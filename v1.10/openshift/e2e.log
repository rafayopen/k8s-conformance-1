Jul 11 00:08:41.324: INFO: Overriding default scale value of zero to 1
Jul 11 00:08:41.324: INFO: Overriding default milliseconds value of zero to 5000
I0711 00:08:41.448716    9515 e2e.go:333] Starting e2e run "915151bd-849e-11e8-9117-0e046f2b5c78" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1531267721 - Will randomize all specs
Will run 164 of 891 specs

I0711 00:08:41.475535    9515 e2e.go:56] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
Jul 11 00:08:41.475: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:08:41.478: INFO: Waiting up to 4h0m0s for all (but 1) nodes to be schedulable
Jul 11 00:08:41.558: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 11 00:08:41.621: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 11 00:08:41.621: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jul 11 00:08:41.637: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Jul 11 00:08:41.637: INFO: Dumping network health container logs from all nodes to file /data/src/github.com/openshift/origin/_output/scripts/conformance-k8s/artifacts/nethealth.txt
Jul 11 00:08:41.653: INFO: e2e test version: v1.10.6-beta.0.41+f7ef7a267727c2
Jul 11 00:08:41.668: INFO: kube-apiserver version: v1.10.0+b81c8f8
I0711 00:08:41.668061    9515 e2e.go:56] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
SSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:08:41.668: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
Jul 11 00:08:42.396: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap e2e-tests-configmap-7jz74/configmap-test-9202af8d-849e-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:08:42.461: INFO: Waiting up to 5m0s for pod "pod-configmaps-92054710-849e-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-configmap-7jz74" to be "success or failure"
Jul 11 00:08:42.476: INFO: Pod "pod-configmaps-92054710-849e-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.866696ms
Jul 11 00:08:44.493: INFO: Pod "pod-configmaps-92054710-849e-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031917358s
Jul 11 00:08:46.511: INFO: Pod "pod-configmaps-92054710-849e-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050281352s
STEP: Saw pod success
Jul 11 00:08:46.511: INFO: Pod "pod-configmaps-92054710-849e-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:08:46.527: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-configmaps-92054710-849e-11e8-9117-0e046f2b5c78 container env-test: <nil>
STEP: delete the pod
Jul 11 00:08:46.597: INFO: Waiting for pod pod-configmaps-92054710-849e-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:08:46.612: INFO: Pod pod-configmaps-92054710-849e-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:08:46.612: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7jz74" for this suite.
Jul 11 00:08:52.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:08:53.342: INFO: namespace: e2e-tests-configmap-7jz74, resource: bindings, ignored listing per whitelist
Jul 11 00:08:54.112: INFO: namespace e2e-tests-configmap-7jz74 deletion completed in 7.483363235s

• [SLOW TEST:12.444 seconds]
[sig-api-machinery] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:29
  should be consumable via the environment  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:08:54.112: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:08:54.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9961d591-849e-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-qjtk7" to be "success or failure"
Jul 11 00:08:54.846: INFO: Pod "downwardapi-volume-9961d591-849e-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 24.769012ms
Jul 11 00:08:56.862: INFO: Pod "downwardapi-volume-9961d591-849e-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041222718s
Jul 11 00:08:58.879: INFO: Pod "downwardapi-volume-9961d591-849e-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057318664s
STEP: Saw pod success
Jul 11 00:08:58.879: INFO: Pod "downwardapi-volume-9961d591-849e-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:08:58.894: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod downwardapi-volume-9961d591-849e-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:08:58.964: INFO: Waiting for pod downwardapi-volume-9961d591-849e-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:08:58.979: INFO: Pod downwardapi-volume-9961d591-849e-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:08:58.979: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qjtk7" for this suite.
Jul 11 00:09:05.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:09:05.939: INFO: namespace: e2e-tests-projected-qjtk7, resource: bindings, ignored listing per whitelist
Jul 11 00:09:06.455: INFO: namespace e2e-tests-projected-qjtk7 deletion completed in 7.459704088s

• [SLOW TEST:12.343 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide container's cpu limit [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed  [Flaky] [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:09:06.455: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed  [Flaky] [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Jul 11 00:09:13.283: INFO: Asynchronously running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:09:21.061: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dm774" for this suite.
Jul 11 00:09:27.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:09:27.711: INFO: namespace: e2e-tests-pods-dm774, resource: bindings, ignored listing per whitelist
Jul 11 00:09:28.533: INFO: namespace e2e-tests-pods-dm774 deletion completed in 7.454938107s

• [SLOW TEST:22.078 seconds]
[k8s.io] [sig-node] Pods Extended
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  [k8s.io] Delete Grace Period
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should be submitted and removed  [Flaky] [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:09:28.533: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:09:29.249: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-aded0f20-849e-11e8-9117-0e046f2b5c78
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-aded0f20-849e-11e8-9117-0e046f2b5c78
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:10:46.226: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-fzmf2" for this suite.
Jul 11 00:11:08.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:11:09.353: INFO: namespace: e2e-tests-configmap-fzmf2, resource: bindings, ignored listing per whitelist
Jul 11 00:11:09.714: INFO: namespace e2e-tests-configmap-fzmf2 deletion completed in 23.457412912s

• [SLOW TEST:101.181 seconds]
[sig-storage] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:11:09.714: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-22zgj A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-22zgj;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-22zgj A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-22zgj;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-22zgj.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-22zgj.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-22zgj.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-22zgj.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-22zgj.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-22zgj.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-22zgj.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-22zgj.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-22zgj.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-22zgj.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-22zgj.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-22zgj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-22zgj.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 236.110.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.110.236_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 236.110.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.110.236_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-22zgj A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-22zgj;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-22zgj A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-22zgj;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-22zgj.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-22zgj.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-22zgj.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-22zgj.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-22zgj.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-22zgj.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-22zgj.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-22zgj.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-22zgj.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-22zgj.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-22zgj.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-22zgj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-22zgj.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 236.110.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.110.236_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 236.110.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.110.236_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 11 00:11:33.040: INFO: DNS probes using dns-test-ea4564fb-849e-11e8-9117-0e046f2b5c78 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:11:33.154: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-22zgj" for this suite.
Jul 11 00:11:39.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:11:40.161: INFO: namespace: e2e-tests-dns-22zgj, resource: bindings, ignored listing per whitelist
Jul 11 00:11:40.649: INFO: namespace e2e-tests-dns-22zgj deletion completed in 7.478122971s

• [SLOW TEST:30.935 seconds]
[sig-network] DNS
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:11:40.649: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:11:41.392: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:66
Jul 11 00:11:41.426: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-v8xwv/daemonsets","resourceVersion":"3636"},"items":null}

Jul 11 00:11:41.442: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-v8xwv/pods","resourceVersion":"3636"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:11:41.503: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-v8xwv" for this suite.
Jul 11 00:11:47.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:11:48.773: INFO: namespace: e2e-tests-daemonsets-v8xwv, resource: bindings, ignored listing per whitelist
Jul 11 00:11:48.972: INFO: namespace e2e-tests-daemonsets-v8xwv deletion completed in 7.452682864s

S [SKIPPING] [8.322 seconds]
[sig-apps] Daemon set [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674

  Jul 11 00:11:41.392: Requires at least 2 nodes (not -1)

  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:299
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:11:48.972: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name configmap-test-volume-019c73ea-849f-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:11:49.831: INFO: Waiting up to 5m0s for pod "pod-configmaps-01a429a2-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-configmap-lg76m" to be "success or failure"
Jul 11 00:11:49.867: INFO: Pod "pod-configmaps-01a429a2-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 36.164287ms
Jul 11 00:11:51.883: INFO: Pod "pod-configmaps-01a429a2-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05219338s
Jul 11 00:11:53.899: INFO: Pod "pod-configmaps-01a429a2-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067911972s
STEP: Saw pod success
Jul 11 00:11:53.899: INFO: Pod "pod-configmaps-01a429a2-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:11:53.915: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-configmaps-01a429a2-849f-11e8-9117-0e046f2b5c78 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 00:11:53.961: INFO: Waiting for pod pod-configmaps-01a429a2-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:11:53.977: INFO: Pod pod-configmaps-01a429a2-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:11:53.977: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lg76m" for this suite.
Jul 11 00:12:00.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:12:01.345: INFO: namespace: e2e-tests-configmap-lg76m, resource: bindings, ignored listing per whitelist
Jul 11 00:12:01.468: INFO: namespace e2e-tests-configmap-lg76m deletion completed in 7.462375415s

• [SLOW TEST:12.497 seconds]
[sig-storage] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:12:01.468: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 11 00:12:02.201: INFO: Waiting up to 5m0s for pod "pod-09131867-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-gqttd" to be "success or failure"
Jul 11 00:12:02.217: INFO: Pod "pod-09131867-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.630056ms
Jul 11 00:12:04.233: INFO: Pod "pod-09131867-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031563366s
Jul 11 00:12:06.248: INFO: Pod "pod-09131867-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047523305s
STEP: Saw pod success
Jul 11 00:12:06.249: INFO: Pod "pod-09131867-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:12:06.264: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-09131867-849f-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:12:06.306: INFO: Waiting for pod pod-09131867-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:12:06.321: INFO: Pod pod-09131867-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:12:06.321: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gqttd" for this suite.
Jul 11 00:12:12.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:12:13.386: INFO: namespace: e2e-tests-emptydir-gqttd, resource: bindings, ignored listing per whitelist
Jul 11 00:12:13.823: INFO: namespace e2e-tests-emptydir-gqttd deletion completed in 7.473035124s

• [SLOW TEST:12.355 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:12:13.823: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name configmap-test-volume-map-106d622f-849f-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:12:14.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-106fef19-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-configmap-cfwhg" to be "success or failure"
Jul 11 00:12:14.565: INFO: Pod "pod-configmaps-106fef19-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.399354ms
Jul 11 00:12:16.580: INFO: Pod "pod-configmaps-106fef19-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031129667s
Jul 11 00:12:18.597: INFO: Pod "pod-configmaps-106fef19-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047596596s
STEP: Saw pod success
Jul 11 00:12:18.597: INFO: Pod "pod-configmaps-106fef19-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:12:18.612: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-configmaps-106fef19-849f-11e8-9117-0e046f2b5c78 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 00:12:18.656: INFO: Waiting for pod pod-configmaps-106fef19-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:12:18.671: INFO: Pod pod-configmaps-106fef19-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:12:18.671: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cfwhg" for this suite.
Jul 11 00:12:24.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:12:25.625: INFO: namespace: e2e-tests-configmap-cfwhg, resource: bindings, ignored listing per whitelist
Jul 11 00:12:26.188: INFO: namespace e2e-tests-configmap-cfwhg deletion completed in 7.488333957s

• [SLOW TEST:12.365 seconds]
[sig-storage] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:12:26.189: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 11 00:12:26.900: INFO: Waiting up to 5m0s for pod "pod-17ccb2e9-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-wshs6" to be "success or failure"
Jul 11 00:12:26.917: INFO: Pod "pod-17ccb2e9-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.272815ms
Jul 11 00:12:28.932: INFO: Pod "pod-17ccb2e9-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032034702s
Jul 11 00:12:30.948: INFO: Pod "pod-17ccb2e9-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048067002s
STEP: Saw pod success
Jul 11 00:12:30.948: INFO: Pod "pod-17ccb2e9-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:12:30.965: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-17ccb2e9-849f-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:12:31.011: INFO: Waiting for pod pod-17ccb2e9-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:12:31.026: INFO: Pod pod-17ccb2e9-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:12:31.026: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wshs6" for this suite.
Jul 11 00:12:37.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:12:38.234: INFO: namespace: e2e-tests-emptydir-wshs6, resource: bindings, ignored listing per whitelist
Jul 11 00:12:38.511: INFO: namespace e2e-tests-emptydir-wshs6 deletion completed in 7.456395497s

• [SLOW TEST:12.323 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:12:38.512: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-4m2ds
Jul 11 00:12:43.318: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-4m2ds
STEP: checking the pod's current state and verifying that restartCount is present
Jul 11 00:12:43.334: INFO: Initial restart count of pod liveness-http is 0
Jul 11 00:12:57.461: INFO: Restart count of pod e2e-tests-container-probe-4m2ds/liveness-http is now 1 (14.127272495s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:12:57.481: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4m2ds" for this suite.
Jul 11 00:13:03.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:13:04.514: INFO: namespace: e2e-tests-container-probe-4m2ds, resource: bindings, ignored listing per whitelist
Jul 11 00:13:04.967: INFO: namespace e2e-tests-container-probe-4m2ds deletion completed in 7.456709996s

• [SLOW TEST:26.455 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should be restarted with a /healthz http liveness probe  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:13:04.967: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name projected-configmap-test-volume-map-2ef58abb-849f-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:13:05.796: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2efa167b-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-796v9" to be "success or failure"
Jul 11 00:13:05.818: INFO: Pod "pod-projected-configmaps-2efa167b-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 22.364688ms
Jul 11 00:13:07.834: INFO: Pod "pod-projected-configmaps-2efa167b-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03868652s
Jul 11 00:13:09.851: INFO: Pod "pod-projected-configmaps-2efa167b-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055406682s
STEP: Saw pod success
Jul 11 00:13:09.851: INFO: Pod "pod-projected-configmaps-2efa167b-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:13:09.867: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-projected-configmaps-2efa167b-849f-11e8-9117-0e046f2b5c78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 00:13:09.912: INFO: Waiting for pod pod-projected-configmaps-2efa167b-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:13:09.928: INFO: Pod pod-projected-configmaps-2efa167b-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:13:09.928: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-796v9" for this suite.
Jul 11 00:13:16.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:13:17.169: INFO: namespace: e2e-tests-projected-796v9, resource: bindings, ignored listing per whitelist
Jul 11 00:13:17.479: INFO: namespace e2e-tests-projected-796v9 deletion completed in 7.521561103s

• [SLOW TEST:12.512 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:13:17.479: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating secret with name secret-test-3663f2ed-849f-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 00:13:18.240: INFO: Waiting up to 5m0s for pod "pod-secrets-3666a4d2-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-secrets-mlspk" to be "success or failure"
Jul 11 00:13:18.255: INFO: Pod "pod-secrets-3666a4d2-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.738548ms
Jul 11 00:13:20.271: INFO: Pod "pod-secrets-3666a4d2-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031223587s
Jul 11 00:13:22.287: INFO: Pod "pod-secrets-3666a4d2-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047644969s
STEP: Saw pod success
Jul 11 00:13:22.287: INFO: Pod "pod-secrets-3666a4d2-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:13:22.304: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-secrets-3666a4d2-849f-11e8-9117-0e046f2b5c78 container secret-volume-test: <nil>
STEP: delete the pod
Jul 11 00:13:22.355: INFO: Waiting for pod pod-secrets-3666a4d2-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:13:22.370: INFO: Pod pod-secrets-3666a4d2-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:13:22.370: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mlspk" for this suite.
Jul 11 00:13:28.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:13:29.509: INFO: namespace: e2e-tests-secrets-mlspk, resource: bindings, ignored listing per whitelist
Jul 11 00:13:29.875: INFO: namespace e2e-tests-secrets-mlspk deletion completed in 7.474754529s

• [SLOW TEST:12.396 seconds]
[sig-storage] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:13:29.875: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:13:30.595: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3dc3d75d-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-x66ns" to be "success or failure"
Jul 11 00:13:30.614: INFO: Pod "downwardapi-volume-3dc3d75d-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 18.777286ms
Jul 11 00:13:32.630: INFO: Pod "downwardapi-volume-3dc3d75d-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034845008s
Jul 11 00:13:34.646: INFO: Pod "downwardapi-volume-3dc3d75d-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050660449s
STEP: Saw pod success
Jul 11 00:13:34.646: INFO: Pod "downwardapi-volume-3dc3d75d-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:13:34.662: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod downwardapi-volume-3dc3d75d-849f-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:13:34.709: INFO: Waiting for pod downwardapi-volume-3dc3d75d-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:13:34.724: INFO: Pod downwardapi-volume-3dc3d75d-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:13:34.724: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x66ns" for this suite.
Jul 11 00:13:40.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:13:41.827: INFO: namespace: e2e-tests-projected-x66ns, resource: bindings, ignored listing per whitelist
Jul 11 00:13:42.227: INFO: namespace e2e-tests-projected-x66ns deletion completed in 7.474175548s

• [SLOW TEST:12.352 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide container's cpu request [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:13:42.227: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:13:42.923: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig version'
Jul 11 00:13:43.086: INFO: stderr: ""
Jul 11 00:13:43.086: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"10+\", GitVersion:\"v1.10.6-beta.0.41+f7ef7a267727c2\", GitCommit:\"f7ef7a267727c2b32df51dab2f7d32773595e00e\", GitTreeState:\"clean\", BuildDate:\"2018-07-11T00:03:32Z\", GoVersion:\"go1.9.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"10+\", GitVersion:\"v1.10.0+b81c8f8\", GitCommit:\"b81c8f8\", GitTreeState:\"clean\", BuildDate:\"2018-07-10T23:12:56Z\", GoVersion:\"go1.9.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:13:43.086: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6kjtj" for this suite.
Jul 11 00:13:49.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:13:50.467: INFO: namespace: e2e-tests-kubectl-6kjtj, resource: bindings, ignored listing per whitelist
Jul 11 00:13:50.604: INFO: namespace e2e-tests-kubectl-6kjtj deletion completed in 7.488687229s

• [SLOW TEST:8.376 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should check is all data is printed  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:13:50.604: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should be submitted and removed  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Jul 11 00:13:57.415: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-4a1e1d7b-849f-11e8-9117-0e046f2b5c78", GenerateName:"", Namespace:"e2e-tests-pods-c8rqs", SelfLink:"/api/v1/namespaces/e2e-tests-pods-c8rqs/pods/pod-submit-remove-4a1e1d7b-849f-11e8-9117-0e046f2b5c78", UID:"4a2526dc-849f-11e8-94a2-42010a8e0002", ResourceVersion:"4561", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63666864831, loc:(*time.Location)(0x6765fc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"time":"295429235", "name":"foo"}, Annotations:map[string]string{"openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-k2kp8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc4212dc4c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"k8s.gcr.io/nginx-slim-amd64:0.20", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-k2kp8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc4212dc540), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc421e32e98), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"node-role.kubernetes.io/compute":"true"}, ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"prtest-7ef3e0b-4-ig-n-hp69", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc4212dc580), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-bb8rw"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63666864831, loc:(*time.Location)(0x6765fc0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63666864837, loc:(*time.Location)(0x6765fc0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63666864831, loc:(*time.Location)(0x6765fc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.142.0.4", PodIP:"172.16.6.7", StartTime:(*v1.Time)(0xc421c15b40), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc421c15b60), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"k8s.gcr.io/nginx-slim-amd64:0.20", ImageID:"docker-pullable://k8s.gcr.io/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b", ContainerID:"docker://fb45963967bc96755b269d94027a3231e0c3538f52dba668796dde68148f3479"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 11 00:14:02.474: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:14:02.490: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-c8rqs" for this suite.
Jul 11 00:14:08.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:14:09.821: INFO: namespace: e2e-tests-pods-c8rqs, resource: bindings, ignored listing per whitelist
Jul 11 00:14:10.011: INFO: namespace e2e-tests-pods-c8rqs deletion completed in 7.505626843s

• [SLOW TEST:19.408 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should be submitted and removed  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:14:10.011: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 11 00:14:10.778: INFO: Waiting up to 5m0s for pod "pod-55b766ad-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-jknbj" to be "success or failure"
Jul 11 00:14:10.793: INFO: Pod "pod-55b766ad-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 14.953052ms
Jul 11 00:14:12.810: INFO: Pod "pod-55b766ad-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031524621s
Jul 11 00:14:14.826: INFO: Pod "pod-55b766ad-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047756707s
STEP: Saw pod success
Jul 11 00:14:14.826: INFO: Pod "pod-55b766ad-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:14:14.842: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-55b766ad-849f-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:14:14.890: INFO: Waiting for pod pod-55b766ad-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:14:14.905: INFO: Pod pod-55b766ad-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:14:14.905: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jknbj" for this suite.
Jul 11 00:14:20.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:14:22.044: INFO: namespace: e2e-tests-emptydir-jknbj, resource: bindings, ignored listing per whitelist
Jul 11 00:14:22.417: INFO: namespace e2e-tests-emptydir-jknbj deletion completed in 7.482970237s

• [SLOW TEST:12.406 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:14:22.418: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating projection with secret that has name projected-secret-test-map-5d162e22-849f-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 00:14:23.157: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d189487-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-wv6bj" to be "success or failure"
Jul 11 00:14:23.171: INFO: Pod "pod-projected-secrets-5d189487-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 14.847151ms
Jul 11 00:14:25.192: INFO: Pod "pod-projected-secrets-5d189487-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035708199s
Jul 11 00:14:27.208: INFO: Pod "pod-projected-secrets-5d189487-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051259024s
STEP: Saw pod success
Jul 11 00:14:27.208: INFO: Pod "pod-projected-secrets-5d189487-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:14:27.223: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-projected-secrets-5d189487-849f-11e8-9117-0e046f2b5c78 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 11 00:14:27.270: INFO: Waiting for pod pod-projected-secrets-5d189487-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:14:27.285: INFO: Pod pod-projected-secrets-5d189487-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:14:27.285: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wv6bj" for this suite.
Jul 11 00:14:33.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:14:34.807: INFO: namespace: e2e-tests-projected-wv6bj, resource: bindings, ignored listing per whitelist
Jul 11 00:14:34.807: INFO: namespace e2e-tests-projected-wv6bj deletion completed in 7.492291516s

• [SLOW TEST:12.389 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:14:34.807: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating secret with name secret-test-64755813-849f-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 00:14:35.531: INFO: Waiting up to 5m0s for pod "pod-secrets-6477e009-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-secrets-7tfwx" to be "success or failure"
Jul 11 00:14:35.546: INFO: Pod "pod-secrets-6477e009-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 14.719778ms
Jul 11 00:14:37.562: INFO: Pod "pod-secrets-6477e009-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030761936s
Jul 11 00:14:39.578: INFO: Pod "pod-secrets-6477e009-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046726475s
STEP: Saw pod success
Jul 11 00:14:39.578: INFO: Pod "pod-secrets-6477e009-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:14:39.593: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-secrets-6477e009-849f-11e8-9117-0e046f2b5c78 container secret-volume-test: <nil>
STEP: delete the pod
Jul 11 00:14:39.638: INFO: Waiting for pod pod-secrets-6477e009-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:14:39.654: INFO: Pod pod-secrets-6477e009-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:14:39.654: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7tfwx" for this suite.
Jul 11 00:14:45.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:14:46.646: INFO: namespace: e2e-tests-secrets-7tfwx, resource: bindings, ignored listing per whitelist
Jul 11 00:14:47.215: INFO: namespace e2e-tests-secrets-7tfwx deletion completed in 7.532278475s

• [SLOW TEST:12.408 seconds]
[sig-storage] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:14:47.215: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-r5tfg
Jul 11 00:14:53.957: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-r5tfg
STEP: checking the pod's current state and verifying that restartCount is present
Jul 11 00:14:53.980: INFO: Initial restart count of pod liveness-exec is 0
Jul 11 00:15:44.411: INFO: Restart count of pod e2e-tests-container-probe-r5tfg/liveness-exec is now 1 (50.430776636s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:15:44.432: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-r5tfg" for this suite.
Jul 11 00:15:50.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:15:51.595: INFO: namespace: e2e-tests-container-probe-r5tfg, resource: bindings, ignored listing per whitelist
Jul 11 00:15:51.919: INFO: namespace e2e-tests-container-probe-r5tfg deletion completed in 7.459009124s

• [SLOW TEST:64.704 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:15:51.920: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating Redis RC
Jul 11 00:15:52.658: INFO: namespace e2e-tests-kubectl-2xsb6
Jul 11 00:15:52.658: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-2xsb6'
Jul 11 00:15:53.649: INFO: stderr: ""
Jul 11 00:15:53.650: INFO: stdout: "replicationcontroller \"redis-master\" created\n"
STEP: Waiting for Redis master to start.
Jul 11 00:15:54.666: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 00:15:54.666: INFO: Found 0 / 1
Jul 11 00:15:55.666: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 00:15:55.666: INFO: Found 0 / 1
Jul 11 00:15:56.667: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 00:15:56.667: INFO: Found 0 / 1
Jul 11 00:15:57.666: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 00:15:57.666: INFO: Found 1 / 1
Jul 11 00:15:57.666: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 11 00:15:57.682: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 00:15:57.682: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 11 00:15:57.682: INFO: wait on redis-master startup in e2e-tests-kubectl-2xsb6 
Jul 11 00:15:57.682: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig logs redis-master-kq2dd redis-master --namespace=e2e-tests-kubectl-2xsb6'
Jul 11 00:15:57.864: INFO: stderr: ""
Jul 11 00:15:57.864: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.8 (6737a5e6/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 Jul 00:15:56.440 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Jul 00:15:56.440 # Server started, Redis version 3.2.8\n1:M 11 Jul 00:15:56.441 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Jul 00:15:56.441 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 11 00:15:57.864: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-2xsb6'
Jul 11 00:15:58.051: INFO: stderr: ""
Jul 11 00:15:58.051: INFO: stdout: "service \"rm2\" exposed\n"
Jul 11 00:15:58.073: INFO: Service rm2 in namespace e2e-tests-kubectl-2xsb6 found.
STEP: exposing service
Jul 11 00:16:00.104: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-2xsb6'
Jul 11 00:16:00.296: INFO: stderr: ""
Jul 11 00:16:00.296: INFO: stdout: "service \"rm3\" exposed\n"
Jul 11 00:16:00.312: INFO: Service rm3 in namespace e2e-tests-kubectl-2xsb6 found.
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:16:02.347: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2xsb6" for this suite.
Jul 11 00:16:24.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:16:25.828: INFO: namespace: e2e-tests-kubectl-2xsb6, resource: bindings, ignored listing per whitelist
Jul 11 00:16:25.861: INFO: namespace e2e-tests-kubectl-2xsb6 deletion completed in 23.484454943s

• [SLOW TEST:33.941 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should create services for rc  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:16:25.861: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:16:26.593: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6aa3845-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-z4th2" to be "success or failure"
Jul 11 00:16:26.610: INFO: Pod "downwardapi-volume-a6aa3845-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.24406ms
Jul 11 00:16:28.626: INFO: Pod "downwardapi-volume-a6aa3845-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032289075s
Jul 11 00:16:30.642: INFO: Pod "downwardapi-volume-a6aa3845-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04819433s
STEP: Saw pod success
Jul 11 00:16:30.642: INFO: Pod "downwardapi-volume-a6aa3845-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:16:30.657: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod downwardapi-volume-a6aa3845-849f-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:16:30.705: INFO: Waiting for pod downwardapi-volume-a6aa3845-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:16:30.721: INFO: Pod downwardapi-volume-a6aa3845-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:16:30.721: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z4th2" for this suite.
Jul 11 00:16:36.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:16:37.534: INFO: namespace: e2e-tests-projected-z4th2, resource: bindings, ignored listing per whitelist
Jul 11 00:16:38.214: INFO: namespace e2e-tests-projected-z4th2 deletion completed in 7.464308663s

• [SLOW TEST:12.353 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide container's memory limit [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:16:38.214: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward api env vars
Jul 11 00:16:38.938: INFO: Waiting up to 5m0s for pod "downward-api-ae049354-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-ktr89" to be "success or failure"
Jul 11 00:16:38.963: INFO: Pod "downward-api-ae049354-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 24.942674ms
Jul 11 00:16:40.979: INFO: Pod "downward-api-ae049354-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04093192s
Jul 11 00:16:42.995: INFO: Pod "downward-api-ae049354-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056684261s
STEP: Saw pod success
Jul 11 00:16:42.995: INFO: Pod "downward-api-ae049354-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:16:43.010: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod downward-api-ae049354-849f-11e8-9117-0e046f2b5c78 container dapi-container: <nil>
STEP: delete the pod
Jul 11 00:16:43.059: INFO: Waiting for pod downward-api-ae049354-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:16:43.075: INFO: Pod downward-api-ae049354-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:16:43.075: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ktr89" for this suite.
Jul 11 00:16:49.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:16:49.823: INFO: namespace: e2e-tests-downward-api-ktr89, resource: bindings, ignored listing per whitelist
Jul 11 00:16:50.543: INFO: namespace e2e-tests-downward-api-ktr89 deletion completed in 7.439427581s

• [SLOW TEST:12.329 seconds]
[sig-api-machinery] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:16:50.543: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run pod
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1355
[It] should create a pod from an image when restart is Never  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Jul 11 00:16:51.212: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=k8s.gcr.io/nginx-slim-amd64:0.20 --namespace=e2e-tests-kubectl-cpbxq'
Jul 11 00:16:51.377: INFO: stderr: ""
Jul 11 00:16:51.377: INFO: stdout: "pod \"e2e-test-nginx-pod\" created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1360
Jul 11 00:16:51.392: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-cpbxq'
Jul 11 00:16:51.567: INFO: stderr: ""
Jul 11 00:16:51.567: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:16:51.567: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cpbxq" for this suite.
Jul 11 00:16:57.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:16:58.996: INFO: namespace: e2e-tests-kubectl-cpbxq, resource: bindings, ignored listing per whitelist
Jul 11 00:16:59.058: INFO: namespace e2e-tests-kubectl-cpbxq deletion completed in 7.462113142s

• [SLOW TEST:8.515 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should create a pod from an image when restart is Never  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:16:59.058: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name projected-configmap-test-volume-ba6facf3-849f-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:16:59.776: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba728c67-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-6zv2f" to be "success or failure"
Jul 11 00:16:59.792: INFO: Pod "pod-projected-configmaps-ba728c67-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.876028ms
Jul 11 00:17:01.808: INFO: Pod "pod-projected-configmaps-ba728c67-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031582745s
Jul 11 00:17:03.825: INFO: Pod "pod-projected-configmaps-ba728c67-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04816032s
STEP: Saw pod success
Jul 11 00:17:03.825: INFO: Pod "pod-projected-configmaps-ba728c67-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:17:03.840: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-projected-configmaps-ba728c67-849f-11e8-9117-0e046f2b5c78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 00:17:03.894: INFO: Waiting for pod pod-projected-configmaps-ba728c67-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:17:03.909: INFO: Pod pod-projected-configmaps-ba728c67-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:17:03.910: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6zv2f" for this suite.
Jul 11 00:17:09.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:17:10.583: INFO: namespace: e2e-tests-projected-6zv2f, resource: bindings, ignored listing per whitelist
Jul 11 00:17:11.387: INFO: namespace e2e-tests-projected-6zv2f deletion completed in 7.448490546s

• [SLOW TEST:12.329 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume as non-root [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:17:11.387: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating the pod
Jul 11 00:17:16.806: INFO: Successfully updated pod "annotationupdatec1cedba9-849f-11e8-9117-0e046f2b5c78"
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:17:18.850: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gwtnd" for this suite.
Jul 11 00:17:40.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:17:41.513: INFO: namespace: e2e-tests-projected-gwtnd, resource: bindings, ignored listing per whitelist
Jul 11 00:17:42.376: INFO: namespace e2e-tests-projected-gwtnd deletion completed in 23.495483641s

• [SLOW TEST:30.989 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should update annotations on modification [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:17:42.376: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap e2e-tests-configmap-hh6xk/configmap-test-d4420420-849f-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:17:43.099: INFO: Waiting up to 5m0s for pod "pod-configmaps-d44472c3-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-configmap-hh6xk" to be "success or failure"
Jul 11 00:17:43.115: INFO: Pod "pod-configmaps-d44472c3-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.666451ms
Jul 11 00:17:45.131: INFO: Pod "pod-configmaps-d44472c3-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031711791s
Jul 11 00:17:47.148: INFO: Pod "pod-configmaps-d44472c3-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048282856s
STEP: Saw pod success
Jul 11 00:17:47.148: INFO: Pod "pod-configmaps-d44472c3-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:17:47.164: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-configmaps-d44472c3-849f-11e8-9117-0e046f2b5c78 container env-test: <nil>
STEP: delete the pod
Jul 11 00:17:47.208: INFO: Waiting for pod pod-configmaps-d44472c3-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:17:47.223: INFO: Pod pod-configmaps-d44472c3-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:17:47.223: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hh6xk" for this suite.
Jul 11 00:17:53.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:17:54.048: INFO: namespace: e2e-tests-configmap-hh6xk, resource: bindings, ignored listing per whitelist
Jul 11 00:17:54.755: INFO: namespace e2e-tests-configmap-hh6xk deletion completed in 7.502210091s

• [SLOW TEST:12.379 seconds]
[sig-api-machinery] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:29
  should be consumable via environment variable  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:17:54.755: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating projection with secret that has name projected-secret-test-map-dba44a2e-849f-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 00:17:55.491: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dba716d8-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-8kfdn" to be "success or failure"
Jul 11 00:17:55.506: INFO: Pod "pod-projected-secrets-dba716d8-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.17917ms
Jul 11 00:17:57.523: INFO: Pod "pod-projected-secrets-dba716d8-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032157201s
Jul 11 00:17:59.539: INFO: Pod "pod-projected-secrets-dba716d8-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048181211s
STEP: Saw pod success
Jul 11 00:17:59.539: INFO: Pod "pod-projected-secrets-dba716d8-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:17:59.555: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-projected-secrets-dba716d8-849f-11e8-9117-0e046f2b5c78 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 11 00:17:59.602: INFO: Waiting for pod pod-projected-secrets-dba716d8-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:17:59.618: INFO: Pod pod-projected-secrets-dba716d8-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:17:59.618: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8kfdn" for this suite.
Jul 11 00:18:05.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:18:07.067: INFO: namespace: e2e-tests-projected-8kfdn, resource: bindings, ignored listing per whitelist
Jul 11 00:18:07.137: INFO: namespace e2e-tests-projected-8kfdn deletion completed in 7.490286889s

• [SLOW TEST:12.382 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings and Item Mode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:18:07.137: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 11 00:18:07.888: INFO: Waiting up to 5m0s for pod "pod-e30b7f0b-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-c7ml5" to be "success or failure"
Jul 11 00:18:07.904: INFO: Pod "pod-e30b7f0b-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.346306ms
Jul 11 00:18:09.920: INFO: Pod "pod-e30b7f0b-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031782104s
Jul 11 00:18:11.937: INFO: Pod "pod-e30b7f0b-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048357613s
STEP: Saw pod success
Jul 11 00:18:11.937: INFO: Pod "pod-e30b7f0b-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:18:11.952: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-e30b7f0b-849f-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:18:11.994: INFO: Waiting for pod pod-e30b7f0b-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:18:12.009: INFO: Pod pod-e30b7f0b-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:18:12.009: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-c7ml5" for this suite.
Jul 11 00:18:18.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:18:18.986: INFO: namespace: e2e-tests-emptydir-c7ml5, resource: bindings, ignored listing per whitelist
Jul 11 00:18:19.498: INFO: namespace e2e-tests-emptydir-c7ml5 deletion completed in 7.459632698s

• [SLOW TEST:12.361 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:18:19.498: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward api env vars
Jul 11 00:18:20.268: INFO: Waiting up to 5m0s for pod "downward-api-ea686f5e-849f-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-hlgfl" to be "success or failure"
Jul 11 00:18:20.283: INFO: Pod "downward-api-ea686f5e-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.697563ms
Jul 11 00:18:22.300: INFO: Pod "downward-api-ea686f5e-849f-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032115146s
Jul 11 00:18:24.316: INFO: Pod "downward-api-ea686f5e-849f-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048743615s
STEP: Saw pod success
Jul 11 00:18:24.316: INFO: Pod "downward-api-ea686f5e-849f-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:18:24.332: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod downward-api-ea686f5e-849f-11e8-9117-0e046f2b5c78 container dapi-container: <nil>
STEP: delete the pod
Jul 11 00:18:24.376: INFO: Waiting for pod downward-api-ea686f5e-849f-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:18:24.392: INFO: Pod downward-api-ea686f5e-849f-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:18:24.392: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hlgfl" for this suite.
Jul 11 00:18:30.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:18:31.748: INFO: namespace: e2e-tests-downward-api-hlgfl, resource: bindings, ignored listing per whitelist
Jul 11 00:18:31.918: INFO: namespace e2e-tests-downward-api-hlgfl deletion completed in 7.495079602s

• [SLOW TEST:12.420 seconds]
[sig-api-machinery] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide host IP as an env var  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:18:31.918: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:69
[It] should proxy through a service and a pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-7rlgf in namespace e2e-tests-proxy-m92rh
I0711 00:18:32.680787    9515 runners.go:175] Created replication controller with name: proxy-service-7rlgf, namespace: e2e-tests-proxy-m92rh, replica count: 1
I0711 00:18:33.731422    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0711 00:18:34.731651    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0711 00:18:35.731875    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0711 00:18:36.732101    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0711 00:18:37.732380    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0711 00:18:38.732664    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0711 00:18:39.732895    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0711 00:18:40.733128    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0711 00:18:41.733380    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0711 00:18:42.733597    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0711 00:18:43.733854    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0711 00:18:44.734151    9515 runners.go:175] proxy-service-7rlgf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 11 00:18:44.750: INFO: setup took 12.116866345s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 11 00:18:44.771: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 21.113769ms)
Jul 11 00:18:44.773: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 23.066593ms)
Jul 11 00:18:44.774: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 23.79429ms)
Jul 11 00:18:44.774: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 23.979885ms)
Jul 11 00:18:44.774: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 24.074218ms)
Jul 11 00:18:44.784: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 34.013194ms)
Jul 11 00:18:44.784: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 33.941929ms)
Jul 11 00:18:44.784: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 33.981203ms)
Jul 11 00:18:44.784: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 33.927577ms)
Jul 11 00:18:44.784: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 34.215916ms)
Jul 11 00:18:44.786: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 35.948223ms)
Jul 11 00:18:44.786: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 35.935034ms)
Jul 11 00:18:44.786: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 35.894815ms)
Jul 11 00:18:44.786: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 35.980078ms)
Jul 11 00:18:44.786: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 35.928937ms)
Jul 11 00:18:44.786: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 36.067828ms)
Jul 11 00:18:44.809: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 23.304498ms)
Jul 11 00:18:44.813: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 26.75854ms)
Jul 11 00:18:44.813: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 27.368168ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 36.23108ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 36.285815ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.191515ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 36.272791ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 36.469738ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 36.251696ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 36.365964ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 36.401227ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 36.348102ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 36.269232ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 36.326551ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.461968ms)
Jul 11 00:18:44.822: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 36.469132ms)
Jul 11 00:18:44.846: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 23.775008ms)
Jul 11 00:18:44.847: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 24.596449ms)
Jul 11 00:18:44.847: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 24.584024ms)
Jul 11 00:18:44.847: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 24.727943ms)
Jul 11 00:18:44.847: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 24.557127ms)
Jul 11 00:18:44.847: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 24.661957ms)
Jul 11 00:18:44.859: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.963088ms)
Jul 11 00:18:44.859: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 37.064587ms)
Jul 11 00:18:44.860: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 37.148386ms)
Jul 11 00:18:44.860: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.946006ms)
Jul 11 00:18:44.860: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 37.042426ms)
Jul 11 00:18:44.860: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 37.02948ms)
Jul 11 00:18:44.860: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 37.108238ms)
Jul 11 00:18:44.860: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 37.153992ms)
Jul 11 00:18:44.860: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 37.139859ms)
Jul 11 00:18:44.860: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 37.030799ms)
Jul 11 00:18:44.885: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 25.288453ms)
Jul 11 00:18:44.885: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 25.173173ms)
Jul 11 00:18:44.885: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 25.405401ms)
Jul 11 00:18:44.885: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 25.600479ms)
Jul 11 00:18:44.885: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 25.72105ms)
Jul 11 00:18:44.898: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 38.052656ms)
Jul 11 00:18:44.898: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 38.239535ms)
Jul 11 00:18:44.898: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 38.305016ms)
Jul 11 00:18:44.898: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 38.24199ms)
Jul 11 00:18:44.898: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 38.198907ms)
Jul 11 00:18:44.904: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 43.888256ms)
Jul 11 00:18:44.904: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 44.262323ms)
Jul 11 00:18:44.904: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 44.583862ms)
Jul 11 00:18:44.904: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 44.687141ms)
Jul 11 00:18:44.917: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 56.99697ms)
Jul 11 00:18:44.917: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 57.065953ms)
Jul 11 00:18:44.942: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 24.77428ms)
Jul 11 00:18:44.943: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 25.67076ms)
Jul 11 00:18:44.943: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 25.715659ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 37.955536ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 38.272643ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 38.196603ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 38.165191ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 38.138931ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 38.063501ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 38.193003ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 38.054836ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 38.495391ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 38.22825ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 38.487719ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 38.402391ms)
Jul 11 00:18:44.955: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 38.459277ms)
Jul 11 00:18:44.979: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 23.949795ms)
Jul 11 00:18:44.980: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 23.82861ms)
Jul 11 00:18:44.981: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 25.491686ms)
Jul 11 00:18:44.981: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 25.26991ms)
Jul 11 00:18:44.981: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 25.67788ms)
Jul 11 00:18:44.992: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 36.778729ms)
Jul 11 00:18:44.992: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 36.899651ms)
Jul 11 00:18:44.992: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.814239ms)
Jul 11 00:18:44.992: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 36.87186ms)
Jul 11 00:18:44.993: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 36.834438ms)
Jul 11 00:18:44.993: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 37.065013ms)
Jul 11 00:18:44.993: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 37.018467ms)
Jul 11 00:18:44.993: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 36.930818ms)
Jul 11 00:18:44.993: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 36.909995ms)
Jul 11 00:18:44.993: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 36.989059ms)
Jul 11 00:18:44.993: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 36.841458ms)
Jul 11 00:18:45.019: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 26.025702ms)
Jul 11 00:18:45.019: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 25.732228ms)
Jul 11 00:18:45.020: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 26.732159ms)
Jul 11 00:18:45.020: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 26.801539ms)
Jul 11 00:18:45.021: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 27.829119ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 38.852083ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 39.10324ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 38.990302ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 38.749341ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 38.868335ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 38.98576ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 38.903196ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 39.09473ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 38.881045ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 39.216784ms)
Jul 11 00:18:45.032: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 39.198739ms)
Jul 11 00:18:45.056: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 23.488637ms)
Jul 11 00:18:45.056: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 23.697677ms)
Jul 11 00:18:45.056: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 24.325162ms)
Jul 11 00:18:45.057: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 24.59016ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 36.716217ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.639132ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.763326ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 36.724608ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 36.74297ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 36.698382ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 37.056565ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 37.132157ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 36.756972ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 36.993356ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 36.741635ms)
Jul 11 00:18:45.069: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 37.044601ms)
Jul 11 00:18:45.094: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 24.942553ms)
Jul 11 00:18:45.094: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 25.261611ms)
Jul 11 00:18:45.094: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 24.926861ms)
Jul 11 00:18:45.094: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 24.971881ms)
Jul 11 00:18:45.094: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 25.008879ms)
Jul 11 00:18:45.094: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 25.055193ms)
Jul 11 00:18:45.094: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 25.167774ms)
Jul 11 00:18:45.094: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 25.220049ms)
Jul 11 00:18:45.094: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 25.230494ms)
Jul 11 00:18:45.107: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 38.218324ms)
Jul 11 00:18:45.107: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 38.017252ms)
Jul 11 00:18:45.107: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 38.117618ms)
Jul 11 00:18:45.107: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 38.205007ms)
Jul 11 00:18:45.107: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 38.033938ms)
Jul 11 00:18:45.107: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 38.253888ms)
Jul 11 00:18:45.107: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 38.111752ms)
Jul 11 00:18:45.132: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 24.521101ms)
Jul 11 00:18:45.132: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 24.840333ms)
Jul 11 00:18:45.132: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 24.751651ms)
Jul 11 00:18:45.133: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 25.06529ms)
Jul 11 00:18:45.133: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 24.999081ms)
Jul 11 00:18:45.133: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 24.986822ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 37.400278ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 37.486227ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 37.425821ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 37.628383ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 37.630208ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 37.655835ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 37.558352ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 37.604083ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 37.670892ms)
Jul 11 00:18:45.145: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 37.589368ms)
Jul 11 00:18:45.168: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 22.798687ms)
Jul 11 00:18:45.170: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 24.338635ms)
Jul 11 00:18:45.172: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 26.207422ms)
Jul 11 00:18:45.172: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 26.201957ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 36.179363ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 36.064538ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 36.263947ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.065935ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 36.220926ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 35.973661ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 36.002236ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 36.022282ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.128101ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 36.090865ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 36.064114ms)
Jul 11 00:18:45.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 36.105188ms)
Jul 11 00:18:45.203: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 21.476305ms)
Jul 11 00:18:45.204: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 21.807669ms)
Jul 11 00:18:45.204: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 22.428548ms)
Jul 11 00:18:45.204: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 22.578333ms)
Jul 11 00:18:45.216: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 34.641877ms)
Jul 11 00:18:45.216: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 34.626941ms)
Jul 11 00:18:45.216: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 34.720956ms)
Jul 11 00:18:45.216: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 34.667757ms)
Jul 11 00:18:45.216: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 34.671943ms)
Jul 11 00:18:45.217: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 34.640079ms)
Jul 11 00:18:45.217: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 34.825024ms)
Jul 11 00:18:45.217: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 34.670343ms)
Jul 11 00:18:45.217: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 34.663774ms)
Jul 11 00:18:45.217: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 34.782117ms)
Jul 11 00:18:45.217: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 34.841768ms)
Jul 11 00:18:45.217: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 34.806194ms)
Jul 11 00:18:45.240: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 23.224542ms)
Jul 11 00:18:45.240: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 23.170021ms)
Jul 11 00:18:45.244: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 27.479526ms)
Jul 11 00:18:45.244: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 27.457361ms)
Jul 11 00:18:45.246: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 29.646614ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 36.123211ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 36.172097ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 36.345279ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 36.241169ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 36.259147ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.230371ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 36.185467ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 36.415232ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 36.358492ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 36.335856ms)
Jul 11 00:18:45.253: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 36.388207ms)
Jul 11 00:18:45.274: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 21.095272ms)
Jul 11 00:18:45.274: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 21.208488ms)
Jul 11 00:18:45.277: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 23.321672ms)
Jul 11 00:18:45.277: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 24.07005ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 34.063118ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 34.19849ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 34.154128ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 34.042298ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 34.286013ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 34.074356ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 34.215719ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 34.298672ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 34.159984ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 34.126141ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 34.292997ms)
Jul 11 00:18:45.287: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 34.257435ms)
Jul 11 00:18:45.307: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 19.776546ms)
Jul 11 00:18:45.308: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 20.394693ms)
Jul 11 00:18:45.308: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 20.752015ms)
Jul 11 00:18:45.308: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 20.955133ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 32.836567ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 32.832194ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 32.979814ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 32.941985ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 33.122972ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 33.136751ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 32.829999ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 32.909758ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 33.004889ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 32.978378ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 33.160332ms)
Jul 11 00:18:45.321: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 33.119993ms)
Jul 11 00:18:45.344: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 23.05331ms)
Jul 11 00:18:45.344: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 23.198003ms)
Jul 11 00:18:45.344: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 23.174374ms)
Jul 11 00:18:45.345: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 23.724859ms)
Jul 11 00:18:45.345: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 23.819357ms)
Jul 11 00:18:45.345: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 23.748619ms)
Jul 11 00:18:45.345: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 23.757597ms)
Jul 11 00:18:45.345: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 23.810977ms)
Jul 11 00:18:45.357: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 36.378053ms)
Jul 11 00:18:45.357: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.237797ms)
Jul 11 00:18:45.357: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 36.101879ms)
Jul 11 00:18:45.357: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 36.086969ms)
Jul 11 00:18:45.357: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 36.128552ms)
Jul 11 00:18:45.357: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 36.399307ms)
Jul 11 00:18:45.357: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 36.429716ms)
Jul 11 00:18:45.357: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 36.181636ms)
Jul 11 00:18:45.381: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 23.618052ms)
Jul 11 00:18:45.381: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 23.628051ms)
Jul 11 00:18:45.381: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 23.644481ms)
Jul 11 00:18:45.381: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 23.735514ms)
Jul 11 00:18:45.381: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 23.636757ms)
Jul 11 00:18:45.381: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 23.779181ms)
Jul 11 00:18:45.381: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 23.859021ms)
Jul 11 00:18:45.394: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 36.856197ms)
Jul 11 00:18:45.394: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 36.899758ms)
Jul 11 00:18:45.394: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 36.742819ms)
Jul 11 00:18:45.394: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 36.846904ms)
Jul 11 00:18:45.394: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.840445ms)
Jul 11 00:18:45.394: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 36.81496ms)
Jul 11 00:18:45.394: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 36.825813ms)
Jul 11 00:18:45.394: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 36.933711ms)
Jul 11 00:18:45.394: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 36.90029ms)
Jul 11 00:18:45.418: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 23.227093ms)
Jul 11 00:18:45.418: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 23.266177ms)
Jul 11 00:18:45.418: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 23.832989ms)
Jul 11 00:18:45.418: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 23.991935ms)
Jul 11 00:18:45.418: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 23.991303ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 36.099884ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 36.324948ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 36.336779ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 36.276512ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 36.329518ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 36.290021ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 36.340851ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 36.273725ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 36.332588ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 36.43497ms)
Jul 11 00:18:45.431: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 36.379741ms)
Jul 11 00:18:45.454: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 22.741678ms)
Jul 11 00:18:45.454: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 22.909807ms)
Jul 11 00:18:45.454: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 23.151651ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 35.55705ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 35.623239ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 35.501293ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 35.478969ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 35.530273ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 35.630307ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 35.601618ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 35.741667ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 35.561822ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 35.73973ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 35.786114ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 35.633693ms)
Jul 11 00:18:45.467: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 35.717753ms)
Jul 11 00:18:45.487: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 19.686369ms)
Jul 11 00:18:45.487: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:1080/proxy/rewri... (200; 20.078525ms)
Jul 11 00:18:45.487: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:160/proxy/: foo (200; 20.16813ms)
Jul 11 00:18:45.487: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname1/proxy/: tls baz (200; 20.533439ms)
Jul 11 00:18:45.492: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 24.929619ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/http:proxy-service-7rlgf-t9m6x:1080/proxy/... (200; 32.594183ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:460/proxy/: tls baz (200; 32.617837ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:462/proxy/: tls qux (200; 32.810973ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x/proxy/rewriteme"... (200; 32.794148ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m92rh/pods/https:proxy-service-7rlgf-t9m6x:443/proxy/... (200; 32.74871ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/pods/proxy-service-7rlgf-t9m6x:162/proxy/: bar (200; 32.779534ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname2/proxy/: bar (200; 32.608047ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname2/proxy/: bar (200; 32.763353ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/proxy-service-7rlgf:portname1/proxy/: foo (200; 32.779753ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/http:proxy-service-7rlgf:portname1/proxy/: foo (200; 32.709851ms)
Jul 11 00:18:45.500: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m92rh/services/https:proxy-service-7rlgf:tlsportname2/proxy/: tls qux (200; 32.715145ms)
STEP: deleting { ReplicationController} proxy-service-7rlgf in namespace e2e-tests-proxy-m92rh
Jul 11 00:18:45.713: INFO: Deleting { ReplicationController} proxy-service-7rlgf took: 147.287075ms
Jul 11 00:18:45.713: INFO: Terminating { ReplicationController} proxy-service-7rlgf pods took: 26.201µs
Jul 11 00:18:52.613: INFO: Garbage collecting { ReplicationController} proxy-service-7rlgf pods took: 7.047470786s
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:18:52.614: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-m92rh" for this suite.
Jul 11 00:18:58.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:18:59.446: INFO: namespace: e2e-tests-proxy-m92rh, resource: bindings, ignored listing per whitelist
Jul 11 00:19:00.124: INFO: namespace e2e-tests-proxy-m92rh deletion completed in 7.480132495s

• [SLOW TEST:28.206 seconds]
[sig-network] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:61
    should proxy through a service and a pod  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:19:00.124: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:264
[It] should create and stop a replication controller  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating a replication controller
Jul 11 00:19:00.843: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:01.156: INFO: stderr: ""
Jul 11 00:19:01.156: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 11 00:19:01.156: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:01.459: INFO: stderr: ""
Jul 11 00:19:01.459: INFO: stdout: "update-demo-nautilus-67p8b update-demo-nautilus-vt88j "
Jul 11 00:19:01.459: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-67p8b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:01.612: INFO: stderr: ""
Jul 11 00:19:01.612: INFO: stdout: ""
Jul 11 00:19:01.612: INFO: update-demo-nautilus-67p8b is created but not running
Jul 11 00:19:06.612: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:06.766: INFO: stderr: ""
Jul 11 00:19:06.766: INFO: stdout: "update-demo-nautilus-67p8b update-demo-nautilus-vt88j "
Jul 11 00:19:06.766: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-67p8b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:06.913: INFO: stderr: ""
Jul 11 00:19:06.913: INFO: stdout: "true"
Jul 11 00:19:06.913: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-67p8b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:07.062: INFO: stderr: ""
Jul 11 00:19:07.062: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jul 11 00:19:07.062: INFO: validating pod update-demo-nautilus-67p8b
Jul 11 00:19:07.082: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 11 00:19:07.082: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 11 00:19:07.082: INFO: update-demo-nautilus-67p8b is verified up and running
Jul 11 00:19:07.082: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vt88j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:07.229: INFO: stderr: ""
Jul 11 00:19:07.229: INFO: stdout: "true"
Jul 11 00:19:07.229: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vt88j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:07.378: INFO: stderr: ""
Jul 11 00:19:07.378: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jul 11 00:19:07.378: INFO: validating pod update-demo-nautilus-vt88j
Jul 11 00:19:07.398: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 11 00:19:07.398: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 11 00:19:07.398: INFO: update-demo-nautilus-vt88j is verified up and running
STEP: using delete to clean up resources
Jul 11 00:19:07.398: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:07.654: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 00:19:07.654: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" deleted\n"
Jul 11 00:19:07.654: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-7nnmk'
Jul 11 00:19:07.825: INFO: stderr: "No resources found.\n"
Jul 11 00:19:07.825: INFO: stdout: ""
Jul 11 00:19:07.825: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-7nnmk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 11 00:19:07.979: INFO: stderr: ""
Jul 11 00:19:07.979: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:19:07.979: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7nnmk" for this suite.
Jul 11 00:19:14.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:19:15.059: INFO: namespace: e2e-tests-kubectl-7nnmk, resource: bindings, ignored listing per whitelist
Jul 11 00:19:15.467: INFO: namespace e2e-tests-kubectl-7nnmk deletion completed in 7.459134026s

• [SLOW TEST:15.343 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should create and stop a replication controller  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:19:15.467: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:19:16.236: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-0bcc327e-84a0-11e8-9117-0e046f2b5c78
STEP: Creating configMap with name cm-test-opt-upd-0bcc32e1-84a0-11e8-9117-0e046f2b5c78
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0bcc327e-84a0-11e8-9117-0e046f2b5c78
STEP: Updating configmap cm-test-opt-upd-0bcc32e1-84a0-11e8-9117-0e046f2b5c78
STEP: Creating configMap with name cm-test-opt-create-0bcc330a-84a0-11e8-9117-0e046f2b5c78
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:20:29.387: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-d98vg" for this suite.
Jul 11 00:20:51.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:20:52.395: INFO: namespace: e2e-tests-configmap-d98vg, resource: bindings, ignored listing per whitelist
Jul 11 00:20:52.865: INFO: namespace e2e-tests-configmap-d98vg deletion completed in 23.449271623s

• [SLOW TEST:97.398 seconds]
[sig-storage] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:20:52.866: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:20:53.644: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 11 00:20:53.715: INFO: Number of nodes with available pods: 0
Jul 11 00:20:53.715: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:20:54.762: INFO: Number of nodes with available pods: 0
Jul 11 00:20:54.762: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:20:55.768: INFO: Number of nodes with available pods: 0
Jul 11 00:20:55.768: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:20:56.762: INFO: Number of nodes with available pods: 0
Jul 11 00:20:56.762: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:20:57.761: INFO: Number of nodes with available pods: 4
Jul 11 00:20:57.761: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 11 00:20:57.868: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:57.868: INFO: Wrong image for pod: daemon-set-bnhqj. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:57.868: INFO: Wrong image for pod: daemon-set-qqpjc. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:57.868: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:58.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:58.913: INFO: Wrong image for pod: daemon-set-bnhqj. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:58.913: INFO: Wrong image for pod: daemon-set-qqpjc. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:58.913: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:59.920: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:59.920: INFO: Wrong image for pod: daemon-set-bnhqj. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:59.920: INFO: Pod daemon-set-bnhqj is not available
Jul 11 00:20:59.920: INFO: Wrong image for pod: daemon-set-qqpjc. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:20:59.920: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:00.914: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:00.914: INFO: Pod daemon-set-59d4r is not available
Jul 11 00:21:00.914: INFO: Wrong image for pod: daemon-set-qqpjc. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:00.914: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:01.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:01.913: INFO: Pod daemon-set-59d4r is not available
Jul 11 00:21:01.913: INFO: Wrong image for pod: daemon-set-qqpjc. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:01.913: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:02.919: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:02.919: INFO: Pod daemon-set-59d4r is not available
Jul 11 00:21:02.919: INFO: Wrong image for pod: daemon-set-qqpjc. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:02.919: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:03.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:03.913: INFO: Wrong image for pod: daemon-set-qqpjc. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:03.913: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:04.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:04.914: INFO: Wrong image for pod: daemon-set-qqpjc. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:04.914: INFO: Pod daemon-set-qqpjc is not available
Jul 11 00:21:04.914: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:05.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:05.913: INFO: Pod daemon-set-nf78g is not available
Jul 11 00:21:05.913: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:06.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:06.913: INFO: Pod daemon-set-nf78g is not available
Jul 11 00:21:06.913: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:07.914: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:07.914: INFO: Pod daemon-set-nf78g is not available
Jul 11 00:21:07.914: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:08.914: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:08.914: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:09.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:09.913: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:09.913: INFO: Pod daemon-set-xclcm is not available
Jul 11 00:21:10.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:10.913: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:10.913: INFO: Pod daemon-set-xclcm is not available
Jul 11 00:21:11.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:11.913: INFO: Wrong image for pod: daemon-set-xclcm. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:11.913: INFO: Pod daemon-set-xclcm is not available
Jul 11 00:21:12.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:12.913: INFO: Pod daemon-set-zdxpf is not available
Jul 11 00:21:13.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:13.913: INFO: Pod daemon-set-zdxpf is not available
Jul 11 00:21:14.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:14.913: INFO: Pod daemon-set-zdxpf is not available
Jul 11 00:21:15.914: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:15.914: INFO: Pod daemon-set-zdxpf is not available
Jul 11 00:21:16.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:17.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:17.913: INFO: Pod daemon-set-2fljb is not available
Jul 11 00:21:18.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:18.913: INFO: Pod daemon-set-2fljb is not available
Jul 11 00:21:19.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:19.913: INFO: Pod daemon-set-2fljb is not available
Jul 11 00:21:20.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:20.913: INFO: Pod daemon-set-2fljb is not available
Jul 11 00:21:21.913: INFO: Wrong image for pod: daemon-set-2fljb. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Jul 11 00:21:21.913: INFO: Pod daemon-set-2fljb is not available
Jul 11 00:21:22.914: INFO: Pod daemon-set-jv27f is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 11 00:21:22.987: INFO: Number of nodes with available pods: 3
Jul 11 00:21:22.988: INFO: Node prtest-7ef3e0b-4-ig-n-qg4c is running more than one daemon pod
Jul 11 00:21:24.032: INFO: Number of nodes with available pods: 3
Jul 11 00:21:24.032: INFO: Node prtest-7ef3e0b-4-ig-n-qg4c is running more than one daemon pod
Jul 11 00:21:25.034: INFO: Number of nodes with available pods: 3
Jul 11 00:21:25.034: INFO: Node prtest-7ef3e0b-4-ig-n-qg4c is running more than one daemon pod
Jul 11 00:21:26.033: INFO: Number of nodes with available pods: 4
Jul 11 00:21:26.033: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:66
STEP: Deleting DaemonSet "daemon-set" with reaper
Jul 11 00:21:33.197: INFO: Number of nodes with available pods: 0
Jul 11 00:21:33.197: INFO: Number of running nodes: 0, number of available pods: 0
Jul 11 00:21:33.212: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-gvztg/daemonsets","resourceVersion":"6871"},"items":null}

Jul 11 00:21:33.228: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-gvztg/pods","resourceVersion":"6871"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:21:33.290: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-gvztg" for this suite.
Jul 11 00:21:39.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:21:39.951: INFO: namespace: e2e-tests-daemonsets-gvztg, resource: bindings, ignored listing per whitelist
Jul 11 00:21:40.871: INFO: namespace e2e-tests-daemonsets-gvztg deletion completed in 7.551372946s

• [SLOW TEST:48.005 seconds]
[sig-apps] Daemon set [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:21:40.871: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating projection with secret that has name projected-secret-test-62814dab-84a0-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 00:21:41.774: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6286c8a8-84a0-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-dzv4p" to be "success or failure"
Jul 11 00:21:41.794: INFO: Pod "pod-projected-secrets-6286c8a8-84a0-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 19.641595ms
Jul 11 00:21:43.810: INFO: Pod "pod-projected-secrets-6286c8a8-84a0-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035929185s
Jul 11 00:21:45.827: INFO: Pod "pod-projected-secrets-6286c8a8-84a0-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052867113s
STEP: Saw pod success
Jul 11 00:21:45.827: INFO: Pod "pod-projected-secrets-6286c8a8-84a0-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:21:45.846: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-projected-secrets-6286c8a8-84a0-11e8-9117-0e046f2b5c78 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 11 00:21:45.896: INFO: Waiting for pod pod-projected-secrets-6286c8a8-84a0-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:21:45.911: INFO: Pod pod-projected-secrets-6286c8a8-84a0-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:21:45.911: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dzv4p" for this suite.
Jul 11 00:21:51.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:21:52.876: INFO: namespace: e2e-tests-projected-dzv4p, resource: bindings, ignored listing per whitelist
Jul 11 00:21:53.431: INFO: namespace e2e-tests-projected-dzv4p deletion completed in 7.490811049s

• [SLOW TEST:12.560 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:21:53.431: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:21:54.160: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-69ed7d75-84a0-11e8-9117-0e046f2b5c78
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-69ed7d75-84a0-11e8-9117-0e046f2b5c78
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:23:11.166: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q77bn" for this suite.
Jul 11 00:23:33.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:23:34.428: INFO: namespace: e2e-tests-projected-q77bn, resource: bindings, ignored listing per whitelist
Jul 11 00:23:34.636: INFO: namespace e2e-tests-projected-q77bn deletion completed in 23.441059097s

• [SLOW TEST:101.205 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  updates should be reflected in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:23:34.637: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should update annotations on modification  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating the pod
Jul 11 00:23:37.941: INFO: Successfully updated pod "annotationupdatea631d841-84a0-11e8-9117-0e046f2b5c78"
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:23:39.985: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5txf6" for this suite.
Jul 11 00:24:02.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:24:02.991: INFO: namespace: e2e-tests-downward-api-5txf6, resource: bindings, ignored listing per whitelist
Jul 11 00:24:03.490: INFO: namespace e2e-tests-downward-api-5txf6 deletion completed in 23.47603945s

• [SLOW TEST:28.853 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should update annotations on modification  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:24:03.490: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-85szx
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a new StaefulSet
Jul 11 00:24:04.256: INFO: Found 0 stateful pods, waiting for 3
Jul 11 00:24:14.279: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:24:14.279: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:24:14.279: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 11 00:24:24.273: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:24:24.273: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:24:24.273: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/nginx-slim-amd64:0.20 to k8s.gcr.io/nginx-slim-amd64:0.21
Jul 11 00:24:24.362: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 11 00:24:24.438: INFO: Updating stateful set ss2
Jul 11 00:24:24.498: INFO: Waiting for Pod e2e-tests-statefulset-85szx/ss2-2 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
STEP: Restoring Pods to the correct revision when they are deleted
Jul 11 00:24:34.621: INFO: Found 2 stateful pods, waiting for 3
Jul 11 00:24:44.639: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:24:44.639: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:24:44.639: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 11 00:24:44.714: INFO: Updating stateful set ss2
Jul 11 00:24:44.750: INFO: Waiting for Pod e2e-tests-statefulset-85szx/ss2-1 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Jul 11 00:24:54.828: INFO: Updating stateful set ss2
Jul 11 00:24:54.861: INFO: Waiting for StatefulSet e2e-tests-statefulset-85szx/ss2 to complete update
Jul 11 00:24:54.861: INFO: Waiting for Pod e2e-tests-statefulset-85szx/ss2-0 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Jul 11 00:25:04.934: INFO: Waiting for StatefulSet e2e-tests-statefulset-85szx/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Jul 11 00:25:14.893: INFO: Deleting all statefulset in ns e2e-tests-statefulset-85szx
Jul 11 00:25:14.908: INFO: Scaling statefulset ss2 to 0
Jul 11 00:25:35.019: INFO: Waiting for statefulset status.replicas updated to 0
Jul 11 00:25:35.035: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:25:35.084: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-85szx" for this suite.
Jul 11 00:25:41.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:25:41.726: INFO: namespace: e2e-tests-statefulset-85szx, resource: bindings, ignored listing per whitelist
Jul 11 00:25:42.614: INFO: namespace e2e-tests-statefulset-85szx deletion completed in 7.500643562s

• [SLOW TEST:99.124 seconds]
[sig-apps] StatefulSet
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:25:42.614: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:25:43.364: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-f28b4f34-84a0-11e8-9117-0e046f2b5c78
STEP: Creating secret with name s-test-opt-upd-f28b4f7c-84a0-11e8-9117-0e046f2b5c78
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f28b4f34-84a0-11e8-9117-0e046f2b5c78
STEP: Updating secret s-test-opt-upd-f28b4f7c-84a0-11e8-9117-0e046f2b5c78
STEP: Creating secret with name s-test-opt-create-f28b4f9a-84a0-11e8-9117-0e046f2b5c78
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:27:18.664: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8nqkx" for this suite.
Jul 11 00:27:40.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:27:42.095: INFO: namespace: e2e-tests-secrets-8nqkx, resource: bindings, ignored listing per whitelist
Jul 11 00:27:42.275: INFO: namespace e2e-tests-secrets-8nqkx deletion completed in 23.550742578s

• [SLOW TEST:119.661 seconds]
[sig-storage] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:27:42.275: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name projected-configmap-test-volume-map-39d9aea4-84a1-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:27:43.034: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39dc252b-84a1-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-h2m9d" to be "success or failure"
Jul 11 00:27:43.050: INFO: Pod "pod-projected-configmaps-39dc252b-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.838347ms
Jul 11 00:27:45.066: INFO: Pod "pod-projected-configmaps-39dc252b-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031946978s
Jul 11 00:27:47.082: INFO: Pod "pod-projected-configmaps-39dc252b-84a1-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048368992s
STEP: Saw pod success
Jul 11 00:27:47.082: INFO: Pod "pod-projected-configmaps-39dc252b-84a1-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:27:47.098: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-projected-configmaps-39dc252b-84a1-11e8-9117-0e046f2b5c78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 00:27:47.143: INFO: Waiting for pod pod-projected-configmaps-39dc252b-84a1-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:27:47.158: INFO: Pod pod-projected-configmaps-39dc252b-84a1-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:27:47.159: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h2m9d" for this suite.
Jul 11 00:27:53.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:27:54.365: INFO: namespace: e2e-tests-projected-h2m9d, resource: bindings, ignored listing per whitelist
Jul 11 00:27:54.669: INFO: namespace e2e-tests-projected-h2m9d deletion completed in 7.481304639s

• [SLOW TEST:12.394 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings as non-root [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:27:54.669: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating secret with name projected-secret-test-4136b255-84a1-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 00:27:55.389: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4139280a-84a1-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-mksz5" to be "success or failure"
Jul 11 00:27:55.410: INFO: Pod "pod-projected-secrets-4139280a-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 20.864076ms
Jul 11 00:27:57.426: INFO: Pod "pod-projected-secrets-4139280a-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036915606s
Jul 11 00:27:59.442: INFO: Pod "pod-projected-secrets-4139280a-84a1-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052710938s
STEP: Saw pod success
Jul 11 00:27:59.442: INFO: Pod "pod-projected-secrets-4139280a-84a1-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:27:59.457: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-projected-secrets-4139280a-84a1-11e8-9117-0e046f2b5c78 container secret-volume-test: <nil>
STEP: delete the pod
Jul 11 00:27:59.505: INFO: Waiting for pod pod-projected-secrets-4139280a-84a1-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:27:59.520: INFO: Pod pod-projected-secrets-4139280a-84a1-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:27:59.520: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mksz5" for this suite.
Jul 11 00:28:05.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:28:06.756: INFO: namespace: e2e-tests-projected-mksz5, resource: bindings, ignored listing per whitelist
Jul 11 00:28:07.029: INFO: namespace e2e-tests-projected-mksz5 deletion completed in 7.479385496s

• [SLOW TEST:12.360 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable in multiple volumes in a pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection] [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:28:07.029: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection] [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name configmap-projected-all-test-volume-4896a0ae-84a1-11e8-9117-0e046f2b5c78
STEP: Creating secret with name secret-projected-all-test-volume-4896a095-84a1-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 11 00:28:07.781: INFO: Waiting up to 5m0s for pod "projected-volume-4896a057-84a1-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-vgjs7" to be "success or failure"
Jul 11 00:28:07.798: INFO: Pod "projected-volume-4896a057-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.347064ms
Jul 11 00:28:09.814: INFO: Pod "projected-volume-4896a057-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032925442s
Jul 11 00:28:11.829: INFO: Pod "projected-volume-4896a057-84a1-11e8-9117-0e046f2b5c78": Phase="Running", Reason="", readiness=true. Elapsed: 4.048012937s
Jul 11 00:28:13.845: INFO: Pod "projected-volume-4896a057-84a1-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063714475s
STEP: Saw pod success
Jul 11 00:28:13.845: INFO: Pod "projected-volume-4896a057-84a1-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:28:13.860: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod projected-volume-4896a057-84a1-11e8-9117-0e046f2b5c78 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 11 00:28:13.910: INFO: Waiting for pod projected-volume-4896a057-84a1-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:28:13.925: INFO: Pod projected-volume-4896a057-84a1-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:28:13.925: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vgjs7" for this suite.
Jul 11 00:28:20.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:28:20.551: INFO: namespace: e2e-tests-projected-vgjs7, resource: bindings, ignored listing per whitelist
Jul 11 00:28:21.392: INFO: namespace e2e-tests-projected-vgjs7 deletion completed in 7.438279471s

• [SLOW TEST:14.364 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should project all components that make up the projection API [Projection] [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:28:21.393: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's cpu limit  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:28:22.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5123ec03-84a1-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-9dpfh" to be "success or failure"
Jul 11 00:28:22.112: INFO: Pod "downwardapi-volume-5123ec03-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.750691ms
Jul 11 00:28:24.127: INFO: Pod "downwardapi-volume-5123ec03-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031513092s
Jul 11 00:28:26.143: INFO: Pod "downwardapi-volume-5123ec03-84a1-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047398426s
STEP: Saw pod success
Jul 11 00:28:26.143: INFO: Pod "downwardapi-volume-5123ec03-84a1-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:28:26.159: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod downwardapi-volume-5123ec03-84a1-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:28:26.204: INFO: Waiting for pod downwardapi-volume-5123ec03-84a1-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:28:26.220: INFO: Pod downwardapi-volume-5123ec03-84a1-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:28:26.220: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9dpfh" for this suite.
Jul 11 00:28:32.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:28:33.585: INFO: namespace: e2e-tests-downward-api-9dpfh, resource: bindings, ignored listing per whitelist
Jul 11 00:28:33.692: INFO: namespace e2e-tests-downward-api-9dpfh deletion completed in 7.442907043s

• [SLOW TEST:12.299 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide container's cpu limit  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:28:33.692: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1287
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Jul 11 00:28:34.406: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=k8s.gcr.io/nginx-slim-amd64:0.20 --namespace=e2e-tests-kubectl-b97xv'
Jul 11 00:28:35.222: INFO: stderr: ""
Jul 11 00:28:35.222: INFO: stdout: "job.batch \"e2e-test-nginx-job\" created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
Jul 11 00:28:35.240: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-b97xv'
Jul 11 00:28:35.522: INFO: stderr: ""
Jul 11 00:28:35.522: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:28:35.522: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b97xv" for this suite.
Jul 11 00:28:57.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:28:58.782: INFO: namespace: e2e-tests-kubectl-b97xv, resource: bindings, ignored listing per whitelist
Jul 11 00:28:59.026: INFO: namespace e2e-tests-kubectl-b97xv deletion completed in 23.474610512s

• [SLOW TEST:25.334 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should create a job from an image when restart is OnFailure  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count  [Slow] [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:28:59.026: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count  [Slow] [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-rbcgr
Jul 11 00:29:03.809: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-rbcgr
STEP: checking the pod's current state and verifying that restartCount is present
Jul 11 00:29:03.825: INFO: Initial restart count of pod liveness-http is 0
Jul 11 00:29:24.014: INFO: Restart count of pod e2e-tests-container-probe-rbcgr/liveness-http is now 1 (20.189018242s elapsed)
Jul 11 00:29:44.174: INFO: Restart count of pod e2e-tests-container-probe-rbcgr/liveness-http is now 2 (40.348698182s elapsed)
Jul 11 00:30:04.331: INFO: Restart count of pod e2e-tests-container-probe-rbcgr/liveness-http is now 3 (1m0.505851303s elapsed)
Jul 11 00:30:24.494: INFO: Restart count of pod e2e-tests-container-probe-rbcgr/liveness-http is now 4 (1m20.669616345s elapsed)
Jul 11 00:31:24.991: INFO: Restart count of pod e2e-tests-container-probe-rbcgr/liveness-http is now 5 (2m21.166107645s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:31:25.018: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-rbcgr" for this suite.
Jul 11 00:31:31.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:31:32.514: INFO: namespace: e2e-tests-container-probe-rbcgr, resource: bindings, ignored listing per whitelist
Jul 11 00:31:32.514: INFO: namespace e2e-tests-container-probe-rbcgr deletion completed in 7.467891088s

• [SLOW TEST:153.488 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should have monotonically increasing restart count  [Slow] [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:31:32.515: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0711 00:31:43.305875    9515 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 11 00:31:43.305: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:31:43.305: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-d7w5h" for this suite.
Jul 11 00:31:49.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:31:50.075: INFO: namespace: e2e-tests-gc-d7w5h, resource: bindings, ignored listing per whitelist
Jul 11 00:31:50.817: INFO: namespace e2e-tests-gc-d7w5h deletion completed in 7.460841595s

• [SLOW TEST:18.302 seconds]
[sig-api-machinery] Garbage collector
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:31:50.817: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should set DefaultMode on files  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:31:51.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cdf3fae3-84a1-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-ws87s" to be "success or failure"
Jul 11 00:31:51.525: INFO: Pod "downwardapi-volume-cdf3fae3-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.051659ms
Jul 11 00:31:53.541: INFO: Pod "downwardapi-volume-cdf3fae3-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032837275s
Jul 11 00:31:55.556: INFO: Pod "downwardapi-volume-cdf3fae3-84a1-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048412726s
STEP: Saw pod success
Jul 11 00:31:55.556: INFO: Pod "downwardapi-volume-cdf3fae3-84a1-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:31:55.571: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod downwardapi-volume-cdf3fae3-84a1-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:31:55.620: INFO: Waiting for pod downwardapi-volume-cdf3fae3-84a1-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:31:55.635: INFO: Pod downwardapi-volume-cdf3fae3-84a1-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:31:55.635: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ws87s" for this suite.
Jul 11 00:32:01.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:32:02.358: INFO: namespace: e2e-tests-downward-api-ws87s, resource: bindings, ignored listing per whitelist
Jul 11 00:32:03.153: INFO: namespace e2e-tests-downward-api-ws87s deletion completed in 7.489199158s

• [SLOW TEST:12.337 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should set DefaultMode on files  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:32:03.154: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test substitution in container's command
Jul 11 00:32:03.881: INFO: Waiting up to 5m0s for pod "var-expansion-d555594c-84a1-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-var-expansion-bz5lj" to be "success or failure"
Jul 11 00:32:03.896: INFO: Pod "var-expansion-d555594c-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.026684ms
Jul 11 00:32:05.912: INFO: Pod "var-expansion-d555594c-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03064094s
Jul 11 00:32:07.928: INFO: Pod "var-expansion-d555594c-84a1-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046466659s
STEP: Saw pod success
Jul 11 00:32:07.928: INFO: Pod "var-expansion-d555594c-84a1-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:32:07.943: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod var-expansion-d555594c-84a1-11e8-9117-0e046f2b5c78 container dapi-container: <nil>
STEP: delete the pod
Jul 11 00:32:07.987: INFO: Waiting for pod var-expansion-d555594c-84a1-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:32:08.002: INFO: Pod var-expansion-d555594c-84a1-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:32:08.002: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-bz5lj" for this suite.
Jul 11 00:32:14.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:32:15.458: INFO: namespace: e2e-tests-var-expansion-bz5lj, resource: bindings, ignored listing per whitelist
Jul 11 00:32:15.504: INFO: namespace e2e-tests-var-expansion-bz5lj deletion completed in 7.473551758s

• [SLOW TEST:12.350 seconds]
[k8s.io] Variable Expansion
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should allow substituting values in a container's command  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:32:15.504: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 11 00:32:16.278: INFO: Waiting up to 5m0s for pod "pod-dcb8dbd3-84a1-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-pt7jt" to be "success or failure"
Jul 11 00:32:16.297: INFO: Pod "pod-dcb8dbd3-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 18.852027ms
Jul 11 00:32:18.313: INFO: Pod "pod-dcb8dbd3-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035484422s
Jul 11 00:32:20.330: INFO: Pod "pod-dcb8dbd3-84a1-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051913656s
STEP: Saw pod success
Jul 11 00:32:20.330: INFO: Pod "pod-dcb8dbd3-84a1-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:32:20.345: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-dcb8dbd3-84a1-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:32:20.390: INFO: Waiting for pod pod-dcb8dbd3-84a1-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:32:20.406: INFO: Pod pod-dcb8dbd3-84a1-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:32:20.406: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pt7jt" for this suite.
Jul 11 00:32:26.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:32:27.133: INFO: namespace: e2e-tests-emptydir-pt7jt, resource: bindings, ignored listing per whitelist
Jul 11 00:32:27.901: INFO: namespace e2e-tests-emptydir-pt7jt deletion completed in 7.464910794s

• [SLOW TEST:12.396 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] ReplicaSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:32:27.901: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:32:28.621: INFO: Creating ReplicaSet my-hostname-basic-e418948a-84a1-11e8-9117-0e046f2b5c78
Jul 11 00:32:28.712: INFO: Pod name my-hostname-basic-e418948a-84a1-11e8-9117-0e046f2b5c78: Found 0 pods out of 1
Jul 11 00:32:33.729: INFO: Pod name my-hostname-basic-e418948a-84a1-11e8-9117-0e046f2b5c78: Found 1 pods out of 1
Jul 11 00:32:33.729: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e418948a-84a1-11e8-9117-0e046f2b5c78" is running
Jul 11 00:32:33.744: INFO: Pod "my-hostname-basic-e418948a-84a1-11e8-9117-0e046f2b5c78-57hm2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-07-11 00:32:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-07-11 00:32:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-07-11 00:32:28 +0000 UTC Reason: Message:}])
Jul 11 00:32:33.745: INFO: Trying to dial the pod
Jul 11 00:32:38.797: INFO: Controller my-hostname-basic-e418948a-84a1-11e8-9117-0e046f2b5c78: Got expected result from replica 1 [my-hostname-basic-e418948a-84a1-11e8-9117-0e046f2b5c78-57hm2]: "my-hostname-basic-e418948a-84a1-11e8-9117-0e046f2b5c78-57hm2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:32:38.797: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-q4mjm" for this suite.
Jul 11 00:32:44.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:32:45.802: INFO: namespace: e2e-tests-replicaset-q4mjm, resource: bindings, ignored listing per whitelist
Jul 11 00:32:46.293: INFO: namespace e2e-tests-replicaset-q4mjm deletion completed in 7.464302801s

• [SLOW TEST:18.392 seconds]
[sig-apps] ReplicaSet
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:32:46.293: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward api env vars
Jul 11 00:32:47.055: INFO: Waiting up to 5m0s for pod "downward-api-ef099fc0-84a1-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-fx69f" to be "success or failure"
Jul 11 00:32:47.072: INFO: Pod "downward-api-ef099fc0-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.533717ms
Jul 11 00:32:49.088: INFO: Pod "downward-api-ef099fc0-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032628658s
Jul 11 00:32:51.104: INFO: Pod "downward-api-ef099fc0-84a1-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048883262s
Jul 11 00:32:53.120: INFO: Pod "downward-api-ef099fc0-84a1-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064800171s
STEP: Saw pod success
Jul 11 00:32:53.120: INFO: Pod "downward-api-ef099fc0-84a1-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:32:53.136: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod downward-api-ef099fc0-84a1-11e8-9117-0e046f2b5c78 container dapi-container: <nil>
STEP: delete the pod
Jul 11 00:32:53.185: INFO: Waiting for pod downward-api-ef099fc0-84a1-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:32:53.200: INFO: Pod downward-api-ef099fc0-84a1-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:32:53.200: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fx69f" for this suite.
Jul 11 00:32:59.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:33:00.008: INFO: namespace: e2e-tests-downward-api-fx69f, resource: bindings, ignored listing per whitelist
Jul 11 00:33:00.739: INFO: namespace e2e-tests-downward-api-fx69f deletion completed in 7.509528265s

• [SLOW TEST:14.445 seconds]
[sig-api-machinery] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide pod name, namespace and IP address as env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:33:00.739: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:33:01.458: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-f7ab2635-84a1-11e8-9117-0e046f2b5c78
STEP: Creating secret with name s-test-opt-upd-f7ab2695-84a1-11e8-9117-0e046f2b5c78
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f7ab2635-84a1-11e8-9117-0e046f2b5c78
STEP: Updating secret s-test-opt-upd-f7ab2695-84a1-11e8-9117-0e046f2b5c78
STEP: Creating secret with name s-test-opt-create-f7ab270e-84a1-11e8-9117-0e046f2b5c78
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:34:10.530: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mnq4p" for this suite.
Jul 11 00:34:32.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:34:33.251: INFO: namespace: e2e-tests-projected-mnq4p, resource: bindings, ignored listing per whitelist
Jul 11 00:34:34.084: INFO: namespace e2e-tests-projected-mnq4p deletion completed in 23.51218089s

• [SLOW TEST:93.345 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  optional updates should be reflected in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:34:34.084: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:34:34.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f485722-84a2-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-gqlr4" to be "success or failure"
Jul 11 00:34:34.803: INFO: Pod "downwardapi-volume-2f485722-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.028185ms
Jul 11 00:34:36.818: INFO: Pod "downwardapi-volume-2f485722-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032826082s
Jul 11 00:34:38.834: INFO: Pod "downwardapi-volume-2f485722-84a2-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048740954s
STEP: Saw pod success
Jul 11 00:34:38.834: INFO: Pod "downwardapi-volume-2f485722-84a2-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:34:38.850: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod downwardapi-volume-2f485722-84a2-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:34:38.894: INFO: Waiting for pod downwardapi-volume-2f485722-84a2-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:34:38.909: INFO: Pod downwardapi-volume-2f485722-84a2-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:34:38.909: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gqlr4" for this suite.
Jul 11 00:34:44.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:34:46.342: INFO: namespace: e2e-tests-projected-gqlr4, resource: bindings, ignored listing per whitelist
Jul 11 00:34:46.418: INFO: namespace e2e-tests-projected-gqlr4 deletion completed in 7.46706725s

• [SLOW TEST:12.334 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:34:46.418: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run default
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1116
[It] should create an rc or deployment from an image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Jul 11 00:34:47.076: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-deployment --image=k8s.gcr.io/nginx-slim-amd64:0.20 --namespace=e2e-tests-kubectl-zk2l5'
Jul 11 00:34:47.347: INFO: stderr: ""
Jul 11 00:34:47.347: INFO: stdout: "deployment.apps \"e2e-test-nginx-deployment\" created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1122
Jul 11 00:34:47.364: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-zk2l5'
Jul 11 00:34:48.711: INFO: stderr: ""
Jul 11 00:34:48.711: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:34:48.711: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zk2l5" for this suite.
Jul 11 00:34:54.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:34:55.471: INFO: namespace: e2e-tests-kubectl-zk2l5, resource: bindings, ignored listing per whitelist
Jul 11 00:34:56.313: INFO: namespace e2e-tests-kubectl-zk2l5 deletion completed in 7.560251375s

• [SLOW TEST:9.895 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should create an rc or deployment from an image  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:34:56.313: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run deployment
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1237
[It] should create a deployment from an image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Jul 11 00:34:57.032: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-deployment --image=k8s.gcr.io/nginx-slim-amd64:0.20 --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-2p9fg'
Jul 11 00:34:57.193: INFO: stderr: ""
Jul 11 00:34:57.193: INFO: stdout: "deployment.apps \"e2e-test-nginx-deployment\" created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1242
Jul 11 00:34:59.241: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-2p9fg'
Jul 11 00:35:00.603: INFO: stderr: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:35:00.603: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2p9fg" for this suite.
Jul 11 00:35:22.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:35:23.466: INFO: namespace: e2e-tests-kubectl-2p9fg, resource: bindings, ignored listing per whitelist
Jul 11 00:35:24.111: INFO: namespace e2e-tests-kubectl-2p9fg deletion completed in 23.465558016s

• [SLOW TEST:27.798 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should create a deployment from an image  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:35:24.111: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-dpqtf
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-dpqtf
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-dpqtf
Jul 11 00:35:24.889: INFO: Found 0 stateful pods, waiting for 1
Jul 11 00:35:34.907: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 11 00:35:34.923: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-dpqtf ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 00:35:35.301: INFO: stderr: ""
Jul 11 00:35:35.301: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 11 00:35:35.317: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 11 00:35:45.334: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 11 00:35:45.334: INFO: Waiting for statefulset status.replicas updated to 0
Jul 11 00:35:45.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999057s
Jul 11 00:35:46.413: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.984540406s
Jul 11 00:35:47.430: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.968536506s
Jul 11 00:35:48.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.951657526s
Jul 11 00:35:49.462: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.935404724s
Jul 11 00:35:50.478: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.919470984s
Jul 11 00:35:51.495: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.90359659s
Jul 11 00:35:52.510: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.887374643s
Jul 11 00:35:53.526: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.871511622s
Jul 11 00:35:54.542: INFO: Verifying statefulset ss doesn't scale past 1 for another 855.511225ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-dpqtf
Jul 11 00:35:55.559: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-dpqtf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 11 00:35:56.011: INFO: stderr: ""
Jul 11 00:35:56.011: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 11 00:35:56.027: INFO: Found 1 stateful pods, waiting for 3
Jul 11 00:36:06.044: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:36:06.044: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:36:06.044: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 11 00:36:06.074: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-dpqtf ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 00:36:06.436: INFO: stderr: ""
Jul 11 00:36:06.436: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 11 00:36:06.436: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-dpqtf ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 00:36:06.782: INFO: stderr: ""
Jul 11 00:36:06.782: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 11 00:36:06.782: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-dpqtf ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 00:36:07.143: INFO: stderr: ""
Jul 11 00:36:07.143: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 11 00:36:07.143: INFO: Waiting for statefulset status.replicas updated to 0
Jul 11 00:36:07.159: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 11 00:36:17.191: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 11 00:36:17.191: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 11 00:36:17.191: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 11 00:36:17.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999039s
Jul 11 00:36:18.255: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984694856s
Jul 11 00:36:19.271: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968587338s
Jul 11 00:36:20.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.952380028s
Jul 11 00:36:21.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.936270801s
Jul 11 00:36:22.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.92006375s
Jul 11 00:36:23.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.904018615s
Jul 11 00:36:24.352: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.887744177s
Jul 11 00:36:25.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.871427119s
Jul 11 00:36:26.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 855.450531ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-dpqtf
Jul 11 00:36:27.400: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-dpqtf ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 11 00:36:27.759: INFO: stderr: ""
Jul 11 00:36:27.759: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 11 00:36:27.759: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-dpqtf ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 11 00:36:28.106: INFO: stderr: ""
Jul 11 00:36:28.106: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 11 00:36:28.106: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-dpqtf ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 11 00:36:28.460: INFO: stderr: ""
Jul 11 00:36:28.460: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 11 00:36:28.460: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Jul 11 00:36:58.525: INFO: Deleting all statefulset in ns e2e-tests-statefulset-dpqtf
Jul 11 00:36:58.541: INFO: Scaling statefulset ss to 0
Jul 11 00:36:58.588: INFO: Waiting for statefulset status.replicas updated to 0
Jul 11 00:36:58.604: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:36:58.653: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-dpqtf" for this suite.
Jul 11 00:37:04.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:37:05.443: INFO: namespace: e2e-tests-statefulset-dpqtf, resource: bindings, ignored listing per whitelist
Jul 11 00:37:06.204: INFO: namespace e2e-tests-statefulset-dpqtf deletion completed in 7.508923956s

• [SLOW TEST:102.093 seconds]
[sig-apps] StatefulSet
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:37:06.205: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name projected-configmap-test-volume-89f703d5-84a2-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:37:06.958: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-89f96fc2-84a2-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-hzn6l" to be "success or failure"
Jul 11 00:37:06.976: INFO: Pod "pod-projected-configmaps-89f96fc2-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.879661ms
Jul 11 00:37:08.992: INFO: Pod "pod-projected-configmaps-89f96fc2-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033932068s
Jul 11 00:37:11.008: INFO: Pod "pod-projected-configmaps-89f96fc2-84a2-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050030476s
STEP: Saw pod success
Jul 11 00:37:11.008: INFO: Pod "pod-projected-configmaps-89f96fc2-84a2-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:37:11.023: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-projected-configmaps-89f96fc2-84a2-11e8-9117-0e046f2b5c78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 00:37:11.071: INFO: Waiting for pod pod-projected-configmaps-89f96fc2-84a2-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:37:11.086: INFO: Pod pod-projected-configmaps-89f96fc2-84a2-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:37:11.086: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hzn6l" for this suite.
Jul 11 00:37:17.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:37:18.284: INFO: namespace: e2e-tests-projected-hzn6l, resource: bindings, ignored listing per whitelist
Jul 11 00:37:18.572: INFO: namespace e2e-tests-projected-hzn6l deletion completed in 7.454667013s

• [SLOW TEST:12.367 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable in multiple volumes in the same pod [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:37:18.572: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name configmap-test-volume-map-916b8c3c-84a2-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:37:19.446: INFO: Waiting up to 5m0s for pod "pod-configmaps-916dfbee-84a2-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-configmap-v7bqn" to be "success or failure"
Jul 11 00:37:19.462: INFO: Pod "pod-configmaps-916dfbee-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.37014ms
Jul 11 00:37:21.478: INFO: Pod "pod-configmaps-916dfbee-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031188881s
Jul 11 00:37:23.493: INFO: Pod "pod-configmaps-916dfbee-84a2-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046973337s
STEP: Saw pod success
Jul 11 00:37:23.493: INFO: Pod "pod-configmaps-916dfbee-84a2-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:37:23.509: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-configmaps-916dfbee-84a2-11e8-9117-0e046f2b5c78 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 00:37:23.551: INFO: Waiting for pod pod-configmaps-916dfbee-84a2-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:37:23.566: INFO: Pod pod-configmaps-916dfbee-84a2-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:37:23.566: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-v7bqn" for this suite.
Jul 11 00:37:29.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:37:30.514: INFO: namespace: e2e-tests-configmap-v7bqn, resource: bindings, ignored listing per whitelist
Jul 11 00:37:31.076: INFO: namespace e2e-tests-configmap-v7bqn deletion completed in 7.480315748s

• [SLOW TEST:12.504 seconds]
[sig-storage] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:37:31.076: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating secret with name secret-test-map-98c9ddd2-84a2-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 00:37:31.811: INFO: Waiting up to 5m0s for pod "pod-secrets-98cc3d2e-84a2-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-secrets-ctwfv" to be "success or failure"
Jul 11 00:37:31.826: INFO: Pod "pod-secrets-98cc3d2e-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 14.89834ms
Jul 11 00:37:33.843: INFO: Pod "pod-secrets-98cc3d2e-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032390315s
Jul 11 00:37:35.859: INFO: Pod "pod-secrets-98cc3d2e-84a2-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048154355s
STEP: Saw pod success
Jul 11 00:37:35.859: INFO: Pod "pod-secrets-98cc3d2e-84a2-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:37:35.874: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-secrets-98cc3d2e-84a2-11e8-9117-0e046f2b5c78 container secret-volume-test: <nil>
STEP: delete the pod
Jul 11 00:37:35.920: INFO: Waiting for pod pod-secrets-98cc3d2e-84a2-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:37:35.935: INFO: Pod pod-secrets-98cc3d2e-84a2-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:37:35.935: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-ctwfv" for this suite.
Jul 11 00:37:42.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:37:42.645: INFO: namespace: e2e-tests-secrets-ctwfv, resource: bindings, ignored listing per whitelist
Jul 11 00:37:43.441: INFO: namespace e2e-tests-secrets-ctwfv deletion completed in 7.476589241s

• [SLOW TEST:12.365 seconds]
[sig-storage] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:37:43.441: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:37:44.195: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-a0314cff-84a2-11e8-9117-0e046f2b5c78
STEP: Creating configMap with name cm-test-opt-upd-a0314d3d-84a2-11e8-9117-0e046f2b5c78
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a0314cff-84a2-11e8-9117-0e046f2b5c78
STEP: Updating configmap cm-test-opt-upd-a0314d3d-84a2-11e8-9117-0e046f2b5c78
STEP: Creating configMap with name cm-test-opt-create-a0314d9d-84a2-11e8-9117-0e046f2b5c78
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:39:17.567: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q6ntq" for this suite.
Jul 11 00:39:39.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:39:40.474: INFO: namespace: e2e-tests-projected-q6ntq, resource: bindings, ignored listing per whitelist
Jul 11 00:39:41.043: INFO: namespace e2e-tests-projected-q6ntq deletion completed in 23.447098525s

• [SLOW TEST:117.602 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  optional updates should be reflected in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:39:41.044: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name configmap-test-volume-e644a0da-84a2-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:39:41.802: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6475264-84a2-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-configmap-h87vg" to be "success or failure"
Jul 11 00:39:41.817: INFO: Pod "pod-configmaps-e6475264-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.787107ms
Jul 11 00:39:43.834: INFO: Pod "pod-configmaps-e6475264-84a2-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031944824s
Jul 11 00:39:45.851: INFO: Pod "pod-configmaps-e6475264-84a2-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049130465s
STEP: Saw pod success
Jul 11 00:39:45.851: INFO: Pod "pod-configmaps-e6475264-84a2-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:39:45.869: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-configmaps-e6475264-84a2-11e8-9117-0e046f2b5c78 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 00:39:45.916: INFO: Waiting for pod pod-configmaps-e6475264-84a2-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:39:45.932: INFO: Pod pod-configmaps-e6475264-84a2-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:39:45.932: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-h87vg" for this suite.
Jul 11 00:39:52.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:39:53.265: INFO: namespace: e2e-tests-configmap-h87vg, resource: bindings, ignored listing per whitelist
Jul 11 00:39:53.433: INFO: namespace e2e-tests-configmap-h87vg deletion completed in 7.472272569s

• [SLOW TEST:12.390 seconds]
[sig-storage] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:39:53.433: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4skbb
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 11 00:39:54.122: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Jul 11 00:40:20.480: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://172.16.4.24:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4skbb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 00:40:20.480: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:40:20.804: INFO: Found all expected endpoints: [netserver-0]
Jul 11 00:40:20.820: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://172.16.6.35:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4skbb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 00:40:20.820: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:40:21.019: INFO: Found all expected endpoints: [netserver-1]
Jul 11 00:40:21.035: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://172.16.2.31:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4skbb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 00:40:21.035: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:40:21.236: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:40:21.236: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4skbb" for this suite.
Jul 11 00:40:43.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:40:44.573: INFO: namespace: e2e-tests-pod-network-test-4skbb, resource: bindings, ignored listing per whitelist
Jul 11 00:40:44.725: INFO: namespace e2e-tests-pod-network-test-4skbb deletion completed in 23.45954298s

• [SLOW TEST:51.292 seconds]
[sig-network] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-auth] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:40:44.725: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: getting the auto-created API token
Jul 11 00:40:45.994: INFO: created pod pod-service-account-defaultsa
Jul 11 00:40:45.994: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 11 00:40:46.019: INFO: created pod pod-service-account-mountsa
Jul 11 00:40:46.019: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 11 00:40:46.042: INFO: created pod pod-service-account-nomountsa
Jul 11 00:40:46.042: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 11 00:40:46.065: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 11 00:40:46.065: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 11 00:40:46.100: INFO: created pod pod-service-account-mountsa-mountspec
Jul 11 00:40:46.100: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 11 00:40:46.121: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 11 00:40:46.121: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 11 00:40:46.143: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 11 00:40:46.143: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 11 00:40:46.163: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 11 00:40:46.163: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 11 00:40:46.185: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 11 00:40:46.185: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:40:46.185: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-6hxrw" for this suite.
Jul 11 00:40:52.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:40:53.340: INFO: namespace: e2e-tests-svcaccounts-6hxrw, resource: bindings, ignored listing per whitelist
Jul 11 00:40:53.676: INFO: namespace e2e-tests-svcaccounts-6hxrw deletion completed in 7.462316479s

• [SLOW TEST:8.950 seconds]
[sig-auth] ServiceAccounts
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:40:53.676: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should set mode on item file  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:40:54.426: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1190470d-84a3-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-666m7" to be "success or failure"
Jul 11 00:40:54.442: INFO: Pod "downwardapi-volume-1190470d-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.417756ms
Jul 11 00:40:56.459: INFO: Pod "downwardapi-volume-1190470d-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032825643s
Jul 11 00:40:58.474: INFO: Pod "downwardapi-volume-1190470d-84a3-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048649104s
STEP: Saw pod success
Jul 11 00:40:58.474: INFO: Pod "downwardapi-volume-1190470d-84a3-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:40:58.490: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod downwardapi-volume-1190470d-84a3-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:40:58.531: INFO: Waiting for pod downwardapi-volume-1190470d-84a3-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:40:58.546: INFO: Pod downwardapi-volume-1190470d-84a3-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:40:58.546: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-666m7" for this suite.
Jul 11 00:41:04.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:41:06.047: INFO: namespace: e2e-tests-downward-api-666m7, resource: bindings, ignored listing per whitelist
Jul 11 00:41:06.062: INFO: namespace e2e-tests-downward-api-666m7 deletion completed in 7.487496812s

• [SLOW TEST:12.387 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should set mode on item file  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:41:06.063: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-mlmsl
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a new StatefulSet
Jul 11 00:41:06.961: INFO: Found 0 stateful pods, waiting for 3
Jul 11 00:41:16.979: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:41:16.979: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:41:16.979: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 00:41:17.030: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-mlmsl ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 00:41:17.385: INFO: stderr: ""
Jul 11 00:41:17.385: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/nginx-slim-amd64:0.20 to k8s.gcr.io/nginx-slim-amd64:0.21
Jul 11 00:41:27.494: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 11 00:41:27.541: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-mlmsl ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 11 00:41:27.957: INFO: stderr: ""
Jul 11 00:41:27.957: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 11 00:41:38.052: INFO: Waiting for StatefulSet e2e-tests-statefulset-mlmsl/ss2 to complete update
Jul 11 00:41:38.052: INFO: Waiting for Pod e2e-tests-statefulset-mlmsl/ss2-0 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Jul 11 00:41:38.052: INFO: Waiting for Pod e2e-tests-statefulset-mlmsl/ss2-1 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Jul 11 00:41:38.052: INFO: Waiting for Pod e2e-tests-statefulset-mlmsl/ss2-2 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Jul 11 00:41:48.085: INFO: Waiting for StatefulSet e2e-tests-statefulset-mlmsl/ss2 to complete update
Jul 11 00:41:48.085: INFO: Waiting for Pod e2e-tests-statefulset-mlmsl/ss2-0 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Jul 11 00:41:48.085: INFO: Waiting for Pod e2e-tests-statefulset-mlmsl/ss2-1 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Jul 11 00:41:58.084: INFO: Waiting for StatefulSet e2e-tests-statefulset-mlmsl/ss2 to complete update
STEP: Rolling back to a previous revision
Jul 11 00:42:08.084: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-mlmsl ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 00:42:08.441: INFO: stderr: ""
Jul 11 00:42:08.442: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 11 00:42:18.553: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 11 00:42:18.601: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-mlmsl ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 11 00:42:18.957: INFO: stderr: ""
Jul 11 00:42:18.957: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 11 00:42:29.064: INFO: Waiting for StatefulSet e2e-tests-statefulset-mlmsl/ss2 to complete update
Jul 11 00:42:29.064: INFO: Waiting for Pod e2e-tests-statefulset-mlmsl/ss2-0 to have revision ss2-76cb68b6ff update revision ss2-56dd5fb9c4
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Jul 11 00:42:39.097: INFO: Deleting all statefulset in ns e2e-tests-statefulset-mlmsl
Jul 11 00:42:39.112: INFO: Scaling statefulset ss2 to 0
Jul 11 00:42:59.229: INFO: Waiting for statefulset status.replicas updated to 0
Jul 11 00:42:59.246: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:42:59.294: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-mlmsl" for this suite.
Jul 11 00:43:05.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:43:06.204: INFO: namespace: e2e-tests-statefulset-mlmsl, resource: bindings, ignored listing per whitelist
Jul 11 00:43:06.824: INFO: namespace e2e-tests-statefulset-mlmsl deletion completed in 7.500653438s

• [SLOW TEST:120.761 seconds]
[sig-apps] StatefulSet
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should perform rolling updates and roll backs of template modifications [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:43:06.824: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's memory request  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:43:07.595: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60f04f46-84a3-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-hdm54" to be "success or failure"
Jul 11 00:43:07.612: INFO: Pod "downwardapi-volume-60f04f46-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.577675ms
Jul 11 00:43:09.628: INFO: Pod "downwardapi-volume-60f04f46-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032930052s
Jul 11 00:43:11.644: INFO: Pod "downwardapi-volume-60f04f46-84a3-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049122736s
STEP: Saw pod success
Jul 11 00:43:11.645: INFO: Pod "downwardapi-volume-60f04f46-84a3-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:43:11.661: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod downwardapi-volume-60f04f46-84a3-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:43:11.707: INFO: Waiting for pod downwardapi-volume-60f04f46-84a3-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:43:11.723: INFO: Pod downwardapi-volume-60f04f46-84a3-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:43:11.724: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hdm54" for this suite.
Jul 11 00:43:17.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:43:18.881: INFO: namespace: e2e-tests-downward-api-hdm54, resource: bindings, ignored listing per whitelist
Jul 11 00:43:19.245: INFO: namespace e2e-tests-downward-api-hdm54 deletion completed in 7.491740186s

• [SLOW TEST:12.421 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide container's memory request  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:43:19.245: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:53
[It] should serve a basic endpoint from pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating service endpoint-test2 in namespace e2e-tests-services-vkwzp
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-vkwzp to expose endpoints map[]
Jul 11 00:43:19.994: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vkwzp exposes endpoints map[] (25.302252ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-vkwzp
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-vkwzp to expose endpoints map[pod1:[80]]
Jul 11 00:43:23.165: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vkwzp exposes endpoints map[pod1:[80]] (3.125992987s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-vkwzp
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-vkwzp to expose endpoints map[pod1:[80] pod2:[80]]
Jul 11 00:43:26.387: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vkwzp exposes endpoints map[pod1:[80] pod2:[80]] (3.183595866s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-vkwzp
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-vkwzp to expose endpoints map[pod2:[80]]
Jul 11 00:43:26.442: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vkwzp exposes endpoints map[pod2:[80]] (37.331078ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-vkwzp
STEP: waiting up to 1m0s for service endpoint-test2 in namespace e2e-tests-services-vkwzp to expose endpoints map[]
Jul 11 00:43:26.477: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-vkwzp exposes endpoints map[] (18.322959ms elapsed)
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:43:26.510: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-vkwzp" for this suite.
Jul 11 00:43:48.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:43:49.246: INFO: namespace: e2e-tests-services-vkwzp, resource: bindings, ignored listing per whitelist
Jul 11 00:43:49.995: INFO: namespace e2e-tests-services-vkwzp deletion completed in 23.45487977s
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:58

• [SLOW TEST:30.751 seconds]
[sig-network] Services
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:43:49.996: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test substitution in container's args
Jul 11 00:43:50.712: INFO: Waiting up to 5m0s for pod "var-expansion-7aa2ac1d-84a3-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-var-expansion-j6mzr" to be "success or failure"
Jul 11 00:43:50.730: INFO: Pod "var-expansion-7aa2ac1d-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.579109ms
Jul 11 00:43:52.746: INFO: Pod "var-expansion-7aa2ac1d-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033716137s
Jul 11 00:43:54.761: INFO: Pod "var-expansion-7aa2ac1d-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049414879s
Jul 11 00:43:56.778: INFO: Pod "var-expansion-7aa2ac1d-84a3-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065568708s
STEP: Saw pod success
Jul 11 00:43:56.778: INFO: Pod "var-expansion-7aa2ac1d-84a3-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:43:56.793: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod var-expansion-7aa2ac1d-84a3-11e8-9117-0e046f2b5c78 container dapi-container: <nil>
STEP: delete the pod
Jul 11 00:43:56.843: INFO: Waiting for pod var-expansion-7aa2ac1d-84a3-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:43:56.859: INFO: Pod var-expansion-7aa2ac1d-84a3-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:43:56.859: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-j6mzr" for this suite.
Jul 11 00:44:02.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:44:03.748: INFO: namespace: e2e-tests-var-expansion-j6mzr, resource: bindings, ignored listing per whitelist
Jul 11 00:44:04.386: INFO: namespace e2e-tests-var-expansion-j6mzr deletion completed in 7.497541474s

• [SLOW TEST:14.390 seconds]
[k8s.io] Variable Expansion
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should allow substituting values in a container's args  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:44:04.386: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:44:05.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-833a7612-84a3-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-sr9qn" to be "success or failure"
Jul 11 00:44:05.137: INFO: Pod "downwardapi-volume-833a7612-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.577484ms
Jul 11 00:44:07.154: INFO: Pod "downwardapi-volume-833a7612-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03295544s
Jul 11 00:44:09.169: INFO: Pod "downwardapi-volume-833a7612-84a3-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048781553s
STEP: Saw pod success
Jul 11 00:44:09.170: INFO: Pod "downwardapi-volume-833a7612-84a3-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:44:09.186: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod downwardapi-volume-833a7612-84a3-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:44:09.230: INFO: Waiting for pod downwardapi-volume-833a7612-84a3-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:44:09.246: INFO: Pod downwardapi-volume-833a7612-84a3-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:44:09.246: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sr9qn" for this suite.
Jul 11 00:44:15.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:44:16.340: INFO: namespace: e2e-tests-projected-sr9qn, resource: bindings, ignored listing per whitelist
Jul 11 00:44:17.019: INFO: namespace e2e-tests-projected-sr9qn deletion completed in 7.743480281s

• [SLOW TEST:12.633 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should set DefaultMode on files [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Projected 
  should set mode on item file [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:44:17.019: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:44:17.737: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8abd6d01-84a3-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-sqvp5" to be "success or failure"
Jul 11 00:44:17.752: INFO: Pod "downwardapi-volume-8abd6d01-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.546678ms
Jul 11 00:44:19.769: INFO: Pod "downwardapi-volume-8abd6d01-84a3-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031743806s
Jul 11 00:44:21.785: INFO: Pod "downwardapi-volume-8abd6d01-84a3-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048150085s
STEP: Saw pod success
Jul 11 00:44:21.785: INFO: Pod "downwardapi-volume-8abd6d01-84a3-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:44:21.800: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod downwardapi-volume-8abd6d01-84a3-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:44:21.844: INFO: Waiting for pod downwardapi-volume-8abd6d01-84a3-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:44:21.859: INFO: Pod downwardapi-volume-8abd6d01-84a3-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:44:21.859: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sqvp5" for this suite.
Jul 11 00:44:27.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:44:28.522: INFO: namespace: e2e-tests-projected-sqvp5, resource: bindings, ignored listing per whitelist
Jul 11 00:44:29.381: INFO: namespace e2e-tests-projected-sqvp5 deletion completed in 7.493227024s

• [SLOW TEST:12.362 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should set mode on item file [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] [sig-node] Events
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:44:29.381: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-92213fd5-84a3-11e8-9117-0e046f2b5c78,GenerateName:,Namespace:e2e-tests-events-w44xk,SelfLink:/api/v1/namespaces/e2e-tests-events-w44xk/pods/send-events-92213fd5-84a3-11e8-9117-0e046f2b5c78,UID:92232b56-84a3-11e8-94a2-42010a8e0002,ResourceVersion:12902,Generation:0,CreationTimestamp:2018-07-11 00:44:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 98842884,},Annotations:map[string]string{openshift.io/scc: anyuid,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-m9hcq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m9hcq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-m9hcq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{node-role.kubernetes.io/compute: true,},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:prtest-7ef3e0b-4-ig-n-qg4c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,},ImagePullSecrets:[{default-dockercfg-cxkwz}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 00:44:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 00:44:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 00:44:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:172.16.4.36,StartTime:2018-07-11 00:44:30 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2018-07-11 00:44:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64@sha256:2dd4032e98a0450d95a0ac71a5e465f542a900812d8c41bc6ca635aed1a5fc91 docker://5697b56404f7c545dd77cfc1d628292dbde65b70e4f5587ac0741d3e32dd0a36}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
STEP: checking for scheduler event about the pod
Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:44:38.240: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-w44xk" for this suite.
Jul 11 00:44:44.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:44:45.203: INFO: namespace: e2e-tests-events-w44xk, resource: bindings, ignored listing per whitelist
Jul 11 00:44:45.745: INFO: namespace e2e-tests-events-w44xk deletion completed in 7.476432602s

• [SLOW TEST:16.364 seconds]
[k8s.io] [sig-node] Events
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:44:45.746: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-f6mhl
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 11 00:44:46.414: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Jul 11 00:45:12.773: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.46:8080/dial?request=hostName&protocol=udp&host=172.16.4.37&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-f6mhl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 00:45:12.773: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:45:13.023: INFO: Waiting for endpoints: map[]
Jul 11 00:45:13.039: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.46:8080/dial?request=hostName&protocol=udp&host=172.16.2.42&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-f6mhl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 00:45:13.039: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:45:13.231: INFO: Waiting for endpoints: map[]
Jul 11 00:45:13.246: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.46:8080/dial?request=hostName&protocol=udp&host=172.16.6.45&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-f6mhl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 00:45:13.246: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:45:13.440: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:45:13.440: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-f6mhl" for this suite.
Jul 11 00:45:35.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:45:36.118: INFO: namespace: e2e-tests-pod-network-test-f6mhl, resource: bindings, ignored listing per whitelist
Jul 11 00:45:36.921: INFO: namespace e2e-tests-pod-network-test-f6mhl deletion completed in 23.452276402s

• [SLOW TEST:51.176 seconds]
[sig-network] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:45:36.922: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Jul 11 00:45:37.619: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 11 00:46:37.866: INFO: Waiting for terminating namespaces to be deleted...
Jul 11 00:46:37.898: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 11 00:46:37.945: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 11 00:46:37.945: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jul 11 00:46:37.961: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Jul 11 00:46:37.961: INFO: 
Logging pods the kubelet thinks is on node prtest-7ef3e0b-4-ig-n-4p7t before test
Jul 11 00:46:37.982: INFO: sync-n9gsj from openshift-node started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 00:46:37.982: INFO: 	Container sync ready: true, restart count 0
Jul 11 00:46:37.982: INFO: sdn-cwjj9 from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 00:46:37.982: INFO: 	Container sdn ready: true, restart count 0
Jul 11 00:46:37.982: INFO: ovs-2kx2m from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 00:46:37.982: INFO: 	Container openvswitch ready: true, restart count 0
Jul 11 00:46:37.982: INFO: 
Logging pods the kubelet thinks is on node prtest-7ef3e0b-4-ig-n-hp69 before test
Jul 11 00:46:38.003: INFO: sync-rv446 from openshift-node started at 2018-07-10 23:59:33 +0000 UTC (1 container statuses recorded)
Jul 11 00:46:38.003: INFO: 	Container sync ready: true, restart count 0
Jul 11 00:46:38.003: INFO: ovs-ll87z from openshift-sdn started at 2018-07-10 23:59:33 +0000 UTC (1 container statuses recorded)
Jul 11 00:46:38.003: INFO: 	Container openvswitch ready: true, restart count 0
Jul 11 00:46:38.003: INFO: sdn-847kp from openshift-sdn started at 2018-07-10 23:59:33 +0000 UTC (1 container statuses recorded)
Jul 11 00:46:38.003: INFO: 	Container sdn ready: true, restart count 0
Jul 11 00:46:38.003: INFO: 
Logging pods the kubelet thinks is on node prtest-7ef3e0b-4-ig-n-qg4c before test
Jul 11 00:46:38.070: INFO: ovs-r2zxw from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 00:46:38.070: INFO: 	Container openvswitch ready: true, restart count 0
Jul 11 00:46:38.070: INFO: sdn-5zv8h from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 00:46:38.070: INFO: 	Container sdn ready: true, restart count 0
Jul 11 00:46:38.070: INFO: sync-kzrv2 from openshift-node started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 00:46:38.070: INFO: 	Container sync ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.154029f34813f2df], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:46:39.190: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-ksxcj" for this suite.
Jul 11 00:47:01.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:47:02.441: INFO: namespace: e2e-tests-sched-pred-ksxcj, resource: bindings, ignored listing per whitelist
Jul 11 00:47:02.688: INFO: namespace e2e-tests-sched-pred-ksxcj deletion completed in 23.469818823s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:85.767 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:47:02.689: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 2 Minute to see if the garbage collector mistakenly deletes the rs
STEP: expected 0 Deploymentss, got 1 Deployments
STEP: Gathering metrics
W0711 00:47:08.637345    9515 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 11 00:47:08.637: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:47:08.637: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-v69rw" for this suite.
Jul 11 00:47:14.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:47:15.269: INFO: namespace: e2e-tests-gc-v69rw, resource: bindings, ignored listing per whitelist
Jul 11 00:47:16.195: INFO: namespace e2e-tests-gc-v69rw deletion completed in 7.542019021s

• [SLOW TEST:13.506 seconds]
[sig-api-machinery] Garbage collector
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:47:16.195: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 11 00:47:17.051: INFO: Number of nodes with available pods: 0
Jul 11 00:47:17.051: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:47:18.097: INFO: Number of nodes with available pods: 0
Jul 11 00:47:18.097: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:47:19.097: INFO: Number of nodes with available pods: 0
Jul 11 00:47:19.097: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:47:20.097: INFO: Number of nodes with available pods: 2
Jul 11 00:47:20.097: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:47:21.097: INFO: Number of nodes with available pods: 4
Jul 11 00:47:21.097: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 11 00:47:21.201: INFO: Number of nodes with available pods: 3
Jul 11 00:47:21.201: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:47:22.250: INFO: Number of nodes with available pods: 3
Jul 11 00:47:22.250: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:47:23.247: INFO: Number of nodes with available pods: 3
Jul 11 00:47:23.247: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:47:24.246: INFO: Number of nodes with available pods: 4
Jul 11 00:47:24.246: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:66
STEP: Deleting DaemonSet "daemon-set" with reaper
Jul 11 00:47:33.360: INFO: Number of nodes with available pods: 0
Jul 11 00:47:33.360: INFO: Number of running nodes: 0, number of available pods: 0
Jul 11 00:47:33.375: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-xjsrl/daemonsets","resourceVersion":"13656"},"items":null}

Jul 11 00:47:33.391: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-xjsrl/pods","resourceVersion":"13656"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:47:33.471: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-xjsrl" for this suite.
Jul 11 00:47:39.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:47:40.853: INFO: namespace: e2e-tests-daemonsets-xjsrl, resource: bindings, ignored listing per whitelist
Jul 11 00:47:41.008: INFO: namespace e2e-tests-daemonsets-xjsrl deletion completed in 7.520331809s

• [SLOW TEST:24.813 seconds]
[sig-apps] Daemon set [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:47:41.008: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward api env vars
Jul 11 00:47:41.767: INFO: Waiting up to 5m0s for pod "downward-api-045a2718-84a4-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-vrcz9" to be "success or failure"
Jul 11 00:47:41.785: INFO: Pod "downward-api-045a2718-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.71597ms
Jul 11 00:47:43.801: INFO: Pod "downward-api-045a2718-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033858214s
Jul 11 00:47:45.818: INFO: Pod "downward-api-045a2718-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050881988s
Jul 11 00:47:47.834: INFO: Pod "downward-api-045a2718-84a4-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067214392s
STEP: Saw pod success
Jul 11 00:47:47.834: INFO: Pod "downward-api-045a2718-84a4-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:47:47.850: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod downward-api-045a2718-84a4-11e8-9117-0e046f2b5c78 container dapi-container: <nil>
STEP: delete the pod
Jul 11 00:47:47.896: INFO: Waiting for pod downward-api-045a2718-84a4-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:47:47.912: INFO: Pod downward-api-045a2718-84a4-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:47:47.912: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vrcz9" for this suite.
Jul 11 00:47:53.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:47:55.169: INFO: namespace: e2e-tests-downward-api-vrcz9, resource: bindings, ignored listing per whitelist
Jul 11 00:47:55.430: INFO: namespace e2e-tests-downward-api-vrcz9 deletion completed in 7.481019119s

• [SLOW TEST:14.423 seconds]
[sig-api-machinery] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide default limits.cpu/memory from node allocatable  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint)  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:47:55.430: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint)  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test override command
Jul 11 00:47:56.153: INFO: Waiting up to 5m0s for pod "client-containers-0ceeda01-84a4-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-containers-td6px" to be "success or failure"
Jul 11 00:47:56.168: INFO: Pod "client-containers-0ceeda01-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.46608ms
Jul 11 00:47:58.185: INFO: Pod "client-containers-0ceeda01-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032082088s
Jul 11 00:48:00.201: INFO: Pod "client-containers-0ceeda01-84a4-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048365212s
STEP: Saw pod success
Jul 11 00:48:00.201: INFO: Pod "client-containers-0ceeda01-84a4-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:48:00.217: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod client-containers-0ceeda01-84a4-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:48:00.267: INFO: Waiting for pod client-containers-0ceeda01-84a4-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:48:00.283: INFO: Pod client-containers-0ceeda01-84a4-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:48:00.283: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-td6px" for this suite.
Jul 11 00:48:06.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:48:07.332: INFO: namespace: e2e-tests-containers-td6px, resource: bindings, ignored listing per whitelist
Jul 11 00:48:07.849: INFO: namespace e2e-tests-containers-td6px deletion completed in 7.536666819s

• [SLOW TEST:12.418 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should be able to override the image's default command (docker entrypoint)  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:48:07.849: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:48:32.666: INFO: Container started at 2018-07-11 00:48:11 +0000 UTC, pod became ready at 2018-07-11 00:48:32 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:48:32.666: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-mtwj7" for this suite.
Jul 11 00:48:54.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:48:55.486: INFO: namespace: e2e-tests-container-probe-mtwj7, resource: bindings, ignored listing per whitelist
Jul 11 00:48:56.254: INFO: namespace e2e-tests-container-probe-mtwj7 deletion completed in 23.558382011s

• [SLOW TEST:48.404 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  with readiness probe should not be ready before initial delay and never restart  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:48:56.254: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl logs
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1010
STEP: creating an rc
Jul 11 00:48:56.989: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-n5glk'
Jul 11 00:48:57.938: INFO: stderr: ""
Jul 11 00:48:57.938: INFO: stdout: "replicationcontroller \"redis-master\" created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Waiting for Redis master to start.
Jul 11 00:48:58.955: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 00:48:58.955: INFO: Found 0 / 1
Jul 11 00:48:59.955: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 00:48:59.955: INFO: Found 0 / 1
Jul 11 00:49:00.955: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 00:49:00.955: INFO: Found 1 / 1
Jul 11 00:49:00.955: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 11 00:49:00.971: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 00:49:00.971: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 11 00:49:00.971: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig logs redis-master-vxftj redis-master --namespace=e2e-tests-kubectl-n5glk'
Jul 11 00:49:01.149: INFO: stderr: ""
Jul 11 00:49:01.149: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.8 (6737a5e6/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 Jul 00:49:00.074 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Jul 00:49:00.074 # Server started, Redis version 3.2.8\n1:M 11 Jul 00:49:00.074 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Jul 00:49:00.074 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 11 00:49:01.149: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-vxftj redis-master --namespace=e2e-tests-kubectl-n5glk --tail=1'
Jul 11 00:49:01.333: INFO: stderr: ""
Jul 11 00:49:01.334: INFO: stdout: "1:M 11 Jul 00:49:00.074 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 11 00:49:01.334: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-vxftj redis-master --namespace=e2e-tests-kubectl-n5glk --limit-bytes=1'
Jul 11 00:49:01.529: INFO: stderr: ""
Jul 11 00:49:01.529: INFO: stdout: " "
STEP: exposing timestamps
Jul 11 00:49:01.529: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-vxftj redis-master --namespace=e2e-tests-kubectl-n5glk --tail=1 --timestamps'
Jul 11 00:49:01.708: INFO: stderr: ""
Jul 11 00:49:01.708: INFO: stdout: "2018-07-11T00:49:00.074850922Z 1:M 11 Jul 00:49:00.074 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 11 00:49:04.208: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-vxftj redis-master --namespace=e2e-tests-kubectl-n5glk --since=1s'
Jul 11 00:49:04.386: INFO: stderr: ""
Jul 11 00:49:04.386: INFO: stdout: ""
Jul 11 00:49:04.386: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig log redis-master-vxftj redis-master --namespace=e2e-tests-kubectl-n5glk --since=24h'
Jul 11 00:49:04.560: INFO: stderr: ""
Jul 11 00:49:04.560: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.8 (6737a5e6/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 Jul 00:49:00.074 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Jul 00:49:00.074 # Server started, Redis version 3.2.8\n1:M 11 Jul 00:49:00.074 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Jul 00:49:00.074 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1015
STEP: using delete to clean up resources
Jul 11 00:49:04.560: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-n5glk'
Jul 11 00:49:04.821: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 00:49:04.821: INFO: stdout: "replicationcontroller \"redis-master\" deleted\n"
Jul 11 00:49:04.821: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-n5glk'
Jul 11 00:49:04.991: INFO: stderr: "No resources found.\n"
Jul 11 00:49:04.991: INFO: stdout: ""
Jul 11 00:49:04.991: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=nginx --namespace=e2e-tests-kubectl-n5glk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 11 00:49:05.143: INFO: stderr: ""
Jul 11 00:49:05.143: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:49:05.143: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n5glk" for this suite.
Jul 11 00:49:27.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:49:27.923: INFO: namespace: e2e-tests-kubectl-n5glk, resource: bindings, ignored listing per whitelist
Jul 11 00:49:28.658: INFO: namespace e2e-tests-kubectl-n5glk deletion completed in 23.469362509s

• [SLOW TEST:32.404 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should be able to retrieve and filter logs  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:49:28.658: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test override all
Jul 11 00:49:29.368: INFO: Waiting up to 5m0s for pod "client-containers-447bfa07-84a4-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-containers-92cvt" to be "success or failure"
Jul 11 00:49:29.384: INFO: Pod "client-containers-447bfa07-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.24605ms
Jul 11 00:49:31.416: INFO: Pod "client-containers-447bfa07-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047556682s
Jul 11 00:49:33.432: INFO: Pod "client-containers-447bfa07-84a4-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063619054s
STEP: Saw pod success
Jul 11 00:49:33.432: INFO: Pod "client-containers-447bfa07-84a4-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:49:33.448: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod client-containers-447bfa07-84a4-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:49:33.493: INFO: Waiting for pod client-containers-447bfa07-84a4-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:49:33.508: INFO: Pod client-containers-447bfa07-84a4-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:49:33.508: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-92cvt" for this suite.
Jul 11 00:49:39.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:49:40.624: INFO: namespace: e2e-tests-containers-92cvt, resource: bindings, ignored listing per whitelist
Jul 11 00:49:41.024: INFO: namespace e2e-tests-containers-92cvt deletion completed in 7.486278964s

• [SLOW TEST:12.366 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should be able to override the image's default command and arguments  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:49:41.024: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating secret with name secret-test-4bdc3aaa-84a4-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 00:49:41.760: INFO: Waiting up to 5m0s for pod "pod-secrets-4be03568-84a4-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-secrets-lrts9" to be "success or failure"
Jul 11 00:49:41.775: INFO: Pod "pod-secrets-4be03568-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.243818ms
Jul 11 00:49:43.792: INFO: Pod "pod-secrets-4be03568-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031520419s
Jul 11 00:49:45.809: INFO: Pod "pod-secrets-4be03568-84a4-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049149641s
STEP: Saw pod success
Jul 11 00:49:45.809: INFO: Pod "pod-secrets-4be03568-84a4-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:49:45.825: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-secrets-4be03568-84a4-11e8-9117-0e046f2b5c78 container secret-volume-test: <nil>
STEP: delete the pod
Jul 11 00:49:45.871: INFO: Waiting for pod pod-secrets-4be03568-84a4-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:49:45.886: INFO: Pod pod-secrets-4be03568-84a4-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:49:45.886: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lrts9" for this suite.
Jul 11 00:49:51.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:49:52.962: INFO: namespace: e2e-tests-secrets-lrts9, resource: bindings, ignored listing per whitelist
Jul 11 00:49:53.414: INFO: namespace e2e-tests-secrets-lrts9 deletion completed in 7.496461795s

• [SLOW TEST:12.390 seconds]
[sig-storage] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:49:53.414: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating all guestbook components
Jul 11 00:49:54.122: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 11 00:49:54.122: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:49:54.397: INFO: stderr: ""
Jul 11 00:49:54.397: INFO: stdout: "service \"redis-slave\" created\n"
Jul 11 00:49:54.398: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 11 00:49:54.398: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:49:54.668: INFO: stderr: ""
Jul 11 00:49:54.668: INFO: stdout: "service \"redis-master\" created\n"
Jul 11 00:49:54.668: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 11 00:49:54.668: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:49:54.941: INFO: stderr: ""
Jul 11 00:49:54.941: INFO: stdout: "service \"frontend\" created\n"
Jul 11 00:49:54.942: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend-amd64:v5
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 11 00:49:54.942: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:49:55.209: INFO: stderr: ""
Jul 11 00:49:55.209: INFO: stdout: "deployment.extensions \"frontend\" created\n"
Jul 11 00:49:55.209: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 11 00:49:55.209: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:49:55.626: INFO: stderr: ""
Jul 11 00:49:55.626: INFO: stdout: "deployment.extensions \"redis-master\" created\n"
Jul 11 00:49:55.626: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave-amd64:v2
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 11 00:49:55.626: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:49:55.898: INFO: stderr: ""
Jul 11 00:49:55.898: INFO: stdout: "deployment.extensions \"redis-slave\" created\n"
STEP: validating guestbook app
Jul 11 00:49:55.898: INFO: Waiting for all frontend pods to be Running.
Jul 11 00:50:10.950: INFO: Waiting for frontend to serve content.
Jul 11 00:50:14.032: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'No route to host [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('No route to hos...', 113)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\StreamCo in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul 11 00:50:19.060: INFO: Trying to add a new entry to the guestbook.
Jul 11 00:50:19.090: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 11 00:50:19.156: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:50:19.346: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 00:50:19.346: INFO: stdout: "service \"redis-slave\" deleted\n"
STEP: using delete to clean up resources
Jul 11 00:50:19.346: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:50:19.522: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 00:50:19.522: INFO: stdout: "service \"redis-master\" deleted\n"
STEP: using delete to clean up resources
Jul 11 00:50:19.522: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:50:19.690: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 00:50:19.690: INFO: stdout: "service \"frontend\" deleted\n"
STEP: using delete to clean up resources
Jul 11 00:50:19.690: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:50:21.031: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 00:50:21.031: INFO: stdout: "deployment.extensions \"frontend\" deleted\n"
STEP: using delete to clean up resources
Jul 11 00:50:21.031: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:50:22.381: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 00:50:22.381: INFO: stdout: "deployment.extensions \"redis-master\" deleted\n"
STEP: using delete to clean up resources
Jul 11 00:50:22.381: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7t4rt'
Jul 11 00:50:23.722: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 00:50:23.722: INFO: stdout: "deployment.extensions \"redis-slave\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:50:23.722: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7t4rt" for this suite.
Jul 11 00:51:01.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:51:02.876: INFO: namespace: e2e-tests-kubectl-7t4rt, resource: bindings, ignored listing per whitelist
Jul 11 00:51:03.233: INFO: namespace e2e-tests-kubectl-7t4rt deletion completed in 39.481855815s

• [SLOW TEST:69.820 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should create and stop a working application  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] Service endpoints latency
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:51:03.233: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-fgjdj
I0711 00:51:03.961153    9515 runners.go:175] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-fgjdj, replica count: 1
I0711 00:51:05.011688    9515 runners.go:175] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0711 00:51:06.011960    9515 runners.go:175] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0711 00:51:07.012222    9515 runners.go:175] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 11 00:51:07.136: INFO: Created: latency-svc-sv8vl
Jul 11 00:51:07.150: INFO: Got endpoints: latency-svc-sv8vl [37.727338ms]
Jul 11 00:51:07.284: INFO: Created: latency-svc-pbdxn
Jul 11 00:51:07.304: INFO: Got endpoints: latency-svc-pbdxn [154.400473ms]
Jul 11 00:51:07.328: INFO: Created: latency-svc-mzfwn
Jul 11 00:51:07.350: INFO: Created: latency-svc-fgflq
Jul 11 00:51:07.361: INFO: Got endpoints: latency-svc-mzfwn [210.823711ms]
Jul 11 00:51:07.369: INFO: Created: latency-svc-z92xj
Jul 11 00:51:07.380: INFO: Got endpoints: latency-svc-fgflq [230.050165ms]
Jul 11 00:51:07.402: INFO: Created: latency-svc-qmm8n
Jul 11 00:51:07.437: INFO: Got endpoints: latency-svc-z92xj [287.222055ms]
Jul 11 00:51:07.438: INFO: Got endpoints: latency-svc-qmm8n [287.516927ms]
Jul 11 00:51:07.438: INFO: Created: latency-svc-pgldw
Jul 11 00:51:07.451: INFO: Got endpoints: latency-svc-pgldw [301.301442ms]
Jul 11 00:51:07.469: INFO: Created: latency-svc-4mj29
Jul 11 00:51:07.551: INFO: Got endpoints: latency-svc-4mj29 [400.559612ms]
Jul 11 00:51:07.567: INFO: Created: latency-svc-ksmwv
Jul 11 00:51:07.570: INFO: Created: latency-svc-mgdxp
Jul 11 00:51:07.570: INFO: Got endpoints: latency-svc-mgdxp [420.385264ms]
Jul 11 00:51:07.601: INFO: Got endpoints: latency-svc-ksmwv [451.537941ms]
Jul 11 00:51:07.603: INFO: Created: latency-svc-jrl9m
Jul 11 00:51:07.631: INFO: Created: latency-svc-k76sx
Jul 11 00:51:07.634: INFO: Got endpoints: latency-svc-jrl9m [484.008114ms]
Jul 11 00:51:07.653: INFO: Created: latency-svc-hdl8h
Jul 11 00:51:07.658: INFO: Got endpoints: latency-svc-k76sx [508.303781ms]
Jul 11 00:51:07.664: INFO: Created: latency-svc-6bkg2
Jul 11 00:51:07.682: INFO: Got endpoints: latency-svc-hdl8h [532.47777ms]
Jul 11 00:51:07.719: INFO: Got endpoints: latency-svc-6bkg2 [569.327562ms]
Jul 11 00:51:07.720: INFO: Created: latency-svc-8bsmn
Jul 11 00:51:07.724: INFO: Got endpoints: latency-svc-8bsmn [573.779964ms]
Jul 11 00:51:07.733: INFO: Created: latency-svc-n5ppk
Jul 11 00:51:07.752: INFO: Got endpoints: latency-svc-n5ppk [602.14969ms]
Jul 11 00:51:07.763: INFO: Created: latency-svc-fsf4j
Jul 11 00:51:07.777: INFO: Got endpoints: latency-svc-fsf4j [472.411583ms]
Jul 11 00:51:07.797: INFO: Created: latency-svc-w6j99
Jul 11 00:51:07.799: INFO: Created: latency-svc-lvpvk
Jul 11 00:51:07.811: INFO: Created: latency-svc-t6p7v
Jul 11 00:51:07.818: INFO: Created: latency-svc-bmd7n
Jul 11 00:51:07.827: INFO: Got endpoints: latency-svc-w6j99 [446.864357ms]
Jul 11 00:51:07.827: INFO: Got endpoints: latency-svc-t6p7v [389.771989ms]
Jul 11 00:51:07.827: INFO: Got endpoints: latency-svc-lvpvk [466.108722ms]
Jul 11 00:51:07.833: INFO: Got endpoints: latency-svc-bmd7n [395.675981ms]
Jul 11 00:51:07.842: INFO: Created: latency-svc-nqj8b
Jul 11 00:51:07.851: INFO: Created: latency-svc-s8c6v
Jul 11 00:51:07.855: INFO: Got endpoints: latency-svc-nqj8b [403.947171ms]
Jul 11 00:51:07.868: INFO: Got endpoints: latency-svc-s8c6v [317.005189ms]
Jul 11 00:51:07.871: INFO: Created: latency-svc-b5ldt
Jul 11 00:51:07.882: INFO: Got endpoints: latency-svc-b5ldt [311.267441ms]
Jul 11 00:51:07.886: INFO: Created: latency-svc-txqj7
Jul 11 00:51:07.891: INFO: Created: latency-svc-6w4xg
Jul 11 00:51:07.915: INFO: Got endpoints: latency-svc-txqj7 [313.797648ms]
Jul 11 00:51:07.917: INFO: Created: latency-svc-77rqp
Jul 11 00:51:07.923: INFO: Got endpoints: latency-svc-6w4xg [289.008257ms]
Jul 11 00:51:07.940: INFO: Got endpoints: latency-svc-77rqp [281.642978ms]
Jul 11 00:51:07.949: INFO: Created: latency-svc-t2cx9
Jul 11 00:51:07.962: INFO: Got endpoints: latency-svc-t2cx9 [279.721903ms]
Jul 11 00:51:07.975: INFO: Created: latency-svc-9mpw5
Jul 11 00:51:07.987: INFO: Got endpoints: latency-svc-9mpw5 [267.708374ms]
Jul 11 00:51:07.994: INFO: Created: latency-svc-5shkn
Jul 11 00:51:08.010: INFO: Created: latency-svc-4gclk
Jul 11 00:51:08.016: INFO: Got endpoints: latency-svc-5shkn [291.764589ms]
Jul 11 00:51:08.018: INFO: Got endpoints: latency-svc-4gclk [266.382013ms]
Jul 11 00:51:08.024: INFO: Created: latency-svc-cnj4w
Jul 11 00:51:08.036: INFO: Got endpoints: latency-svc-cnj4w [259.21873ms]
Jul 11 00:51:08.043: INFO: Created: latency-svc-j8wcx
Jul 11 00:51:08.050: INFO: Created: latency-svc-jd8gl
Jul 11 00:51:08.058: INFO: Got endpoints: latency-svc-j8wcx [230.822959ms]
Jul 11 00:51:08.064: INFO: Created: latency-svc-95r6x
Jul 11 00:51:08.069: INFO: Got endpoints: latency-svc-jd8gl [241.872259ms]
Jul 11 00:51:08.070: INFO: Got endpoints: latency-svc-95r6x [243.462961ms]
Jul 11 00:51:08.080: INFO: Created: latency-svc-5j89b
Jul 11 00:51:08.090: INFO: Created: latency-svc-zlxhh
Jul 11 00:51:08.090: INFO: Got endpoints: latency-svc-5j89b [256.518982ms]
Jul 11 00:51:08.102: INFO: Created: latency-svc-mks6w
Jul 11 00:51:08.102: INFO: Got endpoints: latency-svc-zlxhh [247.100842ms]
Jul 11 00:51:08.113: INFO: Got endpoints: latency-svc-mks6w [244.92211ms]
Jul 11 00:51:08.123: INFO: Created: latency-svc-djd5q
Jul 11 00:51:08.139: INFO: Created: latency-svc-j2kfh
Jul 11 00:51:08.141: INFO: Got endpoints: latency-svc-j2kfh [225.864829ms]
Jul 11 00:51:08.141: INFO: Got endpoints: latency-svc-djd5q [259.661107ms]
Jul 11 00:51:08.144: INFO: Created: latency-svc-7flvw
Jul 11 00:51:08.157: INFO: Created: latency-svc-tqxv7
Jul 11 00:51:08.163: INFO: Got endpoints: latency-svc-7flvw [240.06717ms]
Jul 11 00:51:08.165: INFO: Got endpoints: latency-svc-tqxv7 [224.834569ms]
Jul 11 00:51:08.174: INFO: Created: latency-svc-5f6wv
Jul 11 00:51:08.177: INFO: Got endpoints: latency-svc-5f6wv [214.784772ms]
Jul 11 00:51:08.190: INFO: Created: latency-svc-d9bwc
Jul 11 00:51:08.195: INFO: Got endpoints: latency-svc-d9bwc [207.72586ms]
Jul 11 00:51:08.204: INFO: Created: latency-svc-kwflq
Jul 11 00:51:08.210: INFO: Got endpoints: latency-svc-kwflq [193.998203ms]
Jul 11 00:51:08.218: INFO: Created: latency-svc-jhq95
Jul 11 00:51:08.222: INFO: Got endpoints: latency-svc-jhq95 [203.528547ms]
Jul 11 00:51:08.230: INFO: Created: latency-svc-vb5j4
Jul 11 00:51:08.233: INFO: Got endpoints: latency-svc-vb5j4 [197.393302ms]
Jul 11 00:51:08.237: INFO: Created: latency-svc-vl2x7
Jul 11 00:51:08.243: INFO: Created: latency-svc-pkblz
Jul 11 00:51:08.254: INFO: Got endpoints: latency-svc-pkblz [185.585423ms]
Jul 11 00:51:08.255: INFO: Got endpoints: latency-svc-vl2x7 [196.927115ms]
Jul 11 00:51:08.259: INFO: Created: latency-svc-bpdks
Jul 11 00:51:08.281: INFO: Created: latency-svc-njhlv
Jul 11 00:51:08.281: INFO: Got endpoints: latency-svc-bpdks [210.134643ms]
Jul 11 00:51:08.289: INFO: Created: latency-svc-9rpsz
Jul 11 00:51:08.296: INFO: Got endpoints: latency-svc-9rpsz [193.538897ms]
Jul 11 00:51:08.296: INFO: Got endpoints: latency-svc-njhlv [206.155347ms]
Jul 11 00:51:08.309: INFO: Created: latency-svc-8bhwc
Jul 11 00:51:08.329: INFO: Got endpoints: latency-svc-8bhwc [216.006594ms]
Jul 11 00:51:08.331: INFO: Created: latency-svc-rsftj
Jul 11 00:51:08.342: INFO: Got endpoints: latency-svc-rsftj [200.894735ms]
Jul 11 00:51:08.352: INFO: Created: latency-svc-vkv56
Jul 11 00:51:08.365: INFO: Created: latency-svc-dd75n
Jul 11 00:51:08.366: INFO: Got endpoints: latency-svc-vkv56 [224.756248ms]
Jul 11 00:51:08.374: INFO: Got endpoints: latency-svc-dd75n [211.020148ms]
Jul 11 00:51:08.379: INFO: Created: latency-svc-5r8p8
Jul 11 00:51:08.387: INFO: Created: latency-svc-ft85c
Jul 11 00:51:08.395: INFO: Got endpoints: latency-svc-ft85c [217.692594ms]
Jul 11 00:51:08.395: INFO: Got endpoints: latency-svc-5r8p8 [230.049833ms]
Jul 11 00:51:08.404: INFO: Created: latency-svc-gpdvz
Jul 11 00:51:08.420: INFO: Created: latency-svc-fnfjr
Jul 11 00:51:08.428: INFO: Got endpoints: latency-svc-gpdvz [233.055049ms]
Jul 11 00:51:08.431: INFO: Got endpoints: latency-svc-fnfjr [221.722308ms]
Jul 11 00:51:08.452: INFO: Created: latency-svc-9dnjx
Jul 11 00:51:08.452: INFO: Got endpoints: latency-svc-9dnjx [230.298207ms]
Jul 11 00:51:08.458: INFO: Created: latency-svc-r8twn
Jul 11 00:51:08.464: INFO: Got endpoints: latency-svc-r8twn [230.9306ms]
Jul 11 00:51:08.474: INFO: Created: latency-svc-k5fgp
Jul 11 00:51:08.479: INFO: Got endpoints: latency-svc-k5fgp [224.388855ms]
Jul 11 00:51:08.497: INFO: Created: latency-svc-mzjxx
Jul 11 00:51:08.497: INFO: Got endpoints: latency-svc-mzjxx [242.204076ms]
Jul 11 00:51:08.504: INFO: Created: latency-svc-wbjmp
Jul 11 00:51:08.516: INFO: Got endpoints: latency-svc-wbjmp [235.70062ms]
Jul 11 00:51:08.526: INFO: Created: latency-svc-m98dp
Jul 11 00:51:08.538: INFO: Got endpoints: latency-svc-m98dp [242.087747ms]
Jul 11 00:51:08.543: INFO: Created: latency-svc-27hfv
Jul 11 00:51:08.551: INFO: Got endpoints: latency-svc-27hfv [255.264047ms]
Jul 11 00:51:08.559: INFO: Created: latency-svc-wvhf6
Jul 11 00:51:08.580: INFO: Created: latency-svc-pnt8w
Jul 11 00:51:08.583: INFO: Got endpoints: latency-svc-wvhf6 [254.000201ms]
Jul 11 00:51:08.594: INFO: Created: latency-svc-mxnss
Jul 11 00:51:08.600: INFO: Got endpoints: latency-svc-mxnss [234.140569ms]
Jul 11 00:51:08.600: INFO: Got endpoints: latency-svc-pnt8w [258.281684ms]
Jul 11 00:51:08.610: INFO: Created: latency-svc-79jkm
Jul 11 00:51:08.620: INFO: Created: latency-svc-xnzvh
Jul 11 00:51:08.623: INFO: Got endpoints: latency-svc-79jkm [248.349434ms]
Jul 11 00:51:08.655: INFO: Got endpoints: latency-svc-xnzvh [260.592443ms]
Jul 11 00:51:08.657: INFO: Created: latency-svc-v7p2l
Jul 11 00:51:08.673: INFO: Created: latency-svc-7jl8c
Jul 11 00:51:08.689: INFO: Got endpoints: latency-svc-v7p2l [293.682868ms]
Jul 11 00:51:08.689: INFO: Created: latency-svc-jj2jn
Jul 11 00:51:08.694: INFO: Got endpoints: latency-svc-7jl8c [265.739317ms]
Jul 11 00:51:08.706: INFO: Created: latency-svc-rxd9l
Jul 11 00:51:08.715: INFO: Got endpoints: latency-svc-rxd9l [262.40168ms]
Jul 11 00:51:08.715: INFO: Got endpoints: latency-svc-jj2jn [283.377493ms]
Jul 11 00:51:08.728: INFO: Created: latency-svc-5qmjp
Jul 11 00:51:08.728: INFO: Created: latency-svc-sx2qg
Jul 11 00:51:08.728: INFO: Created: latency-svc-4sp6h
Jul 11 00:51:08.733: INFO: Created: latency-svc-vbmzb
Jul 11 00:51:08.749: INFO: Got endpoints: latency-svc-sx2qg [252.592904ms]
Jul 11 00:51:08.749: INFO: Got endpoints: latency-svc-4sp6h [270.545045ms]
Jul 11 00:51:08.749: INFO: Got endpoints: latency-svc-5qmjp [285.105578ms]
Jul 11 00:51:08.750: INFO: Created: latency-svc-kld4r
Jul 11 00:51:08.751: INFO: Got endpoints: latency-svc-vbmzb [234.612236ms]
Jul 11 00:51:08.762: INFO: Got endpoints: latency-svc-kld4r [223.864516ms]
Jul 11 00:51:08.771: INFO: Created: latency-svc-p7x29
Jul 11 00:51:08.784: INFO: Got endpoints: latency-svc-p7x29 [232.487222ms]
Jul 11 00:51:08.796: INFO: Created: latency-svc-zj42h
Jul 11 00:51:08.802: INFO: Created: latency-svc-n5cw9
Jul 11 00:51:08.813: INFO: Got endpoints: latency-svc-zj42h [230.431659ms]
Jul 11 00:51:08.815: INFO: Created: latency-svc-2sntf
Jul 11 00:51:08.817: INFO: Got endpoints: latency-svc-n5cw9 [216.64252ms]
Jul 11 00:51:08.830: INFO: Got endpoints: latency-svc-2sntf [229.686663ms]
Jul 11 00:51:08.831: INFO: Created: latency-svc-4hjfd
Jul 11 00:51:08.842: INFO: Got endpoints: latency-svc-4hjfd [219.752297ms]
Jul 11 00:51:08.851: INFO: Created: latency-svc-72v82
Jul 11 00:51:08.860: INFO: Got endpoints: latency-svc-72v82 [204.291644ms]
Jul 11 00:51:08.863: INFO: Created: latency-svc-kblvj
Jul 11 00:51:08.871: INFO: Created: latency-svc-67wn8
Jul 11 00:51:08.878: INFO: Got endpoints: latency-svc-kblvj [189.314491ms]
Jul 11 00:51:08.885: INFO: Got endpoints: latency-svc-67wn8 [191.154034ms]
Jul 11 00:51:08.886: INFO: Created: latency-svc-vkpr2
Jul 11 00:51:08.904: INFO: Got endpoints: latency-svc-vkpr2 [189.585144ms]
Jul 11 00:51:08.917: INFO: Created: latency-svc-wwf8p
Jul 11 00:51:08.917: INFO: Got endpoints: latency-svc-wwf8p [202.530498ms]
Jul 11 00:51:08.926: INFO: Created: latency-svc-k69n6
Jul 11 00:51:08.936: INFO: Created: latency-svc-qds5w
Jul 11 00:51:08.940: INFO: Got endpoints: latency-svc-k69n6 [190.939632ms]
Jul 11 00:51:08.945: INFO: Got endpoints: latency-svc-qds5w [195.388549ms]
Jul 11 00:51:08.950: INFO: Created: latency-svc-b2r7b
Jul 11 00:51:08.958: INFO: Created: latency-svc-ptcwb
Jul 11 00:51:08.981: INFO: Created: latency-svc-qdb59
Jul 11 00:51:08.983: INFO: Got endpoints: latency-svc-ptcwb [220.698826ms]
Jul 11 00:51:08.983: INFO: Got endpoints: latency-svc-b2r7b [233.228406ms]
Jul 11 00:51:08.996: INFO: Created: latency-svc-bk45c
Jul 11 00:51:09.001: INFO: Created: latency-svc-vv8vk
Jul 11 00:51:09.004: INFO: Got endpoints: latency-svc-qdb59 [252.874084ms]
Jul 11 00:51:09.004: INFO: Got endpoints: latency-svc-bk45c [220.246242ms]
Jul 11 00:51:09.016: INFO: Got endpoints: latency-svc-vv8vk [202.990987ms]
Jul 11 00:51:09.029: INFO: Created: latency-svc-qksmj
Jul 11 00:51:09.048: INFO: Got endpoints: latency-svc-qksmj [230.762777ms]
Jul 11 00:51:09.053: INFO: Created: latency-svc-kpxbh
Jul 11 00:51:09.061: INFO: Got endpoints: latency-svc-kpxbh [230.700559ms]
Jul 11 00:51:09.061: INFO: Created: latency-svc-v2gbb
Jul 11 00:51:09.076: INFO: Got endpoints: latency-svc-v2gbb [233.426534ms]
Jul 11 00:51:09.079: INFO: Created: latency-svc-rlxc8
Jul 11 00:51:09.095: INFO: Created: latency-svc-gkn7n
Jul 11 00:51:09.098: INFO: Got endpoints: latency-svc-rlxc8 [237.874906ms]
Jul 11 00:51:09.103: INFO: Created: latency-svc-86hq8
Jul 11 00:51:09.109: INFO: Created: latency-svc-8lllb
Jul 11 00:51:09.125: INFO: Created: latency-svc-cxhx6
Jul 11 00:51:09.136: INFO: Got endpoints: latency-svc-8lllb [231.411535ms]
Jul 11 00:51:09.136: INFO: Got endpoints: latency-svc-86hq8 [250.933197ms]
Jul 11 00:51:09.136: INFO: Got endpoints: latency-svc-gkn7n [257.88999ms]
Jul 11 00:51:09.145: INFO: Got endpoints: latency-svc-cxhx6 [227.941238ms]
Jul 11 00:51:09.148: INFO: Created: latency-svc-8ckxb
Jul 11 00:51:09.163: INFO: Got endpoints: latency-svc-8ckxb [222.710581ms]
Jul 11 00:51:09.166: INFO: Created: latency-svc-gxncx
Jul 11 00:51:09.195: INFO: Got endpoints: latency-svc-gxncx [249.98168ms]
Jul 11 00:51:09.195: INFO: Created: latency-svc-8gkws
Jul 11 00:51:09.201: INFO: Got endpoints: latency-svc-8gkws [218.811604ms]
Jul 11 00:51:09.214: INFO: Created: latency-svc-nbc9z
Jul 11 00:51:09.214: INFO: Got endpoints: latency-svc-nbc9z [230.809579ms]
Jul 11 00:51:09.222: INFO: Created: latency-svc-k7rjh
Jul 11 00:51:09.231: INFO: Created: latency-svc-5qsxs
Jul 11 00:51:09.236: INFO: Got endpoints: latency-svc-k7rjh [232.116934ms]
Jul 11 00:51:09.258: INFO: Got endpoints: latency-svc-5qsxs [253.732381ms]
Jul 11 00:51:09.286: INFO: Created: latency-svc-c8d2l
Jul 11 00:51:09.286: INFO: Created: latency-svc-x5bgr
Jul 11 00:51:09.292: INFO: Got endpoints: latency-svc-x5bgr [244.565376ms]
Jul 11 00:51:09.297: INFO: Created: latency-svc-qjqmb
Jul 11 00:51:09.300: INFO: Got endpoints: latency-svc-c8d2l [283.381118ms]
Jul 11 00:51:09.313: INFO: Got endpoints: latency-svc-qjqmb [252.519537ms]
Jul 11 00:51:09.324: INFO: Created: latency-svc-m97kh
Jul 11 00:51:09.333: INFO: Got endpoints: latency-svc-m97kh [234.904178ms]
Jul 11 00:51:09.337: INFO: Created: latency-svc-rfm22
Jul 11 00:51:09.354: INFO: Created: latency-svc-bb2jd
Jul 11 00:51:09.361: INFO: Got endpoints: latency-svc-rfm22 [285.362224ms]
Jul 11 00:51:09.370: INFO: Got endpoints: latency-svc-bb2jd [233.916792ms]
Jul 11 00:51:09.374: INFO: Created: latency-svc-klzd7
Jul 11 00:51:09.386: INFO: Got endpoints: latency-svc-klzd7 [249.922166ms]
Jul 11 00:51:09.390: INFO: Created: latency-svc-8jkc8
Jul 11 00:51:09.405: INFO: Got endpoints: latency-svc-8jkc8 [269.61386ms]
Jul 11 00:51:09.414: INFO: Created: latency-svc-xltr5
Jul 11 00:51:09.430: INFO: Created: latency-svc-f9rrp
Jul 11 00:51:09.440: INFO: Got endpoints: latency-svc-xltr5 [294.29136ms]
Jul 11 00:51:09.451: INFO: Got endpoints: latency-svc-f9rrp [288.215922ms]
Jul 11 00:51:09.458: INFO: Created: latency-svc-hllht
Jul 11 00:51:09.459: INFO: Got endpoints: latency-svc-hllht [263.686814ms]
Jul 11 00:51:09.482: INFO: Created: latency-svc-spdx5
Jul 11 00:51:09.482: INFO: Created: latency-svc-txwxb
Jul 11 00:51:09.507: INFO: Created: latency-svc-nxjnk
Jul 11 00:51:09.513: INFO: Got endpoints: latency-svc-spdx5 [299.701875ms]
Jul 11 00:51:09.514: INFO: Got endpoints: latency-svc-txwxb [312.028977ms]
Jul 11 00:51:09.535: INFO: Got endpoints: latency-svc-nxjnk [299.305904ms]
Jul 11 00:51:09.535: INFO: Created: latency-svc-xt4fc
Jul 11 00:51:09.551: INFO: Got endpoints: latency-svc-xt4fc [293.315901ms]
Jul 11 00:51:09.578: INFO: Created: latency-svc-bg252
Jul 11 00:51:09.578: INFO: Got endpoints: latency-svc-bg252 [285.144273ms]
Jul 11 00:51:09.581: INFO: Created: latency-svc-vpz2l
Jul 11 00:51:09.581: INFO: Got endpoints: latency-svc-vpz2l [281.157869ms]
Jul 11 00:51:09.582: INFO: Created: latency-svc-9qmxr
Jul 11 00:51:09.595: INFO: Got endpoints: latency-svc-9qmxr [281.434334ms]
Jul 11 00:51:09.617: INFO: Created: latency-svc-nxrjf
Jul 11 00:51:09.621: INFO: Created: latency-svc-lm64l
Jul 11 00:51:09.634: INFO: Got endpoints: latency-svc-nxrjf [301.42833ms]
Jul 11 00:51:09.634: INFO: Got endpoints: latency-svc-lm64l [272.930595ms]
Jul 11 00:51:09.660: INFO: Created: latency-svc-gl92r
Jul 11 00:51:09.668: INFO: Created: latency-svc-k4qvq
Jul 11 00:51:09.676: INFO: Got endpoints: latency-svc-gl92r [305.992994ms]
Jul 11 00:51:09.687: INFO: Got endpoints: latency-svc-k4qvq [301.5392ms]
Jul 11 00:51:09.695: INFO: Created: latency-svc-c5shn
Jul 11 00:51:09.706: INFO: Created: latency-svc-t7dbq
Jul 11 00:51:09.716: INFO: Created: latency-svc-7vmlc
Jul 11 00:51:09.732: INFO: Got endpoints: latency-svc-c5shn [326.87665ms]
Jul 11 00:51:09.736: INFO: Got endpoints: latency-svc-t7dbq [296.318034ms]
Jul 11 00:51:09.736: INFO: Got endpoints: latency-svc-7vmlc [284.806329ms]
Jul 11 00:51:09.750: INFO: Created: latency-svc-hbp6d
Jul 11 00:51:09.778: INFO: Created: latency-svc-8jvhq
Jul 11 00:51:09.781: INFO: Got endpoints: latency-svc-8jvhq [268.049131ms]
Jul 11 00:51:09.781: INFO: Got endpoints: latency-svc-hbp6d [322.728502ms]
Jul 11 00:51:09.790: INFO: Created: latency-svc-g2xkf
Jul 11 00:51:09.800: INFO: Created: latency-svc-rxt84
Jul 11 00:51:09.815: INFO: Got endpoints: latency-svc-g2xkf [300.996187ms]
Jul 11 00:51:09.817: INFO: Created: latency-svc-bcl2d
Jul 11 00:51:09.822: INFO: Got endpoints: latency-svc-rxt84 [286.829579ms]
Jul 11 00:51:09.834: INFO: Got endpoints: latency-svc-bcl2d [282.501552ms]
Jul 11 00:51:09.837: INFO: Created: latency-svc-6jd9v
Jul 11 00:51:09.844: INFO: Got endpoints: latency-svc-6jd9v [266.448871ms]
Jul 11 00:51:09.856: INFO: Created: latency-svc-l269k
Jul 11 00:51:09.873: INFO: Created: latency-svc-b7hll
Jul 11 00:51:09.875: INFO: Got endpoints: latency-svc-l269k [293.710537ms]
Jul 11 00:51:09.879: INFO: Created: latency-svc-chhpt
Jul 11 00:51:09.885: INFO: Created: latency-svc-jfc6p
Jul 11 00:51:09.899: INFO: Created: latency-svc-2bt9b
Jul 11 00:51:09.903: INFO: Got endpoints: latency-svc-jfc6p [269.272189ms]
Jul 11 00:51:09.904: INFO: Got endpoints: latency-svc-b7hll [308.573167ms]
Jul 11 00:51:09.909: INFO: Got endpoints: latency-svc-chhpt [275.021351ms]
Jul 11 00:51:09.923: INFO: Created: latency-svc-gptsd
Jul 11 00:51:09.952: INFO: Got endpoints: latency-svc-2bt9b [275.710948ms]
Jul 11 00:51:09.952: INFO: Got endpoints: latency-svc-gptsd [264.369422ms]
Jul 11 00:51:09.989: INFO: Created: latency-svc-ts4gx
Jul 11 00:51:09.989: INFO: Got endpoints: latency-svc-ts4gx [256.93659ms]
Jul 11 00:51:09.990: INFO: Created: latency-svc-rplwk
Jul 11 00:51:09.990: INFO: Created: latency-svc-r9fkc
Jul 11 00:51:10.031: INFO: Created: latency-svc-g599w
Jul 11 00:51:10.036: INFO: Created: latency-svc-pj8gc
Jul 11 00:51:10.037: INFO: Got endpoints: latency-svc-g599w [255.411192ms]
Jul 11 00:51:10.037: INFO: Got endpoints: latency-svc-rplwk [300.707211ms]
Jul 11 00:51:10.037: INFO: Got endpoints: latency-svc-r9fkc [300.920483ms]
Jul 11 00:51:10.040: INFO: Created: latency-svc-pj4j6
Jul 11 00:51:10.062: INFO: Got endpoints: latency-svc-pj4j6 [247.080495ms]
Jul 11 00:51:10.075: INFO: Got endpoints: latency-svc-pj8gc [293.768177ms]
Jul 11 00:51:10.085: INFO: Created: latency-svc-v4j48
Jul 11 00:51:10.090: INFO: Created: latency-svc-vck5c
Jul 11 00:51:10.107: INFO: Got endpoints: latency-svc-v4j48 [284.28447ms]
Jul 11 00:51:10.114: INFO: Created: latency-svc-rjkv5
Jul 11 00:51:10.120: INFO: Got endpoints: latency-svc-vck5c [285.729098ms]
Jul 11 00:51:10.146: INFO: Got endpoints: latency-svc-rjkv5 [301.876757ms]
Jul 11 00:51:10.146: INFO: Created: latency-svc-p8zzs
Jul 11 00:51:10.162: INFO: Got endpoints: latency-svc-p8zzs [287.711965ms]
Jul 11 00:51:10.166: INFO: Created: latency-svc-nb2n9
Jul 11 00:51:10.212: INFO: Got endpoints: latency-svc-nb2n9 [308.061841ms]
Jul 11 00:51:10.222: INFO: Created: latency-svc-5bkq9
Jul 11 00:51:10.222: INFO: Got endpoints: latency-svc-5bkq9 [318.864656ms]
Jul 11 00:51:10.234: INFO: Created: latency-svc-9kz9c
Jul 11 00:51:10.244: INFO: Got endpoints: latency-svc-9kz9c [334.577399ms]
Jul 11 00:51:10.259: INFO: Created: latency-svc-q9tnf
Jul 11 00:51:10.264: INFO: Created: latency-svc-5gkfw
Jul 11 00:51:10.286: INFO: Got endpoints: latency-svc-q9tnf [334.746577ms]
Jul 11 00:51:10.286: INFO: Got endpoints: latency-svc-5gkfw [334.703181ms]
Jul 11 00:51:10.294: INFO: Created: latency-svc-m4kbn
Jul 11 00:51:10.307: INFO: Created: latency-svc-gx9jl
Jul 11 00:51:10.321: INFO: Created: latency-svc-fssk8
Jul 11 00:51:10.322: INFO: Got endpoints: latency-svc-gx9jl [284.640506ms]
Jul 11 00:51:10.322: INFO: Got endpoints: latency-svc-m4kbn [332.429072ms]
Jul 11 00:51:10.334: INFO: Created: latency-svc-scm9w
Jul 11 00:51:10.370: INFO: Created: latency-svc-hmglz
Jul 11 00:51:10.375: INFO: Got endpoints: latency-svc-scm9w [338.217614ms]
Jul 11 00:51:10.396: INFO: Got endpoints: latency-svc-fssk8 [359.045961ms]
Jul 11 00:51:10.396: INFO: Got endpoints: latency-svc-hmglz [334.52327ms]
Jul 11 00:51:10.402: INFO: Created: latency-svc-6wthk
Jul 11 00:51:10.408: INFO: Got endpoints: latency-svc-6wthk [332.801527ms]
Jul 11 00:51:10.418: INFO: Created: latency-svc-mnlmq
Jul 11 00:51:10.424: INFO: Got endpoints: latency-svc-mnlmq [317.555527ms]
Jul 11 00:51:10.427: INFO: Created: latency-svc-rlwst
Jul 11 00:51:10.435: INFO: Got endpoints: latency-svc-rlwst [315.558934ms]
Jul 11 00:51:10.436: INFO: Created: latency-svc-9vbp2
Jul 11 00:51:10.441: INFO: Created: latency-svc-lkwvq
Jul 11 00:51:10.442: INFO: Got endpoints: latency-svc-9vbp2 [296.293614ms]
Jul 11 00:51:10.450: INFO: Got endpoints: latency-svc-lkwvq [287.942833ms]
Jul 11 00:51:10.454: INFO: Created: latency-svc-4pdgj
Jul 11 00:51:10.457: INFO: Created: latency-svc-dzrc7
Jul 11 00:51:10.458: INFO: Got endpoints: latency-svc-4pdgj [246.623753ms]
Jul 11 00:51:10.464: INFO: Got endpoints: latency-svc-dzrc7 [241.687617ms]
Jul 11 00:51:10.475: INFO: Created: latency-svc-ppk78
Jul 11 00:51:10.475: INFO: Created: latency-svc-jhxzp
Jul 11 00:51:10.476: INFO: Got endpoints: latency-svc-ppk78 [232.598761ms]
Jul 11 00:51:10.478: INFO: Got endpoints: latency-svc-jhxzp [191.742682ms]
Jul 11 00:51:10.483: INFO: Created: latency-svc-lxt8k
Jul 11 00:51:10.487: INFO: Got endpoints: latency-svc-lxt8k [200.282488ms]
Jul 11 00:51:10.493: INFO: Created: latency-svc-f9cgw
Jul 11 00:51:10.495: INFO: Got endpoints: latency-svc-f9cgw [172.987264ms]
Jul 11 00:51:10.496: INFO: Created: latency-svc-zdrlc
Jul 11 00:51:10.503: INFO: Created: latency-svc-zctl8
Jul 11 00:51:10.507: INFO: Got endpoints: latency-svc-zdrlc [185.203637ms]
Jul 11 00:51:10.510: INFO: Got endpoints: latency-svc-zctl8 [134.886825ms]
Jul 11 00:51:10.510: INFO: Created: latency-svc-vkrl2
Jul 11 00:51:10.515: INFO: Got endpoints: latency-svc-vkrl2 [119.250974ms]
Jul 11 00:51:10.516: INFO: Created: latency-svc-z9f27
Jul 11 00:51:10.527: INFO: Got endpoints: latency-svc-z9f27 [130.436303ms]
Jul 11 00:51:10.528: INFO: Created: latency-svc-rkk2p
Jul 11 00:51:10.529: INFO: Got endpoints: latency-svc-rkk2p [120.739778ms]
Jul 11 00:51:10.535: INFO: Created: latency-svc-p48jn
Jul 11 00:51:10.536: INFO: Got endpoints: latency-svc-p48jn [111.364797ms]
Jul 11 00:51:10.540: INFO: Created: latency-svc-h6wzk
Jul 11 00:51:10.545: INFO: Got endpoints: latency-svc-h6wzk [110.126962ms]
Jul 11 00:51:10.547: INFO: Created: latency-svc-w7rtt
Jul 11 00:51:10.554: INFO: Got endpoints: latency-svc-w7rtt [111.464178ms]
Jul 11 00:51:10.556: INFO: Created: latency-svc-ndjjp
Jul 11 00:51:10.565: INFO: Created: latency-svc-9nr2p
Jul 11 00:51:10.567: INFO: Got endpoints: latency-svc-ndjjp [116.920125ms]
Jul 11 00:51:10.571: INFO: Created: latency-svc-z7d9k
Jul 11 00:51:10.584: INFO: Got endpoints: latency-svc-z7d9k [119.788132ms]
Jul 11 00:51:10.595: INFO: Got endpoints: latency-svc-9nr2p [136.939484ms]
Jul 11 00:51:10.597: INFO: Created: latency-svc-gjbtm
Jul 11 00:51:10.599: INFO: Got endpoints: latency-svc-gjbtm [122.486906ms]
Jul 11 00:51:10.610: INFO: Created: latency-svc-dxkph
Jul 11 00:51:10.613: INFO: Created: latency-svc-gdwn4
Jul 11 00:51:10.616: INFO: Got endpoints: latency-svc-dxkph [137.793066ms]
Jul 11 00:51:10.620: INFO: Got endpoints: latency-svc-gdwn4 [133.544183ms]
Jul 11 00:51:10.625: INFO: Created: latency-svc-znxsn
Jul 11 00:51:10.631: INFO: Got endpoints: latency-svc-znxsn [136.257003ms]
Jul 11 00:51:10.634: INFO: Created: latency-svc-rjr47
Jul 11 00:51:10.638: INFO: Got endpoints: latency-svc-rjr47 [131.329347ms]
Jul 11 00:51:10.647: INFO: Created: latency-svc-vwjwt
Jul 11 00:51:10.655: INFO: Created: latency-svc-gltvn
Jul 11 00:51:10.664: INFO: Got endpoints: latency-svc-gltvn [148.364545ms]
Jul 11 00:51:10.664: INFO: Got endpoints: latency-svc-vwjwt [153.556178ms]
Jul 11 00:51:10.666: INFO: Created: latency-svc-4f7ww
Jul 11 00:51:10.677: INFO: Got endpoints: latency-svc-4f7ww [150.160624ms]
Jul 11 00:51:10.680: INFO: Created: latency-svc-nx9ql
Jul 11 00:51:10.688: INFO: Created: latency-svc-kcp2k
Jul 11 00:51:10.688: INFO: Got endpoints: latency-svc-nx9ql [159.4068ms]
Jul 11 00:51:10.688: INFO: Got endpoints: latency-svc-kcp2k [152.813798ms]
Jul 11 00:51:10.688: INFO: Latencies: [110.126962ms 111.364797ms 111.464178ms 116.920125ms 119.250974ms 119.788132ms 120.739778ms 122.486906ms 130.436303ms 131.329347ms 133.544183ms 134.886825ms 136.257003ms 136.939484ms 137.793066ms 148.364545ms 150.160624ms 152.813798ms 153.556178ms 154.400473ms 159.4068ms 172.987264ms 185.203637ms 185.585423ms 189.314491ms 189.585144ms 190.939632ms 191.154034ms 191.742682ms 193.538897ms 193.998203ms 195.388549ms 196.927115ms 197.393302ms 200.282488ms 200.894735ms 202.530498ms 202.990987ms 203.528547ms 204.291644ms 206.155347ms 207.72586ms 210.134643ms 210.823711ms 211.020148ms 214.784772ms 216.006594ms 216.64252ms 217.692594ms 218.811604ms 219.752297ms 220.246242ms 220.698826ms 221.722308ms 222.710581ms 223.864516ms 224.388855ms 224.756248ms 224.834569ms 225.864829ms 227.941238ms 229.686663ms 230.049833ms 230.050165ms 230.298207ms 230.431659ms 230.700559ms 230.762777ms 230.809579ms 230.822959ms 230.9306ms 231.411535ms 232.116934ms 232.487222ms 232.598761ms 233.055049ms 233.228406ms 233.426534ms 233.916792ms 234.140569ms 234.612236ms 234.904178ms 235.70062ms 237.874906ms 240.06717ms 241.687617ms 241.872259ms 242.087747ms 242.204076ms 243.462961ms 244.565376ms 244.92211ms 246.623753ms 247.080495ms 247.100842ms 248.349434ms 249.922166ms 249.98168ms 250.933197ms 252.519537ms 252.592904ms 252.874084ms 253.732381ms 254.000201ms 255.264047ms 255.411192ms 256.518982ms 256.93659ms 257.88999ms 258.281684ms 259.21873ms 259.661107ms 260.592443ms 262.40168ms 263.686814ms 264.369422ms 265.739317ms 266.382013ms 266.448871ms 267.708374ms 268.049131ms 269.272189ms 269.61386ms 270.545045ms 272.930595ms 275.021351ms 275.710948ms 279.721903ms 281.157869ms 281.434334ms 281.642978ms 282.501552ms 283.377493ms 283.381118ms 284.28447ms 284.640506ms 284.806329ms 285.105578ms 285.144273ms 285.362224ms 285.729098ms 286.829579ms 287.222055ms 287.516927ms 287.711965ms 287.942833ms 288.215922ms 289.008257ms 291.764589ms 293.315901ms 293.682868ms 293.710537ms 293.768177ms 294.29136ms 296.293614ms 296.318034ms 299.305904ms 299.701875ms 300.707211ms 300.920483ms 300.996187ms 301.301442ms 301.42833ms 301.5392ms 301.876757ms 305.992994ms 308.061841ms 308.573167ms 311.267441ms 312.028977ms 313.797648ms 315.558934ms 317.005189ms 317.555527ms 318.864656ms 322.728502ms 326.87665ms 332.429072ms 332.801527ms 334.52327ms 334.577399ms 334.703181ms 334.746577ms 338.217614ms 359.045961ms 389.771989ms 395.675981ms 400.559612ms 403.947171ms 420.385264ms 446.864357ms 451.537941ms 466.108722ms 472.411583ms 484.008114ms 508.303781ms 532.47777ms 569.327562ms 573.779964ms 602.14969ms]
Jul 11 00:51:10.689: INFO: 50 %ile: 252.592904ms
Jul 11 00:51:10.689: INFO: 90 %ile: 334.577399ms
Jul 11 00:51:10.689: INFO: 99 %ile: 573.779964ms
Jul 11 00:51:10.689: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:51:10.689: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-fgjdj" for this suite.
Jul 11 00:51:24.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:51:25.876: INFO: namespace: e2e-tests-svc-latency-fgjdj, resource: bindings, ignored listing per whitelist
Jul 11 00:51:26.264: INFO: namespace e2e-tests-svc-latency-fgjdj deletion completed in 15.555100051s

• [SLOW TEST:23.031 seconds]
[sig-network] Service endpoints latency
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:51:26.264: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 11 00:51:27.066: INFO: Waiting up to 5m0s for pod "pod-8aa51147-84a4-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-wbcnp" to be "success or failure"
Jul 11 00:51:27.083: INFO: Pod "pod-8aa51147-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.446922ms
Jul 11 00:51:29.099: INFO: Pod "pod-8aa51147-84a4-11e8-9117-0e046f2b5c78": Phase="Running", Reason="", readiness=true. Elapsed: 2.032506785s
Jul 11 00:51:31.115: INFO: Pod "pod-8aa51147-84a4-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048333465s
STEP: Saw pod success
Jul 11 00:51:31.115: INFO: Pod "pod-8aa51147-84a4-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:51:31.130: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-8aa51147-84a4-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:51:31.174: INFO: Waiting for pod pod-8aa51147-84a4-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:51:31.189: INFO: Pod pod-8aa51147-84a4-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:51:31.189: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wbcnp" for this suite.
Jul 11 00:51:37.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:51:38.312: INFO: namespace: e2e-tests-emptydir-wbcnp, resource: bindings, ignored listing per whitelist
Jul 11 00:51:38.738: INFO: namespace e2e-tests-emptydir-wbcnp deletion completed in 7.518496775s

• [SLOW TEST:12.473 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:51:38.738: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 11 00:51:39.613: INFO: Number of nodes with available pods: 0
Jul 11 00:51:39.613: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:51:40.662: INFO: Number of nodes with available pods: 0
Jul 11 00:51:40.662: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:51:41.659: INFO: Number of nodes with available pods: 0
Jul 11 00:51:41.659: INFO: Node prtest-7ef3e0b-4-ig-m-585t is running more than one daemon pod
Jul 11 00:51:42.662: INFO: Number of nodes with available pods: 3
Jul 11 00:51:42.662: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:51:43.660: INFO: Number of nodes with available pods: 4
Jul 11 00:51:43.660: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 11 00:51:43.744: INFO: Number of nodes with available pods: 3
Jul 11 00:51:43.744: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:51:44.791: INFO: Number of nodes with available pods: 3
Jul 11 00:51:44.791: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:51:45.792: INFO: Number of nodes with available pods: 3
Jul 11 00:51:45.792: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:51:46.790: INFO: Number of nodes with available pods: 3
Jul 11 00:51:46.790: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:51:47.790: INFO: Number of nodes with available pods: 3
Jul 11 00:51:47.790: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:51:48.809: INFO: Number of nodes with available pods: 3
Jul 11 00:51:48.809: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:51:49.790: INFO: Number of nodes with available pods: 3
Jul 11 00:51:49.790: INFO: Node prtest-7ef3e0b-4-ig-n-hp69 is running more than one daemon pod
Jul 11 00:51:50.794: INFO: Number of nodes with available pods: 4
Jul 11 00:51:50.794: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:66
STEP: Deleting DaemonSet "daemon-set" with reaper
Jul 11 00:52:02.895: INFO: Number of nodes with available pods: 0
Jul 11 00:52:02.895: INFO: Number of running nodes: 0, number of available pods: 0
Jul 11 00:52:02.911: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-h4sgp/daemonsets","resourceVersion":"16540"},"items":null}

Jul 11 00:52:02.927: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-h4sgp/pods","resourceVersion":"16540"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:52:03.005: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-h4sgp" for this suite.
Jul 11 00:52:09.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:52:10.378: INFO: namespace: e2e-tests-daemonsets-h4sgp, resource: bindings, ignored listing per whitelist
Jul 11 00:52:10.503: INFO: namespace e2e-tests-daemonsets-h4sgp deletion completed in 7.481893775s

• [SLOW TEST:31.765 seconds]
[sig-apps] Daemon set [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:52:10.503: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating secret with name secret-test-a504457d-84a4-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 00:52:11.322: INFO: Waiting up to 5m0s for pod "pod-secrets-a5071cc0-84a4-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-secrets-r4gbw" to be "success or failure"
Jul 11 00:52:11.344: INFO: Pod "pod-secrets-a5071cc0-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 21.885786ms
Jul 11 00:52:13.361: INFO: Pod "pod-secrets-a5071cc0-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038568208s
Jul 11 00:52:15.377: INFO: Pod "pod-secrets-a5071cc0-84a4-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054597278s
STEP: Saw pod success
Jul 11 00:52:15.377: INFO: Pod "pod-secrets-a5071cc0-84a4-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:52:15.393: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-secrets-a5071cc0-84a4-11e8-9117-0e046f2b5c78 container secret-volume-test: <nil>
STEP: delete the pod
Jul 11 00:52:15.438: INFO: Waiting for pod pod-secrets-a5071cc0-84a4-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:52:15.454: INFO: Pod pod-secrets-a5071cc0-84a4-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:52:15.454: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-r4gbw" for this suite.
Jul 11 00:52:21.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:52:22.477: INFO: namespace: e2e-tests-secrets-r4gbw, resource: bindings, ignored listing per whitelist
Jul 11 00:52:22.958: INFO: namespace e2e-tests-secrets-r4gbw deletion completed in 7.474905552s

• [SLOW TEST:12.455 seconds]
[sig-storage] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:52:22.958: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run rc
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1147
[It] should create an rc from an image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Jul 11 00:52:23.676: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-rc --image=k8s.gcr.io/nginx-slim-amd64:0.20 --generator=run/v1 --namespace=e2e-tests-kubectl-gd2b8'
Jul 11 00:52:23.883: INFO: stderr: ""
Jul 11 00:52:23.883: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 11 00:52:23.919: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-4wz74]
Jul 11 00:52:23.919: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-4wz74" in namespace "e2e-tests-kubectl-gd2b8" to be "running and ready"
Jul 11 00:52:23.935: INFO: Pod "e2e-test-nginx-rc-4wz74": Phase="Pending", Reason="", readiness=false. Elapsed: 16.823171ms
Jul 11 00:52:25.952: INFO: Pod "e2e-test-nginx-rc-4wz74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033383563s
Jul 11 00:52:27.969: INFO: Pod "e2e-test-nginx-rc-4wz74": Phase="Running", Reason="", readiness=true. Elapsed: 4.049910432s
Jul 11 00:52:27.969: INFO: Pod "e2e-test-nginx-rc-4wz74" satisfied condition "running and ready"
Jul 11 00:52:27.969: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-4wz74]
Jul 11 00:52:27.969: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-gd2b8'
Jul 11 00:52:28.168: INFO: stderr: ""
[AfterEach] [k8s.io] Kubectl run rc
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1152
Jul 11 00:52:28.168: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-gd2b8'
Jul 11 00:52:28.428: INFO: stderr: ""
Jul 11 00:52:28.428: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:52:28.428: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gd2b8" for this suite.
Jul 11 00:52:46.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:52:47.581: INFO: namespace: e2e-tests-kubectl-gd2b8, resource: bindings, ignored listing per whitelist
Jul 11 00:52:47.958: INFO: namespace e2e-tests-kubectl-gd2b8 deletion completed in 19.49970632s

• [SLOW TEST:24.999 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should create an rc from an image  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:52:47.958: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should contain environment variables for services  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:52:52.891: INFO: Waiting up to 5m0s for pod "client-envvars-bdc53c40-84a4-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-pods-ck4cr" to be "success or failure"
Jul 11 00:52:52.907: INFO: Pod "client-envvars-bdc53c40-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.506026ms
Jul 11 00:52:54.923: INFO: Pod "client-envvars-bdc53c40-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032614497s
Jul 11 00:52:56.940: INFO: Pod "client-envvars-bdc53c40-84a4-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049102132s
STEP: Saw pod success
Jul 11 00:52:56.940: INFO: Pod "client-envvars-bdc53c40-84a4-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:52:56.955: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod client-envvars-bdc53c40-84a4-11e8-9117-0e046f2b5c78 container env3cont: <nil>
STEP: delete the pod
Jul 11 00:52:57.006: INFO: Waiting for pod client-envvars-bdc53c40-84a4-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:52:57.022: INFO: Pod client-envvars-bdc53c40-84a4-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:52:57.022: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-ck4cr" for this suite.
Jul 11 00:53:19.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:53:20.090: INFO: namespace: e2e-tests-pods-ck4cr, resource: bindings, ignored listing per whitelist
Jul 11 00:53:20.518: INFO: namespace e2e-tests-pods-ck4cr deletion completed in 23.466935308s

• [SLOW TEST:32.560 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should contain environment variables for services  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:53:20.519: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0711 00:54:01.327189    9515 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 11 00:54:01.327: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:54:01.327: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-97j5q" for this suite.
Jul 11 00:54:07.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:54:08.763: INFO: namespace: e2e-tests-gc-97j5q, resource: bindings, ignored listing per whitelist
Jul 11 00:54:08.842: INFO: namespace e2e-tests-gc-97j5q deletion completed in 7.49893324s

• [SLOW TEST:48.323 seconds]
[sig-api-machinery] Garbage collector
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:54:08.842: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0711 00:54:19.849636    9515 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 11 00:54:19.849: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:54:19.849: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-cw78g" for this suite.
Jul 11 00:54:25.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:54:27.419: INFO: namespace: e2e-tests-gc-cw78g, resource: bindings, ignored listing per whitelist
Jul 11 00:54:27.435: INFO: namespace e2e-tests-gc-cw78g deletion completed in 7.569003298s

• [SLOW TEST:18.593 seconds]
[sig-api-machinery] Garbage collector
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:54:27.435: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 00:54:28.196: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f69b237e-84a4-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-5l7mw" to be "success or failure"
Jul 11 00:54:28.212: INFO: Pod "downwardapi-volume-f69b237e-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.128152ms
Jul 11 00:54:30.229: INFO: Pod "downwardapi-volume-f69b237e-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033402413s
Jul 11 00:54:32.245: INFO: Pod "downwardapi-volume-f69b237e-84a4-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049905704s
STEP: Saw pod success
Jul 11 00:54:32.245: INFO: Pod "downwardapi-volume-f69b237e-84a4-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:54:32.262: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod downwardapi-volume-f69b237e-84a4-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 00:54:32.306: INFO: Waiting for pod downwardapi-volume-f69b237e-84a4-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:54:32.322: INFO: Pod downwardapi-volume-f69b237e-84a4-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:54:32.322: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5l7mw" for this suite.
Jul 11 00:54:38.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:54:39.473: INFO: namespace: e2e-tests-projected-5l7mw, resource: bindings, ignored listing per whitelist
Jul 11 00:54:39.854: INFO: namespace e2e-tests-projected-5l7mw deletion completed in 7.501995583s

• [SLOW TEST:12.419 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide container's memory request [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd)  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:54:39.854: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd)  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test override arguments
Jul 11 00:54:40.599: INFO: Waiting up to 5m0s for pod "client-containers-fdffb47c-84a4-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-containers-2crmt" to be "success or failure"
Jul 11 00:54:40.615: INFO: Pod "client-containers-fdffb47c-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.419818ms
Jul 11 00:54:42.632: INFO: Pod "client-containers-fdffb47c-84a4-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032541868s
Jul 11 00:54:44.650: INFO: Pod "client-containers-fdffb47c-84a4-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050823038s
STEP: Saw pod success
Jul 11 00:54:44.650: INFO: Pod "client-containers-fdffb47c-84a4-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:54:44.666: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod client-containers-fdffb47c-84a4-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:54:44.710: INFO: Waiting for pod client-containers-fdffb47c-84a4-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:54:44.725: INFO: Pod client-containers-fdffb47c-84a4-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:54:44.725: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-2crmt" for this suite.
Jul 11 00:54:50.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:54:52.280: INFO: namespace: e2e-tests-containers-2crmt, resource: bindings, ignored listing per whitelist
Jul 11 00:54:52.280: INFO: namespace e2e-tests-containers-2crmt deletion completed in 7.525680706s

• [SLOW TEST:12.427 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should be able to override the image's default arguments (docker cmd)  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:54:52.280: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 00:54:53.030: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 11 00:54:53.062: INFO: Number of nodes with available pods: 0
Jul 11 00:54:53.062: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 11 00:54:53.127: INFO: Number of nodes with available pods: 0
Jul 11 00:54:53.127: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:54:54.142: INFO: Number of nodes with available pods: 0
Jul 11 00:54:54.142: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:54:55.143: INFO: Number of nodes with available pods: 0
Jul 11 00:54:55.143: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:54:56.143: INFO: Number of nodes with available pods: 0
Jul 11 00:54:56.143: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:54:57.148: INFO: Number of nodes with available pods: 1
Jul 11 00:54:57.148: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 11 00:54:57.212: INFO: Number of nodes with available pods: 0
Jul 11 00:54:57.212: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 11 00:54:57.245: INFO: Number of nodes with available pods: 0
Jul 11 00:54:57.245: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:54:58.261: INFO: Number of nodes with available pods: 0
Jul 11 00:54:58.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:54:59.261: INFO: Number of nodes with available pods: 0
Jul 11 00:54:59.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:00.261: INFO: Number of nodes with available pods: 0
Jul 11 00:55:00.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:01.261: INFO: Number of nodes with available pods: 0
Jul 11 00:55:01.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:02.265: INFO: Number of nodes with available pods: 0
Jul 11 00:55:02.265: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:03.262: INFO: Number of nodes with available pods: 0
Jul 11 00:55:03.262: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:04.261: INFO: Number of nodes with available pods: 0
Jul 11 00:55:04.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:05.262: INFO: Number of nodes with available pods: 0
Jul 11 00:55:05.262: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:06.261: INFO: Number of nodes with available pods: 0
Jul 11 00:55:06.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:07.261: INFO: Number of nodes with available pods: 0
Jul 11 00:55:07.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:08.261: INFO: Number of nodes with available pods: 0
Jul 11 00:55:08.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:09.262: INFO: Number of nodes with available pods: 0
Jul 11 00:55:09.262: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:10.261: INFO: Number of nodes with available pods: 0
Jul 11 00:55:10.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:11.261: INFO: Number of nodes with available pods: 0
Jul 11 00:55:11.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:12.268: INFO: Number of nodes with available pods: 0
Jul 11 00:55:12.268: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:13.261: INFO: Number of nodes with available pods: 0
Jul 11 00:55:13.261: INFO: Node prtest-7ef3e0b-4-ig-n-4p7t is running more than one daemon pod
Jul 11 00:55:14.262: INFO: Number of nodes with available pods: 1
Jul 11 00:55:14.262: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:66
STEP: Deleting DaemonSet "daemon-set" with reaper
Jul 11 00:55:18.376: INFO: Number of nodes with available pods: 0
Jul 11 00:55:18.376: INFO: Number of running nodes: 0, number of available pods: 0
Jul 11 00:55:18.391: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-8rjz6/daemonsets","resourceVersion":"17908"},"items":null}

Jul 11 00:55:18.407: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-8rjz6/pods","resourceVersion":"17908"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:55:18.502: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-8rjz6" for this suite.
Jul 11 00:55:24.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:55:25.628: INFO: namespace: e2e-tests-daemonsets-8rjz6, resource: bindings, ignored listing per whitelist
Jul 11 00:55:26.022: INFO: namespace e2e-tests-daemonsets-8rjz6 deletion completed in 7.503946219s

• [SLOW TEST:33.741 seconds]
[sig-apps] Daemon set [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:55:26.022: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 11 00:55:26.780: INFO: Waiting up to 5m0s for pod "pod-1987629a-84a5-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-sngxf" to be "success or failure"
Jul 11 00:55:26.797: INFO: Pod "pod-1987629a-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.008065ms
Jul 11 00:55:28.814: INFO: Pod "pod-1987629a-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033217701s
Jul 11 00:55:30.830: INFO: Pod "pod-1987629a-84a5-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049344862s
STEP: Saw pod success
Jul 11 00:55:30.830: INFO: Pod "pod-1987629a-84a5-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:55:30.846: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-1987629a-84a5-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:55:30.894: INFO: Waiting for pod pod-1987629a-84a5-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:55:30.909: INFO: Pod pod-1987629a-84a5-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:55:30.909: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sngxf" for this suite.
Jul 11 00:55:36.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:55:37.839: INFO: namespace: e2e-tests-emptydir-sngxf, resource: bindings, ignored listing per whitelist
Jul 11 00:55:38.537: INFO: namespace e2e-tests-emptydir-sngxf deletion completed in 7.597904498s

• [SLOW TEST:12.515 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:55:38.537: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 11 00:55:39.317: INFO: Waiting up to 5m0s for pod "pod-210001e5-84a5-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-txh7k" to be "success or failure"
Jul 11 00:55:39.334: INFO: Pod "pod-210001e5-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.931427ms
Jul 11 00:55:41.351: INFO: Pod "pod-210001e5-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033818144s
Jul 11 00:55:43.368: INFO: Pod "pod-210001e5-84a5-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050086983s
STEP: Saw pod success
Jul 11 00:55:43.368: INFO: Pod "pod-210001e5-84a5-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:55:43.384: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-210001e5-84a5-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:55:43.446: INFO: Waiting for pod pod-210001e5-84a5-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:55:43.466: INFO: Pod pod-210001e5-84a5-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:55:43.466: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-txh7k" for this suite.
Jul 11 00:55:49.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:55:50.399: INFO: namespace: e2e-tests-emptydir-txh7k, resource: bindings, ignored listing per whitelist
Jul 11 00:55:50.974: INFO: namespace e2e-tests-emptydir-txh7k deletion completed in 7.474675301s

• [SLOW TEST:12.437 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:55:50.974: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:264
[It] should do a rolling update of a replication controller  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating the initial replication controller
Jul 11 00:55:51.758: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:55:52.113: INFO: stderr: ""
Jul 11 00:55:52.113: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 11 00:55:52.113: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:55:52.298: INFO: stderr: ""
Jul 11 00:55:52.298: INFO: stdout: "update-demo-nautilus-8phtz update-demo-nautilus-f6wr5 "
Jul 11 00:55:52.298: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-8phtz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:55:52.451: INFO: stderr: ""
Jul 11 00:55:52.451: INFO: stdout: ""
Jul 11 00:55:52.451: INFO: update-demo-nautilus-8phtz is created but not running
Jul 11 00:55:57.452: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:55:57.604: INFO: stderr: ""
Jul 11 00:55:57.604: INFO: stdout: "update-demo-nautilus-8phtz update-demo-nautilus-f6wr5 "
Jul 11 00:55:57.604: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-8phtz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:55:57.753: INFO: stderr: ""
Jul 11 00:55:57.753: INFO: stdout: "true"
Jul 11 00:55:57.753: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-8phtz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:55:57.901: INFO: stderr: ""
Jul 11 00:55:57.901: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jul 11 00:55:57.901: INFO: validating pod update-demo-nautilus-8phtz
Jul 11 00:55:57.928: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 11 00:55:57.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 11 00:55:57.928: INFO: update-demo-nautilus-8phtz is verified up and running
Jul 11 00:55:57.928: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-f6wr5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:55:58.076: INFO: stderr: ""
Jul 11 00:55:58.076: INFO: stdout: "true"
Jul 11 00:55:58.076: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-f6wr5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:55:58.228: INFO: stderr: ""
Jul 11 00:55:58.228: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jul 11 00:55:58.228: INFO: validating pod update-demo-nautilus-f6wr5
Jul 11 00:55:58.248: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 11 00:55:58.248: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 11 00:55:58.248: INFO: update-demo-nautilus-f6wr5 is verified up and running
STEP: rolling-update to new replication controller
Jul 11 00:55:58.248: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:56:09.479: INFO: stderr: ""
Jul 11 00:56:09.479: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting update-demo-nautilus\nreplicationcontroller \"update-demo-kitten\" rolling updated to \"update-demo-kitten\"\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 11 00:56:09.479: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:56:09.644: INFO: stderr: ""
Jul 11 00:56:09.644: INFO: stdout: "update-demo-kitten-c4r87 update-demo-kitten-jgxgq update-demo-nautilus-8phtz "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jul 11 00:56:14.645: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:56:14.797: INFO: stderr: ""
Jul 11 00:56:14.797: INFO: stdout: "update-demo-kitten-c4r87 update-demo-kitten-jgxgq "
Jul 11 00:56:14.797: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-c4r87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:56:14.947: INFO: stderr: ""
Jul 11 00:56:14.947: INFO: stdout: "true"
Jul 11 00:56:14.947: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-c4r87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:56:15.096: INFO: stderr: ""
Jul 11 00:56:15.096: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0"
Jul 11 00:56:15.096: INFO: validating pod update-demo-kitten-c4r87
Jul 11 00:56:15.152: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 11 00:56:15.152: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 11 00:56:15.152: INFO: update-demo-kitten-c4r87 is verified up and running
Jul 11 00:56:15.153: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-jgxgq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:56:15.301: INFO: stderr: ""
Jul 11 00:56:15.301: INFO: stdout: "true"
Jul 11 00:56:15.301: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-kitten-jgxgq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-26r49'
Jul 11 00:56:15.450: INFO: stderr: ""
Jul 11 00:56:15.450: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0"
Jul 11 00:56:15.450: INFO: validating pod update-demo-kitten-jgxgq
Jul 11 00:56:15.470: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 11 00:56:15.470: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 11 00:56:15.470: INFO: update-demo-kitten-jgxgq is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:56:15.470: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-26r49" for this suite.
Jul 11 00:56:37.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:56:38.259: INFO: namespace: e2e-tests-kubectl-26r49, resource: bindings, ignored listing per whitelist
Jul 11 00:56:38.983: INFO: namespace e2e-tests-kubectl-26r49 deletion completed in 23.48417023s

• [SLOW TEST:48.009 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should do a rolling update of a replication controller  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:56:38.983: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name projected-configmap-test-volume-44fe1ee9-84a5-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 00:56:39.718: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4500e94d-84a5-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-m4dm7" to be "success or failure"
Jul 11 00:56:39.734: INFO: Pod "pod-projected-configmaps-4500e94d-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.094109ms
Jul 11 00:56:41.750: INFO: Pod "pod-projected-configmaps-4500e94d-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032525042s
Jul 11 00:56:43.766: INFO: Pod "pod-projected-configmaps-4500e94d-84a5-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048756336s
STEP: Saw pod success
Jul 11 00:56:43.766: INFO: Pod "pod-projected-configmaps-4500e94d-84a5-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:56:43.782: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-projected-configmaps-4500e94d-84a5-11e8-9117-0e046f2b5c78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 00:56:43.833: INFO: Waiting for pod pod-projected-configmaps-4500e94d-84a5-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:56:43.848: INFO: Pod pod-projected-configmaps-4500e94d-84a5-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:56:43.848: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m4dm7" for this suite.
Jul 11 00:56:49.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:56:50.587: INFO: namespace: e2e-tests-projected-m4dm7, resource: bindings, ignored listing per whitelist
Jul 11 00:56:51.380: INFO: namespace e2e-tests-projected-m4dm7 deletion completed in 7.502828847s

• [SLOW TEST:12.397 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with defaultMode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:56:51.380: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-xslhd
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 11 00:56:52.063: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Jul 11 00:57:16.427: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.68:8080/dial?request=hostName&protocol=http&host=172.16.6.67&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-xslhd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 00:57:16.427: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:57:16.722: INFO: Waiting for endpoints: map[]
Jul 11 00:57:16.738: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.68:8080/dial?request=hostName&protocol=http&host=172.16.4.58&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-xslhd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 00:57:16.738: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:57:16.935: INFO: Waiting for endpoints: map[]
Jul 11 00:57:16.951: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.68:8080/dial?request=hostName&protocol=http&host=172.16.2.63&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-xslhd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 00:57:16.951: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 00:57:17.146: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:57:17.146: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-xslhd" for this suite.
Jul 11 00:57:39.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:57:40.210: INFO: namespace: e2e-tests-pod-network-test-xslhd, resource: bindings, ignored listing per whitelist
Jul 11 00:57:40.708: INFO: namespace e2e-tests-pod-network-test-xslhd deletion completed in 23.532856166s

• [SLOW TEST:49.328 seconds]
[sig-network] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:57:40.709: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 11 00:57:41.440: INFO: Waiting up to 5m0s for pod "pod-69c61c48-84a5-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-nh7xx" to be "success or failure"
Jul 11 00:57:41.470: INFO: Pod "pod-69c61c48-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 29.329544ms
Jul 11 00:57:43.486: INFO: Pod "pod-69c61c48-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045728474s
Jul 11 00:57:45.503: INFO: Pod "pod-69c61c48-84a5-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062396327s
STEP: Saw pod success
Jul 11 00:57:45.503: INFO: Pod "pod-69c61c48-84a5-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 00:57:45.519: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-69c61c48-84a5-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 00:57:45.566: INFO: Waiting for pod pod-69c61c48-84a5-11e8-9117-0e046f2b5c78 to disappear
Jul 11 00:57:45.582: INFO: Pod pod-69c61c48-84a5-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:57:45.582: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nh7xx" for this suite.
Jul 11 00:57:51.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 00:57:52.736: INFO: namespace: e2e-tests-emptydir-nh7xx, resource: bindings, ignored listing per whitelist
Jul 11 00:57:53.094: INFO: namespace e2e-tests-emptydir-nh7xx deletion completed in 7.482051183s

• [SLOW TEST:12.386 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 00:57:53.094: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-pldkc
Jul 11 00:57:57.840: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-pldkc
STEP: checking the pod's current state and verifying that restartCount is present
Jul 11 00:57:57.855: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 00:59:58.851: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-pldkc" for this suite.
Jul 11 01:00:04.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:00:05.648: INFO: namespace: e2e-tests-container-probe-pldkc, resource: bindings, ignored listing per whitelist
Jul 11 01:00:06.396: INFO: namespace e2e-tests-container-probe-pldkc deletion completed in 7.515587421s

• [SLOW TEST:133.302 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:00:06.396: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test use defaults
Jul 11 01:00:07.126: INFO: Waiting up to 5m0s for pod "client-containers-c09fd679-84a5-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-containers-cz7vm" to be "success or failure"
Jul 11 01:00:07.142: INFO: Pod "client-containers-c09fd679-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.935346ms
Jul 11 01:00:09.158: INFO: Pod "client-containers-c09fd679-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032077153s
Jul 11 01:00:11.174: INFO: Pod "client-containers-c09fd679-84a5-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048252843s
STEP: Saw pod success
Jul 11 01:00:11.174: INFO: Pod "client-containers-c09fd679-84a5-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:00:11.190: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod client-containers-c09fd679-84a5-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 01:00:11.237: INFO: Waiting for pod client-containers-c09fd679-84a5-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:00:11.253: INFO: Pod client-containers-c09fd679-84a5-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:00:11.253: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-cz7vm" for this suite.
Jul 11 01:00:17.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:00:18.727: INFO: namespace: e2e-tests-containers-cz7vm, resource: bindings, ignored listing per whitelist
Jul 11 01:00:18.758: INFO: namespace e2e-tests-containers-cz7vm deletion completed in 7.476237659s

• [SLOW TEST:12.362 seconds]
[k8s.io] Docker Containers
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should use the image defaults if command and args are blank  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:00:18.758: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: starting the proxy server
Jul 11 01:00:19.484: INFO: Asynchronously running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:00:19.722: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dp7pd" for this suite.
Jul 11 01:00:25.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:00:27.212: INFO: namespace: e2e-tests-kubectl-dp7pd, resource: bindings, ignored listing per whitelist
Jul 11 01:00:27.227: INFO: namespace e2e-tests-kubectl-dp7pd deletion completed in 7.47538793s

• [SLOW TEST:8.469 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should support proxy with --port 0  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:00:27.228: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test env composition
Jul 11 01:00:28.022: INFO: Waiting up to 5m0s for pod "var-expansion-cd12374f-84a5-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-var-expansion-67gsj" to be "success or failure"
Jul 11 01:00:28.037: INFO: Pod "var-expansion-cd12374f-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.10953ms
Jul 11 01:00:30.053: INFO: Pod "var-expansion-cd12374f-84a5-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031025634s
Jul 11 01:00:32.069: INFO: Pod "var-expansion-cd12374f-84a5-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047416089s
STEP: Saw pod success
Jul 11 01:00:32.069: INFO: Pod "var-expansion-cd12374f-84a5-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:00:32.085: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod var-expansion-cd12374f-84a5-11e8-9117-0e046f2b5c78 container dapi-container: <nil>
STEP: delete the pod
Jul 11 01:00:32.136: INFO: Waiting for pod var-expansion-cd12374f-84a5-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:00:32.151: INFO: Pod var-expansion-cd12374f-84a5-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:00:32.151: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-67gsj" for this suite.
Jul 11 01:00:38.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:00:39.037: INFO: namespace: e2e-tests-var-expansion-67gsj, resource: bindings, ignored listing per whitelist
Jul 11 01:00:39.682: INFO: namespace e2e-tests-var-expansion-67gsj deletion completed in 7.500937391s

• [SLOW TEST:12.454 seconds]
[k8s.io] Variable Expansion
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should allow composing env vars into new env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:00:39.682: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 11 01:00:46.527: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8pt56 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 01:00:46.527: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 01:00:46.741: INFO: Exec stderr: ""
Jul 11 01:00:46.741: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8pt56 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 01:00:46.741: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 01:00:46.935: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 11 01:00:46.936: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8pt56 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 01:00:46.936: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 01:00:47.120: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 11 01:00:47.120: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8pt56 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 01:00:47.120: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 01:00:47.333: INFO: Exec stderr: ""
Jul 11 01:00:47.333: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8pt56 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 01:00:47.333: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 01:00:47.572: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:00:47.572: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-8pt56" for this suite.
Jul 11 01:01:39.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:01:40.492: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-8pt56, resource: bindings, ignored listing per whitelist
Jul 11 01:01:41.076: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-8pt56 deletion completed in 53.474625073s

• [SLOW TEST:61.394 seconds]
[k8s.io] KubeletManagedEtcHosts
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should test kubelet managed /etc/hosts file  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:01:41.076: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should update labels on modification  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating the pod
Jul 11 01:01:46.431: INFO: Successfully updated pod "labelsupdatef910ed81-84a5-11e8-9117-0e046f2b5c78"
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:01:48.475: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zg82z" for this suite.
Jul 11 01:02:10.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:02:11.636: INFO: namespace: e2e-tests-downward-api-zg82z, resource: bindings, ignored listing per whitelist
Jul 11 01:02:11.992: INFO: namespace e2e-tests-downward-api-zg82z deletion completed in 23.487883273s

• [SLOW TEST:30.916 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should update labels on modification  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:02:11.992: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0711 01:02:18.822418    9515 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 11 01:02:18.822: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:02:18.822: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-rddhd" for this suite.
Jul 11 01:02:24.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:02:26.214: INFO: namespace: e2e-tests-gc-rddhd, resource: bindings, ignored listing per whitelist
Jul 11 01:02:26.366: INFO: namespace e2e-tests-gc-rddhd deletion completed in 7.522527017s

• [SLOW TEST:14.374 seconds]
[sig-api-machinery] Garbage collector
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:02:26.366: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 01:02:27.070: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:02:27.223: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-tx2n4" for this suite.
Jul 11 01:02:33.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:02:34.578: INFO: namespace: e2e-tests-custom-resource-definition-tx2n4, resource: bindings, ignored listing per whitelist
Jul 11 01:02:34.779: INFO: namespace e2e-tests-custom-resource-definition-tx2n4 deletion completed in 7.526499755s

• [SLOW TEST:8.412 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:02:34.779: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating projection with secret that has name projected-secret-test-1910752d-84a6-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 01:02:35.553: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1915170a-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-mxbd6" to be "success or failure"
Jul 11 01:02:35.572: INFO: Pod "pod-projected-secrets-1915170a-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 18.700446ms
Jul 11 01:02:37.589: INFO: Pod "pod-projected-secrets-1915170a-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035658364s
Jul 11 01:02:39.605: INFO: Pod "pod-projected-secrets-1915170a-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052313425s
STEP: Saw pod success
Jul 11 01:02:39.605: INFO: Pod "pod-projected-secrets-1915170a-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:02:39.621: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-projected-secrets-1915170a-84a6-11e8-9117-0e046f2b5c78 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 11 01:02:39.670: INFO: Waiting for pod pod-projected-secrets-1915170a-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:02:39.686: INFO: Pod pod-projected-secrets-1915170a-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:02:39.686: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mxbd6" for this suite.
Jul 11 01:02:45.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:02:46.537: INFO: namespace: e2e-tests-projected-mxbd6, resource: bindings, ignored listing per whitelist
Jul 11 01:02:47.264: INFO: namespace e2e-tests-projected-mxbd6 deletion completed in 7.534307761s

• [SLOW TEST:12.485 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:02:47.264: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:53
[It] should serve multiport endpoints from pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-cwbc7
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-cwbc7 to expose endpoints map[]
Jul 11 01:02:47.975: INFO: Get endpoints failed (17.567817ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul 11 01:02:48.990: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-cwbc7 exposes endpoints map[] (1.033275507s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-cwbc7
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-cwbc7 to expose endpoints map[pod1:[100]]
Jul 11 01:02:52.155: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-cwbc7 exposes endpoints map[pod1:[100]] (3.127287747s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-cwbc7
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-cwbc7 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 11 01:02:55.378: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-cwbc7 exposes endpoints map[pod1:[100] pod2:[101]] (3.188646997s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-cwbc7
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-cwbc7 to expose endpoints map[pod2:[101]]
Jul 11 01:02:55.436: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-cwbc7 exposes endpoints map[pod2:[101]] (39.291444ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-cwbc7
STEP: waiting up to 1m0s for service multi-endpoint-test in namespace e2e-tests-services-cwbc7 to expose endpoints map[]
Jul 11 01:02:55.475: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-cwbc7 exposes endpoints map[] (18.439642ms elapsed)
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:02:55.508: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-cwbc7" for this suite.
Jul 11 01:03:17.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:03:18.828: INFO: namespace: e2e-tests-services-cwbc7, resource: bindings, ignored listing per whitelist
Jul 11 01:03:19.061: INFO: namespace e2e-tests-services-cwbc7 deletion completed in 23.509726794s
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:58

• [SLOW TEST:31.797 seconds]
[sig-network] Services
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should get a host IP  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:03:19.061: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should get a host IP  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating pod
Jul 11 01:03:23.884: INFO: Pod pod-hostip-3378a1ac-84a6-11e8-9117-0e046f2b5c78 has hostIP: 10.142.0.5
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:03:23.884: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-l92sd" for this suite.
Jul 11 01:03:45.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:03:47.454: INFO: namespace: e2e-tests-pods-l92sd, resource: bindings, ignored listing per whitelist
Jul 11 01:03:47.502: INFO: namespace e2e-tests-pods-l92sd deletion completed in 23.588310658s

• [SLOW TEST:28.442 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should get a host IP  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:03:47.502: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward api env vars
Jul 11 01:03:48.247: INFO: Waiting up to 5m0s for pod "downward-api-4467b83d-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-72kb8" to be "success or failure"
Jul 11 01:03:48.263: INFO: Pod "downward-api-4467b83d-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.067756ms
Jul 11 01:03:50.280: INFO: Pod "downward-api-4467b83d-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032334372s
Jul 11 01:03:52.295: INFO: Pod "downward-api-4467b83d-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048060199s
Jul 11 01:03:54.311: INFO: Pod "downward-api-4467b83d-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063808613s
STEP: Saw pod success
Jul 11 01:03:54.311: INFO: Pod "downward-api-4467b83d-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:03:54.329: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod downward-api-4467b83d-84a6-11e8-9117-0e046f2b5c78 container dapi-container: <nil>
STEP: delete the pod
Jul 11 01:03:54.374: INFO: Waiting for pod downward-api-4467b83d-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:03:54.390: INFO: Pod downward-api-4467b83d-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:03:54.390: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-72kb8" for this suite.
Jul 11 01:04:00.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:04:01.465: INFO: namespace: e2e-tests-downward-api-72kb8, resource: bindings, ignored listing per whitelist
Jul 11 01:04:01.906: INFO: namespace e2e-tests-downward-api-72kb8 deletion completed in 7.485666645s

• [SLOW TEST:14.403 seconds]
[sig-api-machinery] Downward API
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide pod UID as env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:04:01.906: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl label
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
STEP: creating the pod
Jul 11 01:04:02.623: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-86dkk'
Jul 11 01:04:03.512: INFO: stderr: ""
Jul 11 01:04:03.512: INFO: stdout: "pod \"pause\" created\n"
Jul 11 01:04:03.512: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 11 01:04:03.512: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-86dkk" to be "running and ready"
Jul 11 01:04:03.527: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.397586ms
Jul 11 01:04:05.544: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032282788s
Jul 11 01:04:07.561: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.048893392s
Jul 11 01:04:07.561: INFO: Pod "pause" satisfied condition "running and ready"
Jul 11 01:04:07.561: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 11 01:04:07.561: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-86dkk'
Jul 11 01:04:07.736: INFO: stderr: ""
Jul 11 01:04:07.736: INFO: stdout: "pod \"pause\" labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 11 01:04:07.736: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-86dkk'
Jul 11 01:04:08.006: INFO: stderr: ""
Jul 11 01:04:08.006: INFO: stdout: "NAME      READY     STATUS    RESTARTS   AGE       TESTING-LABEL\npause     1/1       Running   0          5s        testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 11 01:04:08.006: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig label pods pause testing-label- --namespace=e2e-tests-kubectl-86dkk'
Jul 11 01:04:08.192: INFO: stderr: ""
Jul 11 01:04:08.192: INFO: stdout: "pod \"pause\" labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 11 01:04:08.192: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-86dkk'
Jul 11 01:04:08.460: INFO: stderr: ""
Jul 11 01:04:08.460: INFO: stdout: "NAME      READY     STATUS    RESTARTS   AGE       TESTING-LABEL\npause     1/1       Running   0          5s        \n"
[AfterEach] [k8s.io] Kubectl label
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:980
STEP: using delete to clean up resources
Jul 11 01:04:08.460: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-86dkk'
Jul 11 01:04:08.634: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 01:04:08.634: INFO: stdout: "pod \"pause\" deleted\n"
Jul 11 01:04:08.634: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-86dkk'
Jul 11 01:04:08.805: INFO: stderr: "No resources found.\n"
Jul 11 01:04:08.805: INFO: stdout: ""
Jul 11 01:04:08.805: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=pause --namespace=e2e-tests-kubectl-86dkk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 11 01:04:08.953: INFO: stderr: ""
Jul 11 01:04:08.953: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:04:08.953: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-86dkk" for this suite.
Jul 11 01:04:15.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:04:16.173: INFO: namespace: e2e-tests-kubectl-86dkk, resource: bindings, ignored listing per whitelist
Jul 11 01:04:16.506: INFO: namespace e2e-tests-kubectl-86dkk deletion completed in 7.522981679s

• [SLOW TEST:14.600 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should update the label on a resource  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should update labels on modification [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:04:16.507: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating the pod
Jul 11 01:04:21.844: INFO: Successfully updated pod "labelsupdate55b39841-84a6-11e8-9117-0e046f2b5c78"
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:04:23.890: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2fd4x" for this suite.
Jul 11 01:04:45.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:04:47.076: INFO: namespace: e2e-tests-projected-2fd4x, resource: bindings, ignored listing per whitelist
Jul 11 01:04:47.409: INFO: namespace e2e-tests-projected-2fd4x deletion completed in 23.48859692s

• [SLOW TEST:30.902 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should update labels on modification [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:04:47.409: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-znc49
[It] Should recreate evicted statefulset [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-znc49
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-znc49
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-znc49
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-znc49
Jul 11 01:04:54.310: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-znc49, name: ss-0, uid: 6b4ea7c8-84a6-11e8-94a2-42010a8e0002, status phase: Pending. Waiting for statefulset controller to delete.
Jul 11 01:04:55.055: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-znc49, name: ss-0, uid: 6b4ea7c8-84a6-11e8-94a2-42010a8e0002, status phase: Failed. Waiting for statefulset controller to delete.
Jul 11 01:04:55.061: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-znc49, name: ss-0, uid: 6b4ea7c8-84a6-11e8-94a2-42010a8e0002, status phase: Failed. Waiting for statefulset controller to delete.
Jul 11 01:04:55.064: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-znc49
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-znc49
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-znc49 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Jul 11 01:04:59.177: INFO: Deleting all statefulset in ns e2e-tests-statefulset-znc49
Jul 11 01:04:59.193: INFO: Scaling statefulset ss to 0
Jul 11 01:05:19.282: INFO: Waiting for statefulset status.replicas updated to 0
Jul 11 01:05:19.298: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:05:19.349: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-znc49" for this suite.
Jul 11 01:05:25.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:05:26.120: INFO: namespace: e2e-tests-statefulset-znc49, resource: bindings, ignored listing per whitelist
Jul 11 01:05:26.901: INFO: namespace e2e-tests-statefulset-znc49 deletion completed in 7.52303272s

• [SLOW TEST:39.492 seconds]
[sig-apps] StatefulSet
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    Should recreate evicted statefulset [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:05:26.901: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:199
[It] should be submitted and removed  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:05:27.620: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-q2s5c" for this suite.
Jul 11 01:05:49.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:05:50.803: INFO: namespace: e2e-tests-pods-q2s5c, resource: bindings, ignored listing per whitelist
Jul 11 01:05:51.125: INFO: namespace e2e-tests-pods-q2s5c deletion completed in 23.475797744s

• [SLOW TEST:24.223 seconds]
[k8s.io] [sig-node] Pods Extended
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  [k8s.io] Pods Set QOS Class
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should be submitted and removed  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] HostPath
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:05:51.125: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test hostPath mode
Jul 11 01:05:51.843: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-dkfkd" to be "success or failure"
Jul 11 01:05:51.859: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 16.3118ms
Jul 11 01:05:53.875: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032262907s
Jul 11 01:05:55.891: INFO: Pod "pod-host-path-test": Phase="Running", Reason="", readiness=false. Elapsed: 4.048252206s
Jul 11 01:05:57.907: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064398973s
STEP: Saw pod success
Jul 11 01:05:57.907: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 11 01:05:57.923: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 11 01:05:57.976: INFO: Waiting for pod pod-host-path-test to disappear
Jul 11 01:05:57.991: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:05:57.991: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-dkfkd" for this suite.
Jul 11 01:06:04.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:06:04.628: INFO: namespace: e2e-tests-hostpath-dkfkd, resource: bindings, ignored listing per whitelist
Jul 11 01:06:05.476: INFO: namespace e2e-tests-hostpath-dkfkd deletion completed in 7.456313328s

• [SLOW TEST:14.352 seconds]
[sig-storage] HostPath
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:06:05.477: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's memory limit  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 01:06:06.215: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96a8980d-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-dmlsx" to be "success or failure"
Jul 11 01:06:06.233: INFO: Pod "downwardapi-volume-96a8980d-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.779138ms
Jul 11 01:06:08.250: INFO: Pod "downwardapi-volume-96a8980d-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034586163s
Jul 11 01:06:10.266: INFO: Pod "downwardapi-volume-96a8980d-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051027215s
STEP: Saw pod success
Jul 11 01:06:10.266: INFO: Pod "downwardapi-volume-96a8980d-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:06:10.283: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod downwardapi-volume-96a8980d-84a6-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 01:06:10.330: INFO: Waiting for pod downwardapi-volume-96a8980d-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:06:10.346: INFO: Pod downwardapi-volume-96a8980d-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:06:10.346: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dmlsx" for this suite.
Jul 11 01:06:16.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:06:17.094: INFO: namespace: e2e-tests-downward-api-dmlsx, resource: bindings, ignored listing per whitelist
Jul 11 01:06:17.896: INFO: namespace e2e-tests-downward-api-dmlsx deletion completed in 7.509326976s

• [SLOW TEST:12.419 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide container's memory limit  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:06:17.896: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-5s8wn.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5s8wn.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-5s8wn.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-5s8wn.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-5s8wn.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-5s8wn.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 11 01:06:41.024: INFO: DNS probes using dns-test-9e109fe7-84a6-11e8-9117-0e046f2b5c78 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:06:41.048: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-5s8wn" for this suite.
Jul 11 01:06:47.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:06:47.873: INFO: namespace: e2e-tests-dns-5s8wn, resource: bindings, ignored listing per whitelist
Jul 11 01:06:48.547: INFO: namespace e2e-tests-dns-5s8wn deletion completed in 7.46838152s

• [SLOW TEST:30.650 seconds]
[sig-network] DNS
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:06:48.547: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:69
[It] should proxy logs on node using proxy subresource  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 01:06:49.327: INFO: (0) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 30.538856ms)
Jul 11 01:06:49.344: INFO: (1) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.972937ms)
Jul 11 01:06:49.362: INFO: (2) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 18.468681ms)
Jul 11 01:06:49.379: INFO: (3) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.954284ms)
Jul 11 01:06:49.396: INFO: (4) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.912847ms)
Jul 11 01:06:49.413: INFO: (5) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.102869ms)
Jul 11 01:06:49.431: INFO: (6) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.523352ms)
Jul 11 01:06:49.449: INFO: (7) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 18.049682ms)
Jul 11 01:06:49.466: INFO: (8) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.397528ms)
Jul 11 01:06:49.484: INFO: (9) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.601696ms)
Jul 11 01:06:49.501: INFO: (10) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.83701ms)
Jul 11 01:06:49.518: INFO: (11) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.772182ms)
Jul 11 01:06:49.535: INFO: (12) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.987982ms)
Jul 11 01:06:49.552: INFO: (13) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.984754ms)
Jul 11 01:06:49.569: INFO: (14) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.12203ms)
Jul 11 01:06:49.587: INFO: (15) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.767714ms)
Jul 11 01:06:49.605: INFO: (16) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.93806ms)
Jul 11 01:06:49.626: INFO: (17) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 21.496098ms)
Jul 11 01:06:49.645: INFO: (18) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 19.302466ms)
Jul 11 01:06:49.662: INFO: (19) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.092069ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:06:49.663: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-mdgnm" for this suite.
Jul 11 01:06:55.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:06:56.711: INFO: namespace: e2e-tests-proxy-mdgnm, resource: bindings, ignored listing per whitelist
Jul 11 01:06:57.186: INFO: namespace e2e-tests-proxy-mdgnm deletion completed in 7.494029399s

• [SLOW TEST:8.640 seconds]
[sig-network] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:61
    should proxy logs on node using proxy subresource  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:06:57.187: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating secret e2e-tests-secrets-xj2gs/secret-test-b57842c2-84a6-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 01:06:57.927: INFO: Waiting up to 5m0s for pod "pod-configmaps-b57adbe8-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-secrets-xj2gs" to be "success or failure"
Jul 11 01:06:57.942: INFO: Pod "pod-configmaps-b57adbe8-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 14.997359ms
Jul 11 01:06:59.958: INFO: Pod "pod-configmaps-b57adbe8-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030749823s
Jul 11 01:07:01.974: INFO: Pod "pod-configmaps-b57adbe8-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047111115s
STEP: Saw pod success
Jul 11 01:07:01.974: INFO: Pod "pod-configmaps-b57adbe8-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:07:01.990: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-configmaps-b57adbe8-84a6-11e8-9117-0e046f2b5c78 container env-test: <nil>
STEP: delete the pod
Jul 11 01:07:02.035: INFO: Waiting for pod pod-configmaps-b57adbe8-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:07:02.050: INFO: Pod pod-configmaps-b57adbe8-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:07:02.050: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xj2gs" for this suite.
Jul 11 01:07:08.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:07:09.287: INFO: namespace: e2e-tests-secrets-xj2gs, resource: bindings, ignored listing per whitelist
Jul 11 01:07:09.560: INFO: namespace e2e-tests-secrets-xj2gs deletion completed in 7.480412425s

• [SLOW TEST:12.373 seconds]
[sig-api-machinery] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:30
  should be consumable via the environment  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:07:09.560: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 Pods, got 2 Pods
STEP: Gathering metrics
W0711 01:07:10.972615    9515 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 11 01:07:10.972: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:07:10.972: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-d5zjw" for this suite.
Jul 11 01:07:17.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:07:18.431: INFO: namespace: e2e-tests-gc-d5zjw, resource: bindings, ignored listing per whitelist
Jul 11 01:07:18.508: INFO: namespace e2e-tests-gc-d5zjw deletion completed in 7.50641219s

• [SLOW TEST:8.948 seconds]
[sig-api-machinery] Garbage collector
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:07:18.508: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating secret with name secret-test-map-c2307b0f-84a6-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 01:07:19.264: INFO: Waiting up to 5m0s for pod "pod-secrets-c2331dbb-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-secrets-m5fq7" to be "success or failure"
Jul 11 01:07:19.280: INFO: Pod "pod-secrets-c2331dbb-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.924748ms
Jul 11 01:07:21.296: INFO: Pod "pod-secrets-c2331dbb-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032048761s
Jul 11 01:07:23.312: INFO: Pod "pod-secrets-c2331dbb-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048423456s
STEP: Saw pod success
Jul 11 01:07:23.312: INFO: Pod "pod-secrets-c2331dbb-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:07:23.328: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-secrets-c2331dbb-84a6-11e8-9117-0e046f2b5c78 container secret-volume-test: <nil>
STEP: delete the pod
Jul 11 01:07:23.374: INFO: Waiting for pod pod-secrets-c2331dbb-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:07:23.389: INFO: Pod pod-secrets-c2331dbb-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:07:23.389: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-m5fq7" for this suite.
Jul 11 01:07:29.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:07:30.347: INFO: namespace: e2e-tests-secrets-m5fq7, resource: bindings, ignored listing per whitelist
Jul 11 01:07:30.878: INFO: namespace e2e-tests-secrets-m5fq7 deletion completed in 7.459075104s

• [SLOW TEST:12.370 seconds]
[sig-storage] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:07:30.878: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 11 01:07:31.565: INFO: Waiting up to 5m0s for pod "pod-c9890544-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-bvzdh" to be "success or failure"
Jul 11 01:07:31.582: INFO: Pod "pod-c9890544-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.48349ms
Jul 11 01:07:33.598: INFO: Pod "pod-c9890544-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032513783s
Jul 11 01:07:35.614: INFO: Pod "pod-c9890544-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048521022s
STEP: Saw pod success
Jul 11 01:07:35.614: INFO: Pod "pod-c9890544-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:07:35.629: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-c9890544-84a6-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 01:07:35.681: INFO: Waiting for pod pod-c9890544-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:07:35.696: INFO: Pod pod-c9890544-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:07:35.696: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bvzdh" for this suite.
Jul 11 01:07:41.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:07:42.665: INFO: namespace: e2e-tests-emptydir-bvzdh, resource: bindings, ignored listing per whitelist
Jul 11 01:07:43.199: INFO: namespace e2e-tests-emptydir-bvzdh deletion completed in 7.471929219s

• [SLOW TEST:12.321 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:07:43.199: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name projected-configmap-test-volume-d0e7823f-84a6-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 01:07:43.948: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d0ea4469-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-sn8hk" to be "success or failure"
Jul 11 01:07:43.967: INFO: Pod "pod-projected-configmaps-d0ea4469-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 19.296027ms
Jul 11 01:07:45.984: INFO: Pod "pod-projected-configmaps-d0ea4469-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035661319s
Jul 11 01:07:48.000: INFO: Pod "pod-projected-configmaps-d0ea4469-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051538752s
STEP: Saw pod success
Jul 11 01:07:48.000: INFO: Pod "pod-projected-configmaps-d0ea4469-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:07:48.015: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-projected-configmaps-d0ea4469-84a6-11e8-9117-0e046f2b5c78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 01:07:48.062: INFO: Waiting for pod pod-projected-configmaps-d0ea4469-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:07:48.077: INFO: Pod pod-projected-configmaps-d0ea4469-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:07:48.077: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sn8hk" for this suite.
Jul 11 01:07:54.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:07:54.773: INFO: namespace: e2e-tests-projected-sn8hk, resource: bindings, ignored listing per whitelist
Jul 11 01:07:55.698: INFO: namespace e2e-tests-projected-sn8hk deletion completed in 7.591355086s

• [SLOW TEST:12.498 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:07:55.698: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating secret with name secret-test-d85c6a0a-84a6-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 01:07:56.473: INFO: Waiting up to 5m0s for pod "pod-secrets-d85f5da9-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-secrets-fpmzp" to be "success or failure"
Jul 11 01:07:56.488: INFO: Pod "pod-secrets-d85f5da9-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.61546ms
Jul 11 01:07:58.505: INFO: Pod "pod-secrets-d85f5da9-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031807691s
Jul 11 01:08:00.521: INFO: Pod "pod-secrets-d85f5da9-84a6-11e8-9117-0e046f2b5c78": Phase="Running", Reason="", readiness=true. Elapsed: 4.047970445s
Jul 11 01:08:02.537: INFO: Pod "pod-secrets-d85f5da9-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064115764s
STEP: Saw pod success
Jul 11 01:08:02.537: INFO: Pod "pod-secrets-d85f5da9-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:08:02.552: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-secrets-d85f5da9-84a6-11e8-9117-0e046f2b5c78 container secret-env-test: <nil>
STEP: delete the pod
Jul 11 01:08:02.601: INFO: Waiting for pod pod-secrets-d85f5da9-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:08:02.618: INFO: Pod pod-secrets-d85f5da9-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:08:02.618: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-fpmzp" for this suite.
Jul 11 01:08:08.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:08:10.034: INFO: namespace: e2e-tests-secrets-fpmzp, resource: bindings, ignored listing per whitelist
Jul 11 01:08:10.125: INFO: namespace e2e-tests-secrets-fpmzp deletion completed in 7.477804203s

• [SLOW TEST:14.427 seconds]
[sig-api-machinery] Secrets
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:30
  should be consumable from pods in env vars  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:08:10.125: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating projection with secret that has name projected-secret-test-e0ef4011-84a6-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume secrets
Jul 11 01:08:10.845: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e0f29d9f-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-s8997" to be "success or failure"
Jul 11 01:08:10.861: INFO: Pod "pod-projected-secrets-e0f29d9f-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.721653ms
Jul 11 01:08:12.877: INFO: Pod "pod-projected-secrets-e0f29d9f-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031349081s
Jul 11 01:08:14.893: INFO: Pod "pod-projected-secrets-e0f29d9f-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047509142s
STEP: Saw pod success
Jul 11 01:08:14.893: INFO: Pod "pod-projected-secrets-e0f29d9f-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:08:14.909: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-projected-secrets-e0f29d9f-84a6-11e8-9117-0e046f2b5c78 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 11 01:08:14.953: INFO: Waiting for pod pod-projected-secrets-e0f29d9f-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:08:14.968: INFO: Pod pod-projected-secrets-e0f29d9f-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:08:14.968: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s8997" for this suite.
Jul 11 01:08:21.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:08:22.256: INFO: namespace: e2e-tests-projected-s8997, resource: bindings, ignored listing per whitelist
Jul 11 01:08:22.483: INFO: namespace e2e-tests-projected-s8997 deletion completed in 7.485413027s

• [SLOW TEST:12.358 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with defaultMode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:08:22.483: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl replace
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
[It] should update a single-container pod's image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Jul 11 01:08:23.161: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-pod --generator=run-pod/v1 --image=k8s.gcr.io/nginx-slim-amd64:0.20 --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-pv82l'
Jul 11 01:08:23.341: INFO: stderr: ""
Jul 11 01:08:23.341: INFO: stdout: "pod \"e2e-test-nginx-pod\" created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 11 01:08:28.392: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-pv82l -o json'
Jul 11 01:08:28.543: INFO: stderr: ""
Jul 11 01:08:28.543: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2018-07-11T01:08:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-pv82l\",\n        \"resourceVersion\": \"22154\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-pv82l/pods/e2e-test-nginx-pod\",\n        \"uid\": \"e865b2c3-84a6-11e8-94a2-42010a8e0002\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/nginx-slim-amd64:0.20\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5pksw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-j762x\"\n            }\n        ],\n        \"nodeName\": \"prtest-7ef3e0b-4-ig-n-qg4c\",\n        \"nodeSelector\": {\n            \"node-role.kubernetes.io/compute\": \"true\"\n        },\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c37,c14\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5pksw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5pksw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-07-11T01:08:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-07-11T01:08:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-07-11T01:08:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://56b6ec358a52c57e74f70f738166aa2349506e815b763104fea8912a8d2a491c\",\n                \"image\": \"k8s.gcr.io/nginx-slim-amd64:0.20\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2018-07-11T01:08:26Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.142.0.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.4.72\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2018-07-11T01:08:23Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 11 01:08:28.543: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig replace -f - --namespace=e2e-tests-kubectl-pv82l'
Jul 11 01:08:28.823: INFO: stderr: ""
Jul 11 01:08:28.823: INFO: stdout: "pod \"e2e-test-nginx-pod\" replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image busybox
[AfterEach] [k8s.io] Kubectl replace
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
Jul 11 01:08:28.839: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-pv82l'
Jul 11 01:08:29.012: INFO: stderr: ""
Jul 11 01:08:29.012: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:08:29.012: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pv82l" for this suite.
Jul 11 01:08:51.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:08:51.978: INFO: namespace: e2e-tests-kubectl-pv82l, resource: bindings, ignored listing per whitelist
Jul 11 01:08:52.499: INFO: namespace e2e-tests-kubectl-pv82l deletion completed in 23.457530178s

• [SLOW TEST:30.016 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should update a single-container pod's image  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:08:52.499: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 01:08:53.300: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa379e8f-84a6-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-sl8vz" to be "success or failure"
Jul 11 01:08:53.316: INFO: Pod "downwardapi-volume-fa379e8f-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.026918ms
Jul 11 01:08:55.333: INFO: Pod "downwardapi-volume-fa379e8f-84a6-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032740994s
Jul 11 01:08:57.349: INFO: Pod "downwardapi-volume-fa379e8f-84a6-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049079339s
STEP: Saw pod success
Jul 11 01:08:57.349: INFO: Pod "downwardapi-volume-fa379e8f-84a6-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:08:57.365: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod downwardapi-volume-fa379e8f-84a6-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 01:08:57.412: INFO: Waiting for pod downwardapi-volume-fa379e8f-84a6-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:08:57.427: INFO: Pod downwardapi-volume-fa379e8f-84a6-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:08:57.427: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sl8vz" for this suite.
Jul 11 01:09:03.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:09:04.127: INFO: namespace: e2e-tests-projected-sl8vz, resource: bindings, ignored listing per whitelist
Jul 11 01:09:05.008: INFO: namespace e2e-tests-projected-sl8vz deletion completed in 7.55035381s

• [SLOW TEST:12.509 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:09:05.008: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should be updated  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 11 01:09:10.578: INFO: Successfully updated pod "pod-update-01ce2779-84a7-11e8-9117-0e046f2b5c78"
STEP: verifying the updated pod is in kubernetes
Jul 11 01:09:10.609: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:09:10.609: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-8dtlc" for this suite.
Jul 11 01:09:32.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:09:33.844: INFO: namespace: e2e-tests-pods-8dtlc, resource: bindings, ignored listing per whitelist
Jul 11 01:09:34.136: INFO: namespace e2e-tests-pods-8dtlc deletion completed in 23.497184661s

• [SLOW TEST:29.128 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should be updated  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:09:34.136: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 11 01:09:34.869: INFO: Waiting up to 5m0s for pod "pod-13029895-84a7-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-4fxv8" to be "success or failure"
Jul 11 01:09:34.893: INFO: Pod "pod-13029895-84a7-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 24.365131ms
Jul 11 01:09:36.911: INFO: Pod "pod-13029895-84a7-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042220435s
Jul 11 01:09:38.927: INFO: Pod "pod-13029895-84a7-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058267701s
STEP: Saw pod success
Jul 11 01:09:38.927: INFO: Pod "pod-13029895-84a7-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:09:38.943: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-13029895-84a7-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 01:09:38.990: INFO: Waiting for pod pod-13029895-84a7-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:09:39.006: INFO: Pod pod-13029895-84a7-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:09:39.006: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4fxv8" for this suite.
Jul 11 01:09:45.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:09:46.029: INFO: namespace: e2e-tests-emptydir-4fxv8, resource: bindings, ignored listing per whitelist
Jul 11 01:09:46.537: INFO: namespace e2e-tests-emptydir-4fxv8 deletion completed in 7.50178307s

• [SLOW TEST:12.401 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:09:46.537: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-nbj59
Jul 11 01:09:51.305: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-nbj59
STEP: checking the pod's current state and verifying that restartCount is present
Jul 11 01:09:51.320: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:11:52.824: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-nbj59" for this suite.
Jul 11 01:11:58.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:12:00.041: INFO: namespace: e2e-tests-container-probe-nbj59, resource: bindings, ignored listing per whitelist
Jul 11 01:12:00.308: INFO: namespace e2e-tests-container-probe-nbj59 deletion completed in 7.454823465s

• [SLOW TEST:133.771 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should *not* be restarted with a /healthz http liveness probe  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:12:00.308: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Jul 11 01:12:01.002: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 11 01:13:01.215: INFO: Waiting for terminating namespaces to be deleted...
Jul 11 01:13:01.246: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 11 01:13:01.292: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 11 01:13:01.292: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jul 11 01:13:01.308: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Jul 11 01:13:01.308: INFO: 
Logging pods the kubelet thinks is on node prtest-7ef3e0b-4-ig-n-4p7t before test
Jul 11 01:13:01.330: INFO: sync-n9gsj from openshift-node started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:13:01.330: INFO: 	Container sync ready: true, restart count 0
Jul 11 01:13:01.330: INFO: sdn-cwjj9 from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:13:01.330: INFO: 	Container sdn ready: true, restart count 0
Jul 11 01:13:01.330: INFO: ovs-2kx2m from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:13:01.330: INFO: 	Container openvswitch ready: true, restart count 0
Jul 11 01:13:01.330: INFO: 
Logging pods the kubelet thinks is on node prtest-7ef3e0b-4-ig-n-hp69 before test
Jul 11 01:13:01.352: INFO: ovs-ll87z from openshift-sdn started at 2018-07-10 23:59:33 +0000 UTC (1 container statuses recorded)
Jul 11 01:13:01.352: INFO: 	Container openvswitch ready: true, restart count 0
Jul 11 01:13:01.352: INFO: sdn-847kp from openshift-sdn started at 2018-07-10 23:59:33 +0000 UTC (1 container statuses recorded)
Jul 11 01:13:01.352: INFO: 	Container sdn ready: true, restart count 0
Jul 11 01:13:01.352: INFO: sync-rv446 from openshift-node started at 2018-07-10 23:59:33 +0000 UTC (1 container statuses recorded)
Jul 11 01:13:01.352: INFO: 	Container sync ready: true, restart count 0
Jul 11 01:13:01.352: INFO: 
Logging pods the kubelet thinks is on node prtest-7ef3e0b-4-ig-n-qg4c before test
Jul 11 01:13:01.373: INFO: ovs-r2zxw from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:13:01.373: INFO: 	Container openvswitch ready: true, restart count 0
Jul 11 01:13:01.373: INFO: sdn-5zv8h from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:13:01.373: INFO: 	Container sdn ready: true, restart count 0
Jul 11 01:13:01.373: INFO: sync-kzrv2 from openshift-node started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:13:01.373: INFO: 	Container sync ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: verifying the node has the label node prtest-7ef3e0b-4-ig-n-4p7t
STEP: verifying the node has the label node prtest-7ef3e0b-4-ig-n-hp69
STEP: verifying the node has the label node prtest-7ef3e0b-4-ig-n-qg4c
Jul 11 01:13:01.537: INFO: Pod sync-kzrv2 requesting resource cpu=0m on Node prtest-7ef3e0b-4-ig-n-qg4c
Jul 11 01:13:01.537: INFO: Pod sync-n9gsj requesting resource cpu=0m on Node prtest-7ef3e0b-4-ig-n-4p7t
Jul 11 01:13:01.537: INFO: Pod sync-rv446 requesting resource cpu=0m on Node prtest-7ef3e0b-4-ig-n-hp69
Jul 11 01:13:01.537: INFO: Pod ovs-2kx2m requesting resource cpu=100m on Node prtest-7ef3e0b-4-ig-n-4p7t
Jul 11 01:13:01.537: INFO: Pod ovs-ll87z requesting resource cpu=100m on Node prtest-7ef3e0b-4-ig-n-hp69
Jul 11 01:13:01.537: INFO: Pod ovs-r2zxw requesting resource cpu=100m on Node prtest-7ef3e0b-4-ig-n-qg4c
Jul 11 01:13:01.537: INFO: Pod sdn-5zv8h requesting resource cpu=100m on Node prtest-7ef3e0b-4-ig-n-qg4c
Jul 11 01:13:01.537: INFO: Pod sdn-847kp requesting resource cpu=100m on Node prtest-7ef3e0b-4-ig-n-hp69
Jul 11 01:13:01.537: INFO: Pod sdn-cwjj9 requesting resource cpu=100m on Node prtest-7ef3e0b-4-ig-n-4p7t
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e3a00de-84a7-11e8-9117-0e046f2b5c78.15402b63f2603c26], Reason = [Scheduled], Message = [Successfully assigned filler-pod-8e3a00de-84a7-11e8-9117-0e046f2b5c78 to prtest-7ef3e0b-4-ig-n-4p7t]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e3a00de-84a7-11e8-9117-0e046f2b5c78.15402b646b067d9d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause-amd64:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e3a00de-84a7-11e8-9117-0e046f2b5c78.15402b646cdb9092], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e3a00de-84a7-11e8-9117-0e046f2b5c78.15402b6472cc7f10], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e3fcad0-84a7-11e8-9117-0e046f2b5c78.15402b63f4594a4a], Reason = [Scheduled], Message = [Successfully assigned filler-pod-8e3fcad0-84a7-11e8-9117-0e046f2b5c78 to prtest-7ef3e0b-4-ig-n-hp69]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e3fcad0-84a7-11e8-9117-0e046f2b5c78.15402b6453ee6235], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause-amd64:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e3fcad0-84a7-11e8-9117-0e046f2b5c78.15402b645639bc76], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e3fcad0-84a7-11e8-9117-0e046f2b5c78.15402b645d5a09c2], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e4519ea-84a7-11e8-9117-0e046f2b5c78.15402b63f6c38870], Reason = [Scheduled], Message = [Successfully assigned filler-pod-8e4519ea-84a7-11e8-9117-0e046f2b5c78 to prtest-7ef3e0b-4-ig-n-qg4c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e4519ea-84a7-11e8-9117-0e046f2b5c78.15402b646fef2d89], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause-amd64:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e4519ea-84a7-11e8-9117-0e046f2b5c78.15402b6471c77f32], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8e4519ea-84a7-11e8-9117-0e046f2b5c78.15402b6477aec55b], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15402b64efe88130], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 Insufficient cpu.]
STEP: removing the label node off the node prtest-7ef3e0b-4-ig-n-4p7t
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node prtest-7ef3e0b-4-ig-n-hp69
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node prtest-7ef3e0b-4-ig-n-qg4c
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:13:06.995: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-k56fw" for this suite.
Jul 11 01:13:29.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:13:30.414: INFO: namespace: e2e-tests-sched-pred-k56fw, resource: bindings, ignored listing per whitelist
Jul 11 01:13:30.473: INFO: namespace e2e-tests-sched-pred-k56fw deletion completed in 23.447331249s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:90.166 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:13:30.473: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should allow activeDeadlineSeconds to be updated  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 11 01:13:35.784: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9fe19939-84a7-11e8-9117-0e046f2b5c78"
Jul 11 01:13:35.784: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9fe19939-84a7-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-pods-g8v69" to be "terminated due to deadline exceeded"
Jul 11 01:13:35.800: INFO: Pod "pod-update-activedeadlineseconds-9fe19939-84a7-11e8-9117-0e046f2b5c78": Phase="Running", Reason="", readiness=true. Elapsed: 15.606327ms
Jul 11 01:13:37.816: INFO: Pod "pod-update-activedeadlineseconds-9fe19939-84a7-11e8-9117-0e046f2b5c78": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.031510954s
Jul 11 01:13:37.816: INFO: Pod "pod-update-activedeadlineseconds-9fe19939-84a7-11e8-9117-0e046f2b5c78" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:13:37.816: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-g8v69" for this suite.
Jul 11 01:13:43.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:13:45.080: INFO: namespace: e2e-tests-pods-g8v69, resource: bindings, ignored listing per whitelist
Jul 11 01:13:45.303: INFO: namespace e2e-tests-pods-g8v69 deletion completed in 7.458466559s

• [SLOW TEST:14.830 seconds]
[k8s.io] Pods
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should allow activeDeadlineSeconds to be updated  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:13:45.304: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:14:46.067: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-kcg2f" for this suite.
Jul 11 01:15:08.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:15:09.554: INFO: namespace: e2e-tests-container-probe-kcg2f, resource: bindings, ignored listing per whitelist
Jul 11 01:15:09.554: INFO: namespace e2e-tests-container-probe-kcg2f deletion completed in 23.458281512s

• [SLOW TEST:84.250 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  with readiness probe that fails should never be ready and never restart  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:15:09.554: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name configmap-test-volume-daf10dca-84a7-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 01:15:10.282: INFO: Waiting up to 5m0s for pod "pod-configmaps-daf37778-84a7-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-configmap-djqcs" to be "success or failure"
Jul 11 01:15:10.298: INFO: Pod "pod-configmaps-daf37778-84a7-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.879323ms
Jul 11 01:15:12.314: INFO: Pod "pod-configmaps-daf37778-84a7-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031640311s
Jul 11 01:15:14.330: INFO: Pod "pod-configmaps-daf37778-84a7-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047939251s
STEP: Saw pod success
Jul 11 01:15:14.330: INFO: Pod "pod-configmaps-daf37778-84a7-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:15:14.345: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-configmaps-daf37778-84a7-11e8-9117-0e046f2b5c78 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 01:15:14.388: INFO: Waiting for pod pod-configmaps-daf37778-84a7-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:15:14.403: INFO: Pod pod-configmaps-daf37778-84a7-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:15:14.403: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-djqcs" for this suite.
Jul 11 01:15:20.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:15:21.762: INFO: namespace: e2e-tests-configmap-djqcs, resource: bindings, ignored listing per whitelist
Jul 11 01:15:21.898: INFO: namespace e2e-tests-configmap-djqcs deletion completed in 7.466357629s

• [SLOW TEST:12.344 seconds]
[sig-storage] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:15:21.898: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: validating cluster-info
Jul 11 01:15:22.566: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig cluster-info'
Jul 11 01:15:23.333: INFO: stderr: ""
Jul 11 01:15:23.333: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://internal-api.prtest-7ef3e0b-4.origin-ci-int-gce.dev.rhcloud.com:8443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:15:23.333: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-78qvk" for this suite.
Jul 11 01:15:29.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:15:30.697: INFO: namespace: e2e-tests-kubectl-78qvk, resource: bindings, ignored listing per whitelist
Jul 11 01:15:30.836: INFO: namespace e2e-tests-kubectl-78qvk deletion completed in 7.472304141s

• [SLOW TEST:8.937 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-auth] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:15:30.836: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Jul 11 01:15:32.098: INFO: Waiting up to 5m0s for pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-ztdhg" in namespace "e2e-tests-svcaccounts-wbldc" to be "success or failure"
Jul 11 01:15:32.118: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-ztdhg": Phase="Pending", Reason="", readiness=false. Elapsed: 19.179562ms
Jul 11 01:15:34.133: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-ztdhg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035005453s
Jul 11 01:15:36.149: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-ztdhg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050975393s
STEP: Saw pod success
Jul 11 01:15:36.149: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-ztdhg" satisfied condition "success or failure"
Jul 11 01:15:36.165: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-ztdhg container token-test: <nil>
STEP: delete the pod
Jul 11 01:15:36.208: INFO: Waiting for pod pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-ztdhg to disappear
Jul 11 01:15:36.223: INFO: Pod pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-ztdhg no longer exists
STEP: Creating a pod to test consume service account root CA
Jul 11 01:15:36.242: INFO: Waiting up to 5m0s for pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-56hh8" in namespace "e2e-tests-svcaccounts-wbldc" to be "success or failure"
Jul 11 01:15:36.257: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-56hh8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.930501ms
Jul 11 01:15:38.273: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-56hh8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030531945s
Jul 11 01:15:40.289: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-56hh8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046427625s
STEP: Saw pod success
Jul 11 01:15:40.289: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-56hh8" satisfied condition "success or failure"
Jul 11 01:15:40.304: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-56hh8 container root-ca-test: <nil>
STEP: delete the pod
Jul 11 01:15:40.348: INFO: Waiting for pod pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-56hh8 to disappear
Jul 11 01:15:40.364: INFO: Pod pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-56hh8 no longer exists
STEP: Creating a pod to test consume service account namespace
Jul 11 01:15:40.386: INFO: Waiting up to 5m0s for pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-cv47r" in namespace "e2e-tests-svcaccounts-wbldc" to be "success or failure"
Jul 11 01:15:40.405: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-cv47r": Phase="Pending", Reason="", readiness=false. Elapsed: 18.531668ms
Jul 11 01:15:42.420: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-cv47r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03440645s
Jul 11 01:15:44.436: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-cv47r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050333427s
STEP: Saw pod success
Jul 11 01:15:44.436: INFO: Pod "pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-cv47r" satisfied condition "success or failure"
Jul 11 01:15:44.451: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-cv47r container namespace-test: <nil>
STEP: delete the pod
Jul 11 01:15:44.548: INFO: Waiting for pod pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-cv47r to disappear
Jul 11 01:15:44.563: INFO: Pod pod-service-account-e7f4d4cc-84a7-11e8-9117-0e046f2b5c78-cv47r no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:15:44.563: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-wbldc" for this suite.
Jul 11 01:15:50.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:15:51.196: INFO: namespace: e2e-tests-svcaccounts-wbldc, resource: bindings, ignored listing per whitelist
Jul 11 01:15:52.043: INFO: namespace e2e-tests-svcaccounts-wbldc deletion completed in 7.451291705s

• [SLOW TEST:21.207 seconds]
[sig-auth] ServiceAccounts
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:15:52.043: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 11 01:15:52.735: INFO: Waiting up to 5m0s for pod "pod-f440d13b-84a7-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-ngr47" to be "success or failure"
Jul 11 01:15:52.752: INFO: Pod "pod-f440d13b-84a7-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.162277ms
Jul 11 01:15:54.768: INFO: Pod "pod-f440d13b-84a7-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033096137s
Jul 11 01:15:56.784: INFO: Pod "pod-f440d13b-84a7-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049189076s
STEP: Saw pod success
Jul 11 01:15:56.784: INFO: Pod "pod-f440d13b-84a7-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:15:56.800: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-f440d13b-84a7-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 01:15:56.849: INFO: Waiting for pod pod-f440d13b-84a7-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:15:56.865: INFO: Pod pod-f440d13b-84a7-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:15:56.865: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ngr47" for this suite.
Jul 11 01:16:02.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:16:03.707: INFO: namespace: e2e-tests-emptydir-ngr47, resource: bindings, ignored listing per whitelist
Jul 11 01:16:04.344: INFO: namespace e2e-tests-emptydir-ngr47 deletion completed in 7.449975075s

• [SLOW TEST:12.301 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:16:04.345: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Starting the proxy
Jul 11 01:16:05.040: INFO: Asynchronously running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix601337620/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:16:05.116: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pn84q" for this suite.
Jul 11 01:16:11.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:16:11.839: INFO: namespace: e2e-tests-kubectl-pn84q, resource: bindings, ignored listing per whitelist
Jul 11 01:16:12.595: INFO: namespace e2e-tests-kubectl-pn84q deletion completed in 7.450512695s

• [SLOW TEST:8.251 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should support --unix-socket=/path  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:16:12.595: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Jul 11 01:16:13.281: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 11 01:17:13.459: INFO: Waiting for terminating namespaces to be deleted...
Jul 11 01:17:13.490: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 11 01:17:13.536: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 11 01:17:13.536: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jul 11 01:17:13.552: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Jul 11 01:17:13.552: INFO: 
Logging pods the kubelet thinks is on node prtest-7ef3e0b-4-ig-n-4p7t before test
Jul 11 01:17:13.571: INFO: ovs-2kx2m from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:17:13.571: INFO: 	Container openvswitch ready: true, restart count 0
Jul 11 01:17:13.571: INFO: sync-n9gsj from openshift-node started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:17:13.571: INFO: 	Container sync ready: true, restart count 0
Jul 11 01:17:13.571: INFO: sdn-cwjj9 from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:17:13.571: INFO: 	Container sdn ready: true, restart count 0
Jul 11 01:17:13.571: INFO: 
Logging pods the kubelet thinks is on node prtest-7ef3e0b-4-ig-n-hp69 before test
Jul 11 01:17:13.591: INFO: sdn-847kp from openshift-sdn started at 2018-07-10 23:59:33 +0000 UTC (1 container statuses recorded)
Jul 11 01:17:13.591: INFO: 	Container sdn ready: true, restart count 0
Jul 11 01:17:13.591: INFO: sync-rv446 from openshift-node started at 2018-07-10 23:59:33 +0000 UTC (1 container statuses recorded)
Jul 11 01:17:13.591: INFO: 	Container sync ready: true, restart count 0
Jul 11 01:17:13.591: INFO: ovs-ll87z from openshift-sdn started at 2018-07-10 23:59:33 +0000 UTC (1 container statuses recorded)
Jul 11 01:17:13.591: INFO: 	Container openvswitch ready: true, restart count 0
Jul 11 01:17:13.591: INFO: 
Logging pods the kubelet thinks is on node prtest-7ef3e0b-4-ig-n-qg4c before test
Jul 11 01:17:13.611: INFO: ovs-r2zxw from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:17:13.611: INFO: 	Container openvswitch ready: true, restart count 0
Jul 11 01:17:13.611: INFO: sdn-5zv8h from openshift-sdn started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:17:13.611: INFO: 	Container sdn ready: true, restart count 0
Jul 11 01:17:13.611: INFO: sync-kzrv2 from openshift-node started at 2018-07-10 23:59:32 +0000 UTC (1 container statuses recorded)
Jul 11 01:17:13.611: INFO: 	Container sync ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-26ebdcbd-84a8-11e8-9117-0e046f2b5c78 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-26ebdcbd-84a8-11e8-9117-0e046f2b5c78 off the node prtest-7ef3e0b-4-ig-n-qg4c
STEP: verifying the node doesn't have the label kubernetes.io/e2e-26ebdcbd-84a8-11e8-9117-0e046f2b5c78
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:17:21.876: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-hw8xj" for this suite.
Jul 11 01:17:43.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:17:44.738: INFO: namespace: e2e-tests-sched-pred-hw8xj, resource: bindings, ignored listing per whitelist
Jul 11 01:17:45.362: INFO: namespace e2e-tests-sched-pred-hw8xj deletion completed in 23.457128275s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:92.767 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:17:45.362: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:264
[It] should scale a replication controller  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating a replication controller
Jul 11 01:17:46.056: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:46.412: INFO: stderr: ""
Jul 11 01:17:46.412: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 11 01:17:46.412: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:46.563: INFO: stderr: ""
Jul 11 01:17:46.563: INFO: stdout: "update-demo-nautilus-vpzpm update-demo-nautilus-vr4p2 "
Jul 11 01:17:46.563: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vpzpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:46.714: INFO: stderr: ""
Jul 11 01:17:46.714: INFO: stdout: ""
Jul 11 01:17:46.714: INFO: update-demo-nautilus-vpzpm is created but not running
Jul 11 01:17:51.714: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:51.869: INFO: stderr: ""
Jul 11 01:17:51.869: INFO: stdout: "update-demo-nautilus-vpzpm update-demo-nautilus-vr4p2 "
Jul 11 01:17:51.869: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vpzpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:52.024: INFO: stderr: ""
Jul 11 01:17:52.024: INFO: stdout: "true"
Jul 11 01:17:52.024: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vpzpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:52.175: INFO: stderr: ""
Jul 11 01:17:52.175: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jul 11 01:17:52.175: INFO: validating pod update-demo-nautilus-vpzpm
Jul 11 01:17:52.195: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 11 01:17:52.196: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 11 01:17:52.196: INFO: update-demo-nautilus-vpzpm is verified up and running
Jul 11 01:17:52.196: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vr4p2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:52.343: INFO: stderr: ""
Jul 11 01:17:52.343: INFO: stdout: "true"
Jul 11 01:17:52.344: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vr4p2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:52.495: INFO: stderr: ""
Jul 11 01:17:52.495: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jul 11 01:17:52.495: INFO: validating pod update-demo-nautilus-vr4p2
Jul 11 01:17:52.516: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 11 01:17:52.516: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 11 01:17:52.516: INFO: update-demo-nautilus-vr4p2 is verified up and running
STEP: scaling down the replication controller
Jul 11 01:17:52.516: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:52.732: INFO: stderr: ""
Jul 11 01:17:52.732: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 11 01:17:52.732: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:52.883: INFO: stderr: ""
Jul 11 01:17:52.883: INFO: stdout: "update-demo-nautilus-vpzpm update-demo-nautilus-vr4p2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 11 01:17:57.883: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:58.035: INFO: stderr: ""
Jul 11 01:17:58.035: INFO: stdout: "update-demo-nautilus-vpzpm "
Jul 11 01:17:58.035: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vpzpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:58.183: INFO: stderr: ""
Jul 11 01:17:58.183: INFO: stdout: "true"
Jul 11 01:17:58.183: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vpzpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:58.331: INFO: stderr: ""
Jul 11 01:17:58.331: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jul 11 01:17:58.331: INFO: validating pod update-demo-nautilus-vpzpm
Jul 11 01:17:58.347: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 11 01:17:58.347: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 11 01:17:58.347: INFO: update-demo-nautilus-vpzpm is verified up and running
STEP: scaling up the replication controller
Jul 11 01:17:58.347: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:59.582: INFO: stderr: ""
Jul 11 01:17:59.582: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 11 01:17:59.582: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:59.734: INFO: stderr: ""
Jul 11 01:17:59.734: INFO: stdout: "update-demo-nautilus-d2zp2 update-demo-nautilus-vpzpm "
Jul 11 01:17:59.735: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-d2zp2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:17:59.882: INFO: stderr: ""
Jul 11 01:17:59.882: INFO: stdout: ""
Jul 11 01:17:59.882: INFO: update-demo-nautilus-d2zp2 is created but not running
Jul 11 01:18:04.883: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:18:05.035: INFO: stderr: ""
Jul 11 01:18:05.035: INFO: stdout: "update-demo-nautilus-d2zp2 update-demo-nautilus-vpzpm "
Jul 11 01:18:05.035: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-d2zp2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:18:05.185: INFO: stderr: ""
Jul 11 01:18:05.185: INFO: stdout: "true"
Jul 11 01:18:05.185: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-d2zp2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:18:05.332: INFO: stderr: ""
Jul 11 01:18:05.332: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jul 11 01:18:05.332: INFO: validating pod update-demo-nautilus-d2zp2
Jul 11 01:18:05.352: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 11 01:18:05.352: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 11 01:18:05.352: INFO: update-demo-nautilus-d2zp2 is verified up and running
Jul 11 01:18:05.352: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vpzpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:18:05.501: INFO: stderr: ""
Jul 11 01:18:05.501: INFO: stdout: "true"
Jul 11 01:18:05.501: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods update-demo-nautilus-vpzpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:18:05.651: INFO: stderr: ""
Jul 11 01:18:05.651: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Jul 11 01:18:05.651: INFO: validating pod update-demo-nautilus-vpzpm
Jul 11 01:18:05.678: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 11 01:18:05.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 11 01:18:05.678: INFO: update-demo-nautilus-vpzpm is verified up and running
STEP: using delete to clean up resources
Jul 11 01:18:05.678: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:18:05.942: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 11 01:18:05.942: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" deleted\n"
Jul 11 01:18:05.942: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-ql65d'
Jul 11 01:18:06.111: INFO: stderr: "No resources found.\n"
Jul 11 01:18:06.111: INFO: stdout: ""
Jul 11 01:18:06.111: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-ql65d -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 11 01:18:06.263: INFO: stderr: ""
Jul 11 01:18:06.263: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:18:06.263: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ql65d" for this suite.
Jul 11 01:18:28.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:18:29.415: INFO: namespace: e2e-tests-kubectl-ql65d, resource: bindings, ignored listing per whitelist
Jul 11 01:18:29.756: INFO: namespace e2e-tests-kubectl-ql65d deletion completed in 23.463905726s

• [SLOW TEST:44.394 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should scale a replication controller  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:18:29.756: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name projected-configmap-test-volume-map-5247d45d-84a8-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 01:18:30.503: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-524a314c-84a8-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-cqwsd" to be "success or failure"
Jul 11 01:18:30.518: INFO: Pod "pod-projected-configmaps-524a314c-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.128325ms
Jul 11 01:18:32.534: INFO: Pod "pod-projected-configmaps-524a314c-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030939133s
Jul 11 01:18:34.550: INFO: Pod "pod-projected-configmaps-524a314c-84a8-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047165536s
STEP: Saw pod success
Jul 11 01:18:34.550: INFO: Pod "pod-projected-configmaps-524a314c-84a8-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:18:34.566: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-projected-configmaps-524a314c-84a8-11e8-9117-0e046f2b5c78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 01:18:34.612: INFO: Waiting for pod pod-projected-configmaps-524a314c-84a8-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:18:34.627: INFO: Pod pod-projected-configmaps-524a314c-84a8-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:18:34.627: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cqwsd" for this suite.
Jul 11 01:18:40.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:18:42.151: INFO: namespace: e2e-tests-projected-cqwsd, resource: bindings, ignored listing per whitelist
Jul 11 01:18:42.167: INFO: namespace e2e-tests-projected-cqwsd deletion completed in 7.510472783s

• [SLOW TEST:12.411 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings and Item mode set [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:18:42.167: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl rolling-update
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1202
[It] should support rolling-update to same image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Jul 11 01:18:42.905: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig run e2e-test-nginx-rc --image=k8s.gcr.io/nginx-slim-amd64:0.20 --generator=run/v1 --namespace=e2e-tests-kubectl-x6j9h'
Jul 11 01:18:43.064: INFO: stderr: ""
Jul 11 01:18:43.064: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Jul 11 01:18:43.096: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig rolling-update e2e-test-nginx-rc --update-period=1s --image=k8s.gcr.io/nginx-slim-amd64:0.20 --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-x6j9h'
Jul 11 01:18:55.104: INFO: stderr: ""
Jul 11 01:18:55.104: INFO: stdout: "Created e2e-test-nginx-rc-9b4419593c23e95e9ad9d447dbca4d4e\nScaling up e2e-test-nginx-rc-9b4419593c23e95e9ad9d447dbca4d4e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-9b4419593c23e95e9ad9d447dbca4d4e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-9b4419593c23e95e9ad9d447dbca4d4e to e2e-test-nginx-rc\nreplicationcontroller \"e2e-test-nginx-rc\" rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 11 01:18:55.105: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-x6j9h'
Jul 11 01:18:55.293: INFO: stderr: ""
Jul 11 01:18:55.293: INFO: stdout: "e2e-test-nginx-rc-9b4419593c23e95e9ad9d447dbca4d4e-2p97p "
Jul 11 01:18:55.293: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods e2e-test-nginx-rc-9b4419593c23e95e9ad9d447dbca4d4e-2p97p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-x6j9h'
Jul 11 01:18:55.466: INFO: stderr: ""
Jul 11 01:18:55.466: INFO: stdout: "true"
Jul 11 01:18:55.466: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig get pods e2e-test-nginx-rc-9b4419593c23e95e9ad9d447dbca4d4e-2p97p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-x6j9h'
Jul 11 01:18:55.617: INFO: stderr: ""
Jul 11 01:18:55.617: INFO: stdout: "k8s.gcr.io/nginx-slim-amd64:0.20"
Jul 11 01:18:55.617: INFO: e2e-test-nginx-rc-9b4419593c23e95e9ad9d447dbca4d4e-2p97p is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1208
Jul 11 01:18:55.617: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-x6j9h'
Jul 11 01:18:55.901: INFO: stderr: ""
Jul 11 01:18:55.901: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:18:55.901: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x6j9h" for this suite.
Jul 11 01:19:17.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:19:19.308: INFO: namespace: e2e-tests-kubectl-x6j9h, resource: bindings, ignored listing per whitelist
Jul 11 01:19:19.401: INFO: namespace e2e-tests-kubectl-x6j9h deletion completed in 23.457686888s

• [SLOW TEST:37.234 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should support rolling-update to same image  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:19:19.401: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 11 01:19:20.120: INFO: Waiting up to 5m0s for pod "pod-6fdd868a-84a8-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-xr9dm" to be "success or failure"
Jul 11 01:19:20.138: INFO: Pod "pod-6fdd868a-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.606998ms
Jul 11 01:19:22.155: INFO: Pod "pod-6fdd868a-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035064884s
Jul 11 01:19:24.171: INFO: Pod "pod-6fdd868a-84a8-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050961087s
STEP: Saw pod success
Jul 11 01:19:24.171: INFO: Pod "pod-6fdd868a-84a8-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:19:24.186: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-6fdd868a-84a8-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 01:19:24.229: INFO: Waiting for pod pod-6fdd868a-84a8-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:19:24.245: INFO: Pod pod-6fdd868a-84a8-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:19:24.245: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xr9dm" for this suite.
Jul 11 01:19:30.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:19:31.632: INFO: namespace: e2e-tests-emptydir-xr9dm, resource: bindings, ignored listing per whitelist
Jul 11 01:19:31.724: INFO: namespace e2e-tests-emptydir-xr9dm deletion completed in 7.450512799s

• [SLOW TEST:12.323 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:19:31.724: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 11 01:19:32.433: INFO: Waiting up to 5m0s for pod "pod-7734bee7-84a8-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-emptydir-dhrqc" to be "success or failure"
Jul 11 01:19:32.450: INFO: Pod "pod-7734bee7-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.830533ms
Jul 11 01:19:34.466: INFO: Pod "pod-7734bee7-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032779168s
Jul 11 01:19:36.481: INFO: Pod "pod-7734bee7-84a8-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048336751s
STEP: Saw pod success
Jul 11 01:19:36.481: INFO: Pod "pod-7734bee7-84a8-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:19:36.497: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod pod-7734bee7-84a8-11e8-9117-0e046f2b5c78 container test-container: <nil>
STEP: delete the pod
Jul 11 01:19:36.548: INFO: Waiting for pod pod-7734bee7-84a8-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:19:36.563: INFO: Pod pod-7734bee7-84a8-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:19:36.563: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dhrqc" for this suite.
Jul 11 01:19:42.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:19:43.834: INFO: namespace: e2e-tests-emptydir-dhrqc, resource: bindings, ignored listing per whitelist
Jul 11 01:19:44.063: INFO: namespace e2e-tests-emptydir-dhrqc deletion completed in 7.471132537s

• [SLOW TEST:12.339 seconds]
[sig-storage] EmptyDir volumes
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:19:44.063: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide podname only  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 01:19:44.824: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e955102-84a8-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-2svqs" to be "success or failure"
Jul 11 01:19:44.844: INFO: Pod "downwardapi-volume-7e955102-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 20.628888ms
Jul 11 01:19:46.861: INFO: Pod "downwardapi-volume-7e955102-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037228221s
Jul 11 01:19:48.877: INFO: Pod "downwardapi-volume-7e955102-84a8-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053345056s
STEP: Saw pod success
Jul 11 01:19:48.877: INFO: Pod "downwardapi-volume-7e955102-84a8-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:19:48.893: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod downwardapi-volume-7e955102-84a8-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 01:19:48.939: INFO: Waiting for pod downwardapi-volume-7e955102-84a8-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:19:48.955: INFO: Pod downwardapi-volume-7e955102-84a8-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:19:48.955: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2svqs" for this suite.
Jul 11 01:19:55.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:19:56.243: INFO: namespace: e2e-tests-downward-api-2svqs, resource: bindings, ignored listing per whitelist
Jul 11 01:19:56.517: INFO: namespace e2e-tests-downward-api-2svqs deletion completed in 7.519026211s

• [SLOW TEST:12.453 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide podname only  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should provide podname only [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:19:56.517: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 01:19:57.232: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85fcc24a-84a8-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-projected-pvkp6" to be "success or failure"
Jul 11 01:19:57.247: INFO: Pod "downwardapi-volume-85fcc24a-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 14.88002ms
Jul 11 01:19:59.263: INFO: Pod "downwardapi-volume-85fcc24a-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031324575s
Jul 11 01:20:01.279: INFO: Pod "downwardapi-volume-85fcc24a-84a8-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047397128s
STEP: Saw pod success
Jul 11 01:20:01.280: INFO: Pod "downwardapi-volume-85fcc24a-84a8-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:20:01.295: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod downwardapi-volume-85fcc24a-84a8-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 01:20:01.344: INFO: Waiting for pod downwardapi-volume-85fcc24a-84a8-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:20:01.359: INFO: Pod downwardapi-volume-85fcc24a-84a8-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Projected
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:20:01.359: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pvkp6" for this suite.
Jul 11 01:20:07.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:20:08.820: INFO: namespace: e2e-tests-projected-pvkp6, resource: bindings, ignored listing per whitelist
Jul 11 01:20:08.866: INFO: namespace e2e-tests-projected-pvkp6 deletion completed in 7.464904139s

• [SLOW TEST:12.350 seconds]
[sig-storage] Projected
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide podname only [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:20:08.867: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create a job from an image, then delete the job  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: executing a command with run --rm and attach with stdin
Jul 11 01:20:09.581: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig --namespace=e2e-tests-kubectl-bmwh6 run e2e-test-rm-busybox-job --image=busybox --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 11 01:20:13.825: INFO: stderr: "If you don't see a command prompt, try pressing enter.\n"
Jul 11 01:20:13.825: INFO: stdout: "abcd1234stdin closed\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:20:15.864: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bmwh6" for this suite.
Jul 11 01:20:21.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:20:22.606: INFO: namespace: e2e-tests-kubectl-bmwh6, resource: bindings, ignored listing per whitelist
Jul 11 01:20:23.347: INFO: namespace e2e-tests-kubectl-bmwh6 deletion completed in 7.452727906s

• [SLOW TEST:14.480 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should create a job from an image, then delete the job  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:20:23.347: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 01:20:24.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95fccab6-84a8-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-5crwk" to be "success or failure"
Jul 11 01:20:24.110: INFO: Pod "downwardapi-volume-95fccab6-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 25.660687ms
Jul 11 01:20:26.127: INFO: Pod "downwardapi-volume-95fccab6-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042245094s
Jul 11 01:20:28.143: INFO: Pod "downwardapi-volume-95fccab6-84a8-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058511732s
STEP: Saw pod success
Jul 11 01:20:28.143: INFO: Pod "downwardapi-volume-95fccab6-84a8-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:20:28.159: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-qg4c pod downwardapi-volume-95fccab6-84a8-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 01:20:28.205: INFO: Waiting for pod downwardapi-volume-95fccab6-84a8-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:20:28.220: INFO: Pod downwardapi-volume-95fccab6-84a8-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:20:28.220: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5crwk" for this suite.
Jul 11 01:20:34.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:20:35.536: INFO: namespace: e2e-tests-downward-api-5crwk, resource: bindings, ignored listing per whitelist
Jul 11 01:20:35.746: INFO: namespace e2e-tests-downward-api-5crwk deletion completed in 7.496612752s

• [SLOW TEST:12.399 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:20:35.746: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-l9bj4
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 11 01:20:36.489: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Jul 11 01:21:00.883: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.4.84 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-l9bj4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 01:21:00.883: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 01:21:02.094: INFO: Found all expected endpoints: [netserver-0]
Jul 11 01:21:02.109: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.6.97 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-l9bj4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 01:21:02.109: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 01:21:03.301: INFO: Found all expected endpoints: [netserver-1]
Jul 11 01:21:03.316: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.2.89 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-l9bj4 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 11 01:21:03.316: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
Jul 11 01:21:04.505: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:21:04.505: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-l9bj4" for this suite.
Jul 11 01:21:26.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:21:27.701: INFO: namespace: e2e-tests-pod-network-test-l9bj4, resource: bindings, ignored listing per whitelist
Jul 11 01:21:27.976: INFO: namespace e2e-tests-pod-network-test-l9bj4 deletion completed in 23.442283469s

• [SLOW TEST:52.230 seconds]
[sig-network] Networking
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] ReplicationController
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:21:27.976: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating replication controller my-hostname-basic-bc7fb036-84a8-11e8-9117-0e046f2b5c78
Jul 11 01:21:28.702: INFO: Pod name my-hostname-basic-bc7fb036-84a8-11e8-9117-0e046f2b5c78: Found 1 pods out of 1
Jul 11 01:21:28.702: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bc7fb036-84a8-11e8-9117-0e046f2b5c78" are running
Jul 11 01:21:32.733: INFO: Pod "my-hostname-basic-bc7fb036-84a8-11e8-9117-0e046f2b5c78-hw928" is running (conditions: [])
Jul 11 01:21:32.733: INFO: Trying to dial the pod
Jul 11 01:21:37.784: INFO: Controller my-hostname-basic-bc7fb036-84a8-11e8-9117-0e046f2b5c78: Got expected result from replica 1 [my-hostname-basic-bc7fb036-84a8-11e8-9117-0e046f2b5c78-hw928]: "my-hostname-basic-bc7fb036-84a8-11e8-9117-0e046f2b5c78-hw928", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:21:37.784: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-qm98h" for this suite.
Jul 11 01:21:43.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:21:45.232: INFO: namespace: e2e-tests-replication-controller-qm98h, resource: bindings, ignored listing per whitelist
Jul 11 01:21:45.263: INFO: namespace e2e-tests-replication-controller-qm98h deletion completed in 7.449411169s

• [SLOW TEST:17.287 seconds]
[sig-apps] ReplicationController
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:21:45.263: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:53
[It] should provide secure master service  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:21:45.999: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-gjn86" for this suite.
Jul 11 01:21:52.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:21:52.800: INFO: namespace: e2e-tests-services-gjn86, resource: bindings, ignored listing per whitelist
Jul 11 01:21:53.491: INFO: namespace e2e-tests-services-gjn86 deletion completed in 7.462717093s
[AfterEach] [sig-network] Services
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:58

• [SLOW TEST:8.228 seconds]
[sig-network] Services
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[k8s.io] Probing container 
  should be restarted with a docker exec liveness probe with timeout  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:21:53.491: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a docker exec liveness probe with timeout  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 01:21:54.190: INFO: The default exec handler, dockertools.NativeExecHandler, does not support timeouts due to a limitation in the Docker Remote API
[AfterEach] [k8s.io] Probing container
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:21:54.191: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wf6jd" for this suite.
Jul 11 01:22:00.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:22:01.223: INFO: namespace: e2e-tests-container-probe-wf6jd, resource: bindings, ignored listing per whitelist
Jul 11 01:22:01.660: INFO: namespace e2e-tests-container-probe-wf6jd deletion completed in 7.438661451s

S [SKIPPING] [8.169 seconds]
[k8s.io] Probing container
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should be restarted with a docker exec liveness probe with timeout  [Conformance] [It]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674

  Jul 11 01:21:54.190: The default exec handler, dockertools.NativeExecHandler, does not support timeouts due to a limitation in the Docker Remote API

  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:299
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:22:01.660: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 01:22:02.336: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig version --client'
Jul 11 01:22:02.409: INFO: stderr: ""
Jul 11 01:22:02.409: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"10+\", GitVersion:\"v1.10.6-beta.0.41+f7ef7a267727c2\", GitCommit:\"f7ef7a267727c2b32df51dab2f7d32773595e00e\", GitTreeState:\"clean\", BuildDate:\"2018-07-11T00:03:32Z\", GoVersion:\"go1.9.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jul 11 01:22:02.423: INFO: Not supported for server versions before "1.10.6-beta.0.41+f7ef7a267727c2"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:22:02.426: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r65j5" for this suite.
Jul 11 01:22:08.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:22:09.137: INFO: namespace: e2e-tests-kubectl-r65j5, resource: bindings, ignored listing per whitelist
Jul 11 01:22:09.913: INFO: namespace e2e-tests-kubectl-r65j5 deletion completed in 7.457738983s

S [SKIPPING] [8.253 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should check if kubectl describe prints relevant information for rc and pods  [Conformance] [It]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674

    Jul 11 01:22:02.423: Not supported for server versions before "1.10.6-beta.0.41+f7ef7a267727c2"

    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:299
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:22:09.913: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name configmap-test-volume-d57e3110-84a8-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 01:22:10.640: INFO: Waiting up to 5m0s for pod "pod-configmaps-d580cbb2-84a8-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-configmap-zxmnk" to be "success or failure"
Jul 11 01:22:10.657: INFO: Pod "pod-configmaps-d580cbb2-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.694021ms
Jul 11 01:22:12.673: INFO: Pod "pod-configmaps-d580cbb2-84a8-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032641717s
Jul 11 01:22:14.689: INFO: Pod "pod-configmaps-d580cbb2-84a8-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048375567s
STEP: Saw pod success
Jul 11 01:22:14.689: INFO: Pod "pod-configmaps-d580cbb2-84a8-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:22:14.704: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-configmaps-d580cbb2-84a8-11e8-9117-0e046f2b5c78 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 01:22:14.749: INFO: Waiting for pod pod-configmaps-d580cbb2-84a8-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:22:14.764: INFO: Pod pod-configmaps-d580cbb2-84a8-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:22:14.764: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zxmnk" for this suite.
Jul 11 01:22:20.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:22:21.686: INFO: namespace: e2e-tests-configmap-zxmnk, resource: bindings, ignored listing per whitelist
Jul 11 01:22:22.230: INFO: namespace e2e-tests-configmap-zxmnk deletion completed in 7.43692992s

• [SLOW TEST:12.317 seconds]
[sig-storage] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:22:22.230: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-qdn9n
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-qdn9n
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-qdn9n
Jul 11 01:22:22.997: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jul 11 01:22:33.014: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 11 01:22:33.030: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-qdn9n ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 01:22:33.420: INFO: stderr: ""
Jul 11 01:22:33.420: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 11 01:22:33.437: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 11 01:22:43.454: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 11 01:22:43.454: INFO: Waiting for statefulset status.replicas updated to 0
Jul 11 01:22:43.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999907s
Jul 11 01:22:44.536: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982847731s
Jul 11 01:22:45.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.966896768s
Jul 11 01:22:46.568: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.950603203s
Jul 11 01:22:47.584: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.934385802s
Jul 11 01:22:48.601: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.918527099s
Jul 11 01:22:49.617: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.901896924s
Jul 11 01:22:50.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.885584819s
Jul 11 01:22:51.654: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.869198767s
Jul 11 01:22:52.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 848.595969ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-qdn9n
Jul 11 01:22:53.707: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-qdn9n ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 11 01:22:54.059: INFO: stderr: ""
Jul 11 01:22:54.059: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 11 01:22:54.059: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-qdn9n ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 11 01:22:54.409: INFO: stderr: "mv: cannot stat '/tmp/index.html': No such file or directory\n"
Jul 11 01:22:54.409: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Jul 11 01:22:54.409: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-qdn9n ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 11 01:22:54.758: INFO: stderr: "mv: cannot stat '/tmp/index.html': No such file or directory\n"
Jul 11 01:22:54.758: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Jul 11 01:22:54.774: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 01:22:54.774: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 11 01:22:54.774: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 11 01:22:54.790: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-qdn9n ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 01:22:55.138: INFO: stderr: ""
Jul 11 01:22:55.138: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 11 01:22:55.138: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-qdn9n ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 01:22:55.479: INFO: stderr: ""
Jul 11 01:22:55.479: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 11 01:22:55.479: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig exec --namespace=e2e-tests-statefulset-qdn9n ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 11 01:22:55.837: INFO: stderr: ""
Jul 11 01:22:55.837: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 11 01:22:55.837: INFO: Waiting for statefulset status.replicas updated to 0
Jul 11 01:22:55.852: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 11 01:23:05.885: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 11 01:23:05.885: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 11 01:23:05.885: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 11 01:23:05.938: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 11 01:23:05.938: INFO: ss-0  prtest-7ef3e0b-4-ig-n-4p7t  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:22 +0000 UTC  }]
Jul 11 01:23:05.938: INFO: ss-1  prtest-7ef3e0b-4-ig-n-qg4c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:05.938: INFO: ss-2  prtest-7ef3e0b-4-ig-n-hp69  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:05.938: INFO: 
Jul 11 01:23:05.938: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 11 01:23:06.955: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 11 01:23:06.955: INFO: ss-0  prtest-7ef3e0b-4-ig-n-4p7t  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:22 +0000 UTC  }]
Jul 11 01:23:06.955: INFO: ss-1  prtest-7ef3e0b-4-ig-n-qg4c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:06.955: INFO: ss-2  prtest-7ef3e0b-4-ig-n-hp69  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:06.955: INFO: 
Jul 11 01:23:06.955: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 11 01:23:07.972: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 11 01:23:07.972: INFO: ss-1  prtest-7ef3e0b-4-ig-n-qg4c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:07.972: INFO: ss-2  prtest-7ef3e0b-4-ig-n-hp69  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:07.972: INFO: 
Jul 11 01:23:07.972: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 11 01:23:08.988: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 11 01:23:08.988: INFO: ss-1  prtest-7ef3e0b-4-ig-n-qg4c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:08.988: INFO: ss-2  prtest-7ef3e0b-4-ig-n-hp69  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:08.988: INFO: 
Jul 11 01:23:08.988: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 11 01:23:10.004: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 11 01:23:10.004: INFO: ss-1  prtest-7ef3e0b-4-ig-n-qg4c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:10.004: INFO: ss-2  prtest-7ef3e0b-4-ig-n-hp69  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:10.004: INFO: 
Jul 11 01:23:10.004: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 11 01:23:11.020: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 11 01:23:11.020: INFO: ss-1  prtest-7ef3e0b-4-ig-n-qg4c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:11.020: INFO: ss-2  prtest-7ef3e0b-4-ig-n-hp69  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:11.020: INFO: 
Jul 11 01:23:11.020: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 11 01:23:12.035: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 11 01:23:12.035: INFO: ss-1  prtest-7ef3e0b-4-ig-n-qg4c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:12.035: INFO: ss-2  prtest-7ef3e0b-4-ig-n-hp69  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-07-11 01:22:43 +0000 UTC  }]
Jul 11 01:23:12.035: INFO: 
Jul 11 01:23:12.035: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 11 01:23:13.051: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.881937259s
Jul 11 01:23:14.067: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.86611581s
Jul 11 01:23:15.083: INFO: Verifying statefulset ss doesn't scale past 0 for another 850.096008ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-qdn9n
Jul 11 01:23:16.101: INFO: Scaling statefulset ss to 0
Jul 11 01:23:16.149: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Jul 11 01:23:16.164: INFO: Deleting all statefulset in ns e2e-tests-statefulset-qdn9n
Jul 11 01:23:16.180: INFO: Scaling statefulset ss to 0
Jul 11 01:23:16.227: INFO: Waiting for statefulset status.replicas updated to 0
Jul 11 01:23:16.243: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:23:16.292: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-qdn9n" for this suite.
Jul 11 01:23:22.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:23:23.707: INFO: namespace: e2e-tests-statefulset-qdn9n, resource: bindings, ignored listing per whitelist
Jul 11 01:23:23.784: INFO: namespace e2e-tests-statefulset-qdn9n deletion completed in 7.462604569s

• [SLOW TEST:61.554 seconds]
[sig-apps] StatefulSet
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:23:23.784: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: validating api versions
Jul 11 01:23:24.497: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig api-versions'
Jul 11 01:23:24.654: INFO: stderr: ""
Jul 11 01:23:24.655: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nimage.openshift.io/v1\nnetwork.openshift.io/v1\nnetworking.k8s.io/v1\noauth.openshift.io/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:23:24.655: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4q5zm" for this suite.
Jul 11 01:23:30.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:23:31.338: INFO: namespace: e2e-tests-kubectl-4q5zm, resource: bindings, ignored listing per whitelist
Jul 11 01:23:32.153: INFO: namespace e2e-tests-kubectl-4q5zm deletion completed in 7.469371584s

• [SLOW TEST:8.369 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should check if v1 is in available api versions  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:23:32.154: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's cpu request  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 01:23:33.012: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06830bd4-84a9-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-qsnss" to be "success or failure"
Jul 11 01:23:33.038: INFO: Pod "downwardapi-volume-06830bd4-84a9-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 26.568679ms
Jul 11 01:23:35.055: INFO: Pod "downwardapi-volume-06830bd4-84a9-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04302586s
Jul 11 01:23:37.071: INFO: Pod "downwardapi-volume-06830bd4-84a9-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059213603s
STEP: Saw pod success
Jul 11 01:23:37.071: INFO: Pod "downwardapi-volume-06830bd4-84a9-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:23:37.087: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-4p7t pod downwardapi-volume-06830bd4-84a9-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 01:23:37.159: INFO: Waiting for pod downwardapi-volume-06830bd4-84a9-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:23:37.175: INFO: Pod downwardapi-volume-06830bd4-84a9-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:23:37.175: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qsnss" for this suite.
Jul 11 01:23:43.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:23:43.898: INFO: namespace: e2e-tests-downward-api-qsnss, resource: bindings, ignored listing per whitelist
Jul 11 01:23:44.715: INFO: namespace e2e-tests-downward-api-qsnss deletion completed in 7.510560522s

• [SLOW TEST:12.561 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide container's cpu request  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:23:44.715: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: creating Redis RC
Jul 11 01:23:45.405: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-x4xjb'
Jul 11 01:23:45.762: INFO: stderr: ""
Jul 11 01:23:45.762: INFO: stdout: "replicationcontroller \"redis-master\" created\n"
STEP: Waiting for Redis master to start.
Jul 11 01:23:46.780: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 01:23:46.780: INFO: Found 0 / 1
Jul 11 01:23:47.777: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 01:23:47.778: INFO: Found 0 / 1
Jul 11 01:23:48.777: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 01:23:48.777: INFO: Found 1 / 1
Jul 11 01:23:48.777: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 11 01:23:48.793: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 01:23:48.793: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 11 01:23:48.793: INFO: Running '/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/cluster-admin.kubeconfig patch pod redis-master-dgsrh --namespace=e2e-tests-kubectl-x4xjb -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 11 01:23:48.962: INFO: stderr: ""
Jul 11 01:23:48.962: INFO: stdout: "pod \"redis-master-dgsrh\" patched\n"
STEP: checking annotations
Jul 11 01:23:48.978: INFO: Selector matched 1 pods for map[app:redis]
Jul 11 01:23:48.978: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:23:48.978: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x4xjb" for this suite.
Jul 11 01:24:11.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:24:12.252: INFO: namespace: e2e-tests-kubectl-x4xjb, resource: bindings, ignored listing per whitelist
Jul 11 01:24:12.463: INFO: namespace e2e-tests-kubectl-x4xjb deletion completed in 23.454332779s

• [SLOW TEST:27.749 seconds]
[sig-cli] Kubectl client
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
    should add annotations for pods in rc  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:24:12.463: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:69
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 01:24:13.201: INFO: (0) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 28.028721ms)
Jul 11 01:24:13.217: INFO: (1) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.275686ms)
Jul 11 01:24:13.233: INFO: (2) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.225275ms)
Jul 11 01:24:13.249: INFO: (3) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.132697ms)
Jul 11 01:24:13.266: INFO: (4) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.678124ms)
Jul 11 01:24:13.283: INFO: (5) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.884857ms)
Jul 11 01:24:13.300: INFO: (6) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.709804ms)
Jul 11 01:24:13.317: INFO: (7) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.916211ms)
Jul 11 01:24:13.333: INFO: (8) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.791596ms)
Jul 11 01:24:13.350: INFO: (9) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.782476ms)
Jul 11 01:24:13.368: INFO: (10) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.381915ms)
Jul 11 01:24:13.385: INFO: (11) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.96217ms)
Jul 11 01:24:13.401: INFO: (12) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.845961ms)
Jul 11 01:24:13.418: INFO: (13) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.679716ms)
Jul 11 01:24:13.436: INFO: (14) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.36259ms)
Jul 11 01:24:13.453: INFO: (15) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.121657ms)
Jul 11 01:24:13.470: INFO: (16) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.121207ms)
Jul 11 01:24:13.487: INFO: (17) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.285381ms)
Jul 11 01:24:13.504: INFO: (18) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 16.756391ms)
Jul 11 01:24:13.521: INFO: (19) /api/v1/nodes/prtest-7ef3e0b-4-ig-n-4p7t:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 17.472352ms)
[AfterEach] version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:24:13.521: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-gbsv7" for this suite.
Jul 11 01:24:19.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:24:20.289: INFO: namespace: e2e-tests-proxy-gbsv7, resource: bindings, ignored listing per whitelist
Jul 11 01:24:21.018: INFO: namespace e2e-tests-proxy-gbsv7 deletion completed in 7.467826547s

• [SLOW TEST:8.555 seconds]
[sig-network] Proxy
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:61
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:24:21.019: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide node allocatable (memory) as default memory limit if the limit is not set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating a pod to test downward API volume plugin
Jul 11 01:24:21.723: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23a2c493-84a9-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-downward-api-flxhn" to be "success or failure"
Jul 11 01:24:21.739: INFO: Pod "downwardapi-volume-23a2c493-84a9-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 15.078198ms
Jul 11 01:24:23.755: INFO: Pod "downwardapi-volume-23a2c493-84a9-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031653205s
Jul 11 01:24:25.772: INFO: Pod "downwardapi-volume-23a2c493-84a9-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048296553s
STEP: Saw pod success
Jul 11 01:24:25.772: INFO: Pod "downwardapi-volume-23a2c493-84a9-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:24:25.788: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod downwardapi-volume-23a2c493-84a9-11e8-9117-0e046f2b5c78 container client-container: <nil>
STEP: delete the pod
Jul 11 01:24:25.841: INFO: Waiting for pod downwardapi-volume-23a2c493-84a9-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:24:25.856: INFO: Pod downwardapi-volume-23a2c493-84a9-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:24:25.856: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-flxhn" for this suite.
Jul 11 01:24:31.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:24:32.579: INFO: namespace: e2e-tests-downward-api-flxhn, resource: bindings, ignored listing per whitelist
Jul 11 01:24:33.338: INFO: namespace e2e-tests-downward-api-flxhn deletion completed in 7.45299828s

• [SLOW TEST:12.320 seconds]
[sig-storage] Downward API volume
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:24:33.339: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
Jul 11 01:24:34.128: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2b0622e4-84a9-11e8-94a2-42010a8e0002", Controller:(*bool)(0xc4211b0952), BlockOwnerDeletion:(*bool)(0xc4211b0953)}}
Jul 11 01:24:34.146: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2aff732d-84a9-11e8-94a2-42010a8e0002", Controller:(*bool)(0xc4221ecb76), BlockOwnerDeletion:(*bool)(0xc4221ecb77)}}
Jul 11 01:24:34.163: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2b02f69a-84a9-11e8-94a2-42010a8e0002", Controller:(*bool)(0xc4211b0b16), BlockOwnerDeletion:(*bool)(0xc4211b0b17)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:24:39.198: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-jp8mw" for this suite.
Jul 11 01:24:45.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:24:46.567: INFO: namespace: e2e-tests-gc-jp8mw, resource: bindings, ignored listing per whitelist
Jul 11 01:24:46.749: INFO: namespace e2e-tests-gc-jp8mw deletion completed in 7.520563175s

• [SLOW TEST:13.410 seconds]
[sig-api-machinery] Garbage collector
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [k8s.io] [sig-node] PreStop
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:24:46.749: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating server pod server in namespace e2e-tests-prestop-s4gw5
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-s4gw5
STEP: Deleting pre-stop pod
Jul 11 01:25:02.632: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:25:02.649: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-s4gw5" for this suite.
Jul 11 01:25:40.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:25:41.352: INFO: namespace: e2e-tests-prestop-s4gw5, resource: bindings, ignored listing per whitelist
Jul 11 01:25:42.123: INFO: namespace e2e-tests-prestop-s4gw5 deletion completed in 39.445891084s

• [SLOW TEST:55.375 seconds]
[k8s.io] [sig-node] PreStop
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:669
  should call prestop when killing a pod  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
[BeforeEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Jul 11 01:25:42.124: INFO: >>> kubeConfig: /tmp/cluster-admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
STEP: Creating configMap with name configmap-test-volume-map-53fa8f3b-84a9-11e8-9117-0e046f2b5c78
STEP: Creating a pod to test consume configMaps
Jul 11 01:25:42.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-53fd0140-84a9-11e8-9117-0e046f2b5c78" in namespace "e2e-tests-configmap-bbjql" to be "success or failure"
Jul 11 01:25:42.865: INFO: Pod "pod-configmaps-53fd0140-84a9-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 14.810449ms
Jul 11 01:25:44.881: INFO: Pod "pod-configmaps-53fd0140-84a9-11e8-9117-0e046f2b5c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030726483s
Jul 11 01:25:46.897: INFO: Pod "pod-configmaps-53fd0140-84a9-11e8-9117-0e046f2b5c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04640238s
STEP: Saw pod success
Jul 11 01:25:46.897: INFO: Pod "pod-configmaps-53fd0140-84a9-11e8-9117-0e046f2b5c78" satisfied condition "success or failure"
Jul 11 01:25:46.913: INFO: Trying to get logs from node prtest-7ef3e0b-4-ig-n-hp69 pod pod-configmaps-53fd0140-84a9-11e8-9117-0e046f2b5c78 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 11 01:25:47.016: INFO: Waiting for pod pod-configmaps-53fd0140-84a9-11e8-9117-0e046f2b5c78 to disappear
Jul 11 01:25:47.032: INFO: Pod pod-configmaps-53fd0140-84a9-11e8-9117-0e046f2b5c78 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Jul 11 01:25:47.032: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bbjql" for this suite.
Jul 11 01:25:53.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 11 01:25:53.934: INFO: namespace: e2e-tests-configmap-bbjql, resource: bindings, ignored listing per whitelist
Jul 11 01:25:54.508: INFO: namespace e2e-tests-configmap-bbjql deletion completed in 7.446768644s

• [SLOW TEST:12.384 seconds]
[sig-storage] ConfigMap
/data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings  [Conformance]
  /data/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:674
------------------------------
SSSSSSSSJul 11 01:25:54.508: INFO: Running AfterSuite actions on all node
Jul 11 01:25:54.508: INFO: Running AfterSuite actions on node 1
Jul 11 01:25:54.508: INFO: Dumping logs locally to: /data/src/github.com/openshift/origin/_output/scripts/conformance-k8s/artifacts
Jul 11 01:25:54.508: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 161 of 891 Specs in 4633.033 seconds
SUCCESS! -- 161 Passed | 0 Failed | 0 Pending | 730 Skipped PASS
