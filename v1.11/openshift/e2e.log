Nov 28 05:07:34.480: INFO: Overriding default scale value of zero to 1
Nov 28 05:07:34.480: INFO: Overriding default milliseconds value of zero to 5000
I1128 05:07:34.624726    6241 e2e.go:333] Starting e2e run "84268fbc-f2cb-11e8-904b-0a58ac10ae35" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1543381654 - Will randomize all specs
Will run 167 of 997 specs

I1128 05:07:34.658005    6241 e2e.go:59] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
Nov 28 05:07:34.658: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:07:34.660: INFO: Waiting up to 30m0s for all (but 1) nodes to be schedulable
Nov 28 05:07:34.680: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 28 05:07:34.704: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 28 05:07:34.704: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Nov 28 05:07:34.707: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Nov 28 05:07:34.708: INFO: Dumping network health container logs from all nodes to file /tmp/artifacts/nethealth.txt
Nov 28 05:07:34.711: INFO: e2e test version: v1.11.6-beta.0.1+5933b9771b71c2
Nov 28 05:07:34.712: INFO: kube-apiserver version: v1.11.0+d4cacc0
I1128 05:07:34.712776    6241 e2e.go:59] The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.
S
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:07:34.712: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
Nov 28 05:07:34.854: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:07:34.871: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84692f84-f2cb-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-xh2zq" to be "success or failure"
Nov 28 05:07:34.877: INFO: Pod "downwardapi-volume-84692f84-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 5.209674ms
Nov 28 05:07:36.881: INFO: Pod "downwardapi-volume-84692f84-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009463584s
Nov 28 05:07:38.885: INFO: Pod "downwardapi-volume-84692f84-f2cb-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013527267s
STEP: Saw pod success
Nov 28 05:07:38.885: INFO: Pod "downwardapi-volume-84692f84-f2cb-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:07:38.888: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod downwardapi-volume-84692f84-f2cb-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:07:38.910: INFO: Waiting for pod downwardapi-volume-84692f84-f2cb-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:07:38.912: INFO: Pod downwardapi-volume-84692f84-f2cb-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:07:38.912: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xh2zq" for this suite.
Nov 28 05:07:44.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:07:45.037: INFO: namespace: e2e-tests-projected-xh2zq, resource: bindings, ignored listing per whitelist
Nov 28 05:07:45.440: INFO: namespace e2e-tests-projected-xh2zq deletion completed in 6.524080932s

• [SLOW TEST:10.728 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:07:45.441: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating pod
Nov 28 05:07:49.607: INFO: Pod pod-hostip-8acc6dd4-f2cb-11e8-904b-0a58ac10ae35 has hostIP: 10.142.0.4
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:07:49.607: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-7svnk" for this suite.
Nov 28 05:08:11.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:08:12.089: INFO: namespace: e2e-tests-pods-7svnk, resource: bindings, ignored listing per whitelist
Nov 28 05:08:12.140: INFO: namespace e2e-tests-pods-7svnk deletion completed in 22.52868957s

• [SLOW TEST:26.700 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:08:12.141: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1128 05:08:18.310622    6241 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 28 05:08:18.310: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:08:18.310: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2bxfn" for this suite.
Nov 28 05:08:24.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:08:24.375: INFO: namespace: e2e-tests-gc-2bxfn, resource: bindings, ignored listing per whitelist
Nov 28 05:08:24.836: INFO: namespace e2e-tests-gc-2bxfn deletion completed in 6.52203909s

• [SLOW TEST:12.695 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:08:24.836: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:08:24.977: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a245ec14-f2cb-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-jt9ck" to be "success or failure"
Nov 28 05:08:24.980: INFO: Pod "downwardapi-volume-a245ec14-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.462076ms
Nov 28 05:08:26.984: INFO: Pod "downwardapi-volume-a245ec14-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006678309s
Nov 28 05:08:28.988: INFO: Pod "downwardapi-volume-a245ec14-f2cb-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010518704s
STEP: Saw pod success
Nov 28 05:08:28.988: INFO: Pod "downwardapi-volume-a245ec14-f2cb-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:08:28.990: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod downwardapi-volume-a245ec14-f2cb-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:08:29.009: INFO: Waiting for pod downwardapi-volume-a245ec14-f2cb-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:08:29.011: INFO: Pod downwardapi-volume-a245ec14-f2cb-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:08:29.011: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jt9ck" for this suite.
Nov 28 05:08:35.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:08:35.098: INFO: namespace: e2e-tests-downward-api-jt9ck, resource: bindings, ignored listing per whitelist
Nov 28 05:08:35.540: INFO: namespace e2e-tests-downward-api-jt9ck deletion completed in 6.525327211s

• [SLOW TEST:10.705 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:08:35.541: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:08:35.668: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8a66c99-f2cb-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-hblvw" to be "success or failure"
Nov 28 05:08:35.672: INFO: Pod "downwardapi-volume-a8a66c99-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.725115ms
Nov 28 05:08:37.676: INFO: Pod "downwardapi-volume-a8a66c99-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007883534s
Nov 28 05:08:39.681: INFO: Pod "downwardapi-volume-a8a66c99-f2cb-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012124554s
STEP: Saw pod success
Nov 28 05:08:39.681: INFO: Pod "downwardapi-volume-a8a66c99-f2cb-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:08:39.683: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod downwardapi-volume-a8a66c99-f2cb-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:08:39.735: INFO: Waiting for pod downwardapi-volume-a8a66c99-f2cb-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:08:39.742: INFO: Pod downwardapi-volume-a8a66c99-f2cb-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:08:39.742: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hblvw" for this suite.
Nov 28 05:08:45.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:08:45.922: INFO: namespace: e2e-tests-downward-api-hblvw, resource: bindings, ignored listing per whitelist
Nov 28 05:08:46.270: INFO: namespace e2e-tests-downward-api-hblvw deletion completed in 6.524209675s

• [SLOW TEST:10.729 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:08:46.270: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating replication controller my-hostname-basic-af0e6b9f-f2cb-11e8-904b-0a58ac10ae35
Nov 28 05:08:46.417: INFO: Pod name my-hostname-basic-af0e6b9f-f2cb-11e8-904b-0a58ac10ae35: Found 0 pods out of 1
Nov 28 05:08:51.422: INFO: Pod name my-hostname-basic-af0e6b9f-f2cb-11e8-904b-0a58ac10ae35: Found 1 pods out of 1
Nov 28 05:08:51.422: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-af0e6b9f-f2cb-11e8-904b-0a58ac10ae35" are running
Nov 28 05:08:51.425: INFO: Pod "my-hostname-basic-af0e6b9f-f2cb-11e8-904b-0a58ac10ae35-m587l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-11-28 05:08:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-11-28 05:08:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:0001-01-01 00:00:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-11-28 05:08:46 +0000 UTC Reason: Message:}])
Nov 28 05:08:51.425: INFO: Trying to dial the pod
Nov 28 05:08:56.440: INFO: Controller my-hostname-basic-af0e6b9f-f2cb-11e8-904b-0a58ac10ae35: Got expected result from replica 1 [my-hostname-basic-af0e6b9f-f2cb-11e8-904b-0a58ac10ae35-m587l]: "my-hostname-basic-af0e6b9f-f2cb-11e8-904b-0a58ac10ae35-m587l", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:08:56.440: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-xhpxd" for this suite.
Nov 28 05:09:02.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:09:02.550: INFO: namespace: e2e-tests-replication-controller-xhpxd, resource: bindings, ignored listing per whitelist
Nov 28 05:09:02.967: INFO: namespace e2e-tests-replication-controller-xhpxd deletion completed in 6.52274794s

• [SLOW TEST:16.697 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:09:02.967: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 28 05:09:03.127: INFO: Waiting up to 5m0s for pod "pod-b90428d3-f2cb-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-4jkpk" to be "success or failure"
Nov 28 05:09:03.130: INFO: Pod "pod-b90428d3-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13583ms
Nov 28 05:09:05.134: INFO: Pod "pod-b90428d3-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007132856s
Nov 28 05:09:07.137: INFO: Pod "pod-b90428d3-f2cb-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010349091s
STEP: Saw pod success
Nov 28 05:09:07.137: INFO: Pod "pod-b90428d3-f2cb-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:09:07.140: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-b90428d3-f2cb-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:09:07.170: INFO: Waiting for pod pod-b90428d3-f2cb-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:09:07.173: INFO: Pod pod-b90428d3-f2cb-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:09:07.173: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4jkpk" for this suite.
Nov 28 05:09:13.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:09:13.286: INFO: namespace: e2e-tests-emptydir-4jkpk, resource: bindings, ignored listing per whitelist
Nov 28 05:09:13.702: INFO: namespace e2e-tests-emptydir-4jkpk deletion completed in 6.525485746s

• [SLOW TEST:10.735 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:09:13.703: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:09:13.863: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf6a93e7-f2cb-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-66tq8" to be "success or failure"
Nov 28 05:09:13.868: INFO: Pod "downwardapi-volume-bf6a93e7-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.433603ms
Nov 28 05:09:15.872: INFO: Pod "downwardapi-volume-bf6a93e7-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008889443s
Nov 28 05:09:17.876: INFO: Pod "downwardapi-volume-bf6a93e7-f2cb-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012971662s
STEP: Saw pod success
Nov 28 05:09:17.876: INFO: Pod "downwardapi-volume-bf6a93e7-f2cb-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:09:17.880: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod downwardapi-volume-bf6a93e7-f2cb-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:09:17.901: INFO: Waiting for pod downwardapi-volume-bf6a93e7-f2cb-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:09:17.904: INFO: Pod downwardapi-volume-bf6a93e7-f2cb-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:09:17.904: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-66tq8" for this suite.
Nov 28 05:09:23.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:09:24.038: INFO: namespace: e2e-tests-downward-api-66tq8, resource: bindings, ignored listing per whitelist
Nov 28 05:09:24.434: INFO: namespace e2e-tests-downward-api-66tq8 deletion completed in 6.525965125s

• [SLOW TEST:10.731 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:09:24.434: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 28 05:09:30.614: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:30.614: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:30.745: INFO: Exec stderr: ""
Nov 28 05:09:30.745: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:30.745: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:30.856: INFO: Exec stderr: ""
Nov 28 05:09:30.856: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:30.856: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:30.960: INFO: Exec stderr: ""
Nov 28 05:09:30.960: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:30.960: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:31.053: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 28 05:09:31.053: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:31.053: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:31.146: INFO: Exec stderr: ""
Nov 28 05:09:31.146: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:31.146: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:31.254: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 28 05:09:31.254: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:31.254: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:31.367: INFO: Exec stderr: ""
Nov 28 05:09:31.367: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:31.367: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:31.464: INFO: Exec stderr: ""
Nov 28 05:09:31.464: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:31.464: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:31.573: INFO: Exec stderr: ""
Nov 28 05:09:31.573: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-zgdqf PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:09:31.573: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:09:31.668: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:09:31.668: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-zgdqf" for this suite.
Nov 28 05:10:23.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:10:24.099: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-zgdqf, resource: bindings, ignored listing per whitelist
Nov 28 05:10:24.200: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-zgdqf deletion completed in 52.527951729s

• [SLOW TEST:59.766 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:10:24.200: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating projection with secret that has name projected-secret-test-e96e861e-f2cb-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:10:24.363: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e96f5e9c-f2cb-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-lpctb" to be "success or failure"
Nov 28 05:10:24.370: INFO: Pod "pod-projected-secrets-e96f5e9c-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885891ms
Nov 28 05:10:26.374: INFO: Pod "pod-projected-secrets-e96f5e9c-f2cb-11e8-904b-0a58ac10ae35": Phase="Running", Reason="", readiness=true. Elapsed: 2.010903359s
Nov 28 05:10:28.379: INFO: Pod "pod-projected-secrets-e96f5e9c-f2cb-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015612445s
STEP: Saw pod success
Nov 28 05:10:28.379: INFO: Pod "pod-projected-secrets-e96f5e9c-f2cb-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:10:28.382: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-projected-secrets-e96f5e9c-f2cb-11e8-904b-0a58ac10ae35 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 28 05:10:28.405: INFO: Waiting for pod pod-projected-secrets-e96f5e9c-f2cb-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:10:28.408: INFO: Pod pod-projected-secrets-e96f5e9c-f2cb-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:10:28.408: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lpctb" for this suite.
Nov 28 05:10:34.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:10:34.937: INFO: namespace: e2e-tests-projected-lpctb, resource: bindings, ignored listing per whitelist
Nov 28 05:10:34.937: INFO: namespace e2e-tests-projected-lpctb deletion completed in 6.525871601s

• [SLOW TEST:10.737 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:10:34.938: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:10:35.099: INFO: Waiting up to 5m0s for pod "downwardapi-volume-efd477d6-f2cb-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-mzj92" to be "success or failure"
Nov 28 05:10:35.104: INFO: Pod "downwardapi-volume-efd477d6-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175031ms
Nov 28 05:10:37.107: INFO: Pod "downwardapi-volume-efd477d6-f2cb-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007988372s
Nov 28 05:10:39.111: INFO: Pod "downwardapi-volume-efd477d6-f2cb-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011340864s
STEP: Saw pod success
Nov 28 05:10:39.111: INFO: Pod "downwardapi-volume-efd477d6-f2cb-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:10:39.114: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod downwardapi-volume-efd477d6-f2cb-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:10:39.138: INFO: Waiting for pod downwardapi-volume-efd477d6-f2cb-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:10:39.141: INFO: Pod downwardapi-volume-efd477d6-f2cb-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:10:39.141: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mzj92" for this suite.
Nov 28 05:10:45.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:10:45.245: INFO: namespace: e2e-tests-downward-api-mzj92, resource: bindings, ignored listing per whitelist
Nov 28 05:10:45.669: INFO: namespace e2e-tests-downward-api-mzj92 deletion completed in 6.523960792s

• [SLOW TEST:10.732 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:10:45.670: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: starting the proxy server
Nov 28 05:10:45.837: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:10:45.939: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mn88w" for this suite.
Nov 28 05:10:51.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:10:51.994: INFO: namespace: e2e-tests-kubectl-mn88w, resource: bindings, ignored listing per whitelist
Nov 28 05:10:52.472: INFO: namespace e2e-tests-kubectl-mn88w deletion completed in 6.529548241s

• [SLOW TEST:6.803 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should support proxy with --port 0  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:10:52.472: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating the pod
Nov 28 05:10:57.176: INFO: Successfully updated pod "labelsupdatefa47ed8a-f2cb-11e8-904b-0a58ac10ae35"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:10:59.197: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nzp8n" for this suite.
Nov 28 05:11:21.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:11:21.676: INFO: namespace: e2e-tests-downward-api-nzp8n, resource: bindings, ignored listing per whitelist
Nov 28 05:11:21.725: INFO: namespace e2e-tests-downward-api-nzp8n deletion completed in 22.523831932s

• [SLOW TEST:29.253 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:11:21.726: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4gdpp
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 28 05:11:21.862: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Nov 28 05:11:49.962: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://172.16.2.14:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4gdpp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:11:49.962: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:11:50.083: INFO: Found all expected endpoints: [netserver-0]
Nov 28 05:11:50.087: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://172.16.4.11:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4gdpp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:11:50.087: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:11:50.182: INFO: Found all expected endpoints: [netserver-1]
Nov 28 05:11:50.185: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://172.16.6.15:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4gdpp PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:11:50.185: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:11:50.280: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:11:50.280: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4gdpp" for this suite.
Nov 28 05:12:12.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:12:12.368: INFO: namespace: e2e-tests-pod-network-test-4gdpp, resource: bindings, ignored listing per whitelist
Nov 28 05:12:12.811: INFO: namespace e2e-tests-pod-network-test-4gdpp deletion completed in 22.526456009s

• [SLOW TEST:51.085 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:12:12.811: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating projection with secret that has name projected-secret-test-2a2cb0f8-f2cc-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:12:12.998: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2a2d8db0-f2cc-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-vlw85" to be "success or failure"
Nov 28 05:12:13.000: INFO: Pod "pod-projected-secrets-2a2d8db0-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.873123ms
Nov 28 05:12:15.005: INFO: Pod "pod-projected-secrets-2a2d8db0-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007675977s
Nov 28 05:12:17.009: INFO: Pod "pod-projected-secrets-2a2d8db0-f2cc-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011473055s
STEP: Saw pod success
Nov 28 05:12:17.009: INFO: Pod "pod-projected-secrets-2a2d8db0-f2cc-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:12:17.011: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-projected-secrets-2a2d8db0-f2cc-11e8-904b-0a58ac10ae35 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 28 05:12:17.028: INFO: Waiting for pod pod-projected-secrets-2a2d8db0-f2cc-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:12:17.031: INFO: Pod pod-projected-secrets-2a2d8db0-f2cc-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:12:17.031: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vlw85" for this suite.
Nov 28 05:12:23.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:12:23.146: INFO: namespace: e2e-tests-projected-vlw85, resource: bindings, ignored listing per whitelist
Nov 28 05:12:23.557: INFO: namespace e2e-tests-projected-vlw85 deletion completed in 6.52341273s

• [SLOW TEST:10.747 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:12:23.558: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:12:23.677: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig version'
Nov 28 05:12:23.776: INFO: stderr: ""
Nov 28 05:12:23.776: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"11+\", GitVersion:\"v1.11.6-beta.0.1+5933b9771b71c2\", GitCommit:\"5933b9771b71c2d05543a0cd088542013c1446e0\", GitTreeState:\"clean\", BuildDate:\"2018-11-28T05:03:15Z\", GoVersion:\"go1.10.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"11+\", GitVersion:\"v1.11.0+d4cacc0\", GitCommit:\"d4cacc0\", GitTreeState:\"clean\", BuildDate:\"2018-11-28T01:07:54Z\", GoVersion:\"go1.10.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:12:23.776: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-whb6t" for this suite.
Nov 28 05:12:29.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:12:30.155: INFO: namespace: e2e-tests-kubectl-whb6t, resource: bindings, ignored listing per whitelist
Nov 28 05:12:30.306: INFO: namespace e2e-tests-kubectl-whb6t deletion completed in 6.525325687s

• [SLOW TEST:6.749 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should check is all data is printed  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:12:30.307: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward api env vars
Nov 28 05:12:31.455: INFO: Waiting up to 5m0s for pod "downward-api-34958a4b-f2cc-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-zrrcw" to be "success or failure"
Nov 28 05:12:31.458: INFO: Pod "downward-api-34958a4b-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292759ms
Nov 28 05:12:33.462: INFO: Pod "downward-api-34958a4b-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006988206s
Nov 28 05:12:35.466: INFO: Pod "downward-api-34958a4b-f2cc-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010913607s
STEP: Saw pod success
Nov 28 05:12:35.466: INFO: Pod "downward-api-34958a4b-f2cc-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:12:35.468: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod downward-api-34958a4b-f2cc-11e8-904b-0a58ac10ae35 container dapi-container: <nil>
STEP: delete the pod
Nov 28 05:12:35.501: INFO: Waiting for pod downward-api-34958a4b-f2cc-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:12:35.504: INFO: Pod downward-api-34958a4b-f2cc-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:12:35.504: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zrrcw" for this suite.
Nov 28 05:12:41.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:12:41.528: INFO: namespace: e2e-tests-downward-api-zrrcw, resource: bindings, ignored listing per whitelist
Nov 28 05:12:42.032: INFO: namespace e2e-tests-downward-api-zrrcw deletion completed in 6.524783164s

• [SLOW TEST:11.725 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:12:42.032: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-qvlkg
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 28 05:12:42.167: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Nov 28 05:13:06.321: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.2.15 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qvlkg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:13:06.321: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:13:07.432: INFO: Found all expected endpoints: [netserver-0]
Nov 28 05:13:07.435: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.4.15 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qvlkg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:13:07.435: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:13:08.526: INFO: Found all expected endpoints: [netserver-1]
Nov 28 05:13:08.529: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.16.6.17 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qvlkg PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:13:08.529: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:13:09.640: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:13:09.640: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-qvlkg" for this suite.
Nov 28 05:13:31.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:13:31.759: INFO: namespace: e2e-tests-pod-network-test-qvlkg, resource: bindings, ignored listing per whitelist
Nov 28 05:13:32.167: INFO: namespace e2e-tests-pod-network-test-qvlkg deletion completed in 22.522434414s

• [SLOW TEST:50.135 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:13:32.167: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 28 05:13:32.319: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6b6z9,SelfLink:/api/v1/namespaces/e2e-tests-watch-6b6z9/configmaps/e2e-watch-test-label-changed,UID:59772bed-f2cc-11e8-b66f-42010a8e0002,ResourceVersion:5862,Generation:0,CreationTimestamp:2018-11-28 05:13:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 28 05:13:32.319: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6b6z9,SelfLink:/api/v1/namespaces/e2e-tests-watch-6b6z9/configmaps/e2e-watch-test-label-changed,UID:59772bed-f2cc-11e8-b66f-42010a8e0002,ResourceVersion:5864,Generation:0,CreationTimestamp:2018-11-28 05:13:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 28 05:13:32.319: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6b6z9,SelfLink:/api/v1/namespaces/e2e-tests-watch-6b6z9/configmaps/e2e-watch-test-label-changed,UID:59772bed-f2cc-11e8-b66f-42010a8e0002,ResourceVersion:5867,Generation:0,CreationTimestamp:2018-11-28 05:13:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 28 05:13:42.349: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6b6z9,SelfLink:/api/v1/namespaces/e2e-tests-watch-6b6z9/configmaps/e2e-watch-test-label-changed,UID:59772bed-f2cc-11e8-b66f-42010a8e0002,ResourceVersion:5888,Generation:0,CreationTimestamp:2018-11-28 05:13:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 28 05:13:42.349: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6b6z9,SelfLink:/api/v1/namespaces/e2e-tests-watch-6b6z9/configmaps/e2e-watch-test-label-changed,UID:59772bed-f2cc-11e8-b66f-42010a8e0002,ResourceVersion:5889,Generation:0,CreationTimestamp:2018-11-28 05:13:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Nov 28 05:13:42.349: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6b6z9,SelfLink:/api/v1/namespaces/e2e-tests-watch-6b6z9/configmaps/e2e-watch-test-label-changed,UID:59772bed-f2cc-11e8-b66f-42010a8e0002,ResourceVersion:5890,Generation:0,CreationTimestamp:2018-11-28 05:13:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:13:42.349: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-6b6z9" for this suite.
Nov 28 05:13:48.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:13:48.414: INFO: namespace: e2e-tests-watch-6b6z9, resource: bindings, ignored listing per whitelist
Nov 28 05:13:48.879: INFO: namespace e2e-tests-watch-6b6z9 deletion completed in 6.525981493s

• [SLOW TEST:16.712 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:13:48.879: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 28 05:13:49.020: INFO: Waiting up to 5m0s for pod "pod-636bd1e8-f2cc-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-zdtfs" to be "success or failure"
Nov 28 05:13:49.024: INFO: Pod "pod-636bd1e8-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219514ms
Nov 28 05:13:51.028: INFO: Pod "pod-636bd1e8-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007936893s
Nov 28 05:13:53.032: INFO: Pod "pod-636bd1e8-f2cc-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011858485s
STEP: Saw pod success
Nov 28 05:13:53.032: INFO: Pod "pod-636bd1e8-f2cc-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:13:53.035: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-636bd1e8-f2cc-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:13:53.057: INFO: Waiting for pod pod-636bd1e8-f2cc-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:13:53.060: INFO: Pod pod-636bd1e8-f2cc-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:13:53.060: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-zdtfs" for this suite.
Nov 28 05:13:59.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:13:59.106: INFO: namespace: e2e-tests-emptydir-zdtfs, resource: bindings, ignored listing per whitelist
Nov 28 05:13:59.585: INFO: namespace e2e-tests-emptydir-zdtfs deletion completed in 6.521611541s

• [SLOW TEST:10.706 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:13:59.585: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1180
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Nov 28 05:13:59.734: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-deployment --image=k8s.gcr.io/nginx-slim-amd64:0.20 --namespace=e2e-tests-kubectl-d998z'
Nov 28 05:13:59.939: INFO: stderr: ""
Nov 28 05:13:59.939: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1186
Nov 28 05:14:01.947: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-d998z'
Nov 28 05:14:02.065: INFO: stderr: ""
Nov 28 05:14:02.065: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:14:02.065: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d998z" for this suite.
Nov 28 05:14:24.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:14:24.151: INFO: namespace: e2e-tests-kubectl-d998z, resource: bindings, ignored listing per whitelist
Nov 28 05:14:24.591: INFO: namespace e2e-tests-kubectl-d998z deletion completed in 22.522285686s

• [SLOW TEST:25.006 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should create an rc or deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:14:24.592: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating secret with name secret-test-78b5cd59-f2cc-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:14:24.741: INFO: Waiting up to 5m0s for pod "pod-secrets-78b6f5d0-f2cc-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-secrets-mklmj" to be "success or failure"
Nov 28 05:14:24.743: INFO: Pod "pod-secrets-78b6f5d0-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123255ms
Nov 28 05:14:26.746: INFO: Pod "pod-secrets-78b6f5d0-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005475508s
Nov 28 05:14:28.750: INFO: Pod "pod-secrets-78b6f5d0-f2cc-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009280338s
STEP: Saw pod success
Nov 28 05:14:28.750: INFO: Pod "pod-secrets-78b6f5d0-f2cc-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:14:28.753: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-secrets-78b6f5d0-f2cc-11e8-904b-0a58ac10ae35 container secret-volume-test: <nil>
STEP: delete the pod
Nov 28 05:14:28.771: INFO: Waiting for pod pod-secrets-78b6f5d0-f2cc-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:14:28.773: INFO: Pod pod-secrets-78b6f5d0-f2cc-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:14:28.773: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mklmj" for this suite.
Nov 28 05:14:34.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:14:34.891: INFO: namespace: e2e-tests-secrets-mklmj, resource: bindings, ignored listing per whitelist
Nov 28 05:14:35.304: INFO: namespace e2e-tests-secrets-mklmj deletion completed in 6.526360822s

• [SLOW TEST:10.712 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:14:35.304: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test env composition
Nov 28 05:14:35.458: INFO: Waiting up to 5m0s for pod "var-expansion-7f18d4c5-f2cc-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-var-expansion-fhfn7" to be "success or failure"
Nov 28 05:14:35.466: INFO: Pod "var-expansion-7f18d4c5-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 7.585098ms
Nov 28 05:14:37.469: INFO: Pod "var-expansion-7f18d4c5-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011318333s
Nov 28 05:14:39.473: INFO: Pod "var-expansion-7f18d4c5-f2cc-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014747724s
STEP: Saw pod success
Nov 28 05:14:39.473: INFO: Pod "var-expansion-7f18d4c5-f2cc-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:14:39.475: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod var-expansion-7f18d4c5-f2cc-11e8-904b-0a58ac10ae35 container dapi-container: <nil>
STEP: delete the pod
Nov 28 05:14:39.493: INFO: Waiting for pod var-expansion-7f18d4c5-f2cc-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:14:39.496: INFO: Pod var-expansion-7f18d4c5-f2cc-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:14:39.496: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-fhfn7" for this suite.
Nov 28 05:14:45.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:14:45.537: INFO: namespace: e2e-tests-var-expansion-fhfn7, resource: bindings, ignored listing per whitelist
Nov 28 05:14:46.022: INFO: namespace e2e-tests-var-expansion-fhfn7 deletion completed in 6.522510147s

• [SLOW TEST:10.718 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:14:46.022: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:264
[It] should create and stop a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating a replication controller
Nov 28 05:14:46.134: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:46.433: INFO: stderr: ""
Nov 28 05:14:46.433: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 28 05:14:46.433: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:46.549: INFO: stderr: ""
Nov 28 05:14:46.549: INFO: stdout: "update-demo-nautilus-62wrq update-demo-nautilus-7hccl "
Nov 28 05:14:46.549: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-62wrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:46.671: INFO: stderr: ""
Nov 28 05:14:46.671: INFO: stdout: ""
Nov 28 05:14:46.671: INFO: update-demo-nautilus-62wrq is created but not running
Nov 28 05:14:51.671: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:51.805: INFO: stderr: ""
Nov 28 05:14:51.805: INFO: stdout: "update-demo-nautilus-62wrq update-demo-nautilus-7hccl "
Nov 28 05:14:51.805: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-62wrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:51.914: INFO: stderr: ""
Nov 28 05:14:51.914: INFO: stdout: "true"
Nov 28 05:14:51.914: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-62wrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:52.025: INFO: stderr: ""
Nov 28 05:14:52.025: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:14:52.025: INFO: validating pod update-demo-nautilus-62wrq
Nov 28 05:14:52.034: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:14:52.034: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:14:52.034: INFO: update-demo-nautilus-62wrq is verified up and running
Nov 28 05:14:52.034: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-7hccl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:52.143: INFO: stderr: ""
Nov 28 05:14:52.143: INFO: stdout: "true"
Nov 28 05:14:52.143: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-7hccl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:52.256: INFO: stderr: ""
Nov 28 05:14:52.256: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:14:52.256: INFO: validating pod update-demo-nautilus-7hccl
Nov 28 05:14:52.267: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:14:52.267: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:14:52.267: INFO: update-demo-nautilus-7hccl is verified up and running
STEP: using delete to clean up resources
Nov 28 05:14:52.267: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:52.386: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 05:14:52.386: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 28 05:14:52.386: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-99fsq'
Nov 28 05:14:52.501: INFO: stderr: "No resources found.\n"
Nov 28 05:14:52.501: INFO: stdout: ""
Nov 28 05:14:52.501: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-99fsq -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 28 05:14:52.614: INFO: stderr: ""
Nov 28 05:14:52.614: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:14:52.614: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-99fsq" for this suite.
Nov 28 05:15:14.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:15:14.842: INFO: namespace: e2e-tests-kubectl-99fsq, resource: bindings, ignored listing per whitelist
Nov 28 05:15:15.143: INFO: namespace e2e-tests-kubectl-99fsq deletion completed in 22.523778387s

• [SLOW TEST:29.121 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should create and stop a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:15:15.143: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4pxr4 in namespace e2e-tests-proxy-jm6hk
I1128 05:15:15.293931    6241 runners.go:177] Created replication controller with name: proxy-service-4pxr4, namespace: e2e-tests-proxy-jm6hk, replica count: 1
I1128 05:15:16.344575    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1128 05:15:17.345184    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1128 05:15:18.345458    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1128 05:15:19.345794    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1128 05:15:20.346136    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1128 05:15:21.346450    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1128 05:15:22.346780    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1128 05:15:23.347170    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1128 05:15:24.347661    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1128 05:15:25.347999    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1128 05:15:26.348331    6241 runners.go:177] proxy-service-4pxr4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 28 05:15:26.352: INFO: setup took 11.082961911s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 28 05:15:26.368: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 14.880795ms)
Nov 28 05:15:26.375: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 22.729964ms)
Nov 28 05:15:26.375: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 22.662032ms)
Nov 28 05:15:26.375: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 22.882013ms)
Nov 28 05:15:26.375: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 22.704751ms)
Nov 28 05:15:26.375: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 22.718389ms)
Nov 28 05:15:26.375: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 22.773025ms)
Nov 28 05:15:26.380: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 27.018241ms)
Nov 28 05:15:26.383: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 30.10861ms)
Nov 28 05:15:26.383: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 30.361317ms)
Nov 28 05:15:26.383: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 30.34668ms)
Nov 28 05:15:26.383: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 30.523389ms)
Nov 28 05:15:26.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 31.333058ms)
Nov 28 05:15:26.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 31.59169ms)
Nov 28 05:15:26.384: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 31.667887ms)
Nov 28 05:15:26.385: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 32.085401ms)
Nov 28 05:15:26.391: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 6.055233ms)
Nov 28 05:15:26.391: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 6.291768ms)
Nov 28 05:15:26.391: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 6.135319ms)
Nov 28 05:15:26.391: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 6.539713ms)
Nov 28 05:15:26.392: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 6.766491ms)
Nov 28 05:15:26.392: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 6.98567ms)
Nov 28 05:15:26.392: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 6.982507ms)
Nov 28 05:15:26.392: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 6.895167ms)
Nov 28 05:15:26.392: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 6.986151ms)
Nov 28 05:15:26.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.811851ms)
Nov 28 05:15:26.394: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 9.28022ms)
Nov 28 05:15:26.394: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 9.09311ms)
Nov 28 05:15:26.394: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 9.126822ms)
Nov 28 05:15:26.394: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 9.321429ms)
Nov 28 05:15:26.394: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 9.440996ms)
Nov 28 05:15:26.394: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 9.269761ms)
Nov 28 05:15:26.398: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 3.788175ms)
Nov 28 05:15:26.399: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 4.551037ms)
Nov 28 05:15:26.399: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 4.770576ms)
Nov 28 05:15:26.399: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 4.571623ms)
Nov 28 05:15:26.400: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 5.353534ms)
Nov 28 05:15:26.401: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 6.384659ms)
Nov 28 05:15:26.401: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 6.616908ms)
Nov 28 05:15:26.401: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 6.574059ms)
Nov 28 05:15:26.402: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 7.389125ms)
Nov 28 05:15:26.402: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 7.692764ms)
Nov 28 05:15:26.402: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 7.841153ms)
Nov 28 05:15:26.402: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.787941ms)
Nov 28 05:15:26.403: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 7.745595ms)
Nov 28 05:15:26.403: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 8.227853ms)
Nov 28 05:15:26.403: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 8.134785ms)
Nov 28 05:15:26.405: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 9.578972ms)
Nov 28 05:15:26.409: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 4.751835ms)
Nov 28 05:15:26.410: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 5.438496ms)
Nov 28 05:15:26.412: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 6.537354ms)
Nov 28 05:15:26.412: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 6.638112ms)
Nov 28 05:15:26.412: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 6.970274ms)
Nov 28 05:15:26.412: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 6.80596ms)
Nov 28 05:15:26.412: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.50299ms)
Nov 28 05:15:26.412: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 7.397556ms)
Nov 28 05:15:26.412: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.513042ms)
Nov 28 05:15:26.412: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 7.372546ms)
Nov 28 05:15:26.413: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 7.801167ms)
Nov 28 05:15:26.413: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 8.227084ms)
Nov 28 05:15:26.413: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 8.163022ms)
Nov 28 05:15:26.413: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 8.071333ms)
Nov 28 05:15:26.413: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 8.02733ms)
Nov 28 05:15:26.413: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 8.570939ms)
Nov 28 05:15:26.422: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 8.097918ms)
Nov 28 05:15:26.422: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 8.339937ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 8.970596ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 8.964848ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 9.099478ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 9.145831ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 9.026175ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 9.067326ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 9.190046ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 9.251272ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 9.29679ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 9.139064ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 9.339872ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 9.438557ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 9.220828ms)
Nov 28 05:15:26.423: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 9.424361ms)
Nov 28 05:15:26.431: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.257741ms)
Nov 28 05:15:26.431: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 7.291309ms)
Nov 28 05:15:26.431: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.375149ms)
Nov 28 05:15:26.431: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 7.257572ms)
Nov 28 05:15:26.431: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 7.268695ms)
Nov 28 05:15:26.431: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 7.69399ms)
Nov 28 05:15:26.431: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.605647ms)
Nov 28 05:15:26.431: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 7.56206ms)
Nov 28 05:15:26.431: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.71568ms)
Nov 28 05:15:26.434: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 10.744472ms)
Nov 28 05:15:26.434: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 10.633816ms)
Nov 28 05:15:26.434: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 10.631628ms)
Nov 28 05:15:26.434: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 10.896534ms)
Nov 28 05:15:26.434: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 10.615882ms)
Nov 28 05:15:26.434: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 10.913816ms)
Nov 28 05:15:26.434: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 10.620604ms)
Nov 28 05:15:26.441: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 7.009824ms)
Nov 28 05:15:26.441: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.160942ms)
Nov 28 05:15:26.441: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 7.367219ms)
Nov 28 05:15:26.441: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.100507ms)
Nov 28 05:15:26.441: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 7.221483ms)
Nov 28 05:15:26.442: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 7.212261ms)
Nov 28 05:15:26.442: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.17292ms)
Nov 28 05:15:26.442: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.207029ms)
Nov 28 05:15:26.442: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 7.334576ms)
Nov 28 05:15:26.442: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 7.342506ms)
Nov 28 05:15:26.444: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 9.331447ms)
Nov 28 05:15:26.444: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 9.310618ms)
Nov 28 05:15:26.444: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 9.284425ms)
Nov 28 05:15:26.444: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 9.473708ms)
Nov 28 05:15:26.444: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 9.356154ms)
Nov 28 05:15:26.444: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 9.63519ms)
Nov 28 05:15:26.451: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 6.811858ms)
Nov 28 05:15:26.451: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 6.940657ms)
Nov 28 05:15:26.451: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 6.884691ms)
Nov 28 05:15:26.451: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 7.328453ms)
Nov 28 05:15:26.451: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.471451ms)
Nov 28 05:15:26.452: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.47775ms)
Nov 28 05:15:26.452: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 7.864364ms)
Nov 28 05:15:26.453: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 8.729068ms)
Nov 28 05:15:26.453: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 8.774911ms)
Nov 28 05:15:26.453: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 8.770716ms)
Nov 28 05:15:26.453: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 8.841968ms)
Nov 28 05:15:26.453: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 9.012433ms)
Nov 28 05:15:26.453: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 9.163571ms)
Nov 28 05:15:26.453: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 9.060114ms)
Nov 28 05:15:26.453: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 9.11231ms)
Nov 28 05:15:26.453: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 9.306248ms)
Nov 28 05:15:26.459: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 5.471698ms)
Nov 28 05:15:26.459: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 5.253382ms)
Nov 28 05:15:26.459: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 5.322429ms)
Nov 28 05:15:26.459: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 5.470067ms)
Nov 28 05:15:26.459: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 5.409283ms)
Nov 28 05:15:26.463: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 9.59316ms)
Nov 28 05:15:26.463: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 9.665295ms)
Nov 28 05:15:26.463: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 9.992077ms)
Nov 28 05:15:26.463: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 9.70115ms)
Nov 28 05:15:26.464: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 9.826125ms)
Nov 28 05:15:26.464: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 9.753312ms)
Nov 28 05:15:26.464: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 9.86885ms)
Nov 28 05:15:26.464: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 9.918277ms)
Nov 28 05:15:26.464: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 9.842575ms)
Nov 28 05:15:26.464: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 9.837425ms)
Nov 28 05:15:26.464: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 9.944381ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 6.276707ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 6.139063ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 5.960368ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 5.971879ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 6.063753ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 6.259836ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 6.04913ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 6.513116ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 6.128054ms)
Nov 28 05:15:26.470: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 6.318249ms)
Nov 28 05:15:26.474: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 9.524753ms)
Nov 28 05:15:26.474: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 9.79762ms)
Nov 28 05:15:26.474: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 9.616808ms)
Nov 28 05:15:26.475: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 10.812715ms)
Nov 28 05:15:26.475: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 10.661379ms)
Nov 28 05:15:26.475: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 10.543372ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 14.346389ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 14.420004ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 14.400338ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 14.366185ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 14.431884ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 14.554577ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 14.448121ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 14.39138ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 14.43411ms)
Nov 28 05:15:26.489: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 14.527019ms)
Nov 28 05:15:26.492: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 16.768658ms)
Nov 28 05:15:26.492: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 16.565526ms)
Nov 28 05:15:26.492: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 16.742837ms)
Nov 28 05:15:26.492: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 16.653656ms)
Nov 28 05:15:26.492: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 16.778313ms)
Nov 28 05:15:26.492: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 16.938176ms)
Nov 28 05:15:26.496: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 4.257179ms)
Nov 28 05:15:26.496: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 4.387121ms)
Nov 28 05:15:26.496: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 4.467754ms)
Nov 28 05:15:26.500: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 8.081072ms)
Nov 28 05:15:26.500: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 8.015241ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 8.464599ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 8.557502ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 8.706838ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 8.755512ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 8.813125ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 8.796332ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 9.157709ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 8.995056ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 8.861877ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 9.160383ms)
Nov 28 05:15:26.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 9.38291ms)
Nov 28 05:15:26.507: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 5.330442ms)
Nov 28 05:15:26.507: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 5.32234ms)
Nov 28 05:15:26.509: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 7.736414ms)
Nov 28 05:15:26.509: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 7.870671ms)
Nov 28 05:15:26.509: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 7.943893ms)
Nov 28 05:15:26.510: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 8.877795ms)
Nov 28 05:15:26.511: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 8.834332ms)
Nov 28 05:15:26.511: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 9.381869ms)
Nov 28 05:15:26.511: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 9.529169ms)
Nov 28 05:15:26.511: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 9.480892ms)
Nov 28 05:15:26.511: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 9.525067ms)
Nov 28 05:15:26.511: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 9.655153ms)
Nov 28 05:15:26.512: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 9.819743ms)
Nov 28 05:15:26.512: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 9.822809ms)
Nov 28 05:15:26.512: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 9.973376ms)
Nov 28 05:15:26.512: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 10.433396ms)
Nov 28 05:15:26.517: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 4.54928ms)
Nov 28 05:15:26.517: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 4.548495ms)
Nov 28 05:15:26.517: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 4.505014ms)
Nov 28 05:15:26.517: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 4.857087ms)
Nov 28 05:15:26.517: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 4.782368ms)
Nov 28 05:15:26.517: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 4.947556ms)
Nov 28 05:15:26.518: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 5.583388ms)
Nov 28 05:15:26.518: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 5.636272ms)
Nov 28 05:15:26.518: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 5.730599ms)
Nov 28 05:15:26.520: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 7.892745ms)
Nov 28 05:15:26.520: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 7.972104ms)
Nov 28 05:15:26.521: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 8.806936ms)
Nov 28 05:15:26.521: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 8.861967ms)
Nov 28 05:15:26.521: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 8.707251ms)
Nov 28 05:15:26.521: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 8.986694ms)
Nov 28 05:15:26.522: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 10.221029ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 5.816222ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 5.901318ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 5.898322ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 6.22561ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 6.471503ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 6.246359ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 6.39004ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 6.372231ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 6.290331ms)
Nov 28 05:15:26.529: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 6.488735ms)
Nov 28 05:15:26.530: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 7.260668ms)
Nov 28 05:15:26.530: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 7.566703ms)
Nov 28 05:15:26.530: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 7.300031ms)
Nov 28 05:15:26.530: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 7.487108ms)
Nov 28 05:15:26.530: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 7.538997ms)
Nov 28 05:15:26.530: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 7.598046ms)
Nov 28 05:15:26.537: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 6.984269ms)
Nov 28 05:15:26.537: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 6.812479ms)
Nov 28 05:15:26.537: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 6.908218ms)
Nov 28 05:15:26.537: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 6.809856ms)
Nov 28 05:15:26.538: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.373076ms)
Nov 28 05:15:26.538: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.518421ms)
Nov 28 05:15:26.538: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.518243ms)
Nov 28 05:15:26.538: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 7.535166ms)
Nov 28 05:15:26.538: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 7.573117ms)
Nov 28 05:15:26.538: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 7.679251ms)
Nov 28 05:15:26.538: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 7.844752ms)
Nov 28 05:15:26.540: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 9.602182ms)
Nov 28 05:15:26.540: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 9.544045ms)
Nov 28 05:15:26.540: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 9.568756ms)
Nov 28 05:15:26.540: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 9.698462ms)
Nov 28 05:15:26.540: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 9.78977ms)
Nov 28 05:15:26.546: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 5.813384ms)
Nov 28 05:15:26.550: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 9.550253ms)
Nov 28 05:15:26.550: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 9.566364ms)
Nov 28 05:15:26.550: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 9.705845ms)
Nov 28 05:15:26.550: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 9.616912ms)
Nov 28 05:15:26.550: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 9.889776ms)
Nov 28 05:15:26.550: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 9.720412ms)
Nov 28 05:15:26.551: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 9.785287ms)
Nov 28 05:15:26.551: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 10.126823ms)
Nov 28 05:15:26.551: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 10.068311ms)
Nov 28 05:15:26.551: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 10.691871ms)
Nov 28 05:15:26.551: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 10.34002ms)
Nov 28 05:15:26.552: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 10.955331ms)
Nov 28 05:15:26.552: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 11.214899ms)
Nov 28 05:15:26.552: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 10.978841ms)
Nov 28 05:15:26.552: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 10.982341ms)
Nov 28 05:15:26.560: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 8.081182ms)
Nov 28 05:15:26.561: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 8.671381ms)
Nov 28 05:15:26.561: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 8.617439ms)
Nov 28 05:15:26.561: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 8.663674ms)
Nov 28 05:15:26.561: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 8.728141ms)
Nov 28 05:15:26.561: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 8.594972ms)
Nov 28 05:15:26.561: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 8.696328ms)
Nov 28 05:15:26.561: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 8.668435ms)
Nov 28 05:15:26.561: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 8.656959ms)
Nov 28 05:15:26.561: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 8.871701ms)
Nov 28 05:15:26.562: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 10.4001ms)
Nov 28 05:15:26.562: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 10.556776ms)
Nov 28 05:15:26.562: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 10.217213ms)
Nov 28 05:15:26.563: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 11.076937ms)
Nov 28 05:15:26.563: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 11.460146ms)
Nov 28 05:15:26.563: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 11.355793ms)
Nov 28 05:15:26.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 7.519318ms)
Nov 28 05:15:26.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.503735ms)
Nov 28 05:15:26.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.595018ms)
Nov 28 05:15:26.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 7.589067ms)
Nov 28 05:15:26.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 7.599904ms)
Nov 28 05:15:26.572: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 7.950023ms)
Nov 28 05:15:26.572: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 7.932708ms)
Nov 28 05:15:26.572: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.933095ms)
Nov 28 05:15:26.572: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 8.639468ms)
Nov 28 05:15:26.572: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 8.564ms)
Nov 28 05:15:26.574: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 10.196573ms)
Nov 28 05:15:26.575: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 11.02404ms)
Nov 28 05:15:26.575: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 11.056645ms)
Nov 28 05:15:26.575: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 11.038332ms)
Nov 28 05:15:26.575: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 11.251329ms)
Nov 28 05:15:26.575: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 11.305581ms)
Nov 28 05:15:26.582: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s/proxy/rewriteme"... (200; 7.138693ms)
Nov 28 05:15:26.582: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:1080/proxy/rewri... (200; 7.26792ms)
Nov 28 05:15:26.582: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.352772ms)
Nov 28 05:15:26.582: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.251011ms)
Nov 28 05:15:26.582: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:1080/proxy/... (200; 7.148031ms)
Nov 28 05:15:26.583: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/proxy-service-4pxr4-5549s:162/proxy/: bar (200; 7.762518ms)
Nov 28 05:15:26.583: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:462/proxy/: tls qux (200; 8.014049ms)
Nov 28 05:15:26.583: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/http:proxy-service-4pxr4-5549s:160/proxy/: foo (200; 7.912366ms)
Nov 28 05:15:26.583: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname2/proxy/: bar (200; 7.919328ms)
Nov 28 05:15:26.583: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:443/proxy/... (200; 8.126945ms)
Nov 28 05:15:26.583: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/pods/https:proxy-service-4pxr4-5549s:460/proxy/: tls baz (200; 8.115581ms)
Nov 28 05:15:26.584: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname2/proxy/: tls qux (200; 9.24469ms)
Nov 28 05:15:26.584: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/http:proxy-service-4pxr4:portname1/proxy/: foo (200; 9.214525ms)
Nov 28 05:15:26.584: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname2/proxy/: bar (200; 9.185992ms)
Nov 28 05:15:26.584: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/proxy-service-4pxr4:portname1/proxy/: foo (200; 9.25584ms)
Nov 28 05:15:26.584: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-jm6hk/services/https:proxy-service-4pxr4:tlsportname1/proxy/: tls baz (200; 9.253578ms)
STEP: deleting { ReplicationController} proxy-service-4pxr4 in namespace e2e-tests-proxy-jm6hk, will wait for the garbage collector to delete the pods
Nov 28 05:15:26.644: INFO: Deleting { ReplicationController} proxy-service-4pxr4 took: 6.174002ms
Nov 28 05:15:26.745: INFO: Terminating { ReplicationController} proxy-service-4pxr4 pods took: 100.333342ms
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:15:30.346: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-jm6hk" for this suite.
Nov 28 05:15:36.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:15:36.464: INFO: namespace: e2e-tests-proxy-jm6hk, resource: bindings, ignored listing per whitelist
Nov 28 05:15:36.499: INFO: namespace e2e-tests-proxy-jm6hk deletion completed in 6.147530117s

• [SLOW TEST:21.356 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:15:36.499: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:80
Nov 28 05:15:36.612: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 28 05:16:36.644: INFO: Waiting for terminating namespaces to be deleted...
Nov 28 05:16:36.650: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 28 05:16:36.656: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 28 05:16:36.656: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Nov 28 05:16:36.659: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Nov 28 05:16:36.659: INFO: 
Logging pods the kubelet thinks is on node ci-op-qh5ir6tt-623b1-ig-n-fcth before test
Nov 28 05:16:36.668: INFO: ovs-srxhg from openshift-sdn started at 2018-11-28 04:58:01 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.668: INFO: 	Container openvswitch ready: true, restart count 0
Nov 28 05:16:36.668: INFO: sdn-zk2bx from openshift-sdn started at 2018-11-28 04:58:01 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.668: INFO: 	Container sdn ready: true, restart count 0
Nov 28 05:16:36.668: INFO: router-1-vtgwb from default started at 2018-11-28 04:59:03 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.668: INFO: 	Container router ready: true, restart count 0
Nov 28 05:16:36.668: INFO: prometheus-operator-6644b8cd54-pk4l6 from openshift-monitoring started at 2018-11-28 05:00:03 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.668: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 28 05:16:36.668: INFO: kube-state-metrics-7449d589bc-ktklc from openshift-monitoring started at 2018-11-28 05:02:54 +0000 UTC (3 container statuses recorded)
Nov 28 05:16:36.668: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 28 05:16:36.668: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 28 05:16:36.668: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 28 05:16:36.668: INFO: sync-ql29c from openshift-node started at 2018-11-28 04:58:01 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.668: INFO: 	Container sync ready: true, restart count 0
Nov 28 05:16:36.668: INFO: alertmanager-main-0 from openshift-monitoring started at 2018-11-28 05:01:49 +0000 UTC (3 container statuses recorded)
Nov 28 05:16:36.668: INFO: 	Container alertmanager ready: true, restart count 0
Nov 28 05:16:36.668: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 28 05:16:36.668: INFO: 	Container config-reloader ready: true, restart count 0
Nov 28 05:16:36.668: INFO: node-exporter-f2t98 from openshift-monitoring started at 2018-11-28 05:02:40 +0000 UTC (2 container statuses recorded)
Nov 28 05:16:36.668: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 28 05:16:36.668: INFO: 	Container node-exporter ready: true, restart count 0
Nov 28 05:16:36.668: INFO: 
Logging pods the kubelet thinks is on node ci-op-qh5ir6tt-623b1-ig-n-sr0b before test
Nov 28 05:16:36.677: INFO: docker-registry-1-ldgq8 from default started at 2018-11-28 04:59:22 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.677: INFO: 	Container registry ready: true, restart count 0
Nov 28 05:16:36.677: INFO: alertmanager-main-1 from openshift-monitoring started at 2018-11-28 05:02:09 +0000 UTC (3 container statuses recorded)
Nov 28 05:16:36.677: INFO: 	Container alertmanager ready: true, restart count 0
Nov 28 05:16:36.677: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 28 05:16:36.677: INFO: 	Container config-reloader ready: true, restart count 0
Nov 28 05:16:36.677: INFO: ovs-7p9pf from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.677: INFO: 	Container openvswitch ready: true, restart count 0
Nov 28 05:16:36.677: INFO: prometheus-k8s-0 from openshift-monitoring started at 2018-11-28 05:00:57 +0000 UTC (4 container statuses recorded)
Nov 28 05:16:36.677: INFO: 	Container prometheus ready: true, restart count 1
Nov 28 05:16:36.677: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 28 05:16:36.677: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 28 05:16:36.677: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 28 05:16:36.677: INFO: node-exporter-bx8vj from openshift-monitoring started at 2018-11-28 05:02:40 +0000 UTC (2 container statuses recorded)
Nov 28 05:16:36.677: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 28 05:16:36.677: INFO: 	Container node-exporter ready: true, restart count 0
Nov 28 05:16:36.677: INFO: sdn-7rbmb from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.677: INFO: 	Container sdn ready: true, restart count 0
Nov 28 05:16:36.677: INFO: sync-46bjz from openshift-node started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.677: INFO: 	Container sync ready: true, restart count 0
Nov 28 05:16:36.677: INFO: 
Logging pods the kubelet thinks is on node ci-op-qh5ir6tt-623b1-ig-n-x1wp before test
Nov 28 05:16:36.689: INFO: ovs-pcctn from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.689: INFO: 	Container openvswitch ready: true, restart count 0
Nov 28 05:16:36.689: INFO: sdn-dkxph from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.689: INFO: 	Container sdn ready: true, restart count 0
Nov 28 05:16:36.689: INFO: cluster-monitoring-operator-6465f8fbc7-lr66x from openshift-monitoring started at 2018-11-28 04:59:38 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.689: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 28 05:16:36.689: INFO: grafana-6b9f85786f-vm5t5 from openshift-monitoring started at 2018-11-28 05:00:33 +0000 UTC (2 container statuses recorded)
Nov 28 05:16:36.689: INFO: 	Container grafana ready: true, restart count 0
Nov 28 05:16:36.689: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 28 05:16:36.689: INFO: alertmanager-main-2 from openshift-monitoring started at 2018-11-28 05:02:23 +0000 UTC (3 container statuses recorded)
Nov 28 05:16:36.689: INFO: 	Container alertmanager ready: true, restart count 0
Nov 28 05:16:36.689: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 28 05:16:36.689: INFO: 	Container config-reloader ready: true, restart count 0
Nov 28 05:16:36.689: INFO: sync-xtbfj from openshift-node started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:16:36.689: INFO: 	Container sync ready: true, restart count 0
Nov 28 05:16:36.689: INFO: prometheus-k8s-1 from openshift-monitoring started at 2018-11-28 05:01:26 +0000 UTC (4 container statuses recorded)
Nov 28 05:16:36.689: INFO: 	Container prometheus ready: true, restart count 1
Nov 28 05:16:36.689: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 28 05:16:36.689: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 28 05:16:36.689: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 28 05:16:36.689: INFO: node-exporter-svgfr from openshift-monitoring started at 2018-11-28 05:02:40 +0000 UTC (2 container statuses recorded)
Nov 28 05:16:36.689: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 28 05:16:36.689: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: verifying the node has the label node ci-op-qh5ir6tt-623b1-ig-n-fcth
STEP: verifying the node has the label node ci-op-qh5ir6tt-623b1-ig-n-sr0b
STEP: verifying the node has the label node ci-op-qh5ir6tt-623b1-ig-n-x1wp
Nov 28 05:16:36.729: INFO: Pod docker-registry-1-ldgq8 requesting resource cpu=100m on Node ci-op-qh5ir6tt-623b1-ig-n-sr0b
Nov 28 05:16:36.729: INFO: Pod router-1-vtgwb requesting resource cpu=100m on Node ci-op-qh5ir6tt-623b1-ig-n-fcth
Nov 28 05:16:36.729: INFO: Pod alertmanager-main-0 requesting resource cpu=5m on Node ci-op-qh5ir6tt-623b1-ig-n-fcth
Nov 28 05:16:36.729: INFO: Pod alertmanager-main-1 requesting resource cpu=5m on Node ci-op-qh5ir6tt-623b1-ig-n-sr0b
Nov 28 05:16:36.729: INFO: Pod alertmanager-main-2 requesting resource cpu=5m on Node ci-op-qh5ir6tt-623b1-ig-n-x1wp
Nov 28 05:16:36.729: INFO: Pod cluster-monitoring-operator-6465f8fbc7-lr66x requesting resource cpu=20m on Node ci-op-qh5ir6tt-623b1-ig-n-x1wp
Nov 28 05:16:36.729: INFO: Pod grafana-6b9f85786f-vm5t5 requesting resource cpu=100m on Node ci-op-qh5ir6tt-623b1-ig-n-x1wp
Nov 28 05:16:36.729: INFO: Pod kube-state-metrics-7449d589bc-ktklc requesting resource cpu=20m on Node ci-op-qh5ir6tt-623b1-ig-n-fcth
Nov 28 05:16:36.729: INFO: Pod node-exporter-bx8vj requesting resource cpu=10m on Node ci-op-qh5ir6tt-623b1-ig-n-sr0b
Nov 28 05:16:36.730: INFO: Pod node-exporter-f2t98 requesting resource cpu=10m on Node ci-op-qh5ir6tt-623b1-ig-n-fcth
Nov 28 05:16:36.730: INFO: Pod node-exporter-svgfr requesting resource cpu=10m on Node ci-op-qh5ir6tt-623b1-ig-n-x1wp
Nov 28 05:16:36.730: INFO: Pod prometheus-k8s-0 requesting resource cpu=15m on Node ci-op-qh5ir6tt-623b1-ig-n-sr0b
Nov 28 05:16:36.730: INFO: Pod prometheus-k8s-1 requesting resource cpu=15m on Node ci-op-qh5ir6tt-623b1-ig-n-x1wp
Nov 28 05:16:36.730: INFO: Pod prometheus-operator-6644b8cd54-pk4l6 requesting resource cpu=0m on Node ci-op-qh5ir6tt-623b1-ig-n-fcth
Nov 28 05:16:36.730: INFO: Pod sync-46bjz requesting resource cpu=0m on Node ci-op-qh5ir6tt-623b1-ig-n-sr0b
Nov 28 05:16:36.730: INFO: Pod sync-ql29c requesting resource cpu=0m on Node ci-op-qh5ir6tt-623b1-ig-n-fcth
Nov 28 05:16:36.730: INFO: Pod sync-xtbfj requesting resource cpu=0m on Node ci-op-qh5ir6tt-623b1-ig-n-x1wp
Nov 28 05:16:36.730: INFO: Pod ovs-7p9pf requesting resource cpu=100m on Node ci-op-qh5ir6tt-623b1-ig-n-sr0b
Nov 28 05:16:36.730: INFO: Pod ovs-pcctn requesting resource cpu=100m on Node ci-op-qh5ir6tt-623b1-ig-n-x1wp
Nov 28 05:16:36.730: INFO: Pod ovs-srxhg requesting resource cpu=100m on Node ci-op-qh5ir6tt-623b1-ig-n-fcth
Nov 28 05:16:36.730: INFO: Pod sdn-7rbmb requesting resource cpu=100m on Node ci-op-qh5ir6tt-623b1-ig-n-sr0b
Nov 28 05:16:36.730: INFO: Pod sdn-dkxph requesting resource cpu=100m on Node ci-op-qh5ir6tt-623b1-ig-n-x1wp
Nov 28 05:16:36.730: INFO: Pod sdn-zk2bx requesting resource cpu=100m on Node ci-op-qh5ir6tt-623b1-ig-n-fcth
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7647762-f2cc-11e8-904b-0a58ac10ae35.156b31ee45d1ecd5], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-wxnb5/filler-pod-c7647762-f2cc-11e8-904b-0a58ac10ae35 to ci-op-qh5ir6tt-623b1-ig-n-fcth]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7647762-f2cc-11e8-904b-0a58ac10ae35.156b31eec8207df1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7647762-f2cc-11e8-904b-0a58ac10ae35.156b31eecb52b91f], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7647762-f2cc-11e8-904b-0a58ac10ae35.156b31eed3196b4f], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7669450-f2cc-11e8-904b-0a58ac10ae35.156b31ee466b4bc3], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-wxnb5/filler-pod-c7669450-f2cc-11e8-904b-0a58ac10ae35 to ci-op-qh5ir6tt-623b1-ig-n-sr0b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7669450-f2cc-11e8-904b-0a58ac10ae35.156b31eecbc5e5fe], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7669450-f2cc-11e8-904b-0a58ac10ae35.156b31eee65edf6e], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7669450-f2cc-11e8-904b-0a58ac10ae35.156b31eee84ed2d1], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7669450-f2cc-11e8-904b-0a58ac10ae35.156b31eeefd46ff1], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7681d05-f2cc-11e8-904b-0a58ac10ae35.156b31ee4746f138], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-wxnb5/filler-pod-c7681d05-f2cc-11e8-904b-0a58ac10ae35 to ci-op-qh5ir6tt-623b1-ig-n-x1wp]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7681d05-f2cc-11e8-904b-0a58ac10ae35.156b31eeaf7352ea], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7681d05-f2cc-11e8-904b-0a58ac10ae35.156b31eeca8f7490], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7681d05-f2cc-11e8-904b-0a58ac10ae35.156b31eeccf07d2b], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7681d05-f2cc-11e8-904b-0a58ac10ae35.156b31eed4cc1143], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.156b31ef3708a9f3], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 Insufficient cpu.]
STEP: removing the label node off the node ci-op-qh5ir6tt-623b1-ig-n-sr0b
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ci-op-qh5ir6tt-623b1-ig-n-x1wp
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ci-op-qh5ir6tt-623b1-ig-n-fcth
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:16:41.840: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-wxnb5" for this suite.
Nov 28 05:16:47.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:16:47.916: INFO: namespace: e2e-tests-sched-pred-wxnb5, resource: bindings, ignored listing per whitelist
Nov 28 05:16:48.366: INFO: namespace e2e-tests-sched-pred-wxnb5 deletion completed in 6.522350321s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:71

• [SLOW TEST:71.867 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:16:48.366: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:16:49.533: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce6b55f5-f2cc-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-nqxxj" to be "success or failure"
Nov 28 05:16:49.537: INFO: Pod "downwardapi-volume-ce6b55f5-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.239893ms
Nov 28 05:16:51.540: INFO: Pod "downwardapi-volume-ce6b55f5-f2cc-11e8-904b-0a58ac10ae35": Phase="Running", Reason="", readiness=true. Elapsed: 2.006052883s
Nov 28 05:16:53.543: INFO: Pod "downwardapi-volume-ce6b55f5-f2cc-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009608121s
STEP: Saw pod success
Nov 28 05:16:53.543: INFO: Pod "downwardapi-volume-ce6b55f5-f2cc-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:16:53.545: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod downwardapi-volume-ce6b55f5-f2cc-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:16:53.562: INFO: Waiting for pod downwardapi-volume-ce6b55f5-f2cc-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:16:53.564: INFO: Pod downwardapi-volume-ce6b55f5-f2cc-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:16:53.564: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nqxxj" for this suite.
Nov 28 05:16:59.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:16:59.613: INFO: namespace: e2e-tests-downward-api-nqxxj, resource: bindings, ignored listing per whitelist
Nov 28 05:17:00.091: INFO: namespace e2e-tests-downward-api-nqxxj deletion completed in 6.523540366s

• [SLOW TEST:11.724 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:17:00.091: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating the pod
Nov 28 05:17:04.785: INFO: Successfully updated pod "annotationupdated567b1e3-f2cc-11e8-904b-0a58ac10ae35"
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:17:08.825: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zfbn2" for this suite.
Nov 28 05:17:30.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:17:30.952: INFO: namespace: e2e-tests-projected-zfbn2, resource: bindings, ignored listing per whitelist
Nov 28 05:17:31.367: INFO: namespace e2e-tests-projected-zfbn2 deletion completed in 22.537665369s

• [SLOW TEST:31.276 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:17:31.367: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 28 05:17:31.606: INFO: Waiting up to 5m0s for pod "pod-e817ca82-f2cc-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-ktszb" to be "success or failure"
Nov 28 05:17:31.611: INFO: Pod "pod-e817ca82-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 5.262827ms
Nov 28 05:17:33.615: INFO: Pod "pod-e817ca82-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009234538s
Nov 28 05:17:35.619: INFO: Pod "pod-e817ca82-f2cc-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013319415s
STEP: Saw pod success
Nov 28 05:17:35.619: INFO: Pod "pod-e817ca82-f2cc-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:17:35.622: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-e817ca82-f2cc-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:17:35.641: INFO: Waiting for pod pod-e817ca82-f2cc-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:17:35.644: INFO: Pod pod-e817ca82-f2cc-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:17:35.644: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ktszb" for this suite.
Nov 28 05:17:41.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:17:41.748: INFO: namespace: e2e-tests-emptydir-ktszb, resource: bindings, ignored listing per whitelist
Nov 28 05:17:42.171: INFO: namespace e2e-tests-emptydir-ktszb deletion completed in 6.522801656s

• [SLOW TEST:10.804 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:17:42.171: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating secret with name secret-test-ee7ce2cc-f2cc-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:17:43.340: INFO: Waiting up to 5m0s for pod "pod-secrets-ee7d9ad4-f2cc-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-secrets-x4j56" to be "success or failure"
Nov 28 05:17:43.343: INFO: Pod "pod-secrets-ee7d9ad4-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921335ms
Nov 28 05:17:45.347: INFO: Pod "pod-secrets-ee7d9ad4-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006375683s
Nov 28 05:17:47.350: INFO: Pod "pod-secrets-ee7d9ad4-f2cc-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00951888s
STEP: Saw pod success
Nov 28 05:17:47.350: INFO: Pod "pod-secrets-ee7d9ad4-f2cc-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:17:47.352: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-secrets-ee7d9ad4-f2cc-11e8-904b-0a58ac10ae35 container secret-volume-test: <nil>
STEP: delete the pod
Nov 28 05:17:47.368: INFO: Waiting for pod pod-secrets-ee7d9ad4-f2cc-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:17:47.371: INFO: Pod pod-secrets-ee7d9ad4-f2cc-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:17:47.371: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-x4j56" for this suite.
Nov 28 05:17:53.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:17:53.429: INFO: namespace: e2e-tests-secrets-x4j56, resource: bindings, ignored listing per whitelist
Nov 28 05:17:53.899: INFO: namespace e2e-tests-secrets-x4j56 deletion completed in 6.524427992s

• [SLOW TEST:11.729 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:17:53.900: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:17:54.053: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5794128-f2cc-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-hnzt5" to be "success or failure"
Nov 28 05:17:54.061: INFO: Pod "downwardapi-volume-f5794128-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 8.478317ms
Nov 28 05:17:56.065: INFO: Pod "downwardapi-volume-f5794128-f2cc-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012180043s
Nov 28 05:17:58.069: INFO: Pod "downwardapi-volume-f5794128-f2cc-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016178086s
STEP: Saw pod success
Nov 28 05:17:58.069: INFO: Pod "downwardapi-volume-f5794128-f2cc-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:17:58.072: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod downwardapi-volume-f5794128-f2cc-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:17:58.093: INFO: Waiting for pod downwardapi-volume-f5794128-f2cc-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:17:58.095: INFO: Pod downwardapi-volume-f5794128-f2cc-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:17:58.095: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hnzt5" for this suite.
Nov 28 05:18:04.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:18:04.155: INFO: namespace: e2e-tests-projected-hnzt5, resource: bindings, ignored listing per whitelist
Nov 28 05:18:04.623: INFO: namespace e2e-tests-projected-hnzt5 deletion completed in 6.524772339s

• [SLOW TEST:10.724 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:18:04.623: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:80
Nov 28 05:18:04.756: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 28 05:19:04.795: INFO: Waiting for terminating namespaces to be deleted...
Nov 28 05:19:04.800: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 28 05:19:04.809: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 28 05:19:04.810: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Nov 28 05:19:04.813: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Nov 28 05:19:04.813: INFO: 
Logging pods the kubelet thinks is on node ci-op-qh5ir6tt-623b1-ig-n-fcth before test
Nov 28 05:19:04.825: INFO: sync-ql29c from openshift-node started at 2018-11-28 04:58:01 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.825: INFO: 	Container sync ready: true, restart count 0
Nov 28 05:19:04.825: INFO: alertmanager-main-0 from openshift-monitoring started at 2018-11-28 05:01:49 +0000 UTC (3 container statuses recorded)
Nov 28 05:19:04.825: INFO: 	Container alertmanager ready: true, restart count 0
Nov 28 05:19:04.825: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 28 05:19:04.825: INFO: 	Container config-reloader ready: true, restart count 0
Nov 28 05:19:04.825: INFO: node-exporter-f2t98 from openshift-monitoring started at 2018-11-28 05:02:40 +0000 UTC (2 container statuses recorded)
Nov 28 05:19:04.825: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 28 05:19:04.825: INFO: 	Container node-exporter ready: true, restart count 0
Nov 28 05:19:04.825: INFO: kube-state-metrics-7449d589bc-ktklc from openshift-monitoring started at 2018-11-28 05:02:54 +0000 UTC (3 container statuses recorded)
Nov 28 05:19:04.825: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 28 05:19:04.825: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 28 05:19:04.825: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 28 05:19:04.825: INFO: ovs-srxhg from openshift-sdn started at 2018-11-28 04:58:01 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.825: INFO: 	Container openvswitch ready: true, restart count 0
Nov 28 05:19:04.825: INFO: sdn-zk2bx from openshift-sdn started at 2018-11-28 04:58:01 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.825: INFO: 	Container sdn ready: true, restart count 0
Nov 28 05:19:04.825: INFO: router-1-vtgwb from default started at 2018-11-28 04:59:03 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.825: INFO: 	Container router ready: true, restart count 0
Nov 28 05:19:04.825: INFO: prometheus-operator-6644b8cd54-pk4l6 from openshift-monitoring started at 2018-11-28 05:00:03 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.825: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 28 05:19:04.825: INFO: 
Logging pods the kubelet thinks is on node ci-op-qh5ir6tt-623b1-ig-n-sr0b before test
Nov 28 05:19:04.835: INFO: sdn-7rbmb from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.835: INFO: 	Container sdn ready: true, restart count 0
Nov 28 05:19:04.835: INFO: sync-46bjz from openshift-node started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.835: INFO: 	Container sync ready: true, restart count 0
Nov 28 05:19:04.835: INFO: ovs-7p9pf from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.836: INFO: 	Container openvswitch ready: true, restart count 0
Nov 28 05:19:04.836: INFO: prometheus-k8s-0 from openshift-monitoring started at 2018-11-28 05:00:57 +0000 UTC (4 container statuses recorded)
Nov 28 05:19:04.836: INFO: 	Container prometheus ready: true, restart count 1
Nov 28 05:19:04.836: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 28 05:19:04.836: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 28 05:19:04.836: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 28 05:19:04.836: INFO: node-exporter-bx8vj from openshift-monitoring started at 2018-11-28 05:02:40 +0000 UTC (2 container statuses recorded)
Nov 28 05:19:04.836: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 28 05:19:04.836: INFO: 	Container node-exporter ready: true, restart count 0
Nov 28 05:19:04.836: INFO: docker-registry-1-ldgq8 from default started at 2018-11-28 04:59:22 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.836: INFO: 	Container registry ready: true, restart count 0
Nov 28 05:19:04.836: INFO: alertmanager-main-1 from openshift-monitoring started at 2018-11-28 05:02:09 +0000 UTC (3 container statuses recorded)
Nov 28 05:19:04.836: INFO: 	Container alertmanager ready: true, restart count 0
Nov 28 05:19:04.836: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 28 05:19:04.836: INFO: 	Container config-reloader ready: true, restart count 0
Nov 28 05:19:04.836: INFO: 
Logging pods the kubelet thinks is on node ci-op-qh5ir6tt-623b1-ig-n-x1wp before test
Nov 28 05:19:04.845: INFO: sdn-dkxph from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.846: INFO: 	Container sdn ready: true, restart count 0
Nov 28 05:19:04.846: INFO: cluster-monitoring-operator-6465f8fbc7-lr66x from openshift-monitoring started at 2018-11-28 04:59:38 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.846: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 28 05:19:04.846: INFO: grafana-6b9f85786f-vm5t5 from openshift-monitoring started at 2018-11-28 05:00:33 +0000 UTC (2 container statuses recorded)
Nov 28 05:19:04.846: INFO: 	Container grafana ready: true, restart count 0
Nov 28 05:19:04.846: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 28 05:19:04.846: INFO: alertmanager-main-2 from openshift-monitoring started at 2018-11-28 05:02:23 +0000 UTC (3 container statuses recorded)
Nov 28 05:19:04.846: INFO: 	Container alertmanager ready: true, restart count 0
Nov 28 05:19:04.846: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 28 05:19:04.846: INFO: 	Container config-reloader ready: true, restart count 0
Nov 28 05:19:04.846: INFO: ovs-pcctn from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.846: INFO: 	Container openvswitch ready: true, restart count 0
Nov 28 05:19:04.846: INFO: prometheus-k8s-1 from openshift-monitoring started at 2018-11-28 05:01:26 +0000 UTC (4 container statuses recorded)
Nov 28 05:19:04.846: INFO: 	Container prometheus ready: true, restart count 1
Nov 28 05:19:04.846: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 28 05:19:04.846: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 28 05:19:04.846: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 28 05:19:04.846: INFO: node-exporter-svgfr from openshift-monitoring started at 2018-11-28 05:02:40 +0000 UTC (2 container statuses recorded)
Nov 28 05:19:04.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 28 05:19:04.846: INFO: 	Container node-exporter ready: true, restart count 0
Nov 28 05:19:04.846: INFO: sync-xtbfj from openshift-node started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:19:04.846: INFO: 	Container sync ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.156b3210c33bf8f6], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:19:05.884: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-8p2sh" for this suite.
Nov 28 05:19:11.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:19:11.930: INFO: namespace: e2e-tests-sched-pred-8p2sh, resource: bindings, ignored listing per whitelist
Nov 28 05:19:12.418: INFO: namespace e2e-tests-sched-pred-8p2sh deletion completed in 6.530248093s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:71

• [SLOW TEST:67.795 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:19:12.419: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name projected-configmap-test-volume-24445c78-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:19:12.568: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-24455ba7-f2cd-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-dw5rm" to be "success or failure"
Nov 28 05:19:12.573: INFO: Pod "pod-projected-configmaps-24455ba7-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.769584ms
Nov 28 05:19:14.577: INFO: Pod "pod-projected-configmaps-24455ba7-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008298783s
Nov 28 05:19:16.580: INFO: Pod "pod-projected-configmaps-24455ba7-f2cd-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011949941s
STEP: Saw pod success
Nov 28 05:19:16.580: INFO: Pod "pod-projected-configmaps-24455ba7-f2cd-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:19:16.583: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-projected-configmaps-24455ba7-f2cd-11e8-904b-0a58ac10ae35 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:19:16.604: INFO: Waiting for pod pod-projected-configmaps-24455ba7-f2cd-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:19:16.607: INFO: Pod pod-projected-configmaps-24455ba7-f2cd-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:19:16.607: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dw5rm" for this suite.
Nov 28 05:19:22.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:19:22.743: INFO: namespace: e2e-tests-projected-dw5rm, resource: bindings, ignored listing per whitelist
Nov 28 05:19:23.145: INFO: namespace e2e-tests-projected-dw5rm deletion completed in 6.53375688s

• [SLOW TEST:10.726 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:19:23.145: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating secret with name secret-test-2aa79c99-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:19:23.280: INFO: Waiting up to 5m0s for pod "pod-secrets-2aa812a3-f2cd-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-secrets-dv4cs" to be "success or failure"
Nov 28 05:19:23.282: INFO: Pod "pod-secrets-2aa812a3-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 1.997945ms
Nov 28 05:19:25.286: INFO: Pod "pod-secrets-2aa812a3-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006183938s
Nov 28 05:19:27.290: INFO: Pod "pod-secrets-2aa812a3-f2cd-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009854774s
STEP: Saw pod success
Nov 28 05:19:27.290: INFO: Pod "pod-secrets-2aa812a3-f2cd-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:19:27.292: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-secrets-2aa812a3-f2cd-11e8-904b-0a58ac10ae35 container secret-volume-test: <nil>
STEP: delete the pod
Nov 28 05:19:27.311: INFO: Waiting for pod pod-secrets-2aa812a3-f2cd-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:19:27.314: INFO: Pod pod-secrets-2aa812a3-f2cd-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:19:27.314: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-dv4cs" for this suite.
Nov 28 05:19:33.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:19:33.367: INFO: namespace: e2e-tests-secrets-dv4cs, resource: bindings, ignored listing per whitelist
Nov 28 05:19:33.843: INFO: namespace e2e-tests-secrets-dv4cs deletion completed in 6.52428282s

• [SLOW TEST:10.698 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:19:33.844: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:19:33.977: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-310a55c1-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating secret with name s-test-opt-upd-310a5602-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-310a55c1-f2cd-11e8-904b-0a58ac10ae35
STEP: Updating secret s-test-opt-upd-310a5602-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating secret with name s-test-opt-create-310a561a-f2cd-11e8-904b-0a58ac10ae35
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:21:09.654: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ftbkx" for this suite.
Nov 28 05:21:31.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:21:31.776: INFO: namespace: e2e-tests-projected-ftbkx, resource: bindings, ignored listing per whitelist
Nov 28 05:21:32.179: INFO: namespace e2e-tests-projected-ftbkx deletion completed in 22.521185093s

• [SLOW TEST:118.335 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:21:32.179: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward api env vars
Nov 28 05:21:32.339: INFO: Waiting up to 5m0s for pod "downward-api-779366f3-f2cd-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-7bxjm" to be "success or failure"
Nov 28 05:21:32.342: INFO: Pod "downward-api-779366f3-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.968267ms
Nov 28 05:21:34.346: INFO: Pod "downward-api-779366f3-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00696293s
Nov 28 05:21:36.350: INFO: Pod "downward-api-779366f3-f2cd-11e8-904b-0a58ac10ae35": Phase="Running", Reason="", readiness=true. Elapsed: 4.010903382s
Nov 28 05:21:38.354: INFO: Pod "downward-api-779366f3-f2cd-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014847174s
STEP: Saw pod success
Nov 28 05:21:38.354: INFO: Pod "downward-api-779366f3-f2cd-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:21:38.357: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod downward-api-779366f3-f2cd-11e8-904b-0a58ac10ae35 container dapi-container: <nil>
STEP: delete the pod
Nov 28 05:21:38.376: INFO: Waiting for pod downward-api-779366f3-f2cd-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:21:38.379: INFO: Pod downward-api-779366f3-f2cd-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:21:38.379: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7bxjm" for this suite.
Nov 28 05:21:44.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:21:44.421: INFO: namespace: e2e-tests-downward-api-7bxjm, resource: bindings, ignored listing per whitelist
Nov 28 05:21:44.908: INFO: namespace e2e-tests-downward-api-7bxjm deletion completed in 6.525390487s

• [SLOW TEST:12.729 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:21:44.908: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Nov 28 05:21:45.590: INFO: Waiting up to 5m0s for pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-4nvs2" in namespace "e2e-tests-svcaccounts-9rqsg" to be "success or failure"
Nov 28 05:21:45.594: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-4nvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416615ms
Nov 28 05:21:47.598: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-4nvs2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006997899s
Nov 28 05:21:49.601: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-4nvs2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01059287s
STEP: Saw pod success
Nov 28 05:21:49.601: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-4nvs2" satisfied condition "success or failure"
Nov 28 05:21:49.604: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-4nvs2 container token-test: <nil>
STEP: delete the pod
Nov 28 05:21:49.625: INFO: Waiting for pod pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-4nvs2 to disappear
Nov 28 05:21:49.627: INFO: Pod pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-4nvs2 no longer exists
STEP: Creating a pod to test consume service account root CA
Nov 28 05:21:49.636: INFO: Waiting up to 5m0s for pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-t8hl5" in namespace "e2e-tests-svcaccounts-9rqsg" to be "success or failure"
Nov 28 05:21:49.640: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-t8hl5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.60972ms
Nov 28 05:21:51.643: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-t8hl5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007065963s
Nov 28 05:21:53.647: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-t8hl5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01061771s
STEP: Saw pod success
Nov 28 05:21:53.647: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-t8hl5" satisfied condition "success or failure"
Nov 28 05:21:53.649: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-t8hl5 container root-ca-test: <nil>
STEP: delete the pod
Nov 28 05:21:53.667: INFO: Waiting for pod pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-t8hl5 to disappear
Nov 28 05:21:53.669: INFO: Pod pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-t8hl5 no longer exists
STEP: Creating a pod to test consume service account namespace
Nov 28 05:21:53.678: INFO: Waiting up to 5m0s for pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-pzr7c" in namespace "e2e-tests-svcaccounts-9rqsg" to be "success or failure"
Nov 28 05:21:53.681: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-pzr7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37979ms
Nov 28 05:21:55.684: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-pzr7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005546128s
Nov 28 05:21:57.688: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-pzr7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009462171s
STEP: Saw pod success
Nov 28 05:21:57.688: INFO: Pod "pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-pzr7c" satisfied condition "success or failure"
Nov 28 05:21:57.691: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-pzr7c container namespace-test: <nil>
STEP: delete the pod
Nov 28 05:21:57.709: INFO: Waiting for pod pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-pzr7c to disappear
Nov 28 05:21:57.711: INFO: Pod pod-service-account-7f7af1f9-f2cd-11e8-904b-0a58ac10ae35-pzr7c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:21:57.711: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-9rqsg" for this suite.
Nov 28 05:22:03.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:22:03.811: INFO: namespace: e2e-tests-svcaccounts-9rqsg, resource: bindings, ignored listing per whitelist
Nov 28 05:22:04.239: INFO: namespace e2e-tests-svcaccounts-9rqsg deletion completed in 6.523444755s

• [SLOW TEST:19.331 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:22:04.239: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:22:05.378: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8aacfc74-f2cd-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-sbpv8" to be "success or failure"
Nov 28 05:22:05.380: INFO: Pod "downwardapi-volume-8aacfc74-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.618568ms
Nov 28 05:22:07.384: INFO: Pod "downwardapi-volume-8aacfc74-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00621171s
Nov 28 05:22:09.388: INFO: Pod "downwardapi-volume-8aacfc74-f2cd-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009947767s
STEP: Saw pod success
Nov 28 05:22:09.388: INFO: Pod "downwardapi-volume-8aacfc74-f2cd-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:22:09.390: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod downwardapi-volume-8aacfc74-f2cd-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:22:09.410: INFO: Waiting for pod downwardapi-volume-8aacfc74-f2cd-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:22:09.412: INFO: Pod downwardapi-volume-8aacfc74-f2cd-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:22:09.412: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sbpv8" for this suite.
Nov 28 05:22:15.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:22:15.438: INFO: namespace: e2e-tests-downward-api-sbpv8, resource: bindings, ignored listing per whitelist
Nov 28 05:22:15.940: INFO: namespace e2e-tests-downward-api-sbpv8 deletion completed in 6.524086838s

• [SLOW TEST:11.701 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:22:15.940: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name configmap-test-volume-91a83c54-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:22:17.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-91a8cf49-f2cd-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-configmap-p2bxw" to be "success or failure"
Nov 28 05:22:17.096: INFO: Pod "pod-configmaps-91a8cf49-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077151ms
Nov 28 05:22:19.099: INFO: Pod "pod-configmaps-91a8cf49-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007633566s
Nov 28 05:22:21.104: INFO: Pod "pod-configmaps-91a8cf49-f2cd-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011821008s
STEP: Saw pod success
Nov 28 05:22:21.104: INFO: Pod "pod-configmaps-91a8cf49-f2cd-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:22:21.106: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-configmaps-91a8cf49-f2cd-11e8-904b-0a58ac10ae35 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:22:21.128: INFO: Waiting for pod pod-configmaps-91a8cf49-f2cd-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:22:21.130: INFO: Pod pod-configmaps-91a8cf49-f2cd-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:22:21.130: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-p2bxw" for this suite.
Nov 28 05:22:27.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:22:27.179: INFO: namespace: e2e-tests-configmap-p2bxw, resource: bindings, ignored listing per whitelist
Nov 28 05:22:27.662: INFO: namespace e2e-tests-configmap-p2bxw deletion completed in 6.528143117s

• [SLOW TEST:11.722 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:22:27.662: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:22:28.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98a8d969-f2cd-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-rrv2h" to be "success or failure"
Nov 28 05:22:28.841: INFO: Pod "downwardapi-volume-98a8d969-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.909473ms
Nov 28 05:22:30.845: INFO: Pod "downwardapi-volume-98a8d969-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008328562s
Nov 28 05:22:32.851: INFO: Pod "downwardapi-volume-98a8d969-f2cd-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013427323s
STEP: Saw pod success
Nov 28 05:22:32.851: INFO: Pod "downwardapi-volume-98a8d969-f2cd-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:22:32.854: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod downwardapi-volume-98a8d969-f2cd-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:22:32.874: INFO: Waiting for pod downwardapi-volume-98a8d969-f2cd-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:22:32.876: INFO: Pod downwardapi-volume-98a8d969-f2cd-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:22:32.876: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rrv2h" for this suite.
Nov 28 05:22:38.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:22:38.973: INFO: namespace: e2e-tests-downward-api-rrv2h, resource: bindings, ignored listing per whitelist
Nov 28 05:22:39.405: INFO: namespace e2e-tests-downward-api-rrv2h deletion completed in 6.52457763s

• [SLOW TEST:11.743 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:22:39.405: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-kth6c
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-kth6c
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-kth6c
Nov 28 05:22:39.550: INFO: Found 0 stateful pods, waiting for 1
Nov 28 05:22:49.556: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 28 05:22:49.559: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-kth6c ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 05:22:49.807: INFO: stderr: ""
Nov 28 05:22:49.807: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 05:22:49.807: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 28 05:22:49.811: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 28 05:22:59.816: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 28 05:22:59.816: INFO: Waiting for statefulset status.replicas updated to 0
Nov 28 05:22:59.832: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999608s
Nov 28 05:23:00.836: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995831772s
Nov 28 05:23:01.840: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991652895s
Nov 28 05:23:02.845: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987286727s
Nov 28 05:23:03.850: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981982017s
Nov 28 05:23:04.854: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977673243s
Nov 28 05:23:05.859: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97351515s
Nov 28 05:23:06.864: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968095637s
Nov 28 05:23:07.868: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963608347s
Nov 28 05:23:08.872: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.449918ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-kth6c
Nov 28 05:23:09.876: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-kth6c ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 28 05:23:10.091: INFO: stderr: ""
Nov 28 05:23:10.091: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 28 05:23:10.091: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 28 05:23:10.095: INFO: Found 1 stateful pods, waiting for 3
Nov 28 05:23:20.099: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:23:20.100: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:23:20.100: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 28 05:23:20.105: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-kth6c ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 05:23:20.329: INFO: stderr: ""
Nov 28 05:23:20.329: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 05:23:20.329: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 28 05:23:20.329: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-kth6c ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 05:23:20.570: INFO: stderr: ""
Nov 28 05:23:20.570: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 05:23:20.570: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 28 05:23:20.570: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-kth6c ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 05:23:20.791: INFO: stderr: ""
Nov 28 05:23:20.791: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 05:23:20.791: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 28 05:23:20.791: INFO: Waiting for statefulset status.replicas updated to 0
Nov 28 05:23:20.794: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 28 05:23:30.802: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 28 05:23:30.802: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 28 05:23:30.802: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 28 05:23:30.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999742s
Nov 28 05:23:31.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996239914s
Nov 28 05:23:32.823: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991653541s
Nov 28 05:23:33.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987167498s
Nov 28 05:23:34.832: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982510033s
Nov 28 05:23:35.837: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978086688s
Nov 28 05:23:36.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973484607s
Nov 28 05:23:37.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969428821s
Nov 28 05:23:38.849: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965067486s
Nov 28 05:23:39.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.059285ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-kth6c
Nov 28 05:23:40.857: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-kth6c ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 28 05:23:41.081: INFO: stderr: ""
Nov 28 05:23:41.081: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 28 05:23:41.081: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 28 05:23:41.081: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-kth6c ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 28 05:23:41.286: INFO: stderr: ""
Nov 28 05:23:41.286: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 28 05:23:41.286: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 28 05:23:41.286: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-kth6c ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 28 05:23:41.496: INFO: stderr: ""
Nov 28 05:23:41.496: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 28 05:23:41.496: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 28 05:23:41.496: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Nov 28 05:24:11.516: INFO: Deleting all statefulset in ns e2e-tests-statefulset-kth6c
Nov 28 05:24:11.520: INFO: Scaling statefulset ss to 0
Nov 28 05:24:11.530: INFO: Waiting for statefulset status.replicas updated to 0
Nov 28 05:24:11.533: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:24:11.550: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-kth6c" for this suite.
Nov 28 05:24:17.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:24:17.626: INFO: namespace: e2e-tests-statefulset-kth6c, resource: bindings, ignored listing per whitelist
Nov 28 05:24:18.080: INFO: namespace e2e-tests-statefulset-kth6c deletion completed in 6.525201071s

• [SLOW TEST:98.675 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:24:18.080: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:24:18.205: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig version --client'
Nov 28 05:24:18.290: INFO: stderr: ""
Nov 28 05:24:18.290: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"11+\", GitVersion:\"v1.11.6-beta.0.1+5933b9771b71c2\", GitCommit:\"5933b9771b71c2d05543a0cd088542013c1446e0\", GitTreeState:\"clean\", BuildDate:\"2018-11-28T05:03:15Z\", GoVersion:\"go1.10.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Nov 28 05:24:18.292: INFO: Not supported for server versions before "1.11.6-beta.0.1+5933b9771b71c2"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:24:18.292: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zctss" for this suite.
Nov 28 05:24:24.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:24:24.322: INFO: namespace: e2e-tests-kubectl-zctss, resource: bindings, ignored listing per whitelist
Nov 28 05:24:24.825: INFO: namespace e2e-tests-kubectl-zctss deletion completed in 6.528840689s

S [SKIPPING] [6.745 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should check if kubectl describe prints relevant information for rc and pods  [Conformance] [It]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684

    Nov 28 05:24:18.292: Not supported for server versions before "1.11.6-beta.0.1+5933b9771b71c2"

    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:305
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:24:24.825: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating secret with name projected-secret-test-de7c7907-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:24:25.011: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-de7e13e7-f2cd-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-szzmh" to be "success or failure"
Nov 28 05:24:25.016: INFO: Pod "pod-projected-secrets-de7e13e7-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.896216ms
Nov 28 05:24:27.020: INFO: Pod "pod-projected-secrets-de7e13e7-f2cd-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009000524s
Nov 28 05:24:29.024: INFO: Pod "pod-projected-secrets-de7e13e7-f2cd-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013080876s
STEP: Saw pod success
Nov 28 05:24:29.024: INFO: Pod "pod-projected-secrets-de7e13e7-f2cd-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:24:29.027: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-projected-secrets-de7e13e7-f2cd-11e8-904b-0a58ac10ae35 container secret-volume-test: <nil>
STEP: delete the pod
Nov 28 05:24:29.048: INFO: Waiting for pod pod-projected-secrets-de7e13e7-f2cd-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:24:29.050: INFO: Pod pod-projected-secrets-de7e13e7-f2cd-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:24:29.050: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-szzmh" for this suite.
Nov 28 05:24:35.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:24:35.091: INFO: namespace: e2e-tests-projected-szzmh, resource: bindings, ignored listing per whitelist
Nov 28 05:24:35.586: INFO: namespace e2e-tests-projected-szzmh deletion completed in 6.525342526s

• [SLOW TEST:10.761 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:24:35.586: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:24:35.725: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-e4e55bb6-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating configMap with name cm-test-opt-upd-e4e55bed-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e4e55bb6-f2cd-11e8-904b-0a58ac10ae35
STEP: Updating configmap cm-test-opt-upd-e4e55bed-f2cd-11e8-904b-0a58ac10ae35
STEP: Creating configMap with name cm-test-opt-create-e4e55bfe-f2cd-11e8-904b-0a58ac10ae35
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:25:54.279: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-26zkk" for this suite.
Nov 28 05:26:16.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:26:16.384: INFO: namespace: e2e-tests-projected-26zkk, resource: bindings, ignored listing per whitelist
Nov 28 05:26:16.813: INFO: namespace e2e-tests-projected-26zkk deletion completed in 22.529833048s

• [SLOW TEST:101.226 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:26:16.813: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name projected-configmap-test-volume-map-213c57f6-f2ce-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:26:16.975: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-213d1d4a-f2ce-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-4fvx5" to be "success or failure"
Nov 28 05:26:16.980: INFO: Pod "pod-projected-configmaps-213d1d4a-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 5.17427ms
Nov 28 05:26:18.984: INFO: Pod "pod-projected-configmaps-213d1d4a-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008737149s
Nov 28 05:26:20.987: INFO: Pod "pod-projected-configmaps-213d1d4a-f2ce-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012266865s
STEP: Saw pod success
Nov 28 05:26:20.987: INFO: Pod "pod-projected-configmaps-213d1d4a-f2ce-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:26:20.990: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-projected-configmaps-213d1d4a-f2ce-11e8-904b-0a58ac10ae35 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:26:21.013: INFO: Waiting for pod pod-projected-configmaps-213d1d4a-f2ce-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:26:21.015: INFO: Pod pod-projected-configmaps-213d1d4a-f2ce-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:26:21.015: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4fvx5" for this suite.
Nov 28 05:26:27.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:26:27.085: INFO: namespace: e2e-tests-projected-4fvx5, resource: bindings, ignored listing per whitelist
Nov 28 05:26:27.539: INFO: namespace e2e-tests-projected-4fvx5 deletion completed in 6.520605226s

• [SLOW TEST:10.727 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:26:27.539: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:26:27.661: INFO: Waiting up to 5m0s for pod "downwardapi-volume-279b14f4-f2ce-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-8p4pz" to be "success or failure"
Nov 28 05:26:27.668: INFO: Pod "downwardapi-volume-279b14f4-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 7.557695ms
Nov 28 05:26:29.672: INFO: Pod "downwardapi-volume-279b14f4-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011408973s
Nov 28 05:26:31.676: INFO: Pod "downwardapi-volume-279b14f4-f2ce-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014933214s
STEP: Saw pod success
Nov 28 05:26:31.676: INFO: Pod "downwardapi-volume-279b14f4-f2ce-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:26:31.678: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod downwardapi-volume-279b14f4-f2ce-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:26:31.698: INFO: Waiting for pod downwardapi-volume-279b14f4-f2ce-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:26:31.700: INFO: Pod downwardapi-volume-279b14f4-f2ce-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:26:31.700: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8p4pz" for this suite.
Nov 28 05:26:37.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:26:37.794: INFO: namespace: e2e-tests-downward-api-8p4pz, resource: bindings, ignored listing per whitelist
Nov 28 05:26:38.227: INFO: namespace e2e-tests-downward-api-8p4pz deletion completed in 6.523362344s

• [SLOW TEST:10.688 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:26:38.227: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:26:38.376: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dfd0167-f2ce-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-4z2rw" to be "success or failure"
Nov 28 05:26:38.382: INFO: Pod "downwardapi-volume-2dfd0167-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091655ms
Nov 28 05:26:40.385: INFO: Pod "downwardapi-volume-2dfd0167-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00975153s
Nov 28 05:26:42.389: INFO: Pod "downwardapi-volume-2dfd0167-f2ce-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013520976s
STEP: Saw pod success
Nov 28 05:26:42.389: INFO: Pod "downwardapi-volume-2dfd0167-f2ce-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:26:42.392: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod downwardapi-volume-2dfd0167-f2ce-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:26:42.409: INFO: Waiting for pod downwardapi-volume-2dfd0167-f2ce-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:26:42.411: INFO: Pod downwardapi-volume-2dfd0167-f2ce-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:26:42.411: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4z2rw" for this suite.
Nov 28 05:26:48.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:26:48.492: INFO: namespace: e2e-tests-projected-4z2rw, resource: bindings, ignored listing per whitelist
Nov 28 05:26:48.943: INFO: namespace e2e-tests-projected-4z2rw deletion completed in 6.528420375s

• [SLOW TEST:10.716 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:26:48.944: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-ddn4d
I1128 05:26:49.086059    6241 runners.go:177] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-ddn4d, replica count: 1
I1128 05:26:50.137470    6241 runners.go:177] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1128 05:26:51.137819    6241 runners.go:177] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1128 05:26:52.138152    6241 runners.go:177] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 28 05:26:52.246: INFO: Created: latency-svc-6ctrn
Nov 28 05:26:52.255: INFO: Got endpoints: latency-svc-6ctrn [16.543552ms]
Nov 28 05:26:52.268: INFO: Created: latency-svc-pbhzr
Nov 28 05:26:52.270: INFO: Got endpoints: latency-svc-pbhzr [15.723991ms]
Nov 28 05:26:52.273: INFO: Created: latency-svc-zjwz8
Nov 28 05:26:52.277: INFO: Created: latency-svc-p6lfm
Nov 28 05:26:52.282: INFO: Got endpoints: latency-svc-p6lfm [26.592983ms]
Nov 28 05:26:52.282: INFO: Got endpoints: latency-svc-zjwz8 [26.961333ms]
Nov 28 05:26:52.284: INFO: Created: latency-svc-s7fn2
Nov 28 05:26:52.290: INFO: Created: latency-svc-snhxh
Nov 28 05:26:52.293: INFO: Got endpoints: latency-svc-s7fn2 [38.054697ms]
Nov 28 05:26:52.297: INFO: Got endpoints: latency-svc-snhxh [42.310645ms]
Nov 28 05:26:52.297: INFO: Created: latency-svc-xtkwg
Nov 28 05:26:52.304: INFO: Created: latency-svc-r78mk
Nov 28 05:26:52.306: INFO: Got endpoints: latency-svc-xtkwg [50.841184ms]
Nov 28 05:26:52.309: INFO: Got endpoints: latency-svc-r78mk [53.788786ms]
Nov 28 05:26:52.309: INFO: Created: latency-svc-8p5f2
Nov 28 05:26:52.316: INFO: Got endpoints: latency-svc-8p5f2 [60.806868ms]
Nov 28 05:26:52.317: INFO: Created: latency-svc-j5rcs
Nov 28 05:26:52.319: INFO: Got endpoints: latency-svc-j5rcs [63.741078ms]
Nov 28 05:26:52.323: INFO: Created: latency-svc-tlbt2
Nov 28 05:26:52.323: INFO: Created: latency-svc-mfwz2
Nov 28 05:26:52.329: INFO: Got endpoints: latency-svc-mfwz2 [74.276632ms]
Nov 28 05:26:52.329: INFO: Got endpoints: latency-svc-tlbt2 [74.221829ms]
Nov 28 05:26:52.332: INFO: Created: latency-svc-j2lbz
Nov 28 05:26:52.337: INFO: Created: latency-svc-4v9hn
Nov 28 05:26:52.337: INFO: Got endpoints: latency-svc-j2lbz [82.40855ms]
Nov 28 05:26:52.341: INFO: Got endpoints: latency-svc-4v9hn [85.911879ms]
Nov 28 05:26:52.345: INFO: Created: latency-svc-jm5v7
Nov 28 05:26:52.347: INFO: Got endpoints: latency-svc-jm5v7 [92.434987ms]
Nov 28 05:26:52.351: INFO: Created: latency-svc-wvwq4
Nov 28 05:26:52.354: INFO: Got endpoints: latency-svc-wvwq4 [98.855943ms]
Nov 28 05:26:52.359: INFO: Created: latency-svc-m5jbf
Nov 28 05:26:52.364: INFO: Created: latency-svc-sp8r5
Nov 28 05:26:52.365: INFO: Got endpoints: latency-svc-m5jbf [94.110555ms]
Nov 28 05:26:52.366: INFO: Got endpoints: latency-svc-sp8r5 [84.592966ms]
Nov 28 05:26:52.372: INFO: Created: latency-svc-bgc2w
Nov 28 05:26:52.376: INFO: Got endpoints: latency-svc-bgc2w [93.832459ms]
Nov 28 05:26:52.376: INFO: Created: latency-svc-57jpb
Nov 28 05:26:52.380: INFO: Created: latency-svc-7gpsd
Nov 28 05:26:52.383: INFO: Got endpoints: latency-svc-57jpb [90.158676ms]
Nov 28 05:26:52.385: INFO: Got endpoints: latency-svc-7gpsd [87.604545ms]
Nov 28 05:26:52.386: INFO: Created: latency-svc-x2xsv
Nov 28 05:26:52.390: INFO: Got endpoints: latency-svc-x2xsv [84.402019ms]
Nov 28 05:26:52.396: INFO: Created: latency-svc-6r8qh
Nov 28 05:26:52.398: INFO: Got endpoints: latency-svc-6r8qh [89.540858ms]
Nov 28 05:26:52.401: INFO: Created: latency-svc-hlhvv
Nov 28 05:26:52.405: INFO: Created: latency-svc-w2sgf
Nov 28 05:26:52.408: INFO: Got endpoints: latency-svc-hlhvv [91.863332ms]
Nov 28 05:26:52.414: INFO: Created: latency-svc-6vnmf
Nov 28 05:26:52.416: INFO: Got endpoints: latency-svc-w2sgf [97.567621ms]
Nov 28 05:26:52.417: INFO: Created: latency-svc-tt7dg
Nov 28 05:26:52.421: INFO: Got endpoints: latency-svc-tt7dg [13.168293ms]
Nov 28 05:26:52.421: INFO: Got endpoints: latency-svc-6vnmf [91.658927ms]
Nov 28 05:26:52.423: INFO: Created: latency-svc-zmmzk
Nov 28 05:26:52.429: INFO: Created: latency-svc-6nszd
Nov 28 05:26:52.430: INFO: Got endpoints: latency-svc-zmmzk [100.719913ms]
Nov 28 05:26:52.433: INFO: Created: latency-svc-nsz2r
Nov 28 05:26:52.435: INFO: Got endpoints: latency-svc-6nszd [97.035168ms]
Nov 28 05:26:52.438: INFO: Created: latency-svc-6vnxc
Nov 28 05:26:52.439: INFO: Got endpoints: latency-svc-nsz2r [97.84317ms]
Nov 28 05:26:52.445: INFO: Got endpoints: latency-svc-6vnxc [97.064407ms]
Nov 28 05:26:52.445: INFO: Created: latency-svc-6s6gf
Nov 28 05:26:52.451: INFO: Got endpoints: latency-svc-6s6gf [97.450514ms]
Nov 28 05:26:52.454: INFO: Created: latency-svc-brr9d
Nov 28 05:26:52.459: INFO: Got endpoints: latency-svc-brr9d [94.669359ms]
Nov 28 05:26:52.462: INFO: Created: latency-svc-4t942
Nov 28 05:26:52.466: INFO: Got endpoints: latency-svc-4t942 [99.861331ms]
Nov 28 05:26:52.472: INFO: Created: latency-svc-vwbhx
Nov 28 05:26:52.472: INFO: Created: latency-svc-mldwd
Nov 28 05:26:52.481: INFO: Created: latency-svc-7jf9c
Nov 28 05:26:52.483: INFO: Got endpoints: latency-svc-mldwd [107.537992ms]
Nov 28 05:26:52.483: INFO: Got endpoints: latency-svc-vwbhx [99.89004ms]
Nov 28 05:26:52.492: INFO: Created: latency-svc-5g45z
Nov 28 05:26:52.496: INFO: Got endpoints: latency-svc-7jf9c [111.225532ms]
Nov 28 05:26:52.498: INFO: Created: latency-svc-nfkhj
Nov 28 05:26:52.499: INFO: Got endpoints: latency-svc-5g45z [108.937869ms]
Nov 28 05:26:52.503: INFO: Created: latency-svc-v6pk7
Nov 28 05:26:52.508: INFO: Got endpoints: latency-svc-nfkhj [109.054924ms]
Nov 28 05:26:52.508: INFO: Got endpoints: latency-svc-v6pk7 [91.885008ms]
Nov 28 05:26:52.516: INFO: Created: latency-svc-rgq5g
Nov 28 05:26:52.518: INFO: Created: latency-svc-bj648
Nov 28 05:26:52.519: INFO: Got endpoints: latency-svc-rgq5g [97.96302ms]
Nov 28 05:26:52.523: INFO: Created: latency-svc-4x6db
Nov 28 05:26:52.525: INFO: Got endpoints: latency-svc-bj648 [104.463604ms]
Nov 28 05:26:52.528: INFO: Created: latency-svc-dq6x8
Nov 28 05:26:52.535: INFO: Got endpoints: latency-svc-dq6x8 [100.681937ms]
Nov 28 05:26:52.535: INFO: Got endpoints: latency-svc-4x6db [105.290697ms]
Nov 28 05:26:52.537: INFO: Created: latency-svc-6b5q2
Nov 28 05:26:52.540: INFO: Got endpoints: latency-svc-6b5q2 [101.241317ms]
Nov 28 05:26:52.540: INFO: Created: latency-svc-g2xp4
Nov 28 05:26:52.544: INFO: Got endpoints: latency-svc-g2xp4 [99.303059ms]
Nov 28 05:26:52.545: INFO: Created: latency-svc-bgzzl
Nov 28 05:26:52.548: INFO: Got endpoints: latency-svc-bgzzl [96.324272ms]
Nov 28 05:26:52.550: INFO: Created: latency-svc-jgbq8
Nov 28 05:26:52.560: INFO: Got endpoints: latency-svc-jgbq8 [100.6806ms]
Nov 28 05:26:52.560: INFO: Created: latency-svc-csd8k
Nov 28 05:26:52.574: INFO: Created: latency-svc-k44sp
Nov 28 05:26:52.575: INFO: Got endpoints: latency-svc-csd8k [108.259031ms]
Nov 28 05:26:52.582: INFO: Got endpoints: latency-svc-k44sp [99.004835ms]
Nov 28 05:26:52.583: INFO: Created: latency-svc-j4r77
Nov 28 05:26:52.588: INFO: Created: latency-svc-q2f2q
Nov 28 05:26:52.590: INFO: Got endpoints: latency-svc-j4r77 [106.91624ms]
Nov 28 05:26:52.592: INFO: Got endpoints: latency-svc-q2f2q [95.83389ms]
Nov 28 05:26:52.596: INFO: Created: latency-svc-c5lw5
Nov 28 05:26:52.601: INFO: Got endpoints: latency-svc-c5lw5 [102.314154ms]
Nov 28 05:26:52.603: INFO: Created: latency-svc-9jhch
Nov 28 05:26:52.608: INFO: Got endpoints: latency-svc-9jhch [100.403641ms]
Nov 28 05:26:52.611: INFO: Created: latency-svc-xxsjl
Nov 28 05:26:52.617: INFO: Got endpoints: latency-svc-xxsjl [108.803672ms]
Nov 28 05:26:52.617: INFO: Created: latency-svc-zsf4c
Nov 28 05:26:52.622: INFO: Created: latency-svc-v676w
Nov 28 05:26:52.625: INFO: Got endpoints: latency-svc-zsf4c [106.04401ms]
Nov 28 05:26:52.629: INFO: Got endpoints: latency-svc-v676w [103.577823ms]
Nov 28 05:26:52.630: INFO: Created: latency-svc-4n9fq
Nov 28 05:26:52.640: INFO: Got endpoints: latency-svc-4n9fq [104.870945ms]
Nov 28 05:26:52.645: INFO: Created: latency-svc-lpwzf
Nov 28 05:26:52.649: INFO: Got endpoints: latency-svc-lpwzf [114.124338ms]
Nov 28 05:26:52.651: INFO: Created: latency-svc-8xf2l
Nov 28 05:26:52.658: INFO: Created: latency-svc-mk57z
Nov 28 05:26:52.658: INFO: Got endpoints: latency-svc-8xf2l [117.847341ms]
Nov 28 05:26:52.664: INFO: Created: latency-svc-k9n26
Nov 28 05:26:52.665: INFO: Got endpoints: latency-svc-mk57z [120.763869ms]
Nov 28 05:26:52.670: INFO: Created: latency-svc-bqbfx
Nov 28 05:26:52.673: INFO: Got endpoints: latency-svc-k9n26 [124.635146ms]
Nov 28 05:26:52.673: INFO: Got endpoints: latency-svc-bqbfx [112.674707ms]
Nov 28 05:26:52.676: INFO: Created: latency-svc-pxnwx
Nov 28 05:26:52.679: INFO: Got endpoints: latency-svc-pxnwx [103.835205ms]
Nov 28 05:26:52.682: INFO: Created: latency-svc-5gk4s
Nov 28 05:26:52.685: INFO: Created: latency-svc-zgz8r
Nov 28 05:26:52.686: INFO: Got endpoints: latency-svc-5gk4s [104.029284ms]
Nov 28 05:26:52.691: INFO: Created: latency-svc-7p72j
Nov 28 05:26:52.691: INFO: Got endpoints: latency-svc-zgz8r [101.251314ms]
Nov 28 05:26:52.696: INFO: Got endpoints: latency-svc-7p72j [103.367622ms]
Nov 28 05:26:52.696: INFO: Created: latency-svc-q4hxh
Nov 28 05:26:52.702: INFO: Created: latency-svc-9tsns
Nov 28 05:26:52.702: INFO: Got endpoints: latency-svc-q4hxh [100.522588ms]
Nov 28 05:26:52.707: INFO: Created: latency-svc-mc9lb
Nov 28 05:26:52.712: INFO: Got endpoints: latency-svc-9tsns [104.103524ms]
Nov 28 05:26:52.714: INFO: Created: latency-svc-5lrq8
Nov 28 05:26:52.715: INFO: Got endpoints: latency-svc-mc9lb [97.612249ms]
Nov 28 05:26:52.718: INFO: Got endpoints: latency-svc-5lrq8 [88.882294ms]
Nov 28 05:26:52.720: INFO: Created: latency-svc-c4d54
Nov 28 05:26:52.723: INFO: Got endpoints: latency-svc-c4d54 [97.853011ms]
Nov 28 05:26:52.726: INFO: Created: latency-svc-6h7rl
Nov 28 05:26:52.729: INFO: Got endpoints: latency-svc-6h7rl [89.127794ms]
Nov 28 05:26:52.730: INFO: Created: latency-svc-flkqq
Nov 28 05:26:52.733: INFO: Got endpoints: latency-svc-flkqq [82.99795ms]
Nov 28 05:26:52.735: INFO: Created: latency-svc-wtd8v
Nov 28 05:26:52.739: INFO: Got endpoints: latency-svc-wtd8v [81.124756ms]
Nov 28 05:26:52.740: INFO: Created: latency-svc-qpmjv
Nov 28 05:26:52.743: INFO: Created: latency-svc-xpf4s
Nov 28 05:26:52.744: INFO: Got endpoints: latency-svc-qpmjv [79.664858ms]
Nov 28 05:26:52.748: INFO: Got endpoints: latency-svc-xpf4s [75.034465ms]
Nov 28 05:26:52.753: INFO: Created: latency-svc-fsxp5
Nov 28 05:26:52.754: INFO: Got endpoints: latency-svc-fsxp5 [80.960391ms]
Nov 28 05:26:52.757: INFO: Created: latency-svc-4q4zf
Nov 28 05:26:52.763: INFO: Got endpoints: latency-svc-4q4zf [84.234665ms]
Nov 28 05:26:52.766: INFO: Created: latency-svc-7qlkd
Nov 28 05:26:52.771: INFO: Got endpoints: latency-svc-7qlkd [84.19601ms]
Nov 28 05:26:52.773: INFO: Created: latency-svc-m8q7m
Nov 28 05:26:52.776: INFO: Created: latency-svc-nplv7
Nov 28 05:26:52.778: INFO: Got endpoints: latency-svc-m8q7m [86.091765ms]
Nov 28 05:26:52.783: INFO: Got endpoints: latency-svc-nplv7 [87.536227ms]
Nov 28 05:26:52.785: INFO: Created: latency-svc-7xgxj
Nov 28 05:26:52.789: INFO: Got endpoints: latency-svc-7xgxj [87.071956ms]
Nov 28 05:26:52.792: INFO: Created: latency-svc-rkxnh
Nov 28 05:26:52.796: INFO: Created: latency-svc-w7ztp
Nov 28 05:26:52.798: INFO: Got endpoints: latency-svc-rkxnh [85.451475ms]
Nov 28 05:26:52.802: INFO: Got endpoints: latency-svc-w7ztp [87.02029ms]
Nov 28 05:26:52.804: INFO: Created: latency-svc-rdz2m
Nov 28 05:26:52.806: INFO: Got endpoints: latency-svc-rdz2m [88.387138ms]
Nov 28 05:26:52.810: INFO: Created: latency-svc-ngfll
Nov 28 05:26:52.812: INFO: Created: latency-svc-kjd8b
Nov 28 05:26:52.817: INFO: Got endpoints: latency-svc-ngfll [94.267716ms]
Nov 28 05:26:52.823: INFO: Created: latency-svc-h4h7h
Nov 28 05:26:52.829: INFO: Got endpoints: latency-svc-kjd8b [99.480968ms]
Nov 28 05:26:52.833: INFO: Got endpoints: latency-svc-h4h7h [100.64651ms]
Nov 28 05:26:52.839: INFO: Created: latency-svc-mhfdm
Nov 28 05:26:52.846: INFO: Got endpoints: latency-svc-mhfdm [106.310336ms]
Nov 28 05:26:52.849: INFO: Created: latency-svc-ll6pl
Nov 28 05:26:52.854: INFO: Got endpoints: latency-svc-ll6pl [109.853653ms]
Nov 28 05:26:52.854: INFO: Created: latency-svc-c9r7g
Nov 28 05:26:52.859: INFO: Got endpoints: latency-svc-c9r7g [111.460238ms]
Nov 28 05:26:52.861: INFO: Created: latency-svc-8rz22
Nov 28 05:26:52.868: INFO: Created: latency-svc-4rjdr
Nov 28 05:26:52.870: INFO: Got endpoints: latency-svc-8rz22 [116.428634ms]
Nov 28 05:26:52.874: INFO: Created: latency-svc-xv5rd
Nov 28 05:26:52.877: INFO: Got endpoints: latency-svc-4rjdr [113.505822ms]
Nov 28 05:26:52.879: INFO: Got endpoints: latency-svc-xv5rd [108.011085ms]
Nov 28 05:26:52.879: INFO: Created: latency-svc-dwtqx
Nov 28 05:26:52.884: INFO: Got endpoints: latency-svc-dwtqx [106.337898ms]
Nov 28 05:26:52.891: INFO: Created: latency-svc-k4tp9
Nov 28 05:26:52.892: INFO: Created: latency-svc-qwmcm
Nov 28 05:26:52.895: INFO: Got endpoints: latency-svc-k4tp9 [111.615508ms]
Nov 28 05:26:52.898: INFO: Created: latency-svc-8thjr
Nov 28 05:26:52.902: INFO: Got endpoints: latency-svc-qwmcm [112.633315ms]
Nov 28 05:26:52.902: INFO: Got endpoints: latency-svc-8thjr [104.300555ms]
Nov 28 05:26:52.905: INFO: Created: latency-svc-ldgfh
Nov 28 05:26:52.908: INFO: Created: latency-svc-brzlk
Nov 28 05:26:52.910: INFO: Got endpoints: latency-svc-ldgfh [108.451917ms]
Nov 28 05:26:52.913: INFO: Created: latency-svc-284jn
Nov 28 05:26:52.915: INFO: Got endpoints: latency-svc-brzlk [108.419378ms]
Nov 28 05:26:52.919: INFO: Got endpoints: latency-svc-284jn [101.926341ms]
Nov 28 05:26:52.922: INFO: Created: latency-svc-dgq78
Nov 28 05:26:52.925: INFO: Created: latency-svc-mxz56
Nov 28 05:26:52.926: INFO: Got endpoints: latency-svc-dgq78 [97.228964ms]
Nov 28 05:26:52.933: INFO: Created: latency-svc-sc8jm
Nov 28 05:26:52.934: INFO: Got endpoints: latency-svc-mxz56 [100.375993ms]
Nov 28 05:26:52.941: INFO: Got endpoints: latency-svc-sc8jm [95.12579ms]
Nov 28 05:26:52.947: INFO: Created: latency-svc-lc5js
Nov 28 05:26:52.949: INFO: Got endpoints: latency-svc-lc5js [94.978219ms]
Nov 28 05:26:52.950: INFO: Created: latency-svc-h8x8f
Nov 28 05:26:52.953: INFO: Got endpoints: latency-svc-h8x8f [93.821705ms]
Nov 28 05:26:52.956: INFO: Created: latency-svc-mfxs6
Nov 28 05:26:52.960: INFO: Got endpoints: latency-svc-mfxs6 [89.997035ms]
Nov 28 05:26:52.965: INFO: Created: latency-svc-72jvz
Nov 28 05:26:52.968: INFO: Got endpoints: latency-svc-72jvz [91.450659ms]
Nov 28 05:26:52.970: INFO: Created: latency-svc-fbzmm
Nov 28 05:26:52.974: INFO: Created: latency-svc-h454w
Nov 28 05:26:52.975: INFO: Got endpoints: latency-svc-fbzmm [96.107767ms]
Nov 28 05:26:52.980: INFO: Created: latency-svc-rxdfj
Nov 28 05:26:52.980: INFO: Got endpoints: latency-svc-h454w [96.289729ms]
Nov 28 05:26:52.989: INFO: Created: latency-svc-qv6bf
Nov 28 05:26:52.991: INFO: Got endpoints: latency-svc-rxdfj [96.267333ms]
Nov 28 05:26:52.994: INFO: Got endpoints: latency-svc-qv6bf [91.604837ms]
Nov 28 05:26:52.998: INFO: Created: latency-svc-n6jsj
Nov 28 05:26:53.003: INFO: Got endpoints: latency-svc-n6jsj [100.694689ms]
Nov 28 05:26:53.006: INFO: Created: latency-svc-znvjk
Nov 28 05:26:53.008: INFO: Created: latency-svc-qphml
Nov 28 05:26:53.011: INFO: Created: latency-svc-xjtwf
Nov 28 05:26:53.011: INFO: Created: latency-svc-24v5g
Nov 28 05:26:53.022: INFO: Created: latency-svc-hvwcv
Nov 28 05:26:53.023: INFO: Got endpoints: latency-svc-xjtwf [103.788911ms]
Nov 28 05:26:53.023: INFO: Got endpoints: latency-svc-24v5g [96.605042ms]
Nov 28 05:26:53.023: INFO: Got endpoints: latency-svc-znvjk [112.841534ms]
Nov 28 05:26:53.023: INFO: Got endpoints: latency-svc-qphml [108.337975ms]
Nov 28 05:26:53.026: INFO: Got endpoints: latency-svc-hvwcv [92.44084ms]
Nov 28 05:26:53.029: INFO: Created: latency-svc-7xm7g
Nov 28 05:26:53.033: INFO: Created: latency-svc-b9g9f
Nov 28 05:26:53.036: INFO: Got endpoints: latency-svc-7xm7g [95.14951ms]
Nov 28 05:26:53.038: INFO: Created: latency-svc-z4hbc
Nov 28 05:26:53.042: INFO: Created: latency-svc-hm88r
Nov 28 05:26:53.044: INFO: Got endpoints: latency-svc-z4hbc [90.483998ms]
Nov 28 05:26:53.044: INFO: Got endpoints: latency-svc-b9g9f [94.134729ms]
Nov 28 05:26:53.047: INFO: Got endpoints: latency-svc-hm88r [86.854621ms]
Nov 28 05:26:53.050: INFO: Created: latency-svc-9rnbt
Nov 28 05:26:53.053: INFO: Got endpoints: latency-svc-9rnbt [85.242348ms]
Nov 28 05:26:53.058: INFO: Created: latency-svc-snzzj
Nov 28 05:26:53.062: INFO: Created: latency-svc-srttk
Nov 28 05:26:53.064: INFO: Got endpoints: latency-svc-snzzj [89.227649ms]
Nov 28 05:26:53.068: INFO: Got endpoints: latency-svc-srttk [87.467435ms]
Nov 28 05:26:53.071: INFO: Created: latency-svc-q648q
Nov 28 05:26:53.075: INFO: Got endpoints: latency-svc-q648q [83.757174ms]
Nov 28 05:26:53.078: INFO: Created: latency-svc-hgqt9
Nov 28 05:26:53.083: INFO: Created: latency-svc-p59l8
Nov 28 05:26:53.085: INFO: Got endpoints: latency-svc-hgqt9 [90.874413ms]
Nov 28 05:26:53.092: INFO: Got endpoints: latency-svc-p59l8 [89.287187ms]
Nov 28 05:26:53.094: INFO: Created: latency-svc-8f5tt
Nov 28 05:26:53.099: INFO: Got endpoints: latency-svc-8f5tt [75.686288ms]
Nov 28 05:26:53.099: INFO: Created: latency-svc-qv85n
Nov 28 05:26:53.103: INFO: Got endpoints: latency-svc-qv85n [79.823784ms]
Nov 28 05:26:53.109: INFO: Created: latency-svc-mqskr
Nov 28 05:26:53.113: INFO: Created: latency-svc-5ddvn
Nov 28 05:26:53.113: INFO: Got endpoints: latency-svc-mqskr [90.210015ms]
Nov 28 05:26:53.117: INFO: Got endpoints: latency-svc-5ddvn [93.620438ms]
Nov 28 05:26:53.122: INFO: Created: latency-svc-l6xfn
Nov 28 05:26:53.129: INFO: Created: latency-svc-jc7bp
Nov 28 05:26:53.132: INFO: Got endpoints: latency-svc-l6xfn [105.852883ms]
Nov 28 05:26:53.136: INFO: Got endpoints: latency-svc-jc7bp [99.963929ms]
Nov 28 05:26:53.137: INFO: Created: latency-svc-j2vqn
Nov 28 05:26:53.140: INFO: Created: latency-svc-77h84
Nov 28 05:26:53.144: INFO: Got endpoints: latency-svc-77h84 [100.24328ms]
Nov 28 05:26:53.144: INFO: Got endpoints: latency-svc-j2vqn [100.258592ms]
Nov 28 05:26:53.154: INFO: Created: latency-svc-25dqf
Nov 28 05:26:53.163: INFO: Created: latency-svc-kqtjt
Nov 28 05:26:53.164: INFO: Got endpoints: latency-svc-25dqf [116.728183ms]
Nov 28 05:26:53.167: INFO: Created: latency-svc-hq7j9
Nov 28 05:26:53.168: INFO: Got endpoints: latency-svc-kqtjt [114.76693ms]
Nov 28 05:26:53.175: INFO: Got endpoints: latency-svc-hq7j9 [111.360085ms]
Nov 28 05:26:53.180: INFO: Created: latency-svc-tc2rc
Nov 28 05:26:53.184: INFO: Got endpoints: latency-svc-tc2rc [116.28061ms]
Nov 28 05:26:53.186: INFO: Created: latency-svc-rq87p
Nov 28 05:26:53.190: INFO: Got endpoints: latency-svc-rq87p [115.086688ms]
Nov 28 05:26:53.194: INFO: Created: latency-svc-2l6n6
Nov 28 05:26:53.196: INFO: Got endpoints: latency-svc-2l6n6 [111.551584ms]
Nov 28 05:26:53.199: INFO: Created: latency-svc-2mq9v
Nov 28 05:26:53.204: INFO: Created: latency-svc-fvdd7
Nov 28 05:26:53.204: INFO: Got endpoints: latency-svc-2mq9v [112.299307ms]
Nov 28 05:26:53.209: INFO: Got endpoints: latency-svc-fvdd7 [110.523531ms]
Nov 28 05:26:53.211: INFO: Created: latency-svc-v2tb7
Nov 28 05:26:53.215: INFO: Created: latency-svc-sztwf
Nov 28 05:26:53.216: INFO: Got endpoints: latency-svc-v2tb7 [112.523341ms]
Nov 28 05:26:53.220: INFO: Got endpoints: latency-svc-sztwf [106.086714ms]
Nov 28 05:26:53.221: INFO: Created: latency-svc-hmxzv
Nov 28 05:26:53.224: INFO: Got endpoints: latency-svc-hmxzv [107.131093ms]
Nov 28 05:26:53.225: INFO: Created: latency-svc-vcn4d
Nov 28 05:26:53.229: INFO: Got endpoints: latency-svc-vcn4d [96.789657ms]
Nov 28 05:26:53.236: INFO: Created: latency-svc-bppwv
Nov 28 05:26:53.242: INFO: Got endpoints: latency-svc-bppwv [106.370548ms]
Nov 28 05:26:53.244: INFO: Created: latency-svc-65m7l
Nov 28 05:26:53.247: INFO: Got endpoints: latency-svc-65m7l [103.533917ms]
Nov 28 05:26:53.250: INFO: Created: latency-svc-rxl7x
Nov 28 05:26:53.252: INFO: Got endpoints: latency-svc-rxl7x [107.893095ms]
Nov 28 05:26:53.255: INFO: Created: latency-svc-2z852
Nov 28 05:26:53.261: INFO: Got endpoints: latency-svc-2z852 [97.094867ms]
Nov 28 05:26:53.262: INFO: Created: latency-svc-bbwz6
Nov 28 05:26:53.267: INFO: Created: latency-svc-ntpxz
Nov 28 05:26:53.267: INFO: Got endpoints: latency-svc-bbwz6 [99.016211ms]
Nov 28 05:26:53.273: INFO: Created: latency-svc-9xksg
Nov 28 05:26:53.274: INFO: Got endpoints: latency-svc-ntpxz [98.958319ms]
Nov 28 05:26:53.281: INFO: Got endpoints: latency-svc-9xksg [96.431999ms]
Nov 28 05:26:53.284: INFO: Created: latency-svc-wjp6b
Nov 28 05:26:53.285: INFO: Created: latency-svc-km9gx
Nov 28 05:26:53.288: INFO: Got endpoints: latency-svc-wjp6b [97.577758ms]
Nov 28 05:26:53.294: INFO: Got endpoints: latency-svc-km9gx [97.685134ms]
Nov 28 05:26:53.295: INFO: Created: latency-svc-xdkxt
Nov 28 05:26:53.297: INFO: Got endpoints: latency-svc-xdkxt [92.809106ms]
Nov 28 05:26:53.301: INFO: Created: latency-svc-r2ffz
Nov 28 05:26:53.301: INFO: Created: latency-svc-z9hgs
Nov 28 05:26:53.303: INFO: Got endpoints: latency-svc-r2ffz [93.737326ms]
Nov 28 05:26:53.307: INFO: Got endpoints: latency-svc-z9hgs [91.09096ms]
Nov 28 05:26:53.309: INFO: Created: latency-svc-m7c9b
Nov 28 05:26:53.312: INFO: Created: latency-svc-lbsw2
Nov 28 05:26:53.313: INFO: Got endpoints: latency-svc-m7c9b [93.007453ms]
Nov 28 05:26:53.318: INFO: Got endpoints: latency-svc-lbsw2 [94.219076ms]
Nov 28 05:26:53.323: INFO: Created: latency-svc-p2cmx
Nov 28 05:26:53.323: INFO: Created: latency-svc-j8jjt
Nov 28 05:26:53.326: INFO: Got endpoints: latency-svc-p2cmx [83.992617ms]
Nov 28 05:26:53.326: INFO: Got endpoints: latency-svc-j8jjt [97.513351ms]
Nov 28 05:26:53.327: INFO: Created: latency-svc-7xl6l
Nov 28 05:26:53.333: INFO: Created: latency-svc-slzkh
Nov 28 05:26:53.335: INFO: Got endpoints: latency-svc-7xl6l [87.578064ms]
Nov 28 05:26:53.337: INFO: Got endpoints: latency-svc-slzkh [84.88452ms]
Nov 28 05:26:53.341: INFO: Created: latency-svc-h6zt4
Nov 28 05:26:53.344: INFO: Got endpoints: latency-svc-h6zt4 [82.423311ms]
Nov 28 05:26:53.346: INFO: Created: latency-svc-64t9f
Nov 28 05:26:53.349: INFO: Created: latency-svc-qb5tr
Nov 28 05:26:53.350: INFO: Got endpoints: latency-svc-64t9f [82.728326ms]
Nov 28 05:26:53.353: INFO: Got endpoints: latency-svc-qb5tr [78.735986ms]
Nov 28 05:26:53.359: INFO: Created: latency-svc-9gkcf
Nov 28 05:26:53.360: INFO: Got endpoints: latency-svc-9gkcf [79.421867ms]
Nov 28 05:26:53.362: INFO: Created: latency-svc-lwfjg
Nov 28 05:26:53.365: INFO: Created: latency-svc-q5npw
Nov 28 05:26:53.367: INFO: Got endpoints: latency-svc-lwfjg [79.683837ms]
Nov 28 05:26:53.370: INFO: Got endpoints: latency-svc-q5npw [76.174192ms]
Nov 28 05:26:53.373: INFO: Created: latency-svc-mxpq8
Nov 28 05:26:53.380: INFO: Created: latency-svc-sqqkz
Nov 28 05:26:53.382: INFO: Got endpoints: latency-svc-mxpq8 [85.04865ms]
Nov 28 05:26:53.386: INFO: Got endpoints: latency-svc-sqqkz [82.836431ms]
Nov 28 05:26:53.390: INFO: Created: latency-svc-442r7
Nov 28 05:26:53.393: INFO: Got endpoints: latency-svc-442r7 [86.425922ms]
Nov 28 05:26:53.395: INFO: Created: latency-svc-64sks
Nov 28 05:26:53.399: INFO: Created: latency-svc-vjjcp
Nov 28 05:26:53.401: INFO: Got endpoints: latency-svc-64sks [88.591061ms]
Nov 28 05:26:53.404: INFO: Got endpoints: latency-svc-vjjcp [86.029107ms]
Nov 28 05:26:53.407: INFO: Created: latency-svc-7nc8s
Nov 28 05:26:53.411: INFO: Got endpoints: latency-svc-7nc8s [84.767551ms]
Nov 28 05:26:53.419: INFO: Created: latency-svc-zkqwk
Nov 28 05:26:53.423: INFO: Got endpoints: latency-svc-zkqwk [96.225245ms]
Nov 28 05:26:53.425: INFO: Created: latency-svc-wkz5h
Nov 28 05:26:53.429: INFO: Got endpoints: latency-svc-wkz5h [92.085055ms]
Nov 28 05:26:53.429: INFO: Created: latency-svc-h65jp
Nov 28 05:26:53.434: INFO: Got endpoints: latency-svc-h65jp [99.318192ms]
Nov 28 05:26:53.436: INFO: Created: latency-svc-8dlzv
Nov 28 05:26:53.441: INFO: Got endpoints: latency-svc-8dlzv [97.664805ms]
Nov 28 05:26:53.442: INFO: Created: latency-svc-jzprh
Nov 28 05:26:53.448: INFO: Created: latency-svc-rwzbw
Nov 28 05:26:53.451: INFO: Got endpoints: latency-svc-jzprh [100.913287ms]
Nov 28 05:26:53.453: INFO: Got endpoints: latency-svc-rwzbw [99.982318ms]
Nov 28 05:26:53.455: INFO: Created: latency-svc-p6zhk
Nov 28 05:26:53.460: INFO: Created: latency-svc-b6bhd
Nov 28 05:26:53.462: INFO: Got endpoints: latency-svc-p6zhk [101.949198ms]
Nov 28 05:26:53.466: INFO: Got endpoints: latency-svc-b6bhd [98.250328ms]
Nov 28 05:26:53.467: INFO: Created: latency-svc-6g5rx
Nov 28 05:26:53.473: INFO: Got endpoints: latency-svc-6g5rx [102.9248ms]
Nov 28 05:26:53.477: INFO: Created: latency-svc-5s2wj
Nov 28 05:26:53.479: INFO: Created: latency-svc-vzqcw
Nov 28 05:26:53.487: INFO: Created: latency-svc-fdlz6
Nov 28 05:26:53.488: INFO: Got endpoints: latency-svc-vzqcw [102.27685ms]
Nov 28 05:26:53.488: INFO: Got endpoints: latency-svc-5s2wj [106.159236ms]
Nov 28 05:26:53.491: INFO: Got endpoints: latency-svc-fdlz6 [97.081068ms]
Nov 28 05:26:53.495: INFO: Created: latency-svc-x5znx
Nov 28 05:26:53.501: INFO: Got endpoints: latency-svc-x5znx [99.418112ms]
Nov 28 05:26:53.504: INFO: Created: latency-svc-qdvbk
Nov 28 05:26:53.504: INFO: Created: latency-svc-mkr7p
Nov 28 05:26:53.507: INFO: Created: latency-svc-qzbw4
Nov 28 05:26:53.509: INFO: Got endpoints: latency-svc-mkr7p [97.432027ms]
Nov 28 05:26:53.509: INFO: Got endpoints: latency-svc-qdvbk [104.539324ms]
Nov 28 05:26:53.511: INFO: Created: latency-svc-mnppm
Nov 28 05:26:53.512: INFO: Got endpoints: latency-svc-qzbw4 [88.907371ms]
Nov 28 05:26:53.515: INFO: Got endpoints: latency-svc-mnppm [86.220975ms]
Nov 28 05:26:53.517: INFO: Created: latency-svc-q6l22
Nov 28 05:26:53.521: INFO: Created: latency-svc-5wx82
Nov 28 05:26:53.523: INFO: Got endpoints: latency-svc-q6l22 [88.709766ms]
Nov 28 05:26:53.538: INFO: Got endpoints: latency-svc-5wx82 [96.337908ms]
Nov 28 05:26:53.547: INFO: Created: latency-svc-tsvp8
Nov 28 05:26:53.552: INFO: Created: latency-svc-r7znf
Nov 28 05:26:53.553: INFO: Got endpoints: latency-svc-tsvp8 [102.323123ms]
Nov 28 05:26:53.557: INFO: Got endpoints: latency-svc-r7znf [104.045774ms]
Nov 28 05:26:53.558: INFO: Created: latency-svc-kdn9j
Nov 28 05:26:53.563: INFO: Got endpoints: latency-svc-kdn9j [101.232358ms]
Nov 28 05:26:53.564: INFO: Latencies: [13.168293ms 15.723991ms 26.592983ms 26.961333ms 38.054697ms 42.310645ms 50.841184ms 53.788786ms 60.806868ms 63.741078ms 74.221829ms 74.276632ms 75.034465ms 75.686288ms 76.174192ms 78.735986ms 79.421867ms 79.664858ms 79.683837ms 79.823784ms 80.960391ms 81.124756ms 82.40855ms 82.423311ms 82.728326ms 82.836431ms 82.99795ms 83.757174ms 83.992617ms 84.19601ms 84.234665ms 84.402019ms 84.592966ms 84.767551ms 84.88452ms 85.04865ms 85.242348ms 85.451475ms 85.911879ms 86.029107ms 86.091765ms 86.220975ms 86.425922ms 86.854621ms 87.02029ms 87.071956ms 87.467435ms 87.536227ms 87.578064ms 87.604545ms 88.387138ms 88.591061ms 88.709766ms 88.882294ms 88.907371ms 89.127794ms 89.227649ms 89.287187ms 89.540858ms 89.997035ms 90.158676ms 90.210015ms 90.483998ms 90.874413ms 91.09096ms 91.450659ms 91.604837ms 91.658927ms 91.863332ms 91.885008ms 92.085055ms 92.434987ms 92.44084ms 92.809106ms 93.007453ms 93.620438ms 93.737326ms 93.821705ms 93.832459ms 94.110555ms 94.134729ms 94.219076ms 94.267716ms 94.669359ms 94.978219ms 95.12579ms 95.14951ms 95.83389ms 96.107767ms 96.225245ms 96.267333ms 96.289729ms 96.324272ms 96.337908ms 96.431999ms 96.605042ms 96.789657ms 97.035168ms 97.064407ms 97.081068ms 97.094867ms 97.228964ms 97.432027ms 97.450514ms 97.513351ms 97.567621ms 97.577758ms 97.612249ms 97.664805ms 97.685134ms 97.84317ms 97.853011ms 97.96302ms 98.250328ms 98.855943ms 98.958319ms 99.004835ms 99.016211ms 99.303059ms 99.318192ms 99.418112ms 99.480968ms 99.861331ms 99.89004ms 99.963929ms 99.982318ms 100.24328ms 100.258592ms 100.375993ms 100.403641ms 100.522588ms 100.64651ms 100.6806ms 100.681937ms 100.694689ms 100.719913ms 100.913287ms 101.232358ms 101.241317ms 101.251314ms 101.926341ms 101.949198ms 102.27685ms 102.314154ms 102.323123ms 102.9248ms 103.367622ms 103.533917ms 103.577823ms 103.788911ms 103.835205ms 104.029284ms 104.045774ms 104.103524ms 104.300555ms 104.463604ms 104.539324ms 104.870945ms 105.290697ms 105.852883ms 106.04401ms 106.086714ms 106.159236ms 106.310336ms 106.337898ms 106.370548ms 106.91624ms 107.131093ms 107.537992ms 107.893095ms 108.011085ms 108.259031ms 108.337975ms 108.419378ms 108.451917ms 108.803672ms 108.937869ms 109.054924ms 109.853653ms 110.523531ms 111.225532ms 111.360085ms 111.460238ms 111.551584ms 111.615508ms 112.299307ms 112.523341ms 112.633315ms 112.674707ms 112.841534ms 113.505822ms 114.124338ms 114.76693ms 115.086688ms 116.28061ms 116.428634ms 116.728183ms 117.847341ms 120.763869ms 124.635146ms]
Nov 28 05:26:53.564: INFO: 50 %ile: 97.094867ms
Nov 28 05:26:53.564: INFO: 90 %ile: 111.225532ms
Nov 28 05:26:53.564: INFO: 99 %ile: 120.763869ms
Nov 28 05:26:53.564: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:26:53.564: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-ddn4d" for this suite.
Nov 28 05:27:15.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:27:15.686: INFO: namespace: e2e-tests-svc-latency-ddn4d, resource: bindings, ignored listing per whitelist
Nov 28 05:27:16.090: INFO: namespace e2e-tests-svc-latency-ddn4d deletion completed in 22.521985435s

• [SLOW TEST:27.146 seconds]
[sig-network] Service endpoints latency
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:27:16.090: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:27:16.232: INFO: Waiting up to 5m0s for pod "downwardapi-volume-448e320e-f2ce-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-pvfrb" to be "success or failure"
Nov 28 05:27:16.234: INFO: Pod "downwardapi-volume-448e320e-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023828ms
Nov 28 05:27:18.237: INFO: Pod "downwardapi-volume-448e320e-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005869289s
Nov 28 05:27:20.241: INFO: Pod "downwardapi-volume-448e320e-f2ce-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009573239s
STEP: Saw pod success
Nov 28 05:27:20.241: INFO: Pod "downwardapi-volume-448e320e-f2ce-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:27:20.244: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod downwardapi-volume-448e320e-f2ce-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:27:20.287: INFO: Waiting for pod downwardapi-volume-448e320e-f2ce-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:27:20.289: INFO: Pod downwardapi-volume-448e320e-f2ce-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:27:20.289: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pvfrb" for this suite.
Nov 28 05:27:26.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:27:26.396: INFO: namespace: e2e-tests-projected-pvfrb, resource: bindings, ignored listing per whitelist
Nov 28 05:27:26.814: INFO: namespace e2e-tests-projected-pvfrb deletion completed in 6.521161374s

• [SLOW TEST:10.724 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:27:26.814: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name projected-configmap-test-volume-map-4af447d8-f2ce-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:27:26.967: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4af4cc5c-f2ce-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-ld8w7" to be "success or failure"
Nov 28 05:27:26.971: INFO: Pod "pod-projected-configmaps-4af4cc5c-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.167158ms
Nov 28 05:27:28.975: INFO: Pod "pod-projected-configmaps-4af4cc5c-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007000878s
Nov 28 05:27:30.980: INFO: Pod "pod-projected-configmaps-4af4cc5c-f2ce-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012010783s
STEP: Saw pod success
Nov 28 05:27:30.980: INFO: Pod "pod-projected-configmaps-4af4cc5c-f2ce-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:27:30.982: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-projected-configmaps-4af4cc5c-f2ce-11e8-904b-0a58ac10ae35 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:27:31.000: INFO: Waiting for pod pod-projected-configmaps-4af4cc5c-f2ce-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:27:31.002: INFO: Pod pod-projected-configmaps-4af4cc5c-f2ce-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:27:31.002: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ld8w7" for this suite.
Nov 28 05:27:37.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:27:37.053: INFO: namespace: e2e-tests-projected-ld8w7, resource: bindings, ignored listing per whitelist
Nov 28 05:27:37.529: INFO: namespace e2e-tests-projected-ld8w7 deletion completed in 6.52395519s

• [SLOW TEST:10.715 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:27:37.529: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test substitution in container's command
Nov 28 05:27:37.702: INFO: Waiting up to 5m0s for pod "var-expansion-51559d19-f2ce-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-var-expansion-6jm5r" to be "success or failure"
Nov 28 05:27:37.707: INFO: Pod "var-expansion-51559d19-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.429276ms
Nov 28 05:27:39.711: INFO: Pod "var-expansion-51559d19-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008314458s
Nov 28 05:27:41.715: INFO: Pod "var-expansion-51559d19-f2ce-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012257273s
STEP: Saw pod success
Nov 28 05:27:41.715: INFO: Pod "var-expansion-51559d19-f2ce-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:27:41.718: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod var-expansion-51559d19-f2ce-11e8-904b-0a58ac10ae35 container dapi-container: <nil>
STEP: delete the pod
Nov 28 05:27:41.738: INFO: Waiting for pod var-expansion-51559d19-f2ce-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:27:41.741: INFO: Pod var-expansion-51559d19-f2ce-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:27:41.741: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-6jm5r" for this suite.
Nov 28 05:27:47.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:27:48.227: INFO: namespace: e2e-tests-var-expansion-6jm5r, resource: bindings, ignored listing per whitelist
Nov 28 05:27:48.277: INFO: namespace e2e-tests-var-expansion-6jm5r deletion completed in 6.531259823s

• [SLOW TEST:10.747 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:27:48.277: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test override command
Nov 28 05:27:48.440: INFO: Waiting up to 5m0s for pod "client-containers-57c0d18a-f2ce-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-containers-d8g2s" to be "success or failure"
Nov 28 05:27:48.442: INFO: Pod "client-containers-57c0d18a-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349743ms
Nov 28 05:27:50.446: INFO: Pod "client-containers-57c0d18a-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006441135s
Nov 28 05:27:52.450: INFO: Pod "client-containers-57c0d18a-f2ce-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009968316s
STEP: Saw pod success
Nov 28 05:27:52.450: INFO: Pod "client-containers-57c0d18a-f2ce-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:27:52.452: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod client-containers-57c0d18a-f2ce-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:27:52.470: INFO: Waiting for pod client-containers-57c0d18a-f2ce-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:27:52.472: INFO: Pod client-containers-57c0d18a-f2ce-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:27:52.472: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-d8g2s" for this suite.
Nov 28 05:27:58.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:27:58.525: INFO: namespace: e2e-tests-containers-d8g2s, resource: bindings, ignored listing per whitelist
Nov 28 05:27:58.998: INFO: namespace e2e-tests-containers-d8g2s deletion completed in 6.522455636s

• [SLOW TEST:10.721 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:27:58.998: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 28 05:27:59.136: INFO: Waiting up to 5m0s for pod "pod-5e219530-f2ce-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-5xlxm" to be "success or failure"
Nov 28 05:27:59.140: INFO: Pod "pod-5e219530-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51807ms
Nov 28 05:28:01.143: INFO: Pod "pod-5e219530-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007124984s
Nov 28 05:28:03.147: INFO: Pod "pod-5e219530-f2ce-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010924197s
STEP: Saw pod success
Nov 28 05:28:03.147: INFO: Pod "pod-5e219530-f2ce-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:28:03.150: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-5e219530-f2ce-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:28:03.178: INFO: Waiting for pod pod-5e219530-f2ce-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:28:03.180: INFO: Pod pod-5e219530-f2ce-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:28:03.180: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5xlxm" for this suite.
Nov 28 05:28:09.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:28:09.207: INFO: namespace: e2e-tests-emptydir-5xlxm, resource: bindings, ignored listing per whitelist
Nov 28 05:28:09.707: INFO: namespace e2e-tests-emptydir-5xlxm deletion completed in 6.52283313s

• [SLOW TEST:10.708 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:28:09.707: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:28:09.843: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-648529d5-f2ce-11e8-904b-0a58ac10ae35
STEP: Creating secret with name s-test-opt-upd-64852a43-f2ce-11e8-904b-0a58ac10ae35
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-648529d5-f2ce-11e8-904b-0a58ac10ae35
STEP: Updating secret s-test-opt-upd-64852a43-f2ce-11e8-904b-0a58ac10ae35
STEP: Creating secret with name s-test-opt-create-64852a5f-f2ce-11e8-904b-0a58ac10ae35
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:29:26.384: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-cm2rj" for this suite.
Nov 28 05:29:48.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:29:48.421: INFO: namespace: e2e-tests-secrets-cm2rj, resource: bindings, ignored listing per whitelist
Nov 28 05:29:48.913: INFO: namespace e2e-tests-secrets-cm2rj deletion completed in 22.524824929s

• [SLOW TEST:99.206 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:29:48.913: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating projection with secret that has name projected-secret-test-map-9fa3cc6a-f2ce-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:29:49.057: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9fa4c22d-f2ce-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-g95lt" to be "success or failure"
Nov 28 05:29:49.062: INFO: Pod "pod-projected-secrets-9fa4c22d-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.590231ms
Nov 28 05:29:51.066: INFO: Pod "pod-projected-secrets-9fa4c22d-f2ce-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008830414s
Nov 28 05:29:53.070: INFO: Pod "pod-projected-secrets-9fa4c22d-f2ce-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012684332s
STEP: Saw pod success
Nov 28 05:29:53.070: INFO: Pod "pod-projected-secrets-9fa4c22d-f2ce-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:29:53.072: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-projected-secrets-9fa4c22d-f2ce-11e8-904b-0a58ac10ae35 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 28 05:29:53.091: INFO: Waiting for pod pod-projected-secrets-9fa4c22d-f2ce-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:29:53.093: INFO: Pod pod-projected-secrets-9fa4c22d-f2ce-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:29:53.093: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-g95lt" for this suite.
Nov 28 05:29:59.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:29:59.180: INFO: namespace: e2e-tests-projected-g95lt, resource: bindings, ignored listing per whitelist
Nov 28 05:29:59.620: INFO: namespace e2e-tests-projected-g95lt deletion completed in 6.522797445s

• [SLOW TEST:10.706 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:29:59.620: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:29:59.754: INFO: Creating ReplicaSet my-hostname-basic-a6084b9b-f2ce-11e8-904b-0a58ac10ae35
Nov 28 05:29:59.766: INFO: Pod name my-hostname-basic-a6084b9b-f2ce-11e8-904b-0a58ac10ae35: Found 0 pods out of 1
Nov 28 05:30:04.770: INFO: Pod name my-hostname-basic-a6084b9b-f2ce-11e8-904b-0a58ac10ae35: Found 1 pods out of 1
Nov 28 05:30:04.770: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a6084b9b-f2ce-11e8-904b-0a58ac10ae35" is running
Nov 28 05:30:04.772: INFO: Pod "my-hostname-basic-a6084b9b-f2ce-11e8-904b-0a58ac10ae35-gq8hk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-11-28 05:29:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-11-28 05:30:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:0001-01-01 00:00:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-11-28 05:29:59 +0000 UTC Reason: Message:}])
Nov 28 05:30:04.773: INFO: Trying to dial the pod
Nov 28 05:30:09.789: INFO: Controller my-hostname-basic-a6084b9b-f2ce-11e8-904b-0a58ac10ae35: Got expected result from replica 1 [my-hostname-basic-a6084b9b-f2ce-11e8-904b-0a58ac10ae35-gq8hk]: "my-hostname-basic-a6084b9b-f2ce-11e8-904b-0a58ac10ae35-gq8hk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:30:09.789: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-6h4hs" for this suite.
Nov 28 05:30:15.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:30:15.899: INFO: namespace: e2e-tests-replicaset-6h4hs, resource: bindings, ignored listing per whitelist
Nov 28 05:30:16.326: INFO: namespace e2e-tests-replicaset-6h4hs deletion completed in 6.532051144s

• [SLOW TEST:16.706 seconds]
[sig-apps] ReplicaSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:30:16.326: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Starting the proxy
Nov 28 05:30:16.454: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix690471865/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:30:16.552: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dvgm7" for this suite.
Nov 28 05:30:22.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:30:22.587: INFO: namespace: e2e-tests-kubectl-dvgm7, resource: bindings, ignored listing per whitelist
Nov 28 05:30:23.082: INFO: namespace e2e-tests-kubectl-dvgm7 deletion completed in 6.524986615s

• [SLOW TEST:6.756 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should support --unix-socket=/path  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:30:23.082: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1128 05:30:53.762461    6241 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 28 05:30:53.762: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:30:53.762: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2nh6g" for this suite.
Nov 28 05:30:59.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:30:59.907: INFO: namespace: e2e-tests-gc-2nh6g, resource: bindings, ignored listing per whitelist
Nov 28 05:31:00.291: INFO: namespace e2e-tests-gc-2nh6g deletion completed in 6.525425897s

• [SLOW TEST:37.209 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:31:00.291: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-pckd4
Nov 28 05:31:04.454: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-pckd4
STEP: checking the pod's current state and verifying that restartCount is present
Nov 28 05:31:04.457: INFO: Initial restart count of pod liveness-http is 0
Nov 28 05:31:20.490: INFO: Restart count of pod e2e-tests-container-probe-pckd4/liveness-http is now 1 (16.032572025s elapsed)
Nov 28 05:31:40.526: INFO: Restart count of pod e2e-tests-container-probe-pckd4/liveness-http is now 2 (36.068874953s elapsed)
Nov 28 05:32:00.563: INFO: Restart count of pod e2e-tests-container-probe-pckd4/liveness-http is now 3 (56.106119065s elapsed)
Nov 28 05:32:20.600: INFO: Restart count of pod e2e-tests-container-probe-pckd4/liveness-http is now 4 (1m16.142396116s elapsed)
Nov 28 05:33:24.725: INFO: Restart count of pod e2e-tests-container-probe-pckd4/liveness-http is now 5 (2m20.267362076s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:33:24.733: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-pckd4" for this suite.
Nov 28 05:33:30.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:33:30.835: INFO: namespace: e2e-tests-container-probe-pckd4, resource: bindings, ignored listing per whitelist
Nov 28 05:33:31.263: INFO: namespace e2e-tests-container-probe-pckd4 deletion completed in 6.526077912s

• [SLOW TEST:150.972 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:33:31.263: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:264
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating the initial replication controller
Nov 28 05:33:31.394: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:33:31.740: INFO: stderr: ""
Nov 28 05:33:31.740: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 28 05:33:31.740: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:33:31.868: INFO: stderr: ""
Nov 28 05:33:31.868: INFO: stdout: "update-demo-nautilus-2v8j4 update-demo-nautilus-zhrdk "
Nov 28 05:33:31.868: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2v8j4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:33:31.991: INFO: stderr: ""
Nov 28 05:33:31.991: INFO: stdout: ""
Nov 28 05:33:31.991: INFO: update-demo-nautilus-2v8j4 is created but not running
Nov 28 05:33:36.992: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:33:37.137: INFO: stderr: ""
Nov 28 05:33:37.137: INFO: stdout: "update-demo-nautilus-2v8j4 update-demo-nautilus-zhrdk "
Nov 28 05:33:37.137: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2v8j4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:33:37.244: INFO: stderr: ""
Nov 28 05:33:37.244: INFO: stdout: "true"
Nov 28 05:33:37.244: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2v8j4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:33:37.351: INFO: stderr: ""
Nov 28 05:33:37.351: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:33:37.351: INFO: validating pod update-demo-nautilus-2v8j4
Nov 28 05:33:37.360: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:33:37.360: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:33:37.360: INFO: update-demo-nautilus-2v8j4 is verified up and running
Nov 28 05:33:37.360: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-zhrdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:33:37.472: INFO: stderr: ""
Nov 28 05:33:37.472: INFO: stdout: "true"
Nov 28 05:33:37.472: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-zhrdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:33:37.589: INFO: stderr: ""
Nov 28 05:33:37.589: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:33:37.589: INFO: validating pod update-demo-nautilus-zhrdk
Nov 28 05:33:37.597: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:33:37.597: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:33:37.597: INFO: update-demo-nautilus-zhrdk is verified up and running
STEP: rolling-update to new replication controller
Nov 28 05:33:37.597: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:34:01.044: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 28 05:34:01.044: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 28 05:34:01.044: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:34:01.193: INFO: stderr: ""
Nov 28 05:34:01.193: INFO: stdout: "update-demo-kitten-69mcf update-demo-kitten-wkpnt "
Nov 28 05:34:01.193: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-69mcf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:34:01.355: INFO: stderr: ""
Nov 28 05:34:01.355: INFO: stdout: "true"
Nov 28 05:34:01.355: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-69mcf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:34:01.499: INFO: stderr: ""
Nov 28 05:34:01.499: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0"
Nov 28 05:34:01.499: INFO: validating pod update-demo-kitten-69mcf
Nov 28 05:34:01.509: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 28 05:34:01.509: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 28 05:34:01.509: INFO: update-demo-kitten-69mcf is verified up and running
Nov 28 05:34:01.509: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-wkpnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:34:01.665: INFO: stderr: ""
Nov 28 05:34:01.665: INFO: stdout: "true"
Nov 28 05:34:01.665: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-wkpnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-lpx4r'
Nov 28 05:34:01.816: INFO: stderr: ""
Nov 28 05:34:01.816: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0"
Nov 28 05:34:01.816: INFO: validating pod update-demo-kitten-wkpnt
Nov 28 05:34:01.825: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 28 05:34:01.825: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 28 05:34:01.825: INFO: update-demo-kitten-wkpnt is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:34:01.825: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lpx4r" for this suite.
Nov 28 05:34:23.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:34:23.965: INFO: namespace: e2e-tests-kubectl-lpx4r, resource: bindings, ignored listing per whitelist
Nov 28 05:34:24.356: INFO: namespace e2e-tests-kubectl-lpx4r deletion completed in 22.526286448s

• [SLOW TEST:53.093 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should do a rolling update of a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:34:24.356: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: validating cluster-info
Nov 28 05:34:24.474: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig cluster-info'
Nov 28 05:34:24.585: INFO: stderr: ""
Nov 28 05:34:24.585: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://internal-api.ci-op-qh5ir6tt-623b1.origin-ci-int-gce.dev.rhcloud.com:8443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:34:24.585: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zf6lx" for this suite.
Nov 28 05:34:30.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:34:30.694: INFO: namespace: e2e-tests-kubectl-zf6lx, resource: bindings, ignored listing per whitelist
Nov 28 05:34:31.117: INFO: namespace e2e-tests-kubectl-zf6lx deletion completed in 6.528143512s

• [SLOW TEST:6.761 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:34:31.117: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating secret e2e-tests-secrets-7xrv2/secret-test-47daed3b-f2cf-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:34:32.277: INFO: Waiting up to 5m0s for pod "pod-configmaps-47db6f4d-f2cf-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-secrets-7xrv2" to be "success or failure"
Nov 28 05:34:32.280: INFO: Pod "pod-configmaps-47db6f4d-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.435379ms
Nov 28 05:34:34.284: INFO: Pod "pod-configmaps-47db6f4d-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00622496s
Nov 28 05:34:36.287: INFO: Pod "pod-configmaps-47db6f4d-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009742114s
Nov 28 05:34:38.291: INFO: Pod "pod-configmaps-47db6f4d-f2cf-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013428873s
STEP: Saw pod success
Nov 28 05:34:38.291: INFO: Pod "pod-configmaps-47db6f4d-f2cf-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:34:38.293: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-configmaps-47db6f4d-f2cf-11e8-904b-0a58ac10ae35 container env-test: <nil>
STEP: delete the pod
Nov 28 05:34:38.317: INFO: Waiting for pod pod-configmaps-47db6f4d-f2cf-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:34:38.319: INFO: Pod pod-configmaps-47db6f4d-f2cf-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:34:38.319: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7xrv2" for this suite.
Nov 28 05:34:44.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:34:44.400: INFO: namespace: e2e-tests-secrets-7xrv2, resource: bindings, ignored listing per whitelist
Nov 28 05:34:44.847: INFO: namespace e2e-tests-secrets-7xrv2 deletion completed in 6.523662464s

• [SLOW TEST:13.730 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:34:44.847: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:34:44.998: INFO: (0) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 13.196528ms)
Nov 28 05:34:45.002: INFO: (1) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.001437ms)
Nov 28 05:34:45.008: INFO: (2) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.804219ms)
Nov 28 05:34:45.013: INFO: (3) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.735319ms)
Nov 28 05:34:45.017: INFO: (4) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.952729ms)
Nov 28 05:34:45.021: INFO: (5) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.685515ms)
Nov 28 05:34:45.025: INFO: (6) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.984223ms)
Nov 28 05:34:45.028: INFO: (7) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.569606ms)
Nov 28 05:34:45.032: INFO: (8) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.210114ms)
Nov 28 05:34:45.038: INFO: (9) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.172891ms)
Nov 28 05:34:45.042: INFO: (10) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.954129ms)
Nov 28 05:34:45.046: INFO: (11) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.811883ms)
Nov 28 05:34:45.050: INFO: (12) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.29383ms)
Nov 28 05:34:45.054: INFO: (13) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.94451ms)
Nov 28 05:34:45.058: INFO: (14) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.744726ms)
Nov 28 05:34:45.064: INFO: (15) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.912505ms)
Nov 28 05:34:45.070: INFO: (16) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.404441ms)
Nov 28 05:34:45.074: INFO: (17) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.784601ms)
Nov 28 05:34:45.078: INFO: (18) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.261084ms)
Nov 28 05:34:45.082: INFO: (19) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.718449ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:34:45.082: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-bmpbj" for this suite.
Nov 28 05:34:51.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:34:51.226: INFO: namespace: e2e-tests-proxy-bmpbj, resource: bindings, ignored listing per whitelist
Nov 28 05:34:51.248: INFO: namespace e2e-tests-proxy-bmpbj deletion completed in 6.162622327s

• [SLOW TEST:6.401 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:34:51.249: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Nov 28 05:34:56.413: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-53dacdb5-f2cf-11e8-904b-0a58ac10ae35", GenerateName:"", Namespace:"e2e-tests-pods-lpjsz", SelfLink:"/api/v1/namespaces/e2e-tests-pods-lpjsz/pods/pod-submit-remove-53dacdb5-f2cf-11e8-904b-0a58ac10ae35", UID:"547564d0-f2cf-11e8-b66f-42010a8e0002", ResourceVersion:"13026", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63678980092, loc:(*time.Location)(0x641f720)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"time":"380068559", "name":"foo"}, Annotations:map[string]string{"openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-zw67k", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc42209e480), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"k8s.gcr.io/nginx-slim-amd64:0.20", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zw67k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc42209e500), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc4212e8978), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"node-role.kubernetes.io/compute":"true"}, ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ci-op-qh5ir6tt-623b1-ig-n-sr0b", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc422006120), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-gxgg6"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc4212e89ac), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63678980092, loc:(*time.Location)(0x641f720)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63678980095, loc:(*time.Location)(0x641f720)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63678980092, loc:(*time.Location)(0x641f720)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.142.0.5", PodIP:"172.16.4.40", StartTime:(*v1.Time)(0xc4204885c0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc4204885e0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"k8s.gcr.io/nginx-slim-amd64:0.20", ImageID:"docker-pullable://k8s.gcr.io/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b", ContainerID:"docker://20c0d150615a2b394e988daf4eb0c7179154890e609a9065d56132b56485ba47"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Nov 28 05:35:01.431: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:35:01.434: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lpjsz" for this suite.
Nov 28 05:35:07.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:35:07.487: INFO: namespace: e2e-tests-pods-lpjsz, resource: bindings, ignored listing per whitelist
Nov 28 05:35:07.962: INFO: namespace e2e-tests-pods-lpjsz deletion completed in 6.52443209s

• [SLOW TEST:16.714 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:35:07.962: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name projected-configmap-test-volume-5dd56046-f2cf-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:35:08.173: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5dd63e8a-f2cf-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-8bkdn" to be "success or failure"
Nov 28 05:35:08.184: INFO: Pod "pod-projected-configmaps-5dd63e8a-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 10.711841ms
Nov 28 05:35:10.187: INFO: Pod "pod-projected-configmaps-5dd63e8a-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014310834s
Nov 28 05:35:12.191: INFO: Pod "pod-projected-configmaps-5dd63e8a-f2cf-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018038451s
STEP: Saw pod success
Nov 28 05:35:12.191: INFO: Pod "pod-projected-configmaps-5dd63e8a-f2cf-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:35:12.194: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-projected-configmaps-5dd63e8a-f2cf-11e8-904b-0a58ac10ae35 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:35:12.215: INFO: Waiting for pod pod-projected-configmaps-5dd63e8a-f2cf-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:35:12.217: INFO: Pod pod-projected-configmaps-5dd63e8a-f2cf-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:35:12.217: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8bkdn" for this suite.
Nov 28 05:35:18.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:35:18.299: INFO: namespace: e2e-tests-projected-8bkdn, resource: bindings, ignored listing per whitelist
Nov 28 05:35:18.747: INFO: namespace e2e-tests-projected-8bkdn deletion completed in 6.52581069s

• [SLOW TEST:10.784 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:35:18.747: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating service endpoint-test2 in namespace e2e-tests-services-f8k75
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-f8k75 to expose endpoints map[]
Nov 28 05:35:18.895: INFO: Get endpoints failed (2.391601ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Nov 28 05:35:19.902: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-f8k75 exposes endpoints map[] (1.009646183s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-f8k75
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-f8k75 to expose endpoints map[pod1:[80]]
Nov 28 05:35:23.944: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-f8k75 exposes endpoints map[pod1:[80]] (4.030539772s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-f8k75
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-f8k75 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 28 05:35:28.008: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-f8k75 exposes endpoints map[pod1:[80] pod2:[80]] (4.05203088s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-f8k75
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-f8k75 to expose endpoints map[pod2:[80]]
Nov 28 05:35:29.027: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-f8k75 exposes endpoints map[pod2:[80]] (1.013997044s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-f8k75
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-f8k75 to expose endpoints map[]
Nov 28 05:35:30.038: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-f8k75 exposes endpoints map[] (1.006813877s elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:35:30.053: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-f8k75" for this suite.
Nov 28 05:35:36.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:35:36.186: INFO: namespace: e2e-tests-services-f8k75, resource: bindings, ignored listing per whitelist
Nov 28 05:35:36.592: INFO: namespace e2e-tests-services-f8k75 deletion completed in 6.532538941s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:17.845 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:35:36.592: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name configmap-test-volume-map-6ee287c7-f2cf-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:35:37.745: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ee30ebf-f2cf-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-configmap-kpwhg" to be "success or failure"
Nov 28 05:35:37.748: INFO: Pod "pod-configmaps-6ee30ebf-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.700318ms
Nov 28 05:35:39.752: INFO: Pod "pod-configmaps-6ee30ebf-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007055304s
Nov 28 05:35:41.755: INFO: Pod "pod-configmaps-6ee30ebf-f2cf-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010453313s
STEP: Saw pod success
Nov 28 05:35:41.755: INFO: Pod "pod-configmaps-6ee30ebf-f2cf-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:35:41.758: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-configmaps-6ee30ebf-f2cf-11e8-904b-0a58ac10ae35 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:35:41.777: INFO: Waiting for pod pod-configmaps-6ee30ebf-f2cf-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:35:41.779: INFO: Pod pod-configmaps-6ee30ebf-f2cf-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:35:41.779: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-kpwhg" for this suite.
Nov 28 05:35:47.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:35:47.861: INFO: namespace: e2e-tests-configmap-kpwhg, resource: bindings, ignored listing per whitelist
Nov 28 05:35:48.306: INFO: namespace e2e-tests-configmap-kpwhg deletion completed in 6.523296923s

• [SLOW TEST:11.714 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:35:48.306: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1316
[It] should create a deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Nov 28 05:35:48.475: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-deployment --image=k8s.gcr.io/nginx-slim-amd64:0.20 --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-8h5rd'
Nov 28 05:35:48.584: INFO: stderr: ""
Nov 28 05:35:48.584: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1321
Nov 28 05:35:52.601: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-8h5rd'
Nov 28 05:35:52.718: INFO: stderr: ""
Nov 28 05:35:52.718: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:35:52.718: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8h5rd" for this suite.
Nov 28 05:36:14.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:36:14.846: INFO: namespace: e2e-tests-kubectl-8h5rd, resource: bindings, ignored listing per whitelist
Nov 28 05:36:15.249: INFO: namespace e2e-tests-kubectl-8h5rd deletion completed in 22.52735491s

• [SLOW TEST:26.943 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should create a deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:36:15.249: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name projected-configmap-test-volume-map-85ef84b9-f2cf-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:36:15.420: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-85f02d92-f2cf-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-qbqlj" to be "success or failure"
Nov 28 05:36:15.423: INFO: Pod "pod-projected-configmaps-85f02d92-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.471057ms
Nov 28 05:36:17.427: INFO: Pod "pod-projected-configmaps-85f02d92-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006771451s
Nov 28 05:36:19.431: INFO: Pod "pod-projected-configmaps-85f02d92-f2cf-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010641947s
STEP: Saw pod success
Nov 28 05:36:19.431: INFO: Pod "pod-projected-configmaps-85f02d92-f2cf-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:36:19.434: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-projected-configmaps-85f02d92-f2cf-11e8-904b-0a58ac10ae35 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:36:19.455: INFO: Waiting for pod pod-projected-configmaps-85f02d92-f2cf-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:36:19.457: INFO: Pod pod-projected-configmaps-85f02d92-f2cf-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:36:19.457: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qbqlj" for this suite.
Nov 28 05:36:25.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:36:25.606: INFO: namespace: e2e-tests-projected-qbqlj, resource: bindings, ignored listing per whitelist
Nov 28 05:36:25.985: INFO: namespace e2e-tests-projected-qbqlj deletion completed in 6.524579617s

• [SLOW TEST:10.736 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:36:25.986: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name configmap-test-volume-8c5160f5-f2cf-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:36:26.129: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c51f400-f2cf-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-configmap-p5sfm" to be "success or failure"
Nov 28 05:36:26.134: INFO: Pod "pod-configmaps-8c51f400-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 5.604728ms
Nov 28 05:36:28.138: INFO: Pod "pod-configmaps-8c51f400-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009056621s
Nov 28 05:36:30.141: INFO: Pod "pod-configmaps-8c51f400-f2cf-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012602676s
STEP: Saw pod success
Nov 28 05:36:30.141: INFO: Pod "pod-configmaps-8c51f400-f2cf-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:36:30.144: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-configmaps-8c51f400-f2cf-11e8-904b-0a58ac10ae35 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:36:30.167: INFO: Waiting for pod pod-configmaps-8c51f400-f2cf-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:36:30.169: INFO: Pod pod-configmaps-8c51f400-f2cf-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:36:30.169: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-p5sfm" for this suite.
Nov 28 05:36:36.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:36:36.233: INFO: namespace: e2e-tests-configmap-p5sfm, resource: bindings, ignored listing per whitelist
Nov 28 05:36:36.697: INFO: namespace e2e-tests-configmap-p5sfm deletion completed in 6.524406619s

• [SLOW TEST:10.712 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:36:36.697: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:36:40.912: INFO: Waiting up to 5m0s for pod "client-envvars-9521213f-f2cf-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-pods-d4sl4" to be "success or failure"
Nov 28 05:36:40.916: INFO: Pod "client-envvars-9521213f-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.28535ms
Nov 28 05:36:42.919: INFO: Pod "client-envvars-9521213f-f2cf-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006770136s
Nov 28 05:36:44.923: INFO: Pod "client-envvars-9521213f-f2cf-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010504593s
STEP: Saw pod success
Nov 28 05:36:44.923: INFO: Pod "client-envvars-9521213f-f2cf-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:36:44.926: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod client-envvars-9521213f-f2cf-11e8-904b-0a58ac10ae35 container env3cont: <nil>
STEP: delete the pod
Nov 28 05:36:44.944: INFO: Waiting for pod client-envvars-9521213f-f2cf-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:36:44.946: INFO: Pod client-envvars-9521213f-f2cf-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:36:44.947: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-d4sl4" for this suite.
Nov 28 05:37:06.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:37:06.984: INFO: namespace: e2e-tests-pods-d4sl4, resource: bindings, ignored listing per whitelist
Nov 28 05:37:07.475: INFO: namespace e2e-tests-pods-d4sl4 deletion completed in 22.524495147s

• [SLOW TEST:30.777 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:37:07.475: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 28 05:37:07.609: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vj7fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-vj7fj/configmaps/e2e-watch-test-watch-closed,UID:a50a6b25-f2cf-11e8-b66f-42010a8e0002,ResourceVersion:13899,Generation:0,CreationTimestamp:2018-11-28 05:37:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 28 05:37:07.644: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vj7fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-vj7fj/configmaps/e2e-watch-test-watch-closed,UID:a50a6b25-f2cf-11e8-b66f-42010a8e0002,ResourceVersion:13906,Generation:0,CreationTimestamp:2018-11-28 05:37:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 28 05:37:07.744: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vj7fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-vj7fj/configmaps/e2e-watch-test-watch-closed,UID:a50a6b25-f2cf-11e8-b66f-42010a8e0002,ResourceVersion:13917,Generation:0,CreationTimestamp:2018-11-28 05:37:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 28 05:37:07.745: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vj7fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-vj7fj/configmaps/e2e-watch-test-watch-closed,UID:a50a6b25-f2cf-11e8-b66f-42010a8e0002,ResourceVersion:13918,Generation:0,CreationTimestamp:2018-11-28 05:37:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:37:07.745: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-vj7fj" for this suite.
Nov 28 05:37:13.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:37:13.921: INFO: namespace: e2e-tests-watch-vj7fj, resource: bindings, ignored listing per whitelist
Nov 28 05:37:14.270: INFO: namespace e2e-tests-watch-vj7fj deletion completed in 6.521845371s

• [SLOW TEST:6.796 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:37:14.270: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-nxsqw
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-nxsqw
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-nxsqw
Nov 28 05:37:14.413: INFO: Found 0 stateful pods, waiting for 1
Nov 28 05:37:24.419: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 28 05:37:24.422: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-nxsqw ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 05:37:24.636: INFO: stderr: ""
Nov 28 05:37:24.636: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 05:37:24.636: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 28 05:37:24.639: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 28 05:37:34.645: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 28 05:37:34.645: INFO: Waiting for statefulset status.replicas updated to 0
Nov 28 05:37:34.660: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Nov 28 05:37:34.660: INFO: ss-0  ci-op-qh5ir6tt-623b1-ig-n-fcth  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  }]
Nov 28 05:37:34.660: INFO: 
Nov 28 05:37:34.660: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 28 05:37:35.664: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996797808s
Nov 28 05:37:36.668: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992709566s
Nov 28 05:37:37.672: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988505636s
Nov 28 05:37:38.676: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984387974s
Nov 28 05:37:39.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980214608s
Nov 28 05:37:40.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972305338s
Nov 28 05:37:41.694: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967491481s
Nov 28 05:37:42.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962343658s
Nov 28 05:37:43.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.594653ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-nxsqw
Nov 28 05:37:44.708: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-nxsqw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 28 05:37:44.928: INFO: stderr: ""
Nov 28 05:37:44.928: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 28 05:37:44.928: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 28 05:37:44.928: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-nxsqw ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 28 05:37:45.152: INFO: stderr: "mv: cannot stat '/tmp/index.html': No such file or directory\n"
Nov 28 05:37:45.152: INFO: stdout: ""
Nov 28 05:37:45.152: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Nov 28 05:37:45.152: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-nxsqw ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 28 05:37:45.371: INFO: stderr: "mv: cannot stat '/tmp/index.html': No such file or directory\n"
Nov 28 05:37:45.371: INFO: stdout: ""
Nov 28 05:37:45.371: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Nov 28 05:37:45.375: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Nov 28 05:37:55.381: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:37:55.381: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:37:55.381: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 28 05:37:55.384: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-nxsqw ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 05:37:55.600: INFO: stderr: ""
Nov 28 05:37:55.600: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 05:37:55.600: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 28 05:37:55.600: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-nxsqw ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 05:37:55.818: INFO: stderr: ""
Nov 28 05:37:55.818: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 05:37:55.818: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 28 05:37:55.818: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-nxsqw ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 05:37:56.027: INFO: stderr: ""
Nov 28 05:37:56.027: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 05:37:56.027: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 28 05:37:56.027: INFO: Waiting for statefulset status.replicas updated to 0
Nov 28 05:37:56.031: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 28 05:38:06.042: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 28 05:38:06.042: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 28 05:38:06.042: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 28 05:38:06.055: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Nov 28 05:38:06.055: INFO: ss-0  ci-op-qh5ir6tt-623b1-ig-n-fcth  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  }]
Nov 28 05:38:06.055: INFO: ss-1  ci-op-qh5ir6tt-623b1-ig-n-sr0b  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  }]
Nov 28 05:38:06.055: INFO: ss-2  ci-op-qh5ir6tt-623b1-ig-n-x1wp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  }]
Nov 28 05:38:06.055: INFO: 
Nov 28 05:38:06.055: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 28 05:38:07.059: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Nov 28 05:38:07.059: INFO: ss-0  ci-op-qh5ir6tt-623b1-ig-n-fcth  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  }]
Nov 28 05:38:07.059: INFO: ss-1  ci-op-qh5ir6tt-623b1-ig-n-sr0b  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  }]
Nov 28 05:38:07.059: INFO: ss-2  ci-op-qh5ir6tt-623b1-ig-n-x1wp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  }]
Nov 28 05:38:07.059: INFO: 
Nov 28 05:38:07.059: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 28 05:38:08.064: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Nov 28 05:38:08.064: INFO: ss-0  ci-op-qh5ir6tt-623b1-ig-n-fcth  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  }]
Nov 28 05:38:08.064: INFO: ss-1  ci-op-qh5ir6tt-623b1-ig-n-sr0b  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  }]
Nov 28 05:38:08.064: INFO: ss-2  ci-op-qh5ir6tt-623b1-ig-n-x1wp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  }]
Nov 28 05:38:08.064: INFO: 
Nov 28 05:38:08.064: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 28 05:38:09.069: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Nov 28 05:38:09.069: INFO: ss-0  ci-op-qh5ir6tt-623b1-ig-n-fcth  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  }]
Nov 28 05:38:09.069: INFO: ss-1  ci-op-qh5ir6tt-623b1-ig-n-sr0b  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  }]
Nov 28 05:38:09.069: INFO: ss-2  ci-op-qh5ir6tt-623b1-ig-n-x1wp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:34 +0000 UTC  }]
Nov 28 05:38:09.069: INFO: 
Nov 28 05:38:09.069: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 28 05:38:10.073: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Nov 28 05:38:10.073: INFO: ss-0  ci-op-qh5ir6tt-623b1-ig-n-fcth  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:37:14 +0000 UTC  }]
Nov 28 05:38:10.073: INFO: 
Nov 28 05:38:10.073: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 28 05:38:11.078: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977903099s
Nov 28 05:38:12.082: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973222639s
Nov 28 05:38:13.085: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969328614s
Nov 28 05:38:14.089: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.965855926s
Nov 28 05:38:15.093: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.240886ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-nxsqw
Nov 28 05:38:16.096: INFO: Scaling statefulset ss to 0
Nov 28 05:38:16.113: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Nov 28 05:38:16.115: INFO: Deleting all statefulset in ns e2e-tests-statefulset-nxsqw
Nov 28 05:38:16.121: INFO: Scaling statefulset ss to 0
Nov 28 05:38:16.129: INFO: Waiting for statefulset status.replicas updated to 0
Nov 28 05:38:16.131: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:38:16.141: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-nxsqw" for this suite.
Nov 28 05:38:22.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:38:22.173: INFO: namespace: e2e-tests-statefulset-nxsqw, resource: bindings, ignored listing per whitelist
Nov 28 05:38:22.670: INFO: namespace e2e-tests-statefulset-nxsqw deletion completed in 6.525928074s

• [SLOW TEST:68.400 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:38:22.671: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:80
Nov 28 05:38:22.796: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 28 05:39:22.853: INFO: Waiting for terminating namespaces to be deleted...
Nov 28 05:39:22.861: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 28 05:39:22.878: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 28 05:39:22.878: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Nov 28 05:39:22.881: INFO: Waiting for pods to enter Success, but no pods in "kube-system" match label map[name:e2e-image-puller]
Nov 28 05:39:22.881: INFO: 
Logging pods the kubelet thinks is on node ci-op-qh5ir6tt-623b1-ig-n-fcth before test
Nov 28 05:39:22.890: INFO: sync-ql29c from openshift-node started at 2018-11-28 04:58:01 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.890: INFO: 	Container sync ready: true, restart count 0
Nov 28 05:39:22.890: INFO: alertmanager-main-0 from openshift-monitoring started at 2018-11-28 05:01:49 +0000 UTC (3 container statuses recorded)
Nov 28 05:39:22.890: INFO: 	Container alertmanager ready: true, restart count 0
Nov 28 05:39:22.890: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 28 05:39:22.890: INFO: 	Container config-reloader ready: true, restart count 0
Nov 28 05:39:22.891: INFO: node-exporter-f2t98 from openshift-monitoring started at 2018-11-28 05:02:40 +0000 UTC (2 container statuses recorded)
Nov 28 05:39:22.891: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 28 05:39:22.891: INFO: 	Container node-exporter ready: true, restart count 0
Nov 28 05:39:22.891: INFO: ovs-srxhg from openshift-sdn started at 2018-11-28 04:58:01 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.891: INFO: 	Container openvswitch ready: true, restart count 0
Nov 28 05:39:22.891: INFO: sdn-zk2bx from openshift-sdn started at 2018-11-28 04:58:01 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.891: INFO: 	Container sdn ready: true, restart count 0
Nov 28 05:39:22.891: INFO: router-1-vtgwb from default started at 2018-11-28 04:59:03 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.891: INFO: 	Container router ready: true, restart count 0
Nov 28 05:39:22.891: INFO: prometheus-operator-6644b8cd54-pk4l6 from openshift-monitoring started at 2018-11-28 05:00:03 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.891: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 28 05:39:22.891: INFO: kube-state-metrics-7449d589bc-ktklc from openshift-monitoring started at 2018-11-28 05:02:54 +0000 UTC (3 container statuses recorded)
Nov 28 05:39:22.891: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 28 05:39:22.891: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 28 05:39:22.891: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 28 05:39:22.891: INFO: 
Logging pods the kubelet thinks is on node ci-op-qh5ir6tt-623b1-ig-n-sr0b before test
Nov 28 05:39:22.899: INFO: alertmanager-main-1 from openshift-monitoring started at 2018-11-28 05:02:09 +0000 UTC (3 container statuses recorded)
Nov 28 05:39:22.899: INFO: 	Container alertmanager ready: true, restart count 0
Nov 28 05:39:22.899: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 28 05:39:22.899: INFO: 	Container config-reloader ready: true, restart count 0
Nov 28 05:39:22.899: INFO: docker-registry-1-ldgq8 from default started at 2018-11-28 04:59:22 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.899: INFO: 	Container registry ready: true, restart count 0
Nov 28 05:39:22.899: INFO: sync-46bjz from openshift-node started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.899: INFO: 	Container sync ready: true, restart count 0
Nov 28 05:39:22.899: INFO: ovs-7p9pf from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.899: INFO: 	Container openvswitch ready: true, restart count 0
Nov 28 05:39:22.899: INFO: prometheus-k8s-0 from openshift-monitoring started at 2018-11-28 05:00:57 +0000 UTC (4 container statuses recorded)
Nov 28 05:39:22.899: INFO: 	Container prometheus ready: true, restart count 1
Nov 28 05:39:22.899: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 28 05:39:22.899: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 28 05:39:22.899: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 28 05:39:22.899: INFO: node-exporter-bx8vj from openshift-monitoring started at 2018-11-28 05:02:40 +0000 UTC (2 container statuses recorded)
Nov 28 05:39:22.899: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 28 05:39:22.899: INFO: 	Container node-exporter ready: true, restart count 0
Nov 28 05:39:22.899: INFO: sdn-7rbmb from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.899: INFO: 	Container sdn ready: true, restart count 0
Nov 28 05:39:22.899: INFO: 
Logging pods the kubelet thinks is on node ci-op-qh5ir6tt-623b1-ig-n-x1wp before test
Nov 28 05:39:22.913: INFO: sync-xtbfj from openshift-node started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.913: INFO: 	Container sync ready: true, restart count 0
Nov 28 05:39:22.913: INFO: prometheus-k8s-1 from openshift-monitoring started at 2018-11-28 05:01:26 +0000 UTC (4 container statuses recorded)
Nov 28 05:39:22.913: INFO: 	Container prometheus ready: true, restart count 1
Nov 28 05:39:22.913: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 28 05:39:22.913: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 28 05:39:22.913: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 28 05:39:22.913: INFO: node-exporter-svgfr from openshift-monitoring started at 2018-11-28 05:02:40 +0000 UTC (2 container statuses recorded)
Nov 28 05:39:22.913: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 28 05:39:22.913: INFO: 	Container node-exporter ready: true, restart count 0
Nov 28 05:39:22.913: INFO: grafana-6b9f85786f-vm5t5 from openshift-monitoring started at 2018-11-28 05:00:33 +0000 UTC (2 container statuses recorded)
Nov 28 05:39:22.913: INFO: 	Container grafana ready: true, restart count 0
Nov 28 05:39:22.913: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 28 05:39:22.913: INFO: alertmanager-main-2 from openshift-monitoring started at 2018-11-28 05:02:23 +0000 UTC (3 container statuses recorded)
Nov 28 05:39:22.913: INFO: 	Container alertmanager ready: true, restart count 0
Nov 28 05:39:22.913: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 28 05:39:22.913: INFO: 	Container config-reloader ready: true, restart count 0
Nov 28 05:39:22.913: INFO: ovs-pcctn from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.913: INFO: 	Container openvswitch ready: true, restart count 0
Nov 28 05:39:22.913: INFO: sdn-dkxph from openshift-sdn started at 2018-11-28 04:58:00 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.913: INFO: 	Container sdn ready: true, restart count 0
Nov 28 05:39:22.913: INFO: cluster-monitoring-operator-6465f8fbc7-lr66x from openshift-monitoring started at 2018-11-28 04:59:38 +0000 UTC (1 container statuses recorded)
Nov 28 05:39:22.913: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f81b9961-f2cf-11e8-904b-0a58ac10ae35 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-f81b9961-f2cf-11e8-904b-0a58ac10ae35 off the node ci-op-qh5ir6tt-623b1-ig-n-fcth
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f81b9961-f2cf-11e8-904b-0a58ac10ae35
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:39:30.992: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-zzkd5" for this suite.
Nov 28 05:39:41.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:39:41.421: INFO: namespace: e2e-tests-sched-pred-zzkd5, resource: bindings, ignored listing per whitelist
Nov 28 05:39:41.521: INFO: namespace e2e-tests-sched-pred-zzkd5 deletion completed in 10.525949137s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:71

• [SLOW TEST:78.851 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:39:41.521: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Nov 28 05:39:41.658: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=k8s.gcr.io/nginx-slim-amd64:0.20 --namespace=e2e-tests-kubectl-5ccx7'
Nov 28 05:39:41.824: INFO: stderr: ""
Nov 28 05:39:41.824: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1449
Nov 28 05:39:41.828: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-5ccx7'
Nov 28 05:39:49.517: INFO: stderr: ""
Nov 28 05:39:49.517: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:39:49.517: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5ccx7" for this suite.
Nov 28 05:39:55.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:39:55.605: INFO: namespace: e2e-tests-kubectl-5ccx7, resource: bindings, ignored listing per whitelist
Nov 28 05:39:56.044: INFO: namespace e2e-tests-kubectl-5ccx7 deletion completed in 6.52199786s

• [SLOW TEST:14.522 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:39:56.044: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:199
[It] should be submitted and removed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:39:56.186: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-p8k7v" for this suite.
Nov 28 05:40:18.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:40:18.344: INFO: namespace: e2e-tests-pods-p8k7v, resource: bindings, ignored listing per whitelist
Nov 28 05:40:18.729: INFO: namespace e2e-tests-pods-p8k7v deletion completed in 22.538727508s

• [SLOW TEST:22.685 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should be submitted and removed  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:40:18.729: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-wff6h
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a new StaefulSet
Nov 28 05:40:18.891: INFO: Found 0 stateful pods, waiting for 3
Nov 28 05:40:28.897: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:40:28.897: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:40:28.897: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 28 05:40:38.897: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:40:38.897: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:40:38.897: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/nginx-slim-amd64:0.20 to k8s.gcr.io/nginx-slim-amd64:0.21
Nov 28 05:40:38.935: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 28 05:40:48.972: INFO: Updating stateful set ss2
Nov 28 05:40:48.979: INFO: Waiting for Pod e2e-tests-statefulset-wff6h/ss2-2 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Nov 28 05:40:58.988: INFO: Waiting for Pod e2e-tests-statefulset-wff6h/ss2-2 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
STEP: Restoring Pods to the correct revision when they are deleted
Nov 28 05:41:09.028: INFO: Found 2 stateful pods, waiting for 3
Nov 28 05:41:19.034: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:41:19.034: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:41:19.034: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Nov 28 05:41:29.034: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:41:29.034: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 05:41:29.034: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 28 05:41:29.059: INFO: Updating stateful set ss2
Nov 28 05:41:29.065: INFO: Waiting for Pod e2e-tests-statefulset-wff6h/ss2-1 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Nov 28 05:41:39.073: INFO: Waiting for Pod e2e-tests-statefulset-wff6h/ss2-1 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Nov 28 05:41:49.092: INFO: Updating stateful set ss2
Nov 28 05:41:49.099: INFO: Waiting for StatefulSet e2e-tests-statefulset-wff6h/ss2 to complete update
Nov 28 05:41:49.099: INFO: Waiting for Pod e2e-tests-statefulset-wff6h/ss2-0 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Nov 28 05:41:59.108: INFO: Waiting for StatefulSet e2e-tests-statefulset-wff6h/ss2 to complete update
Nov 28 05:41:59.108: INFO: Waiting for Pod e2e-tests-statefulset-wff6h/ss2-0 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Nov 28 05:42:09.107: INFO: Deleting all statefulset in ns e2e-tests-statefulset-wff6h
Nov 28 05:42:09.110: INFO: Scaling statefulset ss2 to 0
Nov 28 05:42:39.128: INFO: Waiting for statefulset status.replicas updated to 0
Nov 28 05:42:39.131: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:42:39.146: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-wff6h" for this suite.
Nov 28 05:42:45.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:42:45.236: INFO: namespace: e2e-tests-statefulset-wff6h, resource: bindings, ignored listing per whitelist
Nov 28 05:42:45.671: INFO: namespace e2e-tests-statefulset-wff6h deletion completed in 6.521844074s

• [SLOW TEST:146.943 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:42:45.672: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: getting the auto-created API token
Nov 28 05:42:46.315: INFO: created pod pod-service-account-defaultsa
Nov 28 05:42:46.315: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 28 05:42:46.329: INFO: created pod pod-service-account-mountsa
Nov 28 05:42:46.329: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 28 05:42:46.337: INFO: created pod pod-service-account-nomountsa
Nov 28 05:42:46.337: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 28 05:42:46.346: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 28 05:42:46.346: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 28 05:42:46.358: INFO: created pod pod-service-account-mountsa-mountspec
Nov 28 05:42:46.358: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 28 05:42:46.366: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 28 05:42:46.366: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 28 05:42:46.379: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 28 05:42:46.379: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 28 05:42:46.386: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 28 05:42:46.386: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 28 05:42:46.397: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 28 05:42:46.397: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:42:46.398: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-qvgbs" for this suite.
Nov 28 05:42:52.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:42:52.530: INFO: namespace: e2e-tests-svcaccounts-qvgbs, resource: bindings, ignored listing per whitelist
Nov 28 05:42:52.927: INFO: namespace e2e-tests-svcaccounts-qvgbs deletion completed in 6.525689812s

• [SLOW TEST:7.255 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:42:52.927: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-8r8f8
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 28 05:42:53.047: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Nov 28 05:43:19.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.53:8080/dial?request=hostName&protocol=http&host=172.16.6.52&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-8r8f8 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:43:19.167: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:43:19.290: INFO: Waiting for endpoints: map[]
Nov 28 05:43:19.294: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.53:8080/dial?request=hostName&protocol=http&host=172.16.4.50&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-8r8f8 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:43:19.294: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:43:19.398: INFO: Waiting for endpoints: map[]
Nov 28 05:43:19.402: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.53:8080/dial?request=hostName&protocol=http&host=172.16.2.42&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-8r8f8 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:43:19.402: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:43:19.526: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:43:19.526: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-8r8f8" for this suite.
Nov 28 05:43:41.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:43:41.627: INFO: namespace: e2e-tests-pod-network-test-8r8f8, resource: bindings, ignored listing per whitelist
Nov 28 05:43:42.061: INFO: namespace e2e-tests-pod-network-test-8r8f8 deletion completed in 22.529568287s

• [SLOW TEST:49.134 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:43:42.061: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating secret with name secret-test-903fcedd-f2d0-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:43:42.231: INFO: Waiting up to 5m0s for pod "pod-secrets-90404634-f2d0-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-secrets-86pmv" to be "success or failure"
Nov 28 05:43:42.234: INFO: Pod "pod-secrets-90404634-f2d0-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374629ms
Nov 28 05:43:44.238: INFO: Pod "pod-secrets-90404634-f2d0-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006837128s
Nov 28 05:43:46.242: INFO: Pod "pod-secrets-90404634-f2d0-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010854312s
STEP: Saw pod success
Nov 28 05:43:46.242: INFO: Pod "pod-secrets-90404634-f2d0-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:43:46.245: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-secrets-90404634-f2d0-11e8-904b-0a58ac10ae35 container secret-env-test: <nil>
STEP: delete the pod
Nov 28 05:43:46.267: INFO: Waiting for pod pod-secrets-90404634-f2d0-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:43:46.269: INFO: Pod pod-secrets-90404634-f2d0-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:43:46.269: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-86pmv" for this suite.
Nov 28 05:43:52.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:43:52.799: INFO: namespace: e2e-tests-secrets-86pmv, resource: bindings, ignored listing per whitelist
Nov 28 05:43:52.799: INFO: namespace e2e-tests-secrets-86pmv deletion completed in 6.525938736s

• [SLOW TEST:10.737 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:30
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:43:52.799: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:43:52.928: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-96a48a91-f2d0-11e8-904b-0a58ac10ae35
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-96a48a91-f2d0-11e8-904b-0a58ac10ae35
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:45:17.456: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7ck5w" for this suite.
Nov 28 05:45:39.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:45:39.574: INFO: namespace: e2e-tests-configmap-7ck5w, resource: bindings, ignored listing per whitelist
Nov 28 05:45:39.990: INFO: namespace e2e-tests-configmap-7ck5w deletion completed in 22.525999198s

• [SLOW TEST:107.191 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:45:39.990: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:264
[It] should scale a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating a replication controller
Nov 28 05:45:40.158: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:40.516: INFO: stderr: ""
Nov 28 05:45:40.516: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 28 05:45:40.516: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:40.648: INFO: stderr: ""
Nov 28 05:45:40.648: INFO: stdout: "update-demo-nautilus-2ndxj update-demo-nautilus-j78v8 "
Nov 28 05:45:40.648: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2ndxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:40.752: INFO: stderr: ""
Nov 28 05:45:40.752: INFO: stdout: ""
Nov 28 05:45:40.752: INFO: update-demo-nautilus-2ndxj is created but not running
Nov 28 05:45:45.752: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:45.898: INFO: stderr: ""
Nov 28 05:45:45.898: INFO: stdout: "update-demo-nautilus-2ndxj update-demo-nautilus-j78v8 "
Nov 28 05:45:45.898: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2ndxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:46.015: INFO: stderr: ""
Nov 28 05:45:46.015: INFO: stdout: "true"
Nov 28 05:45:46.015: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2ndxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:46.117: INFO: stderr: ""
Nov 28 05:45:46.117: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:45:46.117: INFO: validating pod update-demo-nautilus-2ndxj
Nov 28 05:45:46.126: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:45:46.126: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:45:46.126: INFO: update-demo-nautilus-2ndxj is verified up and running
Nov 28 05:45:46.126: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-j78v8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:46.246: INFO: stderr: ""
Nov 28 05:45:46.246: INFO: stdout: "true"
Nov 28 05:45:46.246: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-j78v8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:46.366: INFO: stderr: ""
Nov 28 05:45:46.366: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:45:46.366: INFO: validating pod update-demo-nautilus-j78v8
Nov 28 05:45:46.374: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:45:46.374: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:45:46.374: INFO: update-demo-nautilus-j78v8 is verified up and running
STEP: scaling down the replication controller
Nov 28 05:45:46.374: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:47.514: INFO: stderr: ""
Nov 28 05:45:47.514: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 28 05:45:47.514: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:47.631: INFO: stderr: ""
Nov 28 05:45:47.631: INFO: stdout: "update-demo-nautilus-2ndxj update-demo-nautilus-j78v8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 28 05:45:52.631: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:52.736: INFO: stderr: ""
Nov 28 05:45:52.736: INFO: stdout: "update-demo-nautilus-2ndxj "
Nov 28 05:45:52.736: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2ndxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:52.843: INFO: stderr: ""
Nov 28 05:45:52.843: INFO: stdout: "true"
Nov 28 05:45:52.843: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2ndxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:52.954: INFO: stderr: ""
Nov 28 05:45:52.954: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:45:52.954: INFO: validating pod update-demo-nautilus-2ndxj
Nov 28 05:45:52.968: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:45:52.968: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:45:52.968: INFO: update-demo-nautilus-2ndxj is verified up and running
STEP: scaling up the replication controller
Nov 28 05:45:52.968: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:54.142: INFO: stderr: ""
Nov 28 05:45:54.142: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 28 05:45:54.142: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:54.254: INFO: stderr: ""
Nov 28 05:45:54.254: INFO: stdout: "update-demo-nautilus-2ndxj update-demo-nautilus-xw758 "
Nov 28 05:45:54.255: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2ndxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:54.373: INFO: stderr: ""
Nov 28 05:45:54.373: INFO: stdout: "true"
Nov 28 05:45:54.373: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2ndxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:54.474: INFO: stderr: ""
Nov 28 05:45:54.474: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:45:54.474: INFO: validating pod update-demo-nautilus-2ndxj
Nov 28 05:45:54.478: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:45:54.478: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:45:54.478: INFO: update-demo-nautilus-2ndxj is verified up and running
Nov 28 05:45:54.478: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-xw758 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:54.582: INFO: stderr: ""
Nov 28 05:45:54.582: INFO: stdout: ""
Nov 28 05:45:54.582: INFO: update-demo-nautilus-xw758 is created but not running
Nov 28 05:45:59.583: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:59.700: INFO: stderr: ""
Nov 28 05:45:59.700: INFO: stdout: "update-demo-nautilus-2ndxj update-demo-nautilus-xw758 "
Nov 28 05:45:59.700: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2ndxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:59.817: INFO: stderr: ""
Nov 28 05:45:59.817: INFO: stdout: "true"
Nov 28 05:45:59.817: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2ndxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:45:59.940: INFO: stderr: ""
Nov 28 05:45:59.940: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:45:59.940: INFO: validating pod update-demo-nautilus-2ndxj
Nov 28 05:45:59.946: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:45:59.946: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:45:59.946: INFO: update-demo-nautilus-2ndxj is verified up and running
Nov 28 05:45:59.946: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-xw758 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:46:00.068: INFO: stderr: ""
Nov 28 05:46:00.068: INFO: stdout: "true"
Nov 28 05:46:00.068: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-xw758 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:46:00.178: INFO: stderr: ""
Nov 28 05:46:00.178: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0"
Nov 28 05:46:00.178: INFO: validating pod update-demo-nautilus-xw758
Nov 28 05:46:00.187: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 28 05:46:00.187: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 28 05:46:00.187: INFO: update-demo-nautilus-xw758 is verified up and running
STEP: using delete to clean up resources
Nov 28 05:46:00.187: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:46:00.310: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 05:46:00.310: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 28 05:46:00.310: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-trrjn'
Nov 28 05:46:00.439: INFO: stderr: "No resources found.\n"
Nov 28 05:46:00.439: INFO: stdout: ""
Nov 28 05:46:00.439: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=e2e-tests-kubectl-trrjn -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 28 05:46:00.559: INFO: stderr: ""
Nov 28 05:46:00.559: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:46:00.559: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-trrjn" for this suite.
Nov 28 05:46:06.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:46:06.621: INFO: namespace: e2e-tests-kubectl-trrjn, resource: bindings, ignored listing per whitelist
Nov 28 05:46:07.087: INFO: namespace e2e-tests-kubectl-trrjn deletion completed in 6.524354489s

• [SLOW TEST:27.097 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should scale a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:46:07.087: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:36
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test hostPath mode
Nov 28 05:46:07.237: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-8wr58" to be "success or failure"
Nov 28 05:46:07.244: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.677718ms
Nov 28 05:46:09.248: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010375222s
Nov 28 05:46:11.251: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014038979s
STEP: Saw pod success
Nov 28 05:46:11.251: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Nov 28 05:46:11.254: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Nov 28 05:46:11.274: INFO: Waiting for pod pod-host-path-test to disappear
Nov 28 05:46:11.276: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:46:11.276: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-8wr58" for this suite.
Nov 28 05:46:17.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:46:17.331: INFO: namespace: e2e-tests-hostpath-8wr58, resource: bindings, ignored listing per whitelist
Nov 28 05:46:17.804: INFO: namespace e2e-tests-hostpath-8wr58 deletion completed in 6.524261212s

• [SLOW TEST:10.717 seconds]
[sig-storage] HostPath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:33
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:46:17.805: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1053
STEP: creating an rc
Nov 28 05:46:17.943: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-49t4r'
Nov 28 05:46:18.167: INFO: stderr: ""
Nov 28 05:46:18.167: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Waiting for Redis master to start.
Nov 28 05:46:19.172: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:19.172: INFO: Found 0 / 1
Nov 28 05:46:20.171: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:20.171: INFO: Found 0 / 1
Nov 28 05:46:21.171: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:21.171: INFO: Found 0 / 1
Nov 28 05:46:22.170: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:22.171: INFO: Found 1 / 1
Nov 28 05:46:22.171: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 28 05:46:22.174: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:22.174: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Nov 28 05:46:22.174: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs redis-master-fvdm2 redis-master --namespace=e2e-tests-kubectl-49t4r'
Nov 28 05:46:22.298: INFO: stderr: ""
Nov 28 05:46:22.298: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Nov 05:46:21.361 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Nov 05:46:21.361 # Server started, Redis version 3.2.12\n1:M 28 Nov 05:46:21.361 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Nov 05:46:21.361 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Nov 28 05:46:22.298: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-fvdm2 redis-master --namespace=e2e-tests-kubectl-49t4r --tail=1'
Nov 28 05:46:22.430: INFO: stderr: ""
Nov 28 05:46:22.430: INFO: stdout: "1:M 28 Nov 05:46:21.361 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Nov 28 05:46:22.430: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-fvdm2 redis-master --namespace=e2e-tests-kubectl-49t4r --limit-bytes=1'
Nov 28 05:46:22.561: INFO: stderr: ""
Nov 28 05:46:22.561: INFO: stdout: " "
STEP: exposing timestamps
Nov 28 05:46:22.561: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-fvdm2 redis-master --namespace=e2e-tests-kubectl-49t4r --tail=1 --timestamps'
Nov 28 05:46:22.692: INFO: stderr: ""
Nov 28 05:46:22.692: INFO: stdout: "2018-11-28T05:46:21.362176165Z 1:M 28 Nov 05:46:21.361 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Nov 28 05:46:25.192: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-fvdm2 redis-master --namespace=e2e-tests-kubectl-49t4r --since=1s'
Nov 28 05:46:25.338: INFO: stderr: ""
Nov 28 05:46:25.338: INFO: stdout: ""
Nov 28 05:46:25.338: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-fvdm2 redis-master --namespace=e2e-tests-kubectl-49t4r --since=24h'
Nov 28 05:46:25.470: INFO: stderr: ""
Nov 28 05:46:25.470: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Nov 05:46:21.361 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Nov 05:46:21.361 # Server started, Redis version 3.2.12\n1:M 28 Nov 05:46:21.361 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Nov 05:46:21.361 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1058
STEP: using delete to clean up resources
Nov 28 05:46:25.470: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-49t4r'
Nov 28 05:46:25.578: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 05:46:25.578: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Nov 28 05:46:25.578: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-49t4r'
Nov 28 05:46:25.681: INFO: stderr: "No resources found.\n"
Nov 28 05:46:25.681: INFO: stdout: ""
Nov 28 05:46:25.681: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=nginx --namespace=e2e-tests-kubectl-49t4r -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 28 05:46:25.784: INFO: stderr: ""
Nov 28 05:46:25.784: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:46:25.785: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-49t4r" for this suite.
Nov 28 05:46:31.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:46:31.809: INFO: namespace: e2e-tests-kubectl-49t4r, resource: bindings, ignored listing per whitelist
Nov 28 05:46:32.314: INFO: namespace e2e-tests-kubectl-49t4r deletion completed in 6.524608193s

• [SLOW TEST:14.509 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should be able to retrieve and filter logs  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:46:32.314: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating Redis RC
Nov 28 05:46:32.445: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-7k2xh'
Nov 28 05:46:32.722: INFO: stderr: ""
Nov 28 05:46:32.722: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 28 05:46:33.726: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:33.726: INFO: Found 0 / 1
Nov 28 05:46:34.726: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:34.726: INFO: Found 0 / 1
Nov 28 05:46:35.726: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:35.726: INFO: Found 0 / 1
Nov 28 05:46:36.726: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:36.726: INFO: Found 1 / 1
Nov 28 05:46:36.726: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 28 05:46:36.730: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:36.730: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 28 05:46:36.730: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig patch pod redis-master-7wl5s --namespace=e2e-tests-kubectl-7k2xh -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 28 05:46:36.873: INFO: stderr: ""
Nov 28 05:46:36.873: INFO: stdout: "pod/redis-master-7wl5s patched\n"
STEP: checking annotations
Nov 28 05:46:36.877: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 05:46:36.877: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:46:36.877: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7k2xh" for this suite.
Nov 28 05:46:58.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:46:58.990: INFO: namespace: e2e-tests-kubectl-7k2xh, resource: bindings, ignored listing per whitelist
Nov 28 05:46:59.410: INFO: namespace e2e-tests-kubectl-7k2xh deletion completed in 22.528761686s

• [SLOW TEST:27.096 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should add annotations for pods in rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:46:59.410: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 28 05:47:00.612: INFO: Waiting up to 5m0s for pod "pod-05e82e73-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-fp4cr" to be "success or failure"
Nov 28 05:47:00.616: INFO: Pod "pod-05e82e73-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.137521ms
Nov 28 05:47:02.619: INFO: Pod "pod-05e82e73-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006925359s
Nov 28 05:47:04.623: INFO: Pod "pod-05e82e73-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010420828s
STEP: Saw pod success
Nov 28 05:47:04.623: INFO: Pod "pod-05e82e73-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:47:04.627: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-05e82e73-f2d1-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:47:04.649: INFO: Waiting for pod pod-05e82e73-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:47:04.652: INFO: Pod pod-05e82e73-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:47:04.652: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fp4cr" for this suite.
Nov 28 05:47:10.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:47:11.128: INFO: namespace: e2e-tests-emptydir-fp4cr, resource: bindings, ignored listing per whitelist
Nov 28 05:47:11.179: INFO: namespace e2e-tests-emptydir-fp4cr deletion completed in 6.522696692s

• [SLOW TEST:11.769 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:47:11.179: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:47:12.321: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ce26ed2-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-p89zx" to be "success or failure"
Nov 28 05:47:12.323: INFO: Pod "downwardapi-volume-0ce26ed2-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.394252ms
Nov 28 05:47:14.327: INFO: Pod "downwardapi-volume-0ce26ed2-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00655241s
Nov 28 05:47:16.331: INFO: Pod "downwardapi-volume-0ce26ed2-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010800602s
STEP: Saw pod success
Nov 28 05:47:16.332: INFO: Pod "downwardapi-volume-0ce26ed2-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:47:16.334: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod downwardapi-volume-0ce26ed2-f2d1-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:47:16.354: INFO: Waiting for pod downwardapi-volume-0ce26ed2-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:47:16.356: INFO: Pod downwardapi-volume-0ce26ed2-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:47:16.356: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p89zx" for this suite.
Nov 28 05:47:22.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:47:22.463: INFO: namespace: e2e-tests-projected-p89zx, resource: bindings, ignored listing per whitelist
Nov 28 05:47:22.884: INFO: namespace e2e-tests-projected-p89zx deletion completed in 6.524685309s

• [SLOW TEST:11.706 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:47:22.885: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating server pod server in namespace e2e-tests-prestop-msjpg
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-msjpg
STEP: Deleting pre-stop pod
Nov 28 05:47:36.082: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:47:36.087: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-msjpg" for this suite.
Nov 28 05:48:14.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:48:14.514: INFO: namespace: e2e-tests-prestop-msjpg, resource: bindings, ignored listing per whitelist
Nov 28 05:48:14.614: INFO: namespace e2e-tests-prestop-msjpg deletion completed in 38.52350807s

• [SLOW TEST:51.730 seconds]
[k8s.io] [sig-node] PreStop
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:48:14.615: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-mb5rk.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-mb5rk.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-mb5rk.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-mb5rk.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-mb5rk.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-mb5rk.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 28 05:48:34.843: INFO: DNS probes using dns-test-32b159b5-f2d1-11e8-904b-0a58ac10ae35 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:48:34.852: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-mb5rk" for this suite.
Nov 28 05:48:40.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:48:40.962: INFO: namespace: e2e-tests-dns-mb5rk, resource: bindings, ignored listing per whitelist
Nov 28 05:48:41.382: INFO: namespace e2e-tests-dns-mb5rk deletion completed in 6.526131189s

• [SLOW TEST:26.768 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:48:41.383: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name configmap-test-volume-42a8be94-f2d1-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:48:41.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-42a99e38-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-configmap-lshjx" to be "success or failure"
Nov 28 05:48:41.544: INFO: Pod "pod-configmaps-42a99e38-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.182364ms
Nov 28 05:48:43.549: INFO: Pod "pod-configmaps-42a99e38-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008722356s
Nov 28 05:48:45.553: INFO: Pod "pod-configmaps-42a99e38-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012482713s
STEP: Saw pod success
Nov 28 05:48:45.553: INFO: Pod "pod-configmaps-42a99e38-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:48:45.556: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-configmaps-42a99e38-f2d1-11e8-904b-0a58ac10ae35 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:48:45.575: INFO: Waiting for pod pod-configmaps-42a99e38-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:48:45.578: INFO: Pod pod-configmaps-42a99e38-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:48:45.578: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lshjx" for this suite.
Nov 28 05:48:51.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:48:51.664: INFO: namespace: e2e-tests-configmap-lshjx, resource: bindings, ignored listing per whitelist
Nov 28 05:48:52.106: INFO: namespace e2e-tests-configmap-lshjx deletion completed in 6.523393592s

• [SLOW TEST:10.723 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed  [Flaky] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:48:52.106: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed  [Flaky] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Nov 28 05:48:57.274: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:49:10.255: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6rw2b" for this suite.
Nov 28 05:49:16.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:49:16.350: INFO: namespace: e2e-tests-pods-6rw2b, resource: bindings, ignored listing per whitelist
Nov 28 05:49:16.782: INFO: namespace e2e-tests-pods-6rw2b deletion completed in 6.522247013s

• [SLOW TEST:24.676 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  [k8s.io] Delete Grace Period
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should be submitted and removed  [Flaky] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:49:16.782: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name configmap-test-volume-map-57c3e47c-f2d1-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:49:16.959: INFO: Waiting up to 5m0s for pod "pod-configmaps-57c45dcf-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-configmap-hwc8n" to be "success or failure"
Nov 28 05:49:16.962: INFO: Pod "pod-configmaps-57c45dcf-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.022161ms
Nov 28 05:49:18.965: INFO: Pod "pod-configmaps-57c45dcf-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006559636s
Nov 28 05:49:20.969: INFO: Pod "pod-configmaps-57c45dcf-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010349284s
STEP: Saw pod success
Nov 28 05:49:20.969: INFO: Pod "pod-configmaps-57c45dcf-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:49:20.971: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-configmaps-57c45dcf-f2d1-11e8-904b-0a58ac10ae35 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:49:20.991: INFO: Waiting for pod pod-configmaps-57c45dcf-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:49:20.994: INFO: Pod pod-configmaps-57c45dcf-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:49:20.994: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hwc8n" for this suite.
Nov 28 05:49:27.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:49:27.094: INFO: namespace: e2e-tests-configmap-hwc8n, resource: bindings, ignored listing per whitelist
Nov 28 05:49:27.521: INFO: namespace e2e-tests-configmap-hwc8n deletion completed in 6.522596169s

• [SLOW TEST:10.739 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:49:27.521: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-hknxx
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 28 05:49:27.653: INFO: Waiting up to 10m0s for all (but 1) nodes to be schedulable
STEP: Creating test pods
Nov 28 05:49:51.780: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.60:8080/dial?request=hostName&protocol=udp&host=172.16.6.59&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-hknxx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:49:51.781: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:49:51.906: INFO: Waiting for endpoints: map[]
Nov 28 05:49:51.910: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.60:8080/dial?request=hostName&protocol=udp&host=172.16.4.56&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-hknxx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:49:51.910: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:49:52.014: INFO: Waiting for endpoints: map[]
Nov 28 05:49:52.017: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.6.60:8080/dial?request=hostName&protocol=udp&host=172.16.2.49&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-hknxx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 28 05:49:52.017: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Nov 28 05:49:52.119: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:49:52.119: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-hknxx" for this suite.
Nov 28 05:50:14.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:50:14.247: INFO: namespace: e2e-tests-pod-network-test-hknxx, resource: bindings, ignored listing per whitelist
Nov 28 05:50:14.647: INFO: namespace e2e-tests-pod-network-test-hknxx deletion completed in 22.523558413s

• [SLOW TEST:47.126 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:50:14.648: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 28 05:50:14.821: INFO: Number of nodes with available pods: 0
Nov 28 05:50:14.821: INFO: Node ci-op-qh5ir6tt-623b1-ig-m-jq6h is running more than one daemon pod
Nov 28 05:50:15.829: INFO: Number of nodes with available pods: 0
Nov 28 05:50:15.829: INFO: Node ci-op-qh5ir6tt-623b1-ig-m-jq6h is running more than one daemon pod
Nov 28 05:50:16.829: INFO: Number of nodes with available pods: 0
Nov 28 05:50:16.829: INFO: Node ci-op-qh5ir6tt-623b1-ig-m-jq6h is running more than one daemon pod
Nov 28 05:50:17.829: INFO: Number of nodes with available pods: 2
Nov 28 05:50:17.829: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 05:50:18.829: INFO: Number of nodes with available pods: 4
Nov 28 05:50:18.829: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 28 05:50:18.849: INFO: Number of nodes with available pods: 3
Nov 28 05:50:18.849: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 05:50:19.857: INFO: Number of nodes with available pods: 3
Nov 28 05:50:19.857: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 05:50:20.864: INFO: Number of nodes with available pods: 3
Nov 28 05:50:20.864: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 05:50:21.857: INFO: Number of nodes with available pods: 4
Nov 28 05:50:21.857: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-9tqm6, will wait for the garbage collector to delete the pods
Nov 28 05:50:21.922: INFO: Deleting {extensions DaemonSet} daemon-set took: 6.08822ms
Nov 28 05:50:22.023: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.402352ms
Nov 28 05:50:30.327: INFO: Number of nodes with available pods: 0
Nov 28 05:50:30.327: INFO: Number of running nodes: 0, number of available pods: 0
Nov 28 05:50:30.332: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-9tqm6/daemonsets","resourceVersion":"17620"},"items":null}

Nov 28 05:50:30.335: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-9tqm6/pods","resourceVersion":"17620"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:50:30.348: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-9tqm6" for this suite.
Nov 28 05:50:36.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:50:36.420: INFO: namespace: e2e-tests-daemonsets-9tqm6, resource: bindings, ignored listing per whitelist
Nov 28 05:50:36.876: INFO: namespace e2e-tests-daemonsets-9tqm6 deletion completed in 6.524665522s

• [SLOW TEST:22.229 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:50:36.877: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
[It] should create an rc from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Nov 28 05:50:37.017: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-rc --image=k8s.gcr.io/nginx-slim-amd64:0.20 --generator=run/v1 --namespace=e2e-tests-kubectl-tggpm'
Nov 28 05:50:37.132: INFO: stderr: ""
Nov 28 05:50:37.132: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Nov 28 05:50:37.139: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-rsswv]
Nov 28 05:50:37.139: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-rsswv" in namespace "e2e-tests-kubectl-tggpm" to be "running and ready"
Nov 28 05:50:37.144: INFO: Pod "e2e-test-nginx-rc-rsswv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.639832ms
Nov 28 05:50:39.147: INFO: Pod "e2e-test-nginx-rc-rsswv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008442386s
Nov 28 05:50:41.151: INFO: Pod "e2e-test-nginx-rc-rsswv": Phase="Running", Reason="", readiness=true. Elapsed: 4.012450575s
Nov 28 05:50:41.151: INFO: Pod "e2e-test-nginx-rc-rsswv" satisfied condition "running and ready"
Nov 28 05:50:41.151: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-rsswv]
Nov 28 05:50:41.152: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-tggpm'
Nov 28 05:50:41.315: INFO: stderr: ""
Nov 28 05:50:41.315: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1221
Nov 28 05:50:41.315: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-tggpm'
Nov 28 05:50:41.430: INFO: stderr: ""
Nov 28 05:50:41.430: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:50:41.430: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tggpm" for this suite.
Nov 28 05:50:47.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:50:47.519: INFO: namespace: e2e-tests-kubectl-tggpm, resource: bindings, ignored listing per whitelist
Nov 28 05:50:47.958: INFO: namespace e2e-tests-kubectl-tggpm deletion completed in 6.524340423s

• [SLOW TEST:11.082 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should create an rc from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:50:47.958: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test override arguments
Nov 28 05:50:49.112: INFO: Waiting up to 5m0s for pod "client-containers-8e1a4ee3-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-containers-wmvgn" to be "success or failure"
Nov 28 05:50:49.115: INFO: Pod "client-containers-8e1a4ee3-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.339977ms
Nov 28 05:50:51.119: INFO: Pod "client-containers-8e1a4ee3-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006392139s
Nov 28 05:50:53.123: INFO: Pod "client-containers-8e1a4ee3-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010227191s
STEP: Saw pod success
Nov 28 05:50:53.123: INFO: Pod "client-containers-8e1a4ee3-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:50:53.126: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod client-containers-8e1a4ee3-f2d1-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:50:53.153: INFO: Waiting for pod client-containers-8e1a4ee3-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:50:53.155: INFO: Pod client-containers-8e1a4ee3-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:50:53.155: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wmvgn" for this suite.
Nov 28 05:50:59.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:50:59.213: INFO: namespace: e2e-tests-containers-wmvgn, resource: bindings, ignored listing per whitelist
Nov 28 05:50:59.684: INFO: namespace e2e-tests-containers-wmvgn deletion completed in 6.524440268s

• [SLOW TEST:11.726 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:50:59.684: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 28 05:50:59.831: INFO: Waiting up to 5m0s for pod "pod-95165ef5-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-mxmq8" to be "success or failure"
Nov 28 05:50:59.836: INFO: Pod "pod-95165ef5-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 5.21755ms
Nov 28 05:51:01.840: INFO: Pod "pod-95165ef5-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00906641s
STEP: Saw pod success
Nov 28 05:51:01.840: INFO: Pod "pod-95165ef5-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:51:01.843: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-95165ef5-f2d1-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:51:01.863: INFO: Waiting for pod pod-95165ef5-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:51:01.865: INFO: Pod pod-95165ef5-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:51:01.865: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mxmq8" for this suite.
Nov 28 05:51:07.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:51:07.892: INFO: namespace: e2e-tests-emptydir-mxmq8, resource: bindings, ignored listing per whitelist
Nov 28 05:51:08.394: INFO: namespace e2e-tests-emptydir-mxmq8 deletion completed in 6.525328183s

• [SLOW TEST:8.710 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:51:08.395: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1128 05:51:48.548300    6241 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 28 05:51:48.548: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:51:48.548: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-8rlv6" for this suite.
Nov 28 05:51:54.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:51:54.619: INFO: namespace: e2e-tests-gc-8rlv6, resource: bindings, ignored listing per whitelist
Nov 28 05:51:55.076: INFO: namespace e2e-tests-gc-8rlv6 deletion completed in 6.523542234s

• [SLOW TEST:46.681 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:51:55.076: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name configmap-test-volume-map-b6267dd8-f2d1-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:51:55.325: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6274a00-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-configmap-r244g" to be "success or failure"
Nov 28 05:51:55.329: INFO: Pod "pod-configmaps-b6274a00-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.473089ms
Nov 28 05:51:57.332: INFO: Pod "pod-configmaps-b6274a00-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00713501s
Nov 28 05:51:59.336: INFO: Pod "pod-configmaps-b6274a00-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010610767s
STEP: Saw pod success
Nov 28 05:51:59.336: INFO: Pod "pod-configmaps-b6274a00-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:51:59.338: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-configmaps-b6274a00-f2d1-11e8-904b-0a58ac10ae35 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 05:51:59.364: INFO: Waiting for pod pod-configmaps-b6274a00-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:51:59.366: INFO: Pod pod-configmaps-b6274a00-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:51:59.366: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-r244g" for this suite.
Nov 28 05:52:05.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:52:05.477: INFO: namespace: e2e-tests-configmap-r244g, resource: bindings, ignored listing per whitelist
Nov 28 05:52:05.896: INFO: namespace e2e-tests-configmap-r244g deletion completed in 6.525685059s

• [SLOW TEST:10.820 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:52:05.896: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-fkq5k
Nov 28 05:52:10.054: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-fkq5k
STEP: checking the pod's current state and verifying that restartCount is present
Nov 28 05:52:10.057: INFO: Initial restart count of pod liveness-http is 0
Nov 28 05:52:30.098: INFO: Restart count of pod e2e-tests-container-probe-fkq5k/liveness-http is now 1 (20.040921808s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:52:30.105: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fkq5k" for this suite.
Nov 28 05:52:36.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:52:36.178: INFO: namespace: e2e-tests-container-probe-fkq5k, resource: bindings, ignored listing per whitelist
Nov 28 05:52:36.632: INFO: namespace e2e-tests-container-probe-fkq5k deletion completed in 6.522578431s

• [SLOW TEST:30.736 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:52:36.632: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-b7zzg
Nov 28 05:52:41.790: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-b7zzg
STEP: checking the pod's current state and verifying that restartCount is present
Nov 28 05:52:41.793: INFO: Initial restart count of pod liveness-exec is 0
Nov 28 05:53:27.887: INFO: Restart count of pod e2e-tests-container-probe-b7zzg/liveness-exec is now 1 (46.0939365s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:53:27.897: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-b7zzg" for this suite.
Nov 28 05:53:33.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:53:33.981: INFO: namespace: e2e-tests-container-probe-b7zzg, resource: bindings, ignored listing per whitelist
Nov 28 05:53:34.462: INFO: namespace e2e-tests-container-probe-b7zzg deletion completed in 6.552553124s

• [SLOW TEST:57.830 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:53:34.463: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 28 05:53:34.651: INFO: Waiting up to 5m0s for pod "pod-f15df749-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-kw4bj" to be "success or failure"
Nov 28 05:53:34.654: INFO: Pod "pod-f15df749-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.367034ms
Nov 28 05:53:36.658: INFO: Pod "pod-f15df749-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00713209s
Nov 28 05:53:38.664: INFO: Pod "pod-f15df749-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012750132s
STEP: Saw pod success
Nov 28 05:53:38.664: INFO: Pod "pod-f15df749-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:53:38.667: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-f15df749-f2d1-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:53:38.686: INFO: Waiting for pod pod-f15df749-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:53:38.689: INFO: Pod pod-f15df749-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:53:38.689: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kw4bj" for this suite.
Nov 28 05:53:44.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:53:44.783: INFO: namespace: e2e-tests-emptydir-kw4bj, resource: bindings, ignored listing per whitelist
Nov 28 05:53:45.218: INFO: namespace e2e-tests-emptydir-kw4bj deletion completed in 6.524374526s

• [SLOW TEST:10.756 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:53:45.218: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 05:53:45.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7c45e9e-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-b5ntr" to be "success or failure"
Nov 28 05:53:45.392: INFO: Pod "downwardapi-volume-f7c45e9e-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.453324ms
Nov 28 05:53:47.395: INFO: Pod "downwardapi-volume-f7c45e9e-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006973537s
Nov 28 05:53:49.398: INFO: Pod "downwardapi-volume-f7c45e9e-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010188262s
STEP: Saw pod success
Nov 28 05:53:49.398: INFO: Pod "downwardapi-volume-f7c45e9e-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:53:49.401: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod downwardapi-volume-f7c45e9e-f2d1-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 05:53:49.420: INFO: Waiting for pod downwardapi-volume-f7c45e9e-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:53:49.422: INFO: Pod downwardapi-volume-f7c45e9e-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:53:49.422: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-b5ntr" for this suite.
Nov 28 05:53:55.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:53:55.457: INFO: namespace: e2e-tests-projected-b5ntr, resource: bindings, ignored listing per whitelist
Nov 28 05:53:55.961: INFO: namespace e2e-tests-projected-b5ntr deletion completed in 6.534939464s

• [SLOW TEST:10.743 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:53:55.961: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 28 05:53:56.108: INFO: Waiting up to 5m0s for pod "pod-fe28a3b1-f2d1-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-fl8tl" to be "success or failure"
Nov 28 05:53:56.110: INFO: Pod "pod-fe28a3b1-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13388ms
Nov 28 05:53:58.114: INFO: Pod "pod-fe28a3b1-f2d1-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005367231s
Nov 28 05:54:00.118: INFO: Pod "pod-fe28a3b1-f2d1-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009355238s
STEP: Saw pod success
Nov 28 05:54:00.118: INFO: Pod "pod-fe28a3b1-f2d1-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:54:00.120: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-fe28a3b1-f2d1-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:54:00.141: INFO: Waiting for pod pod-fe28a3b1-f2d1-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:54:00.143: INFO: Pod pod-fe28a3b1-f2d1-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:54:00.143: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fl8tl" for this suite.
Nov 28 05:54:06.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:54:06.171: INFO: namespace: e2e-tests-emptydir-fl8tl, resource: bindings, ignored listing per whitelist
Nov 28 05:54:06.671: INFO: namespace e2e-tests-emptydir-fl8tl deletion completed in 6.523832107s

• [SLOW TEST:10.710 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:54:06.671: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:54:06.873: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-7dstx" for this suite.
Nov 28 05:54:12.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:54:12.987: INFO: namespace: e2e-tests-services-7dstx, resource: bindings, ignored listing per whitelist
Nov 28 05:54:13.424: INFO: namespace e2e-tests-services-7dstx deletion completed in 6.542929878s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:6.753 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:54:13.425: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap e2e-tests-configmap-gj74p/configmap-test-089241dc-f2d2-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 05:54:13.585: INFO: Waiting up to 5m0s for pod "pod-configmaps-0892b6a4-f2d2-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-configmap-gj74p" to be "success or failure"
Nov 28 05:54:13.588: INFO: Pod "pod-configmaps-0892b6a4-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774882ms
Nov 28 05:54:15.591: INFO: Pod "pod-configmaps-0892b6a4-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006365176s
Nov 28 05:54:17.595: INFO: Pod "pod-configmaps-0892b6a4-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010223188s
Nov 28 05:54:19.599: INFO: Pod "pod-configmaps-0892b6a4-f2d2-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014064888s
STEP: Saw pod success
Nov 28 05:54:19.599: INFO: Pod "pod-configmaps-0892b6a4-f2d2-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:54:19.602: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-configmaps-0892b6a4-f2d2-11e8-904b-0a58ac10ae35 container env-test: <nil>
STEP: delete the pod
Nov 28 05:54:19.623: INFO: Waiting for pod pod-configmaps-0892b6a4-f2d2-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:54:19.625: INFO: Pod pod-configmaps-0892b6a4-f2d2-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:54:19.626: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gj74p" for this suite.
Nov 28 05:54:25.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:54:25.681: INFO: namespace: e2e-tests-configmap-gj74p, resource: bindings, ignored listing per whitelist
Nov 28 05:54:26.158: INFO: namespace e2e-tests-configmap-gj74p deletion completed in 6.52750704s

• [SLOW TEST:12.733 seconds]
[sig-api-machinery] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:29
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:54:26.158: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating secret with name secret-test-map-10266023-f2d2-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 05:54:26.303: INFO: Waiting up to 5m0s for pod "pod-secrets-1027765e-f2d2-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-secrets-s7bwd" to be "success or failure"
Nov 28 05:54:26.306: INFO: Pod "pod-secrets-1027765e-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.225771ms
Nov 28 05:54:28.310: INFO: Pod "pod-secrets-1027765e-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006931631s
Nov 28 05:54:30.314: INFO: Pod "pod-secrets-1027765e-f2d2-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010719238s
STEP: Saw pod success
Nov 28 05:54:30.314: INFO: Pod "pod-secrets-1027765e-f2d2-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:54:30.316: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-secrets-1027765e-f2d2-11e8-904b-0a58ac10ae35 container secret-volume-test: <nil>
STEP: delete the pod
Nov 28 05:54:30.350: INFO: Waiting for pod pod-secrets-1027765e-f2d2-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:54:30.352: INFO: Pod pod-secrets-1027765e-f2d2-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:54:30.353: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-s7bwd" for this suite.
Nov 28 05:54:36.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:54:36.486: INFO: namespace: e2e-tests-secrets-s7bwd, resource: bindings, ignored listing per whitelist
Nov 28 05:54:36.881: INFO: namespace e2e-tests-secrets-s7bwd deletion completed in 6.523986695s

• [SLOW TEST:10.723 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:54:36.881: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-168e4be1-f2d2-11e8-904b-0a58ac10ae35,GenerateName:,Namespace:e2e-tests-events-9jtrv,SelfLink:/api/v1/namespaces/e2e-tests-events-9jtrv/pods/send-events-168e4be1-f2d2-11e8-904b-0a58ac10ae35,UID:168faf48-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:19234,Generation:0,CreationTimestamp:2018-11-28 05:54:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 27843107,},Annotations:map[string]string{openshift.io/scc: anyuid,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-44m46 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-44m46,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-44m46 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{node-role.kubernetes.io/compute: true,},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci-op-qh5ir6tt-623b1-ig-n-x1wp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c36,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:54:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:54:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-11-28 05:54:37 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:172.16.2.57,StartTime:2018-11-28 05:54:37 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2018-11-28 05:54:39 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64@sha256:2dd4032e98a0450d95a0ac71a5e465f542a900812d8c41bc6ca635aed1a5fc91 docker://e0d5481c8612ce5038e3a5d96e57e2371222aead50c7837f32f0ae1a17718c51}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
STEP: checking for scheduler event about the pod
Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:54:45.073: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-9jtrv" for this suite.
Nov 28 05:54:51.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:54:51.180: INFO: namespace: e2e-tests-events-9jtrv, resource: bindings, ignored listing per whitelist
Nov 28 05:54:51.606: INFO: namespace e2e-tests-events-9jtrv deletion completed in 6.52758569s

• [SLOW TEST:14.725 seconds]
[k8s.io] [sig-node] Events
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:54:51.606: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:54:51.783: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:54:52.883: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-zl4gf" for this suite.
Nov 28 05:54:58.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:54:59.029: INFO: namespace: e2e-tests-custom-resource-definition-zl4gf, resource: bindings, ignored listing per whitelist
Nov 28 05:54:59.417: INFO: namespace e2e-tests-custom-resource-definition-zl4gf deletion completed in 6.529279225s

• [SLOW TEST:7.810 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:54:59.417: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1128 05:55:09.636691    6241 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 28 05:55:09.636: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:55:09.636: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mldkh" for this suite.
Nov 28 05:55:15.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:55:15.797: INFO: namespace: e2e-tests-gc-mldkh, resource: bindings, ignored listing per whitelist
Nov 28 05:55:16.182: INFO: namespace e2e-tests-gc-mldkh deletion completed in 6.537984067s

• [SLOW TEST:16.765 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:55:16.182: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 05:55:16.376: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-2e0273ef-f2d2-11e8-904b-0a58ac10ae35
STEP: Creating configMap with name cm-test-opt-upd-2e02743a-f2d2-11e8-904b-0a58ac10ae35
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2e0273ef-f2d2-11e8-904b-0a58ac10ae35
STEP: Updating configmap cm-test-opt-upd-2e02743a-f2d2-11e8-904b-0a58ac10ae35
STEP: Creating configMap with name cm-test-opt-create-2e027626-f2d2-11e8-904b-0a58ac10ae35
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:56:41.045: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mvtz6" for this suite.
Nov 28 05:57:03.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:57:03.186: INFO: namespace: e2e-tests-configmap-mvtz6, resource: bindings, ignored listing per whitelist
Nov 28 05:57:03.582: INFO: namespace e2e-tests-configmap-mvtz6 deletion completed in 22.529420452s

• [SLOW TEST:107.400 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:57:03.582: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward api env vars
Nov 28 05:57:03.785: INFO: Waiting up to 5m0s for pod "downward-api-6e03926f-f2d2-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-ndqx4" to be "success or failure"
Nov 28 05:57:03.795: INFO: Pod "downward-api-6e03926f-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 9.98529ms
Nov 28 05:57:05.799: INFO: Pod "downward-api-6e03926f-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013889477s
Nov 28 05:57:07.806: INFO: Pod "downward-api-6e03926f-f2d2-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021077601s
STEP: Saw pod success
Nov 28 05:57:07.806: INFO: Pod "downward-api-6e03926f-f2d2-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:57:07.814: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod downward-api-6e03926f-f2d2-11e8-904b-0a58ac10ae35 container dapi-container: <nil>
STEP: delete the pod
Nov 28 05:57:07.835: INFO: Waiting for pod downward-api-6e03926f-f2d2-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:57:07.840: INFO: Pod downward-api-6e03926f-f2d2-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:57:07.840: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ndqx4" for this suite.
Nov 28 05:57:13.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:57:13.905: INFO: namespace: e2e-tests-downward-api-ndqx4, resource: bindings, ignored listing per whitelist
Nov 28 05:57:14.399: INFO: namespace e2e-tests-downward-api-ndqx4 deletion completed in 6.545337609s

• [SLOW TEST:10.817 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:57:14.399: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 28 05:57:14.617: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-a,UID:74779c73-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20049,Generation:0,CreationTimestamp:2018-11-28 05:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 28 05:57:14.617: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-a,UID:74779c73-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20049,Generation:0,CreationTimestamp:2018-11-28 05:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 28 05:57:24.628: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-a,UID:74779c73-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20073,Generation:0,CreationTimestamp:2018-11-28 05:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 28 05:57:24.628: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-a,UID:74779c73-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20073,Generation:0,CreationTimestamp:2018-11-28 05:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 28 05:57:34.640: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-a,UID:74779c73-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20090,Generation:0,CreationTimestamp:2018-11-28 05:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 28 05:57:34.640: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-a,UID:74779c73-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20090,Generation:0,CreationTimestamp:2018-11-28 05:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 28 05:57:44.663: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-a,UID:74779c73-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20108,Generation:0,CreationTimestamp:2018-11-28 05:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 28 05:57:44.663: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-a,UID:74779c73-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20108,Generation:0,CreationTimestamp:2018-11-28 05:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 28 05:57:54.675: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-b,UID:8c5b8f64-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20126,Generation:0,CreationTimestamp:2018-11-28 05:57:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 28 05:57:54.675: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-b,UID:8c5b8f64-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20126,Generation:0,CreationTimestamp:2018-11-28 05:57:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 28 05:58:04.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-b,UID:8c5b8f64-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20143,Generation:0,CreationTimestamp:2018-11-28 05:57:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 28 05:58:04.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-jg7z5,SelfLink:/api/v1/namespaces/e2e-tests-watch-jg7z5/configmaps/e2e-watch-test-configmap-b,UID:8c5b8f64-f2d2-11e8-b66f-42010a8e0002,ResourceVersion:20143,Generation:0,CreationTimestamp:2018-11-28 05:57:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:58:14.685: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-jg7z5" for this suite.
Nov 28 05:58:20.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:58:20.829: INFO: namespace: e2e-tests-watch-jg7z5, resource: bindings, ignored listing per whitelist
Nov 28 05:58:21.233: INFO: namespace e2e-tests-watch-jg7z5 deletion completed in 6.538395169s

• [SLOW TEST:66.834 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:58:21.233: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test override all
Nov 28 05:58:21.393: INFO: Waiting up to 5m0s for pod "client-containers-9c46502a-f2d2-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-containers-lfrlr" to be "success or failure"
Nov 28 05:58:21.396: INFO: Pod "client-containers-9c46502a-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.206535ms
Nov 28 05:58:23.401: INFO: Pod "client-containers-9c46502a-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00797212s
Nov 28 05:58:25.411: INFO: Pod "client-containers-9c46502a-f2d2-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017957866s
STEP: Saw pod success
Nov 28 05:58:25.411: INFO: Pod "client-containers-9c46502a-f2d2-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 05:58:25.414: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod client-containers-9c46502a-f2d2-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 05:58:25.435: INFO: Waiting for pod client-containers-9c46502a-f2d2-11e8-904b-0a58ac10ae35 to disappear
Nov 28 05:58:25.437: INFO: Pod client-containers-9c46502a-f2d2-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:58:25.437: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-lfrlr" for this suite.
Nov 28 05:58:31.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:58:31.667: INFO: namespace: e2e-tests-containers-lfrlr, resource: bindings, ignored listing per whitelist
Nov 28 05:58:31.989: INFO: namespace e2e-tests-containers-lfrlr deletion completed in 6.545814516s

• [SLOW TEST:10.756 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:58:31.989: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 05:59:32.228: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-sxbpf" for this suite.
Nov 28 05:59:54.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 05:59:54.295: INFO: namespace: e2e-tests-container-probe-sxbpf, resource: bindings, ignored listing per whitelist
Nov 28 05:59:54.769: INFO: namespace e2e-tests-container-probe-sxbpf deletion completed in 22.535815956s

• [SLOW TEST:82.780 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 05:59:54.769: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1012
STEP: creating the pod
Nov 28 05:59:55.043: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-d5bl6'
Nov 28 05:59:56.097: INFO: stderr: ""
Nov 28 05:59:56.097: INFO: stdout: "pod/pause created\n"
Nov 28 05:59:56.097: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 28 05:59:56.097: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-d5bl6" to be "running and ready"
Nov 28 05:59:56.100: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.109961ms
Nov 28 05:59:58.103: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006704443s
Nov 28 06:00:00.109: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.012418268s
Nov 28 06:00:00.109: INFO: Pod "pause" satisfied condition "running and ready"
Nov 28 06:00:00.109: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 28 06:00:00.109: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-d5bl6'
Nov 28 06:00:00.405: INFO: stderr: ""
Nov 28 06:00:00.405: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 28 06:00:00.405: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-d5bl6'
Nov 28 06:00:00.712: INFO: stderr: ""
Nov 28 06:00:00.712: INFO: stdout: "NAME      READY     STATUS    RESTARTS   AGE       TESTING-LABEL\npause     1/1       Running   0          4s        testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 28 06:00:00.712: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label- --namespace=e2e-tests-kubectl-d5bl6'
Nov 28 06:00:00.992: INFO: stderr: ""
Nov 28 06:00:00.992: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 28 06:00:00.992: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=e2e-tests-kubectl-d5bl6'
Nov 28 06:00:01.368: INFO: stderr: ""
Nov 28 06:00:01.368: INFO: stdout: "NAME      READY     STATUS    RESTARTS   AGE       TESTING-LABEL\npause     1/1       Running   0          5s        \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1018
STEP: using delete to clean up resources
Nov 28 06:00:01.368: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-d5bl6'
Nov 28 06:00:01.683: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 06:00:01.683: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 28 06:00:01.683: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-d5bl6'
Nov 28 06:00:02.004: INFO: stderr: "No resources found.\n"
Nov 28 06:00:02.004: INFO: stdout: ""
Nov 28 06:00:02.004: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=pause --namespace=e2e-tests-kubectl-d5bl6 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 28 06:00:02.261: INFO: stderr: ""
Nov 28 06:00:02.261: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:00:02.261: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d5bl6" for this suite.
Nov 28 06:00:08.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:00:08.326: INFO: namespace: e2e-tests-kubectl-d5bl6, resource: bindings, ignored listing per whitelist
Nov 28 06:00:08.816: INFO: namespace e2e-tests-kubectl-d5bl6 deletion completed in 6.537427207s

• [SLOW TEST:14.047 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should update the label on a resource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:00:08.816: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 28 06:00:13.591: INFO: Successfully updated pod "pod-update-activedeadlineseconds-dc729877-f2d2-11e8-904b-0a58ac10ae35"
Nov 28 06:00:13.591: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-dc729877-f2d2-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-pods-zm69r" to be "terminated due to deadline exceeded"
Nov 28 06:00:13.594: INFO: Pod "pod-update-activedeadlineseconds-dc729877-f2d2-11e8-904b-0a58ac10ae35": Phase="Running", Reason="", readiness=true. Elapsed: 2.440403ms
Nov 28 06:00:15.598: INFO: Pod "pod-update-activedeadlineseconds-dc729877-f2d2-11e8-904b-0a58ac10ae35": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.006617116s
Nov 28 06:00:15.598: INFO: Pod "pod-update-activedeadlineseconds-dc729877-f2d2-11e8-904b-0a58ac10ae35" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:00:15.598: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zm69r" for this suite.
Nov 28 06:00:21.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:00:22.043: INFO: namespace: e2e-tests-pods-zm69r, resource: bindings, ignored listing per whitelist
Nov 28 06:00:22.145: INFO: namespace e2e-tests-pods-zm69r deletion completed in 6.528962022s

• [SLOW TEST:13.329 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:00:22.145: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name projected-configmap-test-volume-e45a1f78-f2d2-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 06:00:22.313: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e45ad150-f2d2-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-kv668" to be "success or failure"
Nov 28 06:00:22.321: INFO: Pod "pod-projected-configmaps-e45ad150-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 8.160745ms
Nov 28 06:00:24.328: INFO: Pod "pod-projected-configmaps-e45ad150-f2d2-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015051031s
Nov 28 06:00:26.336: INFO: Pod "pod-projected-configmaps-e45ad150-f2d2-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023491087s
STEP: Saw pod success
Nov 28 06:00:26.336: INFO: Pod "pod-projected-configmaps-e45ad150-f2d2-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:00:26.341: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-projected-configmaps-e45ad150-f2d2-11e8-904b-0a58ac10ae35 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 06:00:26.368: INFO: Waiting for pod pod-projected-configmaps-e45ad150-f2d2-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:00:26.371: INFO: Pod pod-projected-configmaps-e45ad150-f2d2-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:00:26.371: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kv668" for this suite.
Nov 28 06:00:32.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:00:32.471: INFO: namespace: e2e-tests-projected-kv668, resource: bindings, ignored listing per whitelist
Nov 28 06:00:32.905: INFO: namespace e2e-tests-projected-kv668 deletion completed in 6.529547761s

• [SLOW TEST:10.760 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:00:32.905: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-29cqq
[It] Should recreate evicted statefulset [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-29cqq
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-29cqq
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-29cqq
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-29cqq
Nov 28 06:00:40.193: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-29cqq, name: ss-0, uid: ee8a7f36-f2d2-11e8-b66f-42010a8e0002, status phase: Pending. Waiting for statefulset controller to delete.
Nov 28 06:00:41.184: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-29cqq, name: ss-0, uid: ee8a7f36-f2d2-11e8-b66f-42010a8e0002, status phase: Failed. Waiting for statefulset controller to delete.
Nov 28 06:00:41.187: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-29cqq, name: ss-0, uid: ee8a7f36-f2d2-11e8-b66f-42010a8e0002, status phase: Failed. Waiting for statefulset controller to delete.
Nov 28 06:00:41.190: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-29cqq
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-29cqq
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-29cqq and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Nov 28 06:00:45.221: INFO: Deleting all statefulset in ns e2e-tests-statefulset-29cqq
Nov 28 06:00:45.224: INFO: Scaling statefulset ss to 0
Nov 28 06:00:55.250: INFO: Waiting for statefulset status.replicas updated to 0
Nov 28 06:00:55.254: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:00:55.269: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-29cqq" for this suite.
Nov 28 06:01:01.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:01:01.454: INFO: namespace: e2e-tests-statefulset-29cqq, resource: bindings, ignored listing per whitelist
Nov 28 06:01:01.827: INFO: namespace e2e-tests-statefulset-29cqq deletion completed in 6.545005684s

• [SLOW TEST:28.922 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    Should recreate evicted statefulset [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:01:01.827: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
[It] should update a single-container pod's image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Nov 28 06:01:02.189: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-pod --generator=run-pod/v1 --image=k8s.gcr.io/nginx-slim-amd64:0.20 --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xvb8p'
Nov 28 06:01:02.571: INFO: stderr: ""
Nov 28 06:01:02.571: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Nov 28 06:01:07.625: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xvb8p -o json'
Nov 28 06:01:07.791: INFO: stderr: ""
Nov 28 06:01:07.791: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2018-11-28T06:01:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-xvb8p\",\n        \"resourceVersion\": \"21102\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-xvb8p/pods/e2e-test-nginx-pod\",\n        \"uid\": \"fc579777-f2d2-11e8-b66f-42010a8e0002\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/nginx-slim-amd64:0.20\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-xmbsn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-6r9km\"\n            }\n        ],\n        \"nodeName\": \"ci-op-qh5ir6tt-623b1-ig-n-x1wp\",\n        \"nodeSelector\": {\n            \"node-role.kubernetes.io/compute\": \"true\"\n        },\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c37,c34\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"volumes\": [\n            {\n                \"name\": \"default-token-xmbsn\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-xmbsn\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-11-28T06:01:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-11-28T06:01:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": null,\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-11-28T06:01:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://62637ca7e2f50e39158472f5fdd90b41996c5f4e85b92144e53c31eecaa5b065\",\n                \"image\": \"k8s.gcr.io/nginx-slim-amd64:0.20\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2018-11-28T06:01:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.142.0.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.2.62\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2018-11-28T06:01:02Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 28 06:01:07.791: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig replace -f - --namespace=e2e-tests-kubectl-xvb8p'
Nov 28 06:01:08.202: INFO: stderr: ""
Nov 28 06:01:08.202: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image busybox
[AfterEach] [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1485
Nov 28 06:01:08.206: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xvb8p'
Nov 28 06:01:19.525: INFO: stderr: ""
Nov 28 06:01:19.526: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:01:19.526: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xvb8p" for this suite.
Nov 28 06:01:25.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:01:25.623: INFO: namespace: e2e-tests-kubectl-xvb8p, resource: bindings, ignored listing per whitelist
Nov 28 06:01:26.083: INFO: namespace e2e-tests-kubectl-xvb8p deletion completed in 6.547795283s

• [SLOW TEST:24.256 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should update a single-container pod's image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:01:26.083: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 28 06:01:26.316: INFO: Number of nodes with available pods: 0
Nov 28 06:01:26.316: INFO: Node ci-op-qh5ir6tt-623b1-ig-m-jq6h is running more than one daemon pod
Nov 28 06:01:27.329: INFO: Number of nodes with available pods: 0
Nov 28 06:01:27.329: INFO: Node ci-op-qh5ir6tt-623b1-ig-m-jq6h is running more than one daemon pod
Nov 28 06:01:28.333: INFO: Number of nodes with available pods: 0
Nov 28 06:01:28.333: INFO: Node ci-op-qh5ir6tt-623b1-ig-m-jq6h is running more than one daemon pod
Nov 28 06:01:29.348: INFO: Number of nodes with available pods: 3
Nov 28 06:01:29.348: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:01:30.325: INFO: Number of nodes with available pods: 4
Nov 28 06:01:30.325: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 28 06:01:30.345: INFO: Number of nodes with available pods: 3
Nov 28 06:01:30.346: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:01:31.363: INFO: Number of nodes with available pods: 3
Nov 28 06:01:31.363: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:01:32.354: INFO: Number of nodes with available pods: 3
Nov 28 06:01:32.354: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:01:33.353: INFO: Number of nodes with available pods: 3
Nov 28 06:01:33.353: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:01:34.357: INFO: Number of nodes with available pods: 3
Nov 28 06:01:34.357: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:01:35.362: INFO: Number of nodes with available pods: 3
Nov 28 06:01:35.362: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:01:36.359: INFO: Number of nodes with available pods: 3
Nov 28 06:01:36.359: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:01:37.360: INFO: Number of nodes with available pods: 4
Nov 28 06:01:37.360: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-gld4f, will wait for the garbage collector to delete the pods
Nov 28 06:01:37.429: INFO: Deleting {extensions DaemonSet} daemon-set took: 8.152596ms
Nov 28 06:01:37.529: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.208544ms
Nov 28 06:01:50.335: INFO: Number of nodes with available pods: 0
Nov 28 06:01:50.335: INFO: Number of running nodes: 0, number of available pods: 0
Nov 28 06:01:50.338: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-gld4f/daemonsets","resourceVersion":"21316"},"items":null}

Nov 28 06:01:50.340: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-gld4f/pods","resourceVersion":"21316"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:01:50.357: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-gld4f" for this suite.
Nov 28 06:01:56.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:01:56.400: INFO: namespace: e2e-tests-daemonsets-gld4f, resource: bindings, ignored listing per whitelist
Nov 28 06:01:56.885: INFO: namespace e2e-tests-daemonsets-gld4f deletion completed in 6.523192624s

• [SLOW TEST:30.802 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:01:56.885: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 06:01:57.064: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 28 06:01:57.076: INFO: Number of nodes with available pods: 0
Nov 28 06:01:57.076: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 28 06:01:57.095: INFO: Number of nodes with available pods: 0
Nov 28 06:01:57.095: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:01:58.103: INFO: Number of nodes with available pods: 0
Nov 28 06:01:58.103: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:01:59.099: INFO: Number of nodes with available pods: 0
Nov 28 06:01:59.099: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:00.105: INFO: Number of nodes with available pods: 1
Nov 28 06:02:00.105: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 28 06:02:00.123: INFO: Number of nodes with available pods: 0
Nov 28 06:02:00.123: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 28 06:02:00.138: INFO: Number of nodes with available pods: 0
Nov 28 06:02:00.138: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:01.143: INFO: Number of nodes with available pods: 0
Nov 28 06:02:01.143: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:02.143: INFO: Number of nodes with available pods: 0
Nov 28 06:02:02.143: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:03.154: INFO: Number of nodes with available pods: 0
Nov 28 06:02:03.154: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:04.142: INFO: Number of nodes with available pods: 0
Nov 28 06:02:04.142: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:05.143: INFO: Number of nodes with available pods: 0
Nov 28 06:02:05.143: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:06.142: INFO: Number of nodes with available pods: 0
Nov 28 06:02:06.142: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:07.144: INFO: Number of nodes with available pods: 0
Nov 28 06:02:07.144: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:08.142: INFO: Number of nodes with available pods: 0
Nov 28 06:02:08.142: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:09.142: INFO: Number of nodes with available pods: 0
Nov 28 06:02:09.142: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:10.142: INFO: Number of nodes with available pods: 0
Nov 28 06:02:10.142: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:11.142: INFO: Number of nodes with available pods: 0
Nov 28 06:02:11.142: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:12.142: INFO: Number of nodes with available pods: 0
Nov 28 06:02:12.142: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:02:13.142: INFO: Number of nodes with available pods: 1
Nov 28 06:02:13.142: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-swmbz, will wait for the garbage collector to delete the pods
Nov 28 06:02:13.208: INFO: Deleting {extensions DaemonSet} daemon-set took: 6.168883ms
Nov 28 06:02:13.308: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.289559ms
Nov 28 06:02:20.316: INFO: Number of nodes with available pods: 0
Nov 28 06:02:20.316: INFO: Number of running nodes: 0, number of available pods: 0
Nov 28 06:02:20.322: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-swmbz/daemonsets","resourceVersion":"21497"},"items":null}

Nov 28 06:02:20.325: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-swmbz/pods","resourceVersion":"21497"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:02:20.361: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-swmbz" for this suite.
Nov 28 06:02:26.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:02:26.396: INFO: namespace: e2e-tests-daemonsets-swmbz, resource: bindings, ignored listing per whitelist
Nov 28 06:02:26.891: INFO: namespace e2e-tests-daemonsets-swmbz deletion completed in 6.526602665s

• [SLOW TEST:30.006 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:02:26.892: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 28 06:02:27.022: INFO: Waiting up to 5m0s for pod "pod-2eafcf42-f2d3-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-flfq2" to be "success or failure"
Nov 28 06:02:27.027: INFO: Pod "pod-2eafcf42-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 5.151804ms
Nov 28 06:02:29.031: INFO: Pod "pod-2eafcf42-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009300874s
Nov 28 06:02:31.035: INFO: Pod "pod-2eafcf42-f2d3-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012720717s
STEP: Saw pod success
Nov 28 06:02:31.035: INFO: Pod "pod-2eafcf42-f2d3-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:02:31.038: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-2eafcf42-f2d3-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 06:02:31.064: INFO: Waiting for pod pod-2eafcf42-f2d3-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:02:31.067: INFO: Pod pod-2eafcf42-f2d3-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:02:31.067: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-flfq2" for this suite.
Nov 28 06:02:37.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:02:37.242: INFO: namespace: e2e-tests-emptydir-flfq2, resource: bindings, ignored listing per whitelist
Nov 28 06:02:37.610: INFO: namespace e2e-tests-emptydir-flfq2 deletion completed in 6.538846052s

• [SLOW TEST:10.718 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:02:37.610: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 06:02:37.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3528b753-f2d3-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-q58sj" to be "success or failure"
Nov 28 06:02:37.887: INFO: Pod "downwardapi-volume-3528b753-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.327869ms
Nov 28 06:02:39.894: INFO: Pod "downwardapi-volume-3528b753-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010723868s
Nov 28 06:02:41.898: INFO: Pod "downwardapi-volume-3528b753-f2d3-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014464927s
STEP: Saw pod success
Nov 28 06:02:41.898: INFO: Pod "downwardapi-volume-3528b753-f2d3-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:02:41.901: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod downwardapi-volume-3528b753-f2d3-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 06:02:41.921: INFO: Waiting for pod downwardapi-volume-3528b753-f2d3-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:02:41.923: INFO: Pod downwardapi-volume-3528b753-f2d3-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:02:41.923: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q58sj" for this suite.
Nov 28 06:02:47.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:02:47.994: INFO: namespace: e2e-tests-projected-q58sj, resource: bindings, ignored listing per whitelist
Nov 28 06:02:48.452: INFO: namespace e2e-tests-projected-q58sj deletion completed in 6.524689344s

• [SLOW TEST:10.842 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:02:48.452: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 28 06:02:48.600: INFO: Waiting up to 5m0s for pod "pod-3b8c2e90-f2d3-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-dbn2v" to be "success or failure"
Nov 28 06:02:48.604: INFO: Pod "pod-3b8c2e90-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.648807ms
Nov 28 06:02:50.609: INFO: Pod "pod-3b8c2e90-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009018613s
Nov 28 06:02:52.613: INFO: Pod "pod-3b8c2e90-f2d3-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012782466s
STEP: Saw pod success
Nov 28 06:02:52.613: INFO: Pod "pod-3b8c2e90-f2d3-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:02:52.616: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-3b8c2e90-f2d3-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 06:02:52.638: INFO: Waiting for pod pod-3b8c2e90-f2d3-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:02:52.641: INFO: Pod pod-3b8c2e90-f2d3-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:02:52.641: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dbn2v" for this suite.
Nov 28 06:02:58.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:02:58.803: INFO: namespace: e2e-tests-emptydir-dbn2v, resource: bindings, ignored listing per whitelist
Nov 28 06:02:59.170: INFO: namespace e2e-tests-emptydir-dbn2v deletion completed in 6.525285562s

• [SLOW TEST:10.718 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:02:59.171: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72
STEP: Creating service test in namespace e2e-tests-statefulset-pzcpv
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a new StatefulSet
Nov 28 06:02:59.332: INFO: Found 0 stateful pods, waiting for 3
Nov 28 06:03:09.338: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 06:03:09.338: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 06:03:09.338: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 28 06:03:09.347: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-pzcpv ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 06:03:09.606: INFO: stderr: ""
Nov 28 06:03:09.606: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 06:03:09.606: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/nginx-slim-amd64:0.20 to k8s.gcr.io/nginx-slim-amd64:0.21
Nov 28 06:03:19.640: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 28 06:03:29.659: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-pzcpv ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 28 06:03:29.915: INFO: stderr: ""
Nov 28 06:03:29.915: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 28 06:03:29.915: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 28 06:03:39.937: INFO: Waiting for StatefulSet e2e-tests-statefulset-pzcpv/ss2 to complete update
Nov 28 06:03:39.937: INFO: Waiting for Pod e2e-tests-statefulset-pzcpv/ss2-0 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff
Nov 28 06:03:49.981: INFO: Waiting for StatefulSet e2e-tests-statefulset-pzcpv/ss2 to complete update
STEP: Rolling back to a previous revision
Nov 28 06:03:59.953: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-pzcpv ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 28 06:04:00.385: INFO: stderr: ""
Nov 28 06:04:00.385: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 28 06:04:00.385: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 28 06:04:10.420: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 28 06:04:20.438: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-statefulset-pzcpv ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 28 06:04:20.826: INFO: stderr: ""
Nov 28 06:04:20.826: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 28 06:04:20.826: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 28 06:04:30.844: INFO: Waiting for StatefulSet e2e-tests-statefulset-pzcpv/ss2 to complete update
Nov 28 06:04:30.844: INFO: Waiting for Pod e2e-tests-statefulset-pzcpv/ss2-0 to have revision ss2-76cb68b6ff update revision ss2-56dd5fb9c4
Nov 28 06:04:30.844: INFO: Waiting for Pod e2e-tests-statefulset-pzcpv/ss2-1 to have revision ss2-76cb68b6ff update revision ss2-56dd5fb9c4
Nov 28 06:04:40.852: INFO: Waiting for StatefulSet e2e-tests-statefulset-pzcpv/ss2 to complete update
Nov 28 06:04:40.852: INFO: Waiting for Pod e2e-tests-statefulset-pzcpv/ss2-0 to have revision ss2-76cb68b6ff update revision ss2-56dd5fb9c4
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:83
Nov 28 06:04:50.852: INFO: Deleting all statefulset in ns e2e-tests-statefulset-pzcpv
Nov 28 06:04:50.855: INFO: Scaling statefulset ss2 to 0
Nov 28 06:05:20.872: INFO: Waiting for statefulset status.replicas updated to 0
Nov 28 06:05:20.876: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:05:20.893: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-pzcpv" for this suite.
Nov 28 06:05:26.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:05:27.111: INFO: namespace: e2e-tests-statefulset-pzcpv, resource: bindings, ignored listing per whitelist
Nov 28 06:05:27.435: INFO: namespace e2e-tests-statefulset-pzcpv deletion completed in 6.537848025s

• [SLOW TEST:148.265 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:05:27.436: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward api env vars
Nov 28 06:05:27.645: INFO: Waiting up to 5m0s for pod "downward-api-9a56fe34-f2d3-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-w996w" to be "success or failure"
Nov 28 06:05:27.653: INFO: Pod "downward-api-9a56fe34-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 7.90382ms
Nov 28 06:05:29.661: INFO: Pod "downward-api-9a56fe34-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015958218s
Nov 28 06:05:31.667: INFO: Pod "downward-api-9a56fe34-f2d3-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021985838s
STEP: Saw pod success
Nov 28 06:05:31.667: INFO: Pod "downward-api-9a56fe34-f2d3-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:05:31.670: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod downward-api-9a56fe34-f2d3-11e8-904b-0a58ac10ae35 container dapi-container: <nil>
STEP: delete the pod
Nov 28 06:05:31.688: INFO: Waiting for pod downward-api-9a56fe34-f2d3-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:05:31.691: INFO: Pod downward-api-9a56fe34-f2d3-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:05:31.691: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-w996w" for this suite.
Nov 28 06:05:37.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:05:37.799: INFO: namespace: e2e-tests-downward-api-w996w, resource: bindings, ignored listing per whitelist
Nov 28 06:05:38.228: INFO: namespace e2e-tests-downward-api-w996w deletion completed in 6.533058931s

• [SLOW TEST:10.792 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:05:38.228: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating projection with secret that has name projected-secret-test-a0c14cfb-f2d3-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 06:05:38.407: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a0c22410-f2d3-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-qmnn6" to be "success or failure"
Nov 28 06:05:38.412: INFO: Pod "pod-projected-secrets-a0c22410-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607115ms
Nov 28 06:05:40.417: INFO: Pod "pod-projected-secrets-a0c22410-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009558012s
Nov 28 06:05:42.426: INFO: Pod "pod-projected-secrets-a0c22410-f2d3-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018988323s
STEP: Saw pod success
Nov 28 06:05:42.426: INFO: Pod "pod-projected-secrets-a0c22410-f2d3-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:05:42.429: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-projected-secrets-a0c22410-f2d3-11e8-904b-0a58ac10ae35 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 28 06:05:42.462: INFO: Waiting for pod pod-projected-secrets-a0c22410-f2d3-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:05:42.465: INFO: Pod pod-projected-secrets-a0c22410-f2d3-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:05:42.465: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qmnn6" for this suite.
Nov 28 06:05:48.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:05:48.505: INFO: namespace: e2e-tests-projected-qmnn6, resource: bindings, ignored listing per whitelist
Nov 28 06:05:49.004: INFO: namespace e2e-tests-projected-qmnn6 deletion completed in 6.532541054s

• [SLOW TEST:10.776 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:05:49.004: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2z8kp A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-2z8kp;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2z8kp A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-2z8kp;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2z8kp.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2z8kp.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2z8kp.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2z8kp.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2z8kp.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2z8kp.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-2z8kp.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-2z8kp.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 4.21.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.21.4_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 4.21.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.21.4_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2z8kp A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-2z8kp;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2z8kp A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-2z8kp;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2z8kp.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2z8kp.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2z8kp.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2z8kp.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2z8kp.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-2z8kp.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2z8kp.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-2z8kp.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-2z8kp.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 4.21.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.21.4_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 4.21.30.172.in-addr.arpa. PTR)" && echo OK > /results/172.30.21.4_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 28 06:06:04.359: INFO: DNS probes using dns-test-a72f78b1-f2d3-11e8-904b-0a58ac10ae35 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:06:04.397: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-2z8kp" for this suite.
Nov 28 06:06:10.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:06:10.690: INFO: namespace: e2e-tests-dns-2z8kp, resource: bindings, ignored listing per whitelist
Nov 28 06:06:10.943: INFO: namespace e2e-tests-dns-2z8kp deletion completed in 6.541757727s

• [SLOW TEST:21.939 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:06:10.944: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: validating api versions
Nov 28 06:06:11.075: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig api-versions'
Nov 28 06:06:11.287: INFO: stderr: ""
Nov 28 06:06:11.287: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nimage.openshift.io/v1\nmonitoring.coreos.com/v1\nnetwork.openshift.io/v1\nnetworking.k8s.io/v1\noauth.openshift.io/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:06:11.287: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6v9c7" for this suite.
Nov 28 06:06:17.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:06:17.427: INFO: namespace: e2e-tests-kubectl-6v9c7, resource: bindings, ignored listing per whitelist
Nov 28 06:06:17.828: INFO: namespace e2e-tests-kubectl-6v9c7 deletion completed in 6.52481445s

• [SLOW TEST:6.884 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should check if v1 is in available api versions  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:06:17.828: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 06:06:17.993: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b85b176e-f2d3-11e8-b66f-42010a8e0002", Controller:(*bool)(0xc421f0386a), BlockOwnerDeletion:(*bool)(0xc421f0386b)}}
Nov 28 06:06:18.002: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b857dbc4-f2d3-11e8-b66f-42010a8e0002", Controller:(*bool)(0xc421f03a12), BlockOwnerDeletion:(*bool)(0xc421f03a13)}}
Nov 28 06:06:18.010: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b8596fb0-f2d3-11e8-b66f-42010a8e0002", Controller:(*bool)(0xc4227ab6aa), BlockOwnerDeletion:(*bool)(0xc4227ab6ab)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:06:23.019: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-gdk2m" for this suite.
Nov 28 06:06:29.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:06:29.206: INFO: namespace: e2e-tests-gc-gdk2m, resource: bindings, ignored listing per whitelist
Nov 28 06:06:29.560: INFO: namespace e2e-tests-gc-gdk2m deletion completed in 6.529160452s

• [SLOW TEST:11.732 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:06:29.560: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating secret with name secret-test-bf586cf4-f2d3-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 06:06:30.725: INFO: Waiting up to 5m0s for pod "pod-secrets-bf58fdac-f2d3-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-secrets-w2t85" to be "success or failure"
Nov 28 06:06:30.727: INFO: Pod "pod-secrets-bf58fdac-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473109ms
Nov 28 06:06:32.732: INFO: Pod "pod-secrets-bf58fdac-f2d3-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006967229s
Nov 28 06:06:34.735: INFO: Pod "pod-secrets-bf58fdac-f2d3-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010259848s
STEP: Saw pod success
Nov 28 06:06:34.735: INFO: Pod "pod-secrets-bf58fdac-f2d3-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:06:34.739: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-secrets-bf58fdac-f2d3-11e8-904b-0a58ac10ae35 container secret-volume-test: <nil>
STEP: delete the pod
Nov 28 06:06:34.758: INFO: Waiting for pod pod-secrets-bf58fdac-f2d3-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:06:34.760: INFO: Pod pod-secrets-bf58fdac-f2d3-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:06:34.760: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-w2t85" for this suite.
Nov 28 06:06:40.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:06:41.088: INFO: namespace: e2e-tests-secrets-w2t85, resource: bindings, ignored listing per whitelist
Nov 28 06:06:41.292: INFO: namespace e2e-tests-secrets-w2t85 deletion completed in 6.527962221s

• [SLOW TEST:11.732 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:06:41.292: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-v5psw
Nov 28 06:06:46.466: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-v5psw
STEP: checking the pod's current state and verifying that restartCount is present
Nov 28 06:06:46.469: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:10:47.086: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-v5psw" for this suite.
Nov 28 06:10:53.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:10:53.159: INFO: namespace: e2e-tests-container-probe-v5psw, resource: bindings, ignored listing per whitelist
Nov 28 06:10:53.615: INFO: namespace e2e-tests-container-probe-v5psw deletion completed in 6.525080259s

• [SLOW TEST:252.323 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:10:53.615: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test use defaults
Nov 28 06:10:54.787: INFO: Waiting up to 5m0s for pod "client-containers-5cbe0047-f2d4-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-containers-rd9nf" to be "success or failure"
Nov 28 06:10:54.790: INFO: Pod "client-containers-5cbe0047-f2d4-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.608429ms
Nov 28 06:10:56.800: INFO: Pod "client-containers-5cbe0047-f2d4-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012543537s
Nov 28 06:10:58.809: INFO: Pod "client-containers-5cbe0047-f2d4-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021512948s
STEP: Saw pod success
Nov 28 06:10:58.809: INFO: Pod "client-containers-5cbe0047-f2d4-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:10:58.812: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod client-containers-5cbe0047-f2d4-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 06:10:58.834: INFO: Waiting for pod client-containers-5cbe0047-f2d4-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:10:58.837: INFO: Pod client-containers-5cbe0047-f2d4-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:10:58.837: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-rd9nf" for this suite.
Nov 28 06:11:04.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:11:04.915: INFO: namespace: e2e-tests-containers-rd9nf, resource: bindings, ignored listing per whitelist
Nov 28 06:11:05.393: INFO: namespace e2e-tests-containers-rd9nf deletion completed in 6.541340591s

• [SLOW TEST:11.778 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:11:05.393: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: executing a command with run --rm and attach with stdin
Nov 28 06:11:05.537: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=e2e-tests-kubectl-74hsp run e2e-test-rm-busybox-job --image=busybox --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Nov 28 06:11:09.195: INFO: stderr: "If you don't see a command prompt, try pressing enter.\n"
Nov 28 06:11:09.195: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:11:11.200: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-74hsp" for this suite.
Nov 28 06:11:21.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:11:21.239: INFO: namespace: e2e-tests-kubectl-74hsp, resource: bindings, ignored listing per whitelist
Nov 28 06:11:21.731: INFO: namespace e2e-tests-kubectl-74hsp deletion completed in 10.5257526s

• [SLOW TEST:16.338 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should create a job from an image, then delete the job  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:11:21.731: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1128 06:11:31.873875    6241 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 28 06:11:31.873: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:11:31.874: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-dcs49" for this suite.
Nov 28 06:11:37.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:11:37.919: INFO: namespace: e2e-tests-gc-dcs49, resource: bindings, ignored listing per whitelist
Nov 28 06:11:38.403: INFO: namespace e2e-tests-gc-dcs49 deletion completed in 6.52475785s

• [SLOW TEST:16.672 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:11:38.403: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward api env vars
Nov 28 06:11:38.559: INFO: Waiting up to 5m0s for pod "downward-api-776cbcbb-f2d4-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-9qqbp" to be "success or failure"
Nov 28 06:11:38.562: INFO: Pod "downward-api-776cbcbb-f2d4-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.898406ms
Nov 28 06:11:40.566: INFO: Pod "downward-api-776cbcbb-f2d4-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007184197s
Nov 28 06:11:42.570: INFO: Pod "downward-api-776cbcbb-f2d4-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010987667s
STEP: Saw pod success
Nov 28 06:11:42.570: INFO: Pod "downward-api-776cbcbb-f2d4-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:11:42.573: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod downward-api-776cbcbb-f2d4-11e8-904b-0a58ac10ae35 container dapi-container: <nil>
STEP: delete the pod
Nov 28 06:11:42.595: INFO: Waiting for pod downward-api-776cbcbb-f2d4-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:11:42.597: INFO: Pod downward-api-776cbcbb-f2d4-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:11:42.597: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9qqbp" for this suite.
Nov 28 06:11:48.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:11:48.639: INFO: namespace: e2e-tests-downward-api-9qqbp, resource: bindings, ignored listing per whitelist
Nov 28 06:11:49.128: INFO: namespace e2e-tests-downward-api-9qqbp deletion completed in 6.526181868s

• [SLOW TEST:10.725 seconds]
[sig-api-machinery] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:11:49.128: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 28 06:11:49.278: INFO: Waiting up to 5m0s for pod "pod-7dd1b8cc-f2d4-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-9hffp" to be "success or failure"
Nov 28 06:11:49.281: INFO: Pod "pod-7dd1b8cc-f2d4-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.976707ms
Nov 28 06:11:51.285: INFO: Pod "pod-7dd1b8cc-f2d4-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006987077s
Nov 28 06:11:53.289: INFO: Pod "pod-7dd1b8cc-f2d4-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011033158s
STEP: Saw pod success
Nov 28 06:11:53.290: INFO: Pod "pod-7dd1b8cc-f2d4-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:11:53.293: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-7dd1b8cc-f2d4-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 06:11:53.310: INFO: Waiting for pod pod-7dd1b8cc-f2d4-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:11:53.312: INFO: Pod pod-7dd1b8cc-f2d4-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:11:53.312: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9hffp" for this suite.
Nov 28 06:11:59.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:11:59.429: INFO: namespace: e2e-tests-emptydir-9hffp, resource: bindings, ignored listing per whitelist
Nov 28 06:11:59.836: INFO: namespace e2e-tests-emptydir-9hffp deletion completed in 6.520694203s

• [SLOW TEST:10.708 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:11:59.836: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 06:11:59.993: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
Nov 28 06:11:59.998: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-qjm7c/daemonsets","resourceVersion":"24063"},"items":null}

Nov 28 06:12:00.001: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-qjm7c/pods","resourceVersion":"24063"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:12:00.012: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-qjm7c" for this suite.
Nov 28 06:12:06.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:12:06.146: INFO: namespace: e2e-tests-daemonsets-qjm7c, resource: bindings, ignored listing per whitelist
Nov 28 06:12:06.540: INFO: namespace e2e-tests-daemonsets-qjm7c deletion completed in 6.52405481s

S [SKIPPING] [6.704 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684

  Nov 28 06:11:59.993: Requires at least 2 nodes (not -1)

  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:305
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:12:06.541: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-njqh7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-njqh7 to expose endpoints map[]
Nov 28 06:12:06.679: INFO: Get endpoints failed (5.053268ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Nov 28 06:12:07.682: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-njqh7 exposes endpoints map[] (1.008427625s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-njqh7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-njqh7 to expose endpoints map[pod1:[100]]
Nov 28 06:12:10.720: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-njqh7 exposes endpoints map[pod1:[100]] (3.026926129s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-njqh7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-njqh7 to expose endpoints map[pod2:[101] pod1:[100]]
Nov 28 06:12:14.774: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-njqh7 exposes endpoints map[pod1:[100] pod2:[101]] (4.043943198s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-njqh7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-njqh7 to expose endpoints map[pod2:[101]]
Nov 28 06:12:15.791: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-njqh7 exposes endpoints map[pod2:[101]] (1.013172551s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-njqh7
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-njqh7 to expose endpoints map[]
Nov 28 06:12:16.804: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-njqh7 exposes endpoints map[] (1.006200487s elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:12:16.822: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-njqh7" for this suite.
Nov 28 06:12:22.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:12:22.931: INFO: namespace: e2e-tests-services-njqh7, resource: bindings, ignored listing per whitelist
Nov 28 06:12:23.355: INFO: namespace e2e-tests-services-njqh7 deletion completed in 6.528219989s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:16.815 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:12:23.355: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1371
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Nov 28 06:12:23.484: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=k8s.gcr.io/nginx-slim-amd64:0.20 --namespace=e2e-tests-kubectl-thr2l'
Nov 28 06:12:23.634: INFO: stderr: ""
Nov 28 06:12:23.634: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1376
Nov 28 06:12:23.638: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-thr2l'
Nov 28 06:12:23.765: INFO: stderr: ""
Nov 28 06:12:23.765: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:12:23.765: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-thr2l" for this suite.
Nov 28 06:12:45.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:12:45.813: INFO: namespace: e2e-tests-kubectl-thr2l, resource: bindings, ignored listing per whitelist
Nov 28 06:12:46.298: INFO: namespace e2e-tests-kubectl-thr2l deletion completed in 22.528286938s

• [SLOW TEST:22.942 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:12:46.298: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-pmp9d
Nov 28 06:12:50.462: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-pmp9d
STEP: checking the pod's current state and verifying that restartCount is present
Nov 28 06:12:50.465: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:16:50.966: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-pmp9d" for this suite.
Nov 28 06:16:56.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:16:57.248: INFO: namespace: e2e-tests-container-probe-pmp9d, resource: bindings, ignored listing per whitelist
Nov 28 06:16:57.496: INFO: namespace e2e-tests-container-probe-pmp9d deletion completed in 6.526265656s

• [SLOW TEST:251.198 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:16:57.496: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating secret with name secret-test-map-359e2e4d-f2d5-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 06:16:57.651: INFO: Waiting up to 5m0s for pod "pod-secrets-359ede35-f2d5-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-secrets-bhn2d" to be "success or failure"
Nov 28 06:16:57.656: INFO: Pod "pod-secrets-359ede35-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.996793ms
Nov 28 06:16:59.660: INFO: Pod "pod-secrets-359ede35-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008709564s
Nov 28 06:17:01.664: INFO: Pod "pod-secrets-359ede35-f2d5-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013029781s
STEP: Saw pod success
Nov 28 06:17:01.664: INFO: Pod "pod-secrets-359ede35-f2d5-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:17:01.667: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-secrets-359ede35-f2d5-11e8-904b-0a58ac10ae35 container secret-volume-test: <nil>
STEP: delete the pod
Nov 28 06:17:01.688: INFO: Waiting for pod pod-secrets-359ede35-f2d5-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:17:01.691: INFO: Pod pod-secrets-359ede35-f2d5-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:17:01.691: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bhn2d" for this suite.
Nov 28 06:17:07.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:17:07.725: INFO: namespace: e2e-tests-secrets-bhn2d, resource: bindings, ignored listing per whitelist
Nov 28 06:17:08.220: INFO: namespace e2e-tests-secrets-bhn2d deletion completed in 6.525000059s

• [SLOW TEST:10.723 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:17:08.220: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating projection with secret that has name projected-secret-test-map-3c06722d-f2d5-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume secrets
Nov 28 06:17:08.407: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3c07cc41-f2d5-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-9tvxb" to be "success or failure"
Nov 28 06:17:08.411: INFO: Pod "pod-projected-secrets-3c07cc41-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.741436ms
Nov 28 06:17:10.415: INFO: Pod "pod-projected-secrets-3c07cc41-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007858064s
Nov 28 06:17:12.419: INFO: Pod "pod-projected-secrets-3c07cc41-f2d5-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012054076s
STEP: Saw pod success
Nov 28 06:17:12.419: INFO: Pod "pod-projected-secrets-3c07cc41-f2d5-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:17:12.422: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-projected-secrets-3c07cc41-f2d5-11e8-904b-0a58ac10ae35 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 28 06:17:12.441: INFO: Waiting for pod pod-projected-secrets-3c07cc41-f2d5-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:17:12.444: INFO: Pod pod-projected-secrets-3c07cc41-f2d5-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:17:12.444: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9tvxb" for this suite.
Nov 28 06:17:18.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:17:18.670: INFO: namespace: e2e-tests-projected-9tvxb, resource: bindings, ignored listing per whitelist
Nov 28 06:17:18.970: INFO: namespace e2e-tests-projected-9tvxb deletion completed in 6.522550739s

• [SLOW TEST:10.750 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:17:18.970: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 06:17:19.117: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 28 06:17:19.128: INFO: Number of nodes with available pods: 0
Nov 28 06:17:19.128: INFO: Node ci-op-qh5ir6tt-623b1-ig-m-jq6h is running more than one daemon pod
Nov 28 06:17:20.136: INFO: Number of nodes with available pods: 0
Nov 28 06:17:20.136: INFO: Node ci-op-qh5ir6tt-623b1-ig-m-jq6h is running more than one daemon pod
Nov 28 06:17:21.137: INFO: Number of nodes with available pods: 0
Nov 28 06:17:21.137: INFO: Node ci-op-qh5ir6tt-623b1-ig-m-jq6h is running more than one daemon pod
Nov 28 06:17:22.136: INFO: Number of nodes with available pods: 2
Nov 28 06:17:22.136: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-fcth is running more than one daemon pod
Nov 28 06:17:23.136: INFO: Number of nodes with available pods: 4
Nov 28 06:17:23.136: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 28 06:17:23.163: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:23.163: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:23.163: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:23.163: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:24.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:24.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:24.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:24.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:25.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:25.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:25.175: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:25.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:25.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:26.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:26.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:26.175: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:26.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:26.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:27.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:27.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:27.175: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:27.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:27.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:28.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:28.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:28.175: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:28.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:28.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:29.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:29.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:29.175: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:29.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:29.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:30.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:30.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:30.175: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:30.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:30.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:31.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:31.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:31.175: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:31.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:31.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:32.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:32.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:32.175: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:32.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:32.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:33.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:33.175: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:33.175: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:33.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:33.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:34.174: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:34.174: INFO: Wrong image for pod: daemon-set-cxsm6. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:34.174: INFO: Pod daemon-set-cxsm6 is not available
Nov 28 06:17:34.174: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:34.174: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:35.174: INFO: Pod daemon-set-7m6pn is not available
Nov 28 06:17:35.174: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:35.174: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:35.174: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:36.175: INFO: Pod daemon-set-7m6pn is not available
Nov 28 06:17:36.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:36.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:36.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:37.176: INFO: Pod daemon-set-7m6pn is not available
Nov 28 06:17:37.176: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:37.176: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:37.176: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:38.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:38.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:38.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:39.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:39.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:39.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:40.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:40.175: INFO: Wrong image for pod: daemon-set-n2nhq. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:40.175: INFO: Pod daemon-set-n2nhq is not available
Nov 28 06:17:40.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:41.179: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:41.179: INFO: Pod daemon-set-npj4b is not available
Nov 28 06:17:41.179: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:42.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:42.175: INFO: Pod daemon-set-npj4b is not available
Nov 28 06:17:42.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:43.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:43.175: INFO: Pod daemon-set-npj4b is not available
Nov 28 06:17:43.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:44.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:44.175: INFO: Pod daemon-set-npj4b is not available
Nov 28 06:17:44.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:45.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:45.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:46.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:46.175: INFO: Pod daemon-set-8slbn is not available
Nov 28 06:17:46.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:47.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:47.175: INFO: Pod daemon-set-8slbn is not available
Nov 28 06:17:47.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:48.175: INFO: Wrong image for pod: daemon-set-8slbn. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:48.175: INFO: Pod daemon-set-8slbn is not available
Nov 28 06:17:48.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:49.176: INFO: Pod daemon-set-jdwqv is not available
Nov 28 06:17:49.176: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:50.175: INFO: Pod daemon-set-jdwqv is not available
Nov 28 06:17:50.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:51.175: INFO: Pod daemon-set-jdwqv is not available
Nov 28 06:17:51.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:52.175: INFO: Pod daemon-set-jdwqv is not available
Nov 28 06:17:52.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:53.175: INFO: Wrong image for pod: daemon-set-nq4xd. Expected: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0.
Nov 28 06:17:53.175: INFO: Pod daemon-set-nq4xd is not available
Nov 28 06:17:54.175: INFO: Pod daemon-set-cgbz8 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 28 06:17:54.187: INFO: Number of nodes with available pods: 3
Nov 28 06:17:54.187: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:17:55.195: INFO: Number of nodes with available pods: 3
Nov 28 06:17:55.195: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:17:56.195: INFO: Number of nodes with available pods: 3
Nov 28 06:17:56.195: INFO: Node ci-op-qh5ir6tt-623b1-ig-n-x1wp is running more than one daemon pod
Nov 28 06:17:57.195: INFO: Number of nodes with available pods: 4
Nov 28 06:17:57.195: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-wttr2, will wait for the garbage collector to delete the pods
Nov 28 06:17:57.267: INFO: Deleting {extensions DaemonSet} daemon-set took: 5.965214ms
Nov 28 06:17:57.368: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.392706ms
Nov 28 06:18:09.571: INFO: Number of nodes with available pods: 0
Nov 28 06:18:09.571: INFO: Number of running nodes: 0, number of available pods: 0
Nov 28 06:18:09.574: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-wttr2/daemonsets","resourceVersion":"25287"},"items":null}

Nov 28 06:18:09.577: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-wttr2/pods","resourceVersion":"25287"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:18:09.588: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-wttr2" for this suite.
Nov 28 06:18:15.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:18:15.651: INFO: namespace: e2e-tests-daemonsets-wttr2, resource: bindings, ignored listing per whitelist
Nov 28 06:18:16.118: INFO: namespace e2e-tests-daemonsets-wttr2 deletion completed in 6.525817932s

• [SLOW TEST:57.148 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:18:16.118: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test substitution in container's args
Nov 28 06:18:17.269: INFO: Waiting up to 5m0s for pod "var-expansion-6479ff84-f2d5-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-var-expansion-rlgzh" to be "success or failure"
Nov 28 06:18:17.274: INFO: Pod "var-expansion-6479ff84-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.874516ms
Nov 28 06:18:19.278: INFO: Pod "var-expansion-6479ff84-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008930313s
Nov 28 06:18:21.282: INFO: Pod "var-expansion-6479ff84-f2d5-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013330775s
STEP: Saw pod success
Nov 28 06:18:21.282: INFO: Pod "var-expansion-6479ff84-f2d5-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:18:21.286: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod var-expansion-6479ff84-f2d5-11e8-904b-0a58ac10ae35 container dapi-container: <nil>
STEP: delete the pod
Nov 28 06:18:21.307: INFO: Waiting for pod var-expansion-6479ff84-f2d5-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:18:21.310: INFO: Pod var-expansion-6479ff84-f2d5-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:18:21.310: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-rlgzh" for this suite.
Nov 28 06:18:27.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:18:27.366: INFO: namespace: e2e-tests-var-expansion-rlgzh, resource: bindings, ignored listing per whitelist
Nov 28 06:18:27.838: INFO: namespace e2e-tests-var-expansion-rlgzh deletion completed in 6.524337387s

• [SLOW TEST:11.720 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:18:27.838: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1276
[It] should support rolling-update to same image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: running the image k8s.gcr.io/nginx-slim-amd64:0.20
Nov 28 06:18:27.960: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-rc --image=k8s.gcr.io/nginx-slim-amd64:0.20 --generator=run/v1 --namespace=e2e-tests-kubectl-z4tfc'
Nov 28 06:18:28.129: INFO: stderr: ""
Nov 28 06:18:28.129: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Nov 28 06:18:28.133: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Nov 28 06:18:28.137: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Nov 28 06:18:28.141: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update e2e-test-nginx-rc --update-period=1s --image=k8s.gcr.io/nginx-slim-amd64:0.20 --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-z4tfc'
Nov 28 06:18:43.938: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 28 06:18:43.938: INFO: stdout: "Created e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9\nScaling up e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Nov 28 06:18:43.938: INFO: stdout: "Created e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9\nScaling up e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Nov 28 06:18:43.938: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-z4tfc'
Nov 28 06:18:44.046: INFO: stderr: ""
Nov 28 06:18:44.046: INFO: stdout: "e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9-vmhpm "
Nov 28 06:18:44.046: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9-vmhpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z4tfc'
Nov 28 06:18:44.155: INFO: stderr: ""
Nov 28 06:18:44.155: INFO: stdout: "true"
Nov 28 06:18:44.155: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9-vmhpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-z4tfc'
Nov 28 06:18:44.266: INFO: stderr: ""
Nov 28 06:18:44.266: INFO: stdout: "k8s.gcr.io/nginx-slim-amd64:0.20"
Nov 28 06:18:44.266: INFO: e2e-test-nginx-rc-625fa3ca9d74a4ae36022560ac3242f9-vmhpm is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1282
Nov 28 06:18:44.266: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-z4tfc'
Nov 28 06:18:44.387: INFO: stderr: ""
Nov 28 06:18:44.387: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:18:44.387: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-z4tfc" for this suite.
Nov 28 06:18:50.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:18:50.483: INFO: namespace: e2e-tests-kubectl-z4tfc, resource: bindings, ignored listing per whitelist
Nov 28 06:18:50.916: INFO: namespace e2e-tests-kubectl-z4tfc deletion completed in 6.524105571s

• [SLOW TEST:23.078 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should support rolling-update to same image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:18:50.916: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap e2e-tests-configmap-dmd7z/configmap-test-7937cb07-f2d5-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 06:18:51.069: INFO: Waiting up to 5m0s for pod "pod-configmaps-79386dc6-f2d5-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-configmap-dmd7z" to be "success or failure"
Nov 28 06:18:51.072: INFO: Pod "pod-configmaps-79386dc6-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.205296ms
Nov 28 06:18:53.076: INFO: Pod "pod-configmaps-79386dc6-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007201244s
Nov 28 06:18:55.080: INFO: Pod "pod-configmaps-79386dc6-f2d5-11e8-904b-0a58ac10ae35": Phase="Running", Reason="", readiness=true. Elapsed: 4.010871035s
Nov 28 06:18:57.084: INFO: Pod "pod-configmaps-79386dc6-f2d5-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014819001s
STEP: Saw pod success
Nov 28 06:18:57.084: INFO: Pod "pod-configmaps-79386dc6-f2d5-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:18:57.087: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-x1wp pod pod-configmaps-79386dc6-f2d5-11e8-904b-0a58ac10ae35 container env-test: <nil>
STEP: delete the pod
Nov 28 06:18:57.107: INFO: Waiting for pod pod-configmaps-79386dc6-f2d5-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:18:57.109: INFO: Pod pod-configmaps-79386dc6-f2d5-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:18:57.110: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-dmd7z" for this suite.
Nov 28 06:19:03.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:19:03.296: INFO: namespace: e2e-tests-configmap-dmd7z, resource: bindings, ignored listing per whitelist
Nov 28 06:19:03.639: INFO: namespace e2e-tests-configmap-dmd7z deletion completed in 6.525786472s

• [SLOW TEST:12.723 seconds]
[sig-api-machinery] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:29
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:19:03.639: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 06:19:03.808: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80d09904-f2d5-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-7cg27" to be "success or failure"
Nov 28 06:19:03.812: INFO: Pod "downwardapi-volume-80d09904-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.162344ms
Nov 28 06:19:05.816: INFO: Pod "downwardapi-volume-80d09904-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00782236s
Nov 28 06:19:07.820: INFO: Pod "downwardapi-volume-80d09904-f2d5-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011836019s
STEP: Saw pod success
Nov 28 06:19:07.820: INFO: Pod "downwardapi-volume-80d09904-f2d5-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:19:07.823: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod downwardapi-volume-80d09904-f2d5-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 06:19:07.841: INFO: Waiting for pod downwardapi-volume-80d09904-f2d5-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:19:07.843: INFO: Pod downwardapi-volume-80d09904-f2d5-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:19:07.843: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7cg27" for this suite.
Nov 28 06:19:13.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:19:13.908: INFO: namespace: e2e-tests-projected-7cg27, resource: bindings, ignored listing per whitelist
Nov 28 06:19:14.375: INFO: namespace e2e-tests-projected-7cg27 deletion completed in 6.527714953s

• [SLOW TEST:10.736 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:19:14.375: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 06:19:36.524: INFO: Container started at 2018-11-28 06:19:16 +0000 UTC, pod became ready at 2018-11-28 06:19:35 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:19:36.524: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-hr724" for this suite.
Nov 28 06:19:58.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:19:58.652: INFO: namespace: e2e-tests-container-probe-hr724, resource: bindings, ignored listing per whitelist
Nov 28 06:19:59.056: INFO: namespace e2e-tests-container-probe-hr724 deletion completed in 22.526313382s

• [SLOW TEST:44.681 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:19:59.056: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating Redis RC
Nov 28 06:19:59.190: INFO: namespace e2e-tests-kubectl-j4bbk
Nov 28 06:19:59.190: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-j4bbk'
Nov 28 06:19:59.442: INFO: stderr: ""
Nov 28 06:19:59.442: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 28 06:20:00.446: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 06:20:00.446: INFO: Found 0 / 1
Nov 28 06:20:01.446: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 06:20:01.446: INFO: Found 0 / 1
Nov 28 06:20:02.445: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 06:20:02.445: INFO: Found 0 / 1
Nov 28 06:20:03.446: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 06:20:03.446: INFO: Found 1 / 1
Nov 28 06:20:03.446: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 28 06:20:03.449: INFO: Selector matched 1 pods for map[app:redis]
Nov 28 06:20:03.449: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 28 06:20:03.449: INFO: wait on redis-master startup in e2e-tests-kubectl-j4bbk 
Nov 28 06:20:03.449: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs redis-master-2cxxt redis-master --namespace=e2e-tests-kubectl-j4bbk'
Nov 28 06:20:03.576: INFO: stderr: ""
Nov 28 06:20:03.576: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Nov 06:20:02.259 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Nov 06:20:02.259 # Server started, Redis version 3.2.12\n1:M 28 Nov 06:20:02.260 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Nov 06:20:02.260 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Nov 28 06:20:03.576: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-j4bbk'
Nov 28 06:20:03.705: INFO: stderr: ""
Nov 28 06:20:03.705: INFO: stdout: "service/rm2 exposed\n"
Nov 28 06:20:03.708: INFO: Service rm2 in namespace e2e-tests-kubectl-j4bbk found.
STEP: exposing service
Nov 28 06:20:05.714: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-j4bbk'
Nov 28 06:20:05.841: INFO: stderr: ""
Nov 28 06:20:05.841: INFO: stdout: "service/rm3 exposed\n"
Nov 28 06:20:05.844: INFO: Service rm3 in namespace e2e-tests-kubectl-j4bbk found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:20:07.851: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-j4bbk" for this suite.
Nov 28 06:20:29.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:20:29.905: INFO: namespace: e2e-tests-kubectl-j4bbk, resource: bindings, ignored listing per whitelist
Nov 28 06:20:30.378: INFO: namespace e2e-tests-kubectl-j4bbk deletion completed in 22.523536789s

• [SLOW TEST:31.323 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should create services for rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:20:30.379: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 28 06:20:30.557: INFO: Waiting up to 5m0s for pod "pod-b485db31-f2d5-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-lkllb" to be "success or failure"
Nov 28 06:20:30.561: INFO: Pod "pod-b485db31-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.394968ms
Nov 28 06:20:32.565: INFO: Pod "pod-b485db31-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007769508s
Nov 28 06:20:34.569: INFO: Pod "pod-b485db31-f2d5-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011825227s
STEP: Saw pod success
Nov 28 06:20:34.569: INFO: Pod "pod-b485db31-f2d5-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:20:34.572: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-b485db31-f2d5-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 06:20:34.592: INFO: Waiting for pod pod-b485db31-f2d5-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:20:34.597: INFO: Pod pod-b485db31-f2d5-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:20:34.597: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lkllb" for this suite.
Nov 28 06:20:40.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:20:40.667: INFO: namespace: e2e-tests-emptydir-lkllb, resource: bindings, ignored listing per whitelist
Nov 28 06:20:41.132: INFO: namespace e2e-tests-emptydir-lkllb deletion completed in 6.529075184s

• [SLOW TEST:10.753 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:20:41.132: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name configmap-test-volume-baea3e3c-f2d5-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 06:20:41.280: INFO: Waiting up to 5m0s for pod "pod-configmaps-baeac115-f2d5-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-configmap-8nwxw" to be "success or failure"
Nov 28 06:20:41.288: INFO: Pod "pod-configmaps-baeac115-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 7.84109ms
Nov 28 06:20:43.291: INFO: Pod "pod-configmaps-baeac115-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011671372s
Nov 28 06:20:45.295: INFO: Pod "pod-configmaps-baeac115-f2d5-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015417739s
STEP: Saw pod success
Nov 28 06:20:45.295: INFO: Pod "pod-configmaps-baeac115-f2d5-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:20:45.300: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-configmaps-baeac115-f2d5-11e8-904b-0a58ac10ae35 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 06:20:45.319: INFO: Waiting for pod pod-configmaps-baeac115-f2d5-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:20:45.321: INFO: Pod pod-configmaps-baeac115-f2d5-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:20:45.321: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8nwxw" for this suite.
Nov 28 06:20:51.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:20:51.444: INFO: namespace: e2e-tests-configmap-8nwxw, resource: bindings, ignored listing per whitelist
Nov 28 06:20:51.849: INFO: namespace e2e-tests-configmap-8nwxw deletion completed in 6.524669388s

• [SLOW TEST:10.717 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:20:51.849: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 Pods, got 2 Pods
STEP: Gathering metrics
W1128 06:20:53.010266    6241 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 28 06:20:53.010: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:20:53.010: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-nj9t7" for this suite.
Nov 28 06:20:59.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:20:59.052: INFO: namespace: e2e-tests-gc-nj9t7, resource: bindings, ignored listing per whitelist
Nov 28 06:20:59.537: INFO: namespace e2e-tests-gc-nj9t7 deletion completed in 6.522829058s

• [SLOW TEST:7.688 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:20:59.537: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 28 06:20:59.730: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-52jtl,SelfLink:/api/v1/namespaces/e2e-tests-watch-52jtl/configmaps/e2e-watch-test-resource-version,UID:c5e08b8a-f2d5-11e8-b66f-42010a8e0002,ResourceVersion:26460,Generation:0,CreationTimestamp:2018-11-28 06:20:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 28 06:20:59.730: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-52jtl,SelfLink:/api/v1/namespaces/e2e-tests-watch-52jtl/configmaps/e2e-watch-test-resource-version,UID:c5e08b8a-f2d5-11e8-b66f-42010a8e0002,ResourceVersion:26461,Generation:0,CreationTimestamp:2018-11-28 06:20:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:20:59.730: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-52jtl" for this suite.
Nov 28 06:21:05.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:21:06.107: INFO: namespace: e2e-tests-watch-52jtl, resource: bindings, ignored listing per whitelist
Nov 28 06:21:06.258: INFO: namespace e2e-tests-watch-52jtl deletion completed in 6.523347188s

• [SLOW TEST:6.721 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:21:06.258: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name configmap-projected-all-test-volume-c9e1bcb9-f2d5-11e8-904b-0a58ac10ae35
STEP: Creating secret with name secret-projected-all-test-volume-c9e1bca5-f2d5-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 28 06:21:06.406: INFO: Waiting up to 5m0s for pod "projected-volume-c9e1bc5e-f2d5-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-vxr2x" to be "success or failure"
Nov 28 06:21:06.411: INFO: Pod "projected-volume-c9e1bc5e-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154072ms
Nov 28 06:21:08.417: INFO: Pod "projected-volume-c9e1bc5e-f2d5-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0107783s
Nov 28 06:21:10.421: INFO: Pod "projected-volume-c9e1bc5e-f2d5-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014768532s
STEP: Saw pod success
Nov 28 06:21:10.421: INFO: Pod "projected-volume-c9e1bc5e-f2d5-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:21:10.424: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod projected-volume-c9e1bc5e-f2d5-11e8-904b-0a58ac10ae35 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 28 06:21:10.444: INFO: Waiting for pod projected-volume-c9e1bc5e-f2d5-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:21:10.446: INFO: Pod projected-volume-c9e1bc5e-f2d5-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:21:10.446: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vxr2x" for this suite.
Nov 28 06:21:16.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:21:16.494: INFO: namespace: e2e-tests-projected-vxr2x, resource: bindings, ignored listing per whitelist
Nov 28 06:21:16.974: INFO: namespace e2e-tests-projected-vxr2x deletion completed in 6.524379199s

• [SLOW TEST:10.716 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:21:16.975: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:127
[It] should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 28 06:21:21.663: INFO: Successfully updated pod "pod-update-d048de20-f2d5-11e8-904b-0a58ac10ae35"
STEP: verifying the updated pod is in kubernetes
Nov 28 06:21:21.668: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:21:21.668: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6mxsf" for this suite.
Nov 28 06:21:43.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:21:43.731: INFO: namespace: e2e-tests-pods-6mxsf, resource: bindings, ignored listing per whitelist
Nov 28 06:21:44.196: INFO: namespace e2e-tests-pods-6mxsf deletion completed in 22.524088586s

• [SLOW TEST:27.222 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
  should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:21:44.196: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: creating all guestbook components
Nov 28 06:21:44.356: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Nov 28 06:21:44.356: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:21:44.677: INFO: stderr: ""
Nov 28 06:21:44.677: INFO: stdout: "service/redis-slave created\n"
Nov 28 06:21:44.685: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Nov 28 06:21:44.686: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:21:44.944: INFO: stderr: ""
Nov 28 06:21:44.944: INFO: stdout: "service/redis-master created\n"
Nov 28 06:21:44.944: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 28 06:21:44.944: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:21:45.195: INFO: stderr: ""
Nov 28 06:21:45.195: INFO: stdout: "service/frontend created\n"
Nov 28 06:21:45.196: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend-amd64:v5
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Nov 28 06:21:45.196: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:21:45.453: INFO: stderr: ""
Nov 28 06:21:45.453: INFO: stdout: "deployment.extensions/frontend created\n"
Nov 28 06:21:45.453: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 28 06:21:45.453: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:21:45.708: INFO: stderr: ""
Nov 28 06:21:45.708: INFO: stdout: "deployment.extensions/redis-master created\n"
Nov 28 06:21:45.708: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave-amd64:v2
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Nov 28 06:21:45.709: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:21:45.956: INFO: stderr: ""
Nov 28 06:21:45.956: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Nov 28 06:21:45.956: INFO: Waiting for all frontend pods to be Running.
Nov 28 06:22:11.008: INFO: Waiting for frontend to serve content.
Nov 28 06:22:11.025: INFO: Trying to add a new entry to the guestbook.
Nov 28 06:22:11.046: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov 28 06:22:11.059: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:22:11.186: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 06:22:11.186: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Nov 28 06:22:11.186: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:22:11.308: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 06:22:11.308: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 28 06:22:11.309: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:22:11.429: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 06:22:11.429: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 28 06:22:11.430: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:22:11.540: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 06:22:11.541: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 28 06:22:11.541: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:22:11.654: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 06:22:11.654: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 28 06:22:11.655: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6kttq'
Nov 28 06:22:11.764: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 28 06:22:11.764: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:22:11.764: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6kttq" for this suite.
Nov 28 06:22:51.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:22:51.829: INFO: namespace: e2e-tests-kubectl-6kttq, resource: bindings, ignored listing per whitelist
Nov 28 06:22:52.298: INFO: namespace e2e-tests-kubectl-6kttq deletion completed in 40.528492726s

• [SLOW TEST:68.101 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679
    should create and stop a working application  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:22:52.298: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 06:22:52.469: INFO: Waiting up to 5m0s for pod "downwardapi-volume-091c999d-f2d6-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-downward-api-456dx" to be "success or failure"
Nov 28 06:22:52.472: INFO: Pod "downwardapi-volume-091c999d-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.852593ms
Nov 28 06:22:54.476: INFO: Pod "downwardapi-volume-091c999d-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007050687s
Nov 28 06:22:56.480: INFO: Pod "downwardapi-volume-091c999d-f2d6-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01117678s
STEP: Saw pod success
Nov 28 06:22:56.480: INFO: Pod "downwardapi-volume-091c999d-f2d6-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:22:56.483: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod downwardapi-volume-091c999d-f2d6-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 06:22:56.506: INFO: Waiting for pod downwardapi-volume-091c999d-f2d6-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:22:56.509: INFO: Pod downwardapi-volume-091c999d-f2d6-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:22:56.509: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-456dx" for this suite.
Nov 28 06:23:02.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:23:02.591: INFO: namespace: e2e-tests-downward-api-456dx, resource: bindings, ignored listing per whitelist
Nov 28 06:23:03.033: INFO: namespace e2e-tests-downward-api-456dx deletion completed in 6.520562209s

• [SLOW TEST:10.735 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:23:03.033: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 28 06:23:03.206: INFO: Waiting up to 5m0s for pod "pod-0f82045a-f2d6-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-kwphr" to be "success or failure"
Nov 28 06:23:03.209: INFO: Pod "pod-0f82045a-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.302313ms
Nov 28 06:23:05.212: INFO: Pod "pod-0f82045a-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006065409s
Nov 28 06:23:07.216: INFO: Pod "pod-0f82045a-f2d6-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009946448s
STEP: Saw pod success
Nov 28 06:23:07.216: INFO: Pod "pod-0f82045a-f2d6-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:23:07.219: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod pod-0f82045a-f2d6-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 06:23:07.240: INFO: Waiting for pod pod-0f82045a-f2d6-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:23:07.242: INFO: Pod pod-0f82045a-f2d6-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:23:07.242: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kwphr" for this suite.
Nov 28 06:23:13.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:23:13.341: INFO: namespace: e2e-tests-emptydir-kwphr, resource: bindings, ignored listing per whitelist
Nov 28 06:23:13.769: INFO: namespace e2e-tests-emptydir-kwphr deletion completed in 6.523447744s

• [SLOW TEST:10.736 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:23:13.769: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 06:23:13.926: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-15e831e7-f2d6-11e8-904b-0a58ac10ae35
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-15e831e7-f2d6-11e8-904b-0a58ac10ae35
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:24:42.516: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gtmdb" for this suite.
Nov 28 06:25:04.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:25:04.674: INFO: namespace: e2e-tests-projected-gtmdb, resource: bindings, ignored listing per whitelist
Nov 28 06:25:05.048: INFO: namespace e2e-tests-projected-gtmdb deletion completed in 22.526747134s

• [SLOW TEST:111.279 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:25:05.048: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 28 06:25:06.240: INFO: Waiting up to 5m0s for pod "pod-583eacde-f2d6-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-emptydir-m6v75" to be "success or failure"
Nov 28 06:25:06.243: INFO: Pod "pod-583eacde-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 3.019246ms
Nov 28 06:25:08.247: INFO: Pod "pod-583eacde-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006851668s
Nov 28 06:25:10.251: INFO: Pod "pod-583eacde-f2d6-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01139482s
STEP: Saw pod success
Nov 28 06:25:10.251: INFO: Pod "pod-583eacde-f2d6-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:25:10.254: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-583eacde-f2d6-11e8-904b-0a58ac10ae35 container test-container: <nil>
STEP: delete the pod
Nov 28 06:25:10.278: INFO: Waiting for pod pod-583eacde-f2d6-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:25:10.281: INFO: Pod pod-583eacde-f2d6-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:25:10.281: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-m6v75" for this suite.
Nov 28 06:25:16.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:25:16.609: INFO: namespace: e2e-tests-emptydir-m6v75, resource: bindings, ignored listing per whitelist
Nov 28 06:25:16.811: INFO: namespace e2e-tests-emptydir-m6v75 deletion completed in 6.525315488s

• [SLOW TEST:11.763 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:25:16.811: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating a pod to test downward API volume plugin
Nov 28 06:25:16.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f3b3e39-f2d6-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-dpjwv" to be "success or failure"
Nov 28 06:25:16.965: INFO: Pod "downwardapi-volume-5f3b3e39-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.426761ms
Nov 28 06:25:18.969: INFO: Pod "downwardapi-volume-5f3b3e39-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008734354s
Nov 28 06:25:20.974: INFO: Pod "downwardapi-volume-5f3b3e39-f2d6-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013983082s
STEP: Saw pod success
Nov 28 06:25:20.974: INFO: Pod "downwardapi-volume-5f3b3e39-f2d6-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:25:20.978: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-fcth pod downwardapi-volume-5f3b3e39-f2d6-11e8-904b-0a58ac10ae35 container client-container: <nil>
STEP: delete the pod
Nov 28 06:25:21.004: INFO: Waiting for pod downwardapi-volume-5f3b3e39-f2d6-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:25:21.007: INFO: Pod downwardapi-volume-5f3b3e39-f2d6-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:25:21.007: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dpjwv" for this suite.
Nov 28 06:25:27.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:25:27.104: INFO: namespace: e2e-tests-projected-dpjwv, resource: bindings, ignored listing per whitelist
Nov 28 06:25:27.540: INFO: namespace e2e-tests-projected-dpjwv deletion completed in 6.527262552s

• [SLOW TEST:10.729 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:25:27.540: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
Nov 28 06:25:27.677: INFO: (0) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 7.660292ms)
Nov 28 06:25:27.681: INFO: (1) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.789381ms)
Nov 28 06:25:27.685: INFO: (2) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.425916ms)
Nov 28 06:25:27.689: INFO: (3) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.968801ms)
Nov 28 06:25:27.693: INFO: (4) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.679605ms)
Nov 28 06:25:27.696: INFO: (5) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.008119ms)
Nov 28 06:25:27.700: INFO: (6) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.572722ms)
Nov 28 06:25:27.704: INFO: (7) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.507841ms)
Nov 28 06:25:27.707: INFO: (8) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.221903ms)
Nov 28 06:25:27.710: INFO: (9) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.576579ms)
Nov 28 06:25:27.714: INFO: (10) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.765155ms)
Nov 28 06:25:27.718: INFO: (11) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.794685ms)
Nov 28 06:25:27.722: INFO: (12) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.79504ms)
Nov 28 06:25:27.726: INFO: (13) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.257961ms)
Nov 28 06:25:27.730: INFO: (14) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.571601ms)
Nov 28 06:25:27.733: INFO: (15) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.564351ms)
Nov 28 06:25:27.737: INFO: (16) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.716059ms)
Nov 28 06:25:27.741: INFO: (17) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.942232ms)
Nov 28 06:25:27.745: INFO: (18) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.623847ms)
Nov 28 06:25:27.748: INFO: (19) /api/v1/nodes/ci-op-qh5ir6tt-623b1-ig-n-fcth/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.349103ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:25:27.748: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-tf58t" for this suite.
Nov 28 06:25:33.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:25:33.835: INFO: namespace: e2e-tests-proxy-tf58t, resource: bindings, ignored listing per whitelist
Nov 28 06:25:33.892: INFO: namespace e2e-tests-proxy-tf58t deletion completed in 6.139535729s

• [SLOW TEST:6.352 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:25:33.892: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating the pod
Nov 28 06:25:39.562: INFO: Successfully updated pod "annotationupdate69673ddd-f2d6-11e8-904b-0a58ac10ae35"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:25:43.600: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-km4zv" for this suite.
Nov 28 06:26:05.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:26:05.642: INFO: namespace: e2e-tests-downward-api-km4zv, resource: bindings, ignored listing per whitelist
Nov 28 06:26:06.127: INFO: namespace e2e-tests-downward-api-km4zv deletion completed in 22.52258166s

• [SLOW TEST:32.235 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:26:06.127: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating configMap with name projected-configmap-test-volume-7ca70d0e-f2d6-11e8-904b-0a58ac10ae35
STEP: Creating a pod to test consume configMaps
Nov 28 06:26:06.326: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7ca7dbf0-f2d6-11e8-904b-0a58ac10ae35" in namespace "e2e-tests-projected-jzwv9" to be "success or failure"
Nov 28 06:26:06.328: INFO: Pod "pod-projected-configmaps-7ca7dbf0-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.699034ms
Nov 28 06:26:08.332: INFO: Pod "pod-projected-configmaps-7ca7dbf0-f2d6-11e8-904b-0a58ac10ae35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006660736s
Nov 28 06:26:10.337: INFO: Pod "pod-projected-configmaps-7ca7dbf0-f2d6-11e8-904b-0a58ac10ae35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011148305s
STEP: Saw pod success
Nov 28 06:26:10.337: INFO: Pod "pod-projected-configmaps-7ca7dbf0-f2d6-11e8-904b-0a58ac10ae35" satisfied condition "success or failure"
Nov 28 06:26:10.340: INFO: Trying to get logs from node ci-op-qh5ir6tt-623b1-ig-n-sr0b pod pod-projected-configmaps-7ca7dbf0-f2d6-11e8-904b-0a58ac10ae35 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 28 06:26:10.361: INFO: Waiting for pod pod-projected-configmaps-7ca7dbf0-f2d6-11e8-904b-0a58ac10ae35 to disappear
Nov 28 06:26:10.363: INFO: Pod pod-projected-configmaps-7ca7dbf0-f2d6-11e8-904b-0a58ac10ae35 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:26:10.363: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jzwv9" for this suite.
Nov 28 06:26:16.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:26:16.741: INFO: namespace: e2e-tests-projected-jzwv9, resource: bindings, ignored listing per whitelist
Nov 28 06:26:16.892: INFO: namespace e2e-tests-projected-jzwv9 deletion completed in 6.524833472s

• [SLOW TEST:10.765 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141
STEP: Creating a kubernetes client
Nov 28 06:26:16.892: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
STEP: Creating the pod
Nov 28 06:26:19.579: INFO: Successfully updated pod "labelsupdate830b7303-f2d6-11e8-904b-0a58ac10ae35"
[AfterEach] [sig-storage] Projected
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
Nov 28 06:26:21.600: INFO: Waiting up to 3m0s for all (but 1) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dm9m4" for this suite.
Nov 28 06:26:43.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 28 06:26:43.702: INFO: namespace: e2e-tests-projected-dm9m4, resource: bindings, ignored listing per whitelist
Nov 28 06:26:44.131: INFO: namespace e2e-tests-projected-dm9m4 deletion completed in 22.526508649s

• [SLOW TEST:27.239 seconds]
[sig-storage] Projected
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684
------------------------------
SSNov 28 06:26:44.131: INFO: Running AfterSuite actions on all node
Nov 28 06:26:44.131: INFO: Running AfterSuite actions on node 1
Nov 28 06:26:44.131: INFO: Dumping logs locally to: /tmp/artifacts
Nov 28 06:26:44.132: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 165 of 997 Specs in 4749.474 seconds
SUCCESS! -- 165 Passed | 0 Failed | 0 Pending | 832 Skipped PASS
