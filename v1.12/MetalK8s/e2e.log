Mar 22 13:10:57.886: INFO: Overriding default scale value of zero to 1
Mar 22 13:10:57.886: INFO: Overriding default milliseconds value of zero to 5000
I0322 13:10:58.363391      20 test_context.go:385] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-171814416
I0322 13:10:58.363535      20 e2e.go:304] Starting e2e run "ee994162-4ca3-11e9-99ae-9e98f636e47c" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1553260257 - Will randomize all specs
Will run 188 of 1814 specs

Mar 22 13:10:58.529: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:10:58.532: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 22 13:10:58.558: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 22 13:10:58.602: INFO: 28 / 28 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 22 13:10:58.602: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Mar 22 13:10:58.602: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 22 13:10:58.612: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar 22 13:10:58.612: INFO: e2e test version: v1.12.1
Mar 22 13:10:58.614: INFO: kube-apiserver version: v1.12.3
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:10:58.614: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename statefulset
Mar 22 13:10:58.745: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar 22 13:10:58.769: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-9wwf2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-9wwf2
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Mar 22 13:10:58.963: INFO: Found 0 stateful pods, waiting for 3
Mar 22 13:11:08.986: INFO: Found 2 stateful pods, waiting for 3
Mar 22 13:11:18.975: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 13:11:18.975: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 13:11:18.975: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Mar 22 13:11:28.979: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 13:11:28.979: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 13:11:28.979: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar 22 13:11:29.017: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 22 13:11:39.070: INFO: Updating stateful set ss2
Mar 22 13:11:39.080: INFO: Waiting for Pod e2e-tests-statefulset-9wwf2/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Mar 22 13:11:49.149: INFO: Found 2 stateful pods, waiting for 3
Mar 22 13:11:59.166: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 13:11:59.166: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 13:11:59.166: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 22 13:11:59.221: INFO: Updating stateful set ss2
Mar 22 13:11:59.233: INFO: Waiting for Pod e2e-tests-statefulset-9wwf2/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar 22 13:12:09.276: INFO: Updating stateful set ss2
Mar 22 13:12:09.291: INFO: Waiting for StatefulSet e2e-tests-statefulset-9wwf2/ss2 to complete update
Mar 22 13:12:09.291: INFO: Waiting for Pod e2e-tests-statefulset-9wwf2/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 22 13:12:19.309: INFO: Deleting all statefulset in ns e2e-tests-statefulset-9wwf2
Mar 22 13:12:19.315: INFO: Scaling statefulset ss2 to 0
Mar 22 13:12:39.349: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 13:12:39.354: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:12:39.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-9wwf2" for this suite.
Mar 22 13:12:47.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:12:47.775: INFO: namespace: e2e-tests-statefulset-9wwf2, resource: bindings, ignored listing per whitelist
Mar 22 13:12:48.123: INFO: namespace e2e-tests-statefulset-9wwf2 deletion completed in 8.725658121s

â€¢ [SLOW TEST:109.510 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:12:48.124: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-m5b66
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-30956d9a-4ca4-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:12:48.518: INFO: Waiting up to 5m0s for pod "pod-secrets-309690ff-4ca4-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-secrets-m5b66" to be "success or failure"
Mar 22 13:12:48.565: INFO: Pod "pod-secrets-309690ff-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 46.433311ms
Mar 22 13:12:50.590: INFO: Pod "pod-secrets-309690ff-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071878278s
Mar 22 13:12:52.595: INFO: Pod "pod-secrets-309690ff-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076932954s
Mar 22 13:12:54.601: INFO: Pod "pod-secrets-309690ff-4ca4-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082764083s
STEP: Saw pod success
Mar 22 13:12:54.601: INFO: Pod "pod-secrets-309690ff-4ca4-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:12:54.606: INFO: Trying to get logs from node metalk8s-02 pod pod-secrets-309690ff-4ca4-11e9-99ae-9e98f636e47c container secret-volume-test: <nil>
STEP: delete the pod
Mar 22 13:12:54.654: INFO: Waiting for pod pod-secrets-309690ff-4ca4-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:12:54.669: INFO: Pod pod-secrets-309690ff-4ca4-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:12:54.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-m5b66" for this suite.
Mar 22 13:13:00.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:13:00.749: INFO: namespace: e2e-tests-secrets-m5b66, resource: bindings, ignored listing per whitelist
Mar 22 13:13:00.908: INFO: namespace e2e-tests-secrets-m5b66 deletion completed in 6.230832423s

â€¢ [SLOW TEST:12.784 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:13:00.908: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-mswhr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-38437b08-4ca4-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:13:01.401: INFO: Waiting up to 5m0s for pod "pod-secrets-38447dde-4ca4-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-secrets-mswhr" to be "success or failure"
Mar 22 13:13:01.410: INFO: Pod "pod-secrets-38447dde-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.979037ms
Mar 22 13:13:03.416: INFO: Pod "pod-secrets-38447dde-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015308015s
Mar 22 13:13:05.427: INFO: Pod "pod-secrets-38447dde-4ca4-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025762429s
STEP: Saw pod success
Mar 22 13:13:05.427: INFO: Pod "pod-secrets-38447dde-4ca4-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:13:05.433: INFO: Trying to get logs from node metalk8s-01 pod pod-secrets-38447dde-4ca4-11e9-99ae-9e98f636e47c container secret-env-test: <nil>
STEP: delete the pod
Mar 22 13:13:05.500: INFO: Waiting for pod pod-secrets-38447dde-4ca4-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:13:05.516: INFO: Pod pod-secrets-38447dde-4ca4-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:13:05.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mswhr" for this suite.
Mar 22 13:13:11.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:13:11.774: INFO: namespace: e2e-tests-secrets-mswhr, resource: bindings, ignored listing per whitelist
Mar 22 13:13:11.800: INFO: namespace e2e-tests-secrets-mswhr deletion completed in 6.268488825s

â€¢ [SLOW TEST:10.892 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:13:11.800: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-pds8z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-pds8z
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-pds8z
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-pds8z
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-pds8z
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-pds8z
Mar 22 13:13:16.135: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-pds8z, name: ss-0, uid: 41107608-4ca4-11e9-b0fd-fa163e8e51f5, status phase: Pending. Waiting for statefulset controller to delete.
Mar 22 13:13:16.694: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-pds8z, name: ss-0, uid: 41107608-4ca4-11e9-b0fd-fa163e8e51f5, status phase: Failed. Waiting for statefulset controller to delete.
Mar 22 13:13:16.705: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-pds8z, name: ss-0, uid: 41107608-4ca4-11e9-b0fd-fa163e8e51f5, status phase: Failed. Waiting for statefulset controller to delete.
Mar 22 13:13:16.715: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-pds8z
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-pds8z
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-pds8z and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 22 13:13:20.777: INFO: Deleting all statefulset in ns e2e-tests-statefulset-pds8z
Mar 22 13:13:20.792: INFO: Scaling statefulset ss to 0
Mar 22 13:13:40.826: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 13:13:40.831: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:13:40.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-pds8z" for this suite.
Mar 22 13:13:48.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:13:48.917: INFO: namespace: e2e-tests-statefulset-pds8z, resource: bindings, ignored listing per whitelist
Mar 22 13:13:49.065: INFO: namespace e2e-tests-statefulset-pds8z deletion completed in 8.173559175s

â€¢ [SLOW TEST:37.266 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:13:49.066: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-7hptc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:13:55.326: INFO: Waiting up to 5m0s for pod "client-envvars-5868f91c-4ca4-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-pods-7hptc" to be "success or failure"
Mar 22 13:13:55.333: INFO: Pod "client-envvars-5868f91c-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.728232ms
Mar 22 13:13:57.339: INFO: Pod "client-envvars-5868f91c-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01265193s
Mar 22 13:13:59.347: INFO: Pod "client-envvars-5868f91c-4ca4-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021130351s
STEP: Saw pod success
Mar 22 13:13:59.347: INFO: Pod "client-envvars-5868f91c-4ca4-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:13:59.356: INFO: Trying to get logs from node metalk8s-05 pod client-envvars-5868f91c-4ca4-11e9-99ae-9e98f636e47c container env3cont: <nil>
STEP: delete the pod
Mar 22 13:13:59.447: INFO: Waiting for pod client-envvars-5868f91c-4ca4-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:13:59.452: INFO: Pod client-envvars-5868f91c-4ca4-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:13:59.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-7hptc" for this suite.
Mar 22 13:14:43.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:14:43.643: INFO: namespace: e2e-tests-pods-7hptc, resource: bindings, ignored listing per whitelist
Mar 22 13:14:43.699: INFO: namespace e2e-tests-pods-7hptc deletion completed in 44.239304245s

â€¢ [SLOW TEST:54.633 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:14:43.699: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gf8qf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar 22 13:14:46.466: INFO: Successfully updated pod "annotationupdate755e9067-4ca4-11e9-99ae-9e98f636e47c"
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:14:48.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gf8qf" for this suite.
Mar 22 13:15:13.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:15:13.275: INFO: namespace: e2e-tests-projected-gf8qf, resource: bindings, ignored listing per whitelist
Mar 22 13:15:13.414: INFO: namespace e2e-tests-projected-gf8qf deletion completed in 24.906797781s

â€¢ [SLOW TEST:29.716 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:15:13.415: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7qxsp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Mar 22 13:15:13.659: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-171814416 proxy --unix-socket=/tmp/kubectl-proxy-unix165425711/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:15:13.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7qxsp" for this suite.
Mar 22 13:15:19.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:15:19.924: INFO: namespace: e2e-tests-kubectl-7qxsp, resource: bindings, ignored listing per whitelist
Mar 22 13:15:19.983: INFO: namespace e2e-tests-kubectl-7qxsp deletion completed in 6.23827565s

â€¢ [SLOW TEST:6.569 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:15:19.984: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-xsk4t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 22 13:15:20.306: INFO: Waiting up to 5m0s for pod "pod-8b0cf5dd-4ca4-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-xsk4t" to be "success or failure"
Mar 22 13:15:20.314: INFO: Pod "pod-8b0cf5dd-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.760623ms
Mar 22 13:15:22.327: INFO: Pod "pod-8b0cf5dd-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020742722s
Mar 22 13:15:24.336: INFO: Pod "pod-8b0cf5dd-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029933948s
Mar 22 13:15:26.341: INFO: Pod "pod-8b0cf5dd-4ca4-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034848458s
STEP: Saw pod success
Mar 22 13:15:26.341: INFO: Pod "pod-8b0cf5dd-4ca4-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:15:26.345: INFO: Trying to get logs from node metalk8s-01 pod pod-8b0cf5dd-4ca4-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:15:26.385: INFO: Waiting for pod pod-8b0cf5dd-4ca4-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:15:26.390: INFO: Pod pod-8b0cf5dd-4ca4-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:15:26.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xsk4t" for this suite.
Mar 22 13:15:32.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:15:32.547: INFO: namespace: e2e-tests-emptydir-xsk4t, resource: bindings, ignored listing per whitelist
Mar 22 13:15:32.611: INFO: namespace e2e-tests-emptydir-xsk4t deletion completed in 6.21368744s

â€¢ [SLOW TEST:12.627 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:15:32.611: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-9v654
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-9v654.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-9v654.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-9v654.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-9v654.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-9v654.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-9v654.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 22 13:16:03.096: INFO: DNS probes using e2e-tests-dns-9v654/dns-test-928c0f14-4ca4-11e9-99ae-9e98f636e47c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:16:03.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-9v654" for this suite.
Mar 22 13:16:09.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:16:09.366: INFO: namespace: e2e-tests-dns-9v654, resource: bindings, ignored listing per whitelist
Mar 22 13:16:09.367: INFO: namespace e2e-tests-dns-9v654 deletion completed in 6.222884148s

â€¢ [SLOW TEST:36.757 seconds]
[sig-network] DNS
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:16:09.368: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nd4h5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar 22 13:16:16.222: INFO: Successfully updated pod "labelsupdatea878ef05-4ca4-11e9-99ae-9e98f636e47c"
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:16:18.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nd4h5" for this suite.
Mar 22 13:16:42.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:16:42.390: INFO: namespace: e2e-tests-projected-nd4h5, resource: bindings, ignored listing per whitelist
Mar 22 13:16:42.509: INFO: namespace e2e-tests-projected-nd4h5 deletion completed in 24.244909027s

â€¢ [SLOW TEST:33.141 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:16:42.509: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-fnhs8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-ttmz
STEP: Creating a pod to test atomic-volume-subpath
Mar 22 13:16:42.816: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ttmz" in namespace "e2e-tests-subpath-fnhs8" to be "success or failure"
Mar 22 13:16:42.823: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.984151ms
Mar 22 13:16:44.849: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032651856s
Mar 22 13:16:46.862: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045366088s
Mar 22 13:16:48.870: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 6.053884462s
Mar 22 13:16:50.878: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 8.061337972s
Mar 22 13:16:52.884: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 10.067173678s
Mar 22 13:16:54.891: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 12.074109898s
Mar 22 13:16:56.920: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 14.103552735s
Mar 22 13:16:58.927: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 16.110321437s
Mar 22 13:17:00.934: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 18.117195235s
Mar 22 13:17:02.941: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 20.124260021s
Mar 22 13:17:04.951: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 22.134421968s
Mar 22 13:17:06.970: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Running", Reason="", readiness=false. Elapsed: 24.153922729s
Mar 22 13:17:08.982: INFO: Pod "pod-subpath-test-downwardapi-ttmz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.165484311s
STEP: Saw pod success
Mar 22 13:17:08.982: INFO: Pod "pod-subpath-test-downwardapi-ttmz" satisfied condition "success or failure"
Mar 22 13:17:08.989: INFO: Trying to get logs from node metalk8s-02 pod pod-subpath-test-downwardapi-ttmz container test-container-subpath-downwardapi-ttmz: <nil>
STEP: delete the pod
Mar 22 13:17:09.045: INFO: Waiting for pod pod-subpath-test-downwardapi-ttmz to disappear
Mar 22 13:17:09.050: INFO: Pod pod-subpath-test-downwardapi-ttmz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ttmz
Mar 22 13:17:09.050: INFO: Deleting pod "pod-subpath-test-downwardapi-ttmz" in namespace "e2e-tests-subpath-fnhs8"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:17:09.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-fnhs8" for this suite.
Mar 22 13:17:15.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:17:15.225: INFO: namespace: e2e-tests-subpath-fnhs8, resource: bindings, ignored listing per whitelist
Mar 22 13:17:15.303: INFO: namespace e2e-tests-subpath-fnhs8 deletion completed in 6.234717834s

â€¢ [SLOW TEST:32.793 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:17:15.303: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-zbgjx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Mar 22 13:17:15.555: INFO: Waiting up to 5m0s for pod "var-expansion-cfc18176-4ca4-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-var-expansion-zbgjx" to be "success or failure"
Mar 22 13:17:15.560: INFO: Pod "var-expansion-cfc18176-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.402653ms
Mar 22 13:17:17.589: INFO: Pod "var-expansion-cfc18176-4ca4-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034136199s
STEP: Saw pod success
Mar 22 13:17:17.589: INFO: Pod "var-expansion-cfc18176-4ca4-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:17:17.606: INFO: Trying to get logs from node metalk8s-01 pod var-expansion-cfc18176-4ca4-11e9-99ae-9e98f636e47c container dapi-container: <nil>
STEP: delete the pod
Mar 22 13:17:17.659: INFO: Waiting for pod var-expansion-cfc18176-4ca4-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:17:17.664: INFO: Pod var-expansion-cfc18176-4ca4-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:17:17.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-zbgjx" for this suite.
Mar 22 13:17:23.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:17:23.711: INFO: namespace: e2e-tests-var-expansion-zbgjx, resource: bindings, ignored listing per whitelist
Mar 22 13:17:23.863: INFO: namespace e2e-tests-var-expansion-zbgjx deletion completed in 6.192134262s

â€¢ [SLOW TEST:8.561 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:17:23.864: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-wkxw5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Mar 22 13:17:24.089: INFO: Waiting up to 5m0s for pod "client-containers-d4d7e8cc-4ca4-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-containers-wkxw5" to be "success or failure"
Mar 22 13:17:24.096: INFO: Pod "client-containers-d4d7e8cc-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.395529ms
Mar 22 13:17:26.104: INFO: Pod "client-containers-d4d7e8cc-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01450362s
Mar 22 13:17:28.119: INFO: Pod "client-containers-d4d7e8cc-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030014484s
Mar 22 13:17:30.128: INFO: Pod "client-containers-d4d7e8cc-4ca4-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03872497s
STEP: Saw pod success
Mar 22 13:17:30.128: INFO: Pod "client-containers-d4d7e8cc-4ca4-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:17:30.133: INFO: Trying to get logs from node metalk8s-04 pod client-containers-d4d7e8cc-4ca4-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:17:30.175: INFO: Waiting for pod client-containers-d4d7e8cc-4ca4-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:17:30.179: INFO: Pod client-containers-d4d7e8cc-4ca4-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:17:30.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wkxw5" for this suite.
Mar 22 13:17:36.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:17:36.279: INFO: namespace: e2e-tests-containers-wkxw5, resource: bindings, ignored listing per whitelist
Mar 22 13:17:36.418: INFO: namespace e2e-tests-containers-wkxw5 deletion completed in 6.224098239s

â€¢ [SLOW TEST:12.555 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:17:36.419: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-sr4gh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 22 13:17:36.695: INFO: Waiting up to 5m0s for pod "pod-dc5b5302-4ca4-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-sr4gh" to be "success or failure"
Mar 22 13:17:36.702: INFO: Pod "pod-dc5b5302-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.091318ms
Mar 22 13:17:38.719: INFO: Pod "pod-dc5b5302-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024197695s
Mar 22 13:17:40.728: INFO: Pod "pod-dc5b5302-4ca4-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033816329s
STEP: Saw pod success
Mar 22 13:17:40.728: INFO: Pod "pod-dc5b5302-4ca4-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:17:40.734: INFO: Trying to get logs from node metalk8s-05 pod pod-dc5b5302-4ca4-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:17:40.765: INFO: Waiting for pod pod-dc5b5302-4ca4-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:17:40.770: INFO: Pod pod-dc5b5302-4ca4-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:17:40.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sr4gh" for this suite.
Mar 22 13:17:48.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:17:49.071: INFO: namespace: e2e-tests-emptydir-sr4gh, resource: bindings, ignored listing per whitelist
Mar 22 13:17:49.101: INFO: namespace e2e-tests-emptydir-sr4gh deletion completed in 8.321851057s

â€¢ [SLOW TEST:12.682 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:17:49.101: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-lc5j5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-lc5j5
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lc5j5 to expose endpoints map[]
Mar 22 13:17:49.763: INFO: Get endpoints failed (84.57367ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar 22 13:17:50.769: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lc5j5 exposes endpoints map[] (1.089641904s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-lc5j5
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lc5j5 to expose endpoints map[pod1:[80]]
Mar 22 13:17:54.848: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lc5j5 exposes endpoints map[pod1:[80]] (4.065211859s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-lc5j5
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lc5j5 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 22 13:17:58.959: INFO: Unexpected endpoints: found map[e4c42de6-4ca4-11e9-b0fd-fa163e8e51f5:[80]], expected map[pod1:[80] pod2:[80]] (4.103212866s elapsed, will retry)
Mar 22 13:17:59.976: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lc5j5 exposes endpoints map[pod1:[80] pod2:[80]] (5.120416355s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-lc5j5
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lc5j5 to expose endpoints map[pod2:[80]]
Mar 22 13:18:01.028: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lc5j5 exposes endpoints map[pod2:[80]] (1.041820347s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-lc5j5
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-lc5j5 to expose endpoints map[]
Mar 22 13:18:01.060: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-lc5j5 exposes endpoints map[] (19.251373ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:18:01.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-lc5j5" for this suite.
Mar 22 13:18:25.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:18:25.588: INFO: namespace: e2e-tests-services-lc5j5, resource: bindings, ignored listing per whitelist
Mar 22 13:18:25.779: INFO: namespace e2e-tests-services-lc5j5 deletion completed in 24.670149449s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

â€¢ [SLOW TEST:36.678 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:18:25.779: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2nlsl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 22 13:18:26.073: INFO: Waiting up to 5m0s for pod "pod-f9c8440d-4ca4-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-2nlsl" to be "success or failure"
Mar 22 13:18:26.092: INFO: Pod "pod-f9c8440d-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.036386ms
Mar 22 13:18:28.097: INFO: Pod "pod-f9c8440d-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024698123s
Mar 22 13:18:30.103: INFO: Pod "pod-f9c8440d-4ca4-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030665892s
Mar 22 13:18:32.116: INFO: Pod "pod-f9c8440d-4ca4-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043284266s
STEP: Saw pod success
Mar 22 13:18:32.116: INFO: Pod "pod-f9c8440d-4ca4-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:18:32.121: INFO: Trying to get logs from node metalk8s-04 pod pod-f9c8440d-4ca4-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:18:32.156: INFO: Waiting for pod pod-f9c8440d-4ca4-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:18:32.162: INFO: Pod pod-f9c8440d-4ca4-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:18:32.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2nlsl" for this suite.
Mar 22 13:18:38.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:18:38.320: INFO: namespace: e2e-tests-emptydir-2nlsl, resource: bindings, ignored listing per whitelist
Mar 22 13:18:38.431: INFO: namespace e2e-tests-emptydir-2nlsl deletion completed in 6.257649727s

â€¢ [SLOW TEST:12.652 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:18:38.431: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6qw9x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-014e44af-4ca5-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:18:38.694: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-014f5480-4ca5-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-6qw9x" to be "success or failure"
Mar 22 13:18:38.701: INFO: Pod "pod-projected-secrets-014f5480-4ca5-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.408871ms
Mar 22 13:18:40.707: INFO: Pod "pod-projected-secrets-014f5480-4ca5-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01343623s
Mar 22 13:18:42.721: INFO: Pod "pod-projected-secrets-014f5480-4ca5-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027044871s
STEP: Saw pod success
Mar 22 13:18:42.721: INFO: Pod "pod-projected-secrets-014f5480-4ca5-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:18:42.725: INFO: Trying to get logs from node metalk8s-05 pod pod-projected-secrets-014f5480-4ca5-11e9-99ae-9e98f636e47c container secret-volume-test: <nil>
STEP: delete the pod
Mar 22 13:18:42.762: INFO: Waiting for pod pod-projected-secrets-014f5480-4ca5-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:18:42.772: INFO: Pod pod-projected-secrets-014f5480-4ca5-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:18:42.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6qw9x" for this suite.
Mar 22 13:18:48.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:18:48.895: INFO: namespace: e2e-tests-projected-6qw9x, resource: bindings, ignored listing per whitelist
Mar 22 13:18:48.967: INFO: namespace e2e-tests-projected-6qw9x deletion completed in 6.188191972s

â€¢ [SLOW TEST:10.536 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:18:48.967: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-xcvkd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 22 13:18:49.204: INFO: Waiting up to 5m0s for pod "downward-api-07930ad7-4ca5-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-xcvkd" to be "success or failure"
Mar 22 13:18:49.211: INFO: Pod "downward-api-07930ad7-4ca5-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.361208ms
Mar 22 13:18:51.217: INFO: Pod "downward-api-07930ad7-4ca5-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013043554s
Mar 22 13:18:53.237: INFO: Pod "downward-api-07930ad7-4ca5-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033175883s
STEP: Saw pod success
Mar 22 13:18:53.237: INFO: Pod "downward-api-07930ad7-4ca5-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:18:53.243: INFO: Trying to get logs from node metalk8s-02 pod downward-api-07930ad7-4ca5-11e9-99ae-9e98f636e47c container dapi-container: <nil>
STEP: delete the pod
Mar 22 13:18:53.279: INFO: Waiting for pod downward-api-07930ad7-4ca5-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:18:53.284: INFO: Pod downward-api-07930ad7-4ca5-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:18:53.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xcvkd" for this suite.
Mar 22 13:18:59.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:18:59.460: INFO: namespace: e2e-tests-downward-api-xcvkd, resource: bindings, ignored listing per whitelist
Mar 22 13:18:59.567: INFO: namespace e2e-tests-downward-api-xcvkd deletion completed in 6.275048265s

â€¢ [SLOW TEST:10.600 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:18:59.568: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-wcpvq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Mar 22 13:18:59.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:00.187: INFO: stderr: ""
Mar 22 13:19:00.187: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 22 13:19:00.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:00.306: INFO: stderr: ""
Mar 22 13:19:00.306: INFO: stdout: "update-demo-nautilus-km9pg update-demo-nautilus-tpzwz "
Mar 22 13:19:00.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-km9pg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:00.411: INFO: stderr: ""
Mar 22 13:19:00.411: INFO: stdout: ""
Mar 22 13:19:00.411: INFO: update-demo-nautilus-km9pg is created but not running
Mar 22 13:19:05.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:05.521: INFO: stderr: ""
Mar 22 13:19:05.521: INFO: stdout: "update-demo-nautilus-km9pg update-demo-nautilus-tpzwz "
Mar 22 13:19:05.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-km9pg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:05.636: INFO: stderr: ""
Mar 22 13:19:05.636: INFO: stdout: "true"
Mar 22 13:19:05.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-km9pg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:05.762: INFO: stderr: ""
Mar 22 13:19:05.762: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:19:05.762: INFO: validating pod update-demo-nautilus-km9pg
Mar 22 13:19:05.771: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:19:05.772: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:19:05.772: INFO: update-demo-nautilus-km9pg is verified up and running
Mar 22 13:19:05.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-tpzwz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:05.870: INFO: stderr: ""
Mar 22 13:19:05.870: INFO: stdout: ""
Mar 22 13:19:05.870: INFO: update-demo-nautilus-tpzwz is created but not running
Mar 22 13:19:10.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:10.983: INFO: stderr: ""
Mar 22 13:19:10.983: INFO: stdout: "update-demo-nautilus-km9pg update-demo-nautilus-tpzwz "
Mar 22 13:19:10.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-km9pg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:11.112: INFO: stderr: ""
Mar 22 13:19:11.112: INFO: stdout: "true"
Mar 22 13:19:11.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-km9pg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:11.207: INFO: stderr: ""
Mar 22 13:19:11.207: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:19:11.207: INFO: validating pod update-demo-nautilus-km9pg
Mar 22 13:19:11.215: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:19:11.215: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:19:11.215: INFO: update-demo-nautilus-km9pg is verified up and running
Mar 22 13:19:11.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-tpzwz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:11.312: INFO: stderr: ""
Mar 22 13:19:11.312: INFO: stdout: "true"
Mar 22 13:19:11.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-tpzwz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:11.425: INFO: stderr: ""
Mar 22 13:19:11.425: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:19:11.425: INFO: validating pod update-demo-nautilus-tpzwz
Mar 22 13:19:11.434: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:19:11.434: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:19:11.434: INFO: update-demo-nautilus-tpzwz is verified up and running
STEP: rolling-update to new replication controller
Mar 22 13:19:11.436: INFO: scanned /root for discovery docs: <nil>
Mar 22 13:19:11.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:35.975: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 22 13:19:35.975: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 22 13:19:35.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:36.117: INFO: stderr: ""
Mar 22 13:19:36.117: INFO: stdout: "update-demo-kitten-78zt5 update-demo-kitten-9bgfg "
Mar 22 13:19:36.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-kitten-78zt5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:36.222: INFO: stderr: ""
Mar 22 13:19:36.222: INFO: stdout: "true"
Mar 22 13:19:36.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-kitten-78zt5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:36.314: INFO: stderr: ""
Mar 22 13:19:36.314: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 22 13:19:36.314: INFO: validating pod update-demo-kitten-78zt5
Mar 22 13:19:36.320: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 22 13:19:36.320: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 22 13:19:36.320: INFO: update-demo-kitten-78zt5 is verified up and running
Mar 22 13:19:36.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-kitten-9bgfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:36.419: INFO: stderr: ""
Mar 22 13:19:36.419: INFO: stdout: "true"
Mar 22 13:19:36.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-kitten-9bgfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-wcpvq'
Mar 22 13:19:36.516: INFO: stderr: ""
Mar 22 13:19:36.516: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 22 13:19:36.516: INFO: validating pod update-demo-kitten-9bgfg
Mar 22 13:19:36.529: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 22 13:19:36.529: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 22 13:19:36.529: INFO: update-demo-kitten-9bgfg is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:19:36.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wcpvq" for this suite.
Mar 22 13:20:00.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:20:00.664: INFO: namespace: e2e-tests-kubectl-wcpvq, resource: bindings, ignored listing per whitelist
Mar 22 13:20:00.761: INFO: namespace e2e-tests-kubectl-wcpvq deletion completed in 24.224233959s

â€¢ [SLOW TEST:61.193 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:20:00.761: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-4t8b9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-snmh8
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-l79wf
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:20:08.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-4t8b9" for this suite.
Mar 22 13:20:16.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:20:16.449: INFO: namespace: e2e-tests-namespaces-4t8b9, resource: bindings, ignored listing per whitelist
Mar 22 13:20:16.546: INFO: namespace e2e-tests-namespaces-4t8b9 deletion completed in 8.228404793s
STEP: Destroying namespace "e2e-tests-nsdeletetest-snmh8" for this suite.
Mar 22 13:20:16.550: INFO: Namespace e2e-tests-nsdeletetest-snmh8 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-l79wf" for this suite.
Mar 22 13:20:22.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:20:22.649: INFO: namespace: e2e-tests-nsdeletetest-l79wf, resource: bindings, ignored listing per whitelist
Mar 22 13:20:22.741: INFO: namespace e2e-tests-nsdeletetest-l79wf deletion completed in 6.191162421s

â€¢ [SLOW TEST:21.980 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:20:22.742: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-j6fhn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-j6fhn
Mar 22 13:20:25.038: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-j6fhn
STEP: checking the pod's current state and verifying that restartCount is present
Mar 22 13:20:25.044: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:24:26.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-j6fhn" for this suite.
Mar 22 13:24:32.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:24:32.818: INFO: namespace: e2e-tests-container-probe-j6fhn, resource: bindings, ignored listing per whitelist
Mar 22 13:24:32.825: INFO: namespace e2e-tests-container-probe-j6fhn deletion completed in 6.246868118s

â€¢ [SLOW TEST:250.084 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:24:32.826: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-bppqj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-v95gp in namespace e2e-tests-proxy-bppqj
I0322 13:24:33.105881      20 runners.go:180] Created replication controller with name: proxy-service-v95gp, namespace: e2e-tests-proxy-bppqj, replica count: 1
I0322 13:24:34.156644      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0322 13:24:35.156848      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0322 13:24:36.157047      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0322 13:24:37.157230      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0322 13:24:38.157429      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0322 13:24:39.157592      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0322 13:24:40.157760      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0322 13:24:41.158286      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0322 13:24:42.158489      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0322 13:24:43.158684      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0322 13:24:44.158903      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0322 13:24:45.159206      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0322 13:24:46.159475      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0322 13:24:47.159694      20 runners.go:180] proxy-service-v95gp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 13:24:47.175: INFO: setup took 14.102239094s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 22 13:24:47.187: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 10.521575ms)
Mar 22 13:24:47.188: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 10.86875ms)
Mar 22 13:24:47.188: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 11.10869ms)
Mar 22 13:24:47.188: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 10.568246ms)
Mar 22 13:24:47.192: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 14.081795ms)
Mar 22 13:24:47.192: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 14.234302ms)
Mar 22 13:24:47.192: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 16.737355ms)
Mar 22 13:24:47.193: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 15.840844ms)
Mar 22 13:24:47.193: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 16.201959ms)
Mar 22 13:24:47.195: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 17.939859ms)
Mar 22 13:24:47.195: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 17.635277ms)
Mar 22 13:24:47.204: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 27.238127ms)
Mar 22 13:24:47.204: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 26.570905ms)
Mar 22 13:24:47.204: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 26.774245ms)
Mar 22 13:24:47.209: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 31.410273ms)
Mar 22 13:24:47.210: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 32.722705ms)
Mar 22 13:24:47.263: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 52.92739ms)
Mar 22 13:24:47.264: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 53.31117ms)
Mar 22 13:24:47.264: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 53.378618ms)
Mar 22 13:24:47.264: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 53.223076ms)
Mar 22 13:24:47.264: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 53.689263ms)
Mar 22 13:24:47.264: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 53.338148ms)
Mar 22 13:24:47.268: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 57.967356ms)
Mar 22 13:24:47.269: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 58.579327ms)
Mar 22 13:24:47.270: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 59.326739ms)
Mar 22 13:24:47.270: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 59.664268ms)
Mar 22 13:24:47.270: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 59.654033ms)
Mar 22 13:24:47.271: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 59.992083ms)
Mar 22 13:24:47.272: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 61.233569ms)
Mar 22 13:24:47.272: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 61.555412ms)
Mar 22 13:24:47.274: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 62.952049ms)
Mar 22 13:24:47.275: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 64.083869ms)
Mar 22 13:24:47.281: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 6.066183ms)
Mar 22 13:24:47.282: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 6.903847ms)
Mar 22 13:24:47.282: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 6.978242ms)
Mar 22 13:24:47.285: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 10.063564ms)
Mar 22 13:24:47.285: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 9.806423ms)
Mar 22 13:24:47.285: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 9.801145ms)
Mar 22 13:24:47.285: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 10.090108ms)
Mar 22 13:24:47.285: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 9.840296ms)
Mar 22 13:24:47.285: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 9.922884ms)
Mar 22 13:24:47.285: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 9.980907ms)
Mar 22 13:24:47.286: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 11.180878ms)
Mar 22 13:24:47.289: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 13.572728ms)
Mar 22 13:24:47.289: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 13.75606ms)
Mar 22 13:24:47.289: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 13.816796ms)
Mar 22 13:24:47.289: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 13.704582ms)
Mar 22 13:24:47.291: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 15.110406ms)
Mar 22 13:24:47.297: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 6.025398ms)
Mar 22 13:24:47.300: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 8.820157ms)
Mar 22 13:24:47.300: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 8.827768ms)
Mar 22 13:24:47.300: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 9.025756ms)
Mar 22 13:24:47.300: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 8.852936ms)
Mar 22 13:24:47.300: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 9.097983ms)
Mar 22 13:24:47.300: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 9.053716ms)
Mar 22 13:24:47.300: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 9.135645ms)
Mar 22 13:24:47.300: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 9.344729ms)
Mar 22 13:24:47.300: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 9.449008ms)
Mar 22 13:24:47.301: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 10.283976ms)
Mar 22 13:24:47.302: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 10.760529ms)
Mar 22 13:24:47.303: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 12.564053ms)
Mar 22 13:24:47.307: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 15.533031ms)
Mar 22 13:24:47.307: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 15.56319ms)
Mar 22 13:24:47.307: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 15.554471ms)
Mar 22 13:24:47.316: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 8.588092ms)
Mar 22 13:24:47.318: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 11.01682ms)
Mar 22 13:24:47.362: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 54.450624ms)
Mar 22 13:24:47.362: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 54.27272ms)
Mar 22 13:24:47.362: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 54.621083ms)
Mar 22 13:24:47.362: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 55.186094ms)
Mar 22 13:24:47.362: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 54.579684ms)
Mar 22 13:24:47.362: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 54.501748ms)
Mar 22 13:24:47.362: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 54.494607ms)
Mar 22 13:24:47.363: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 54.889696ms)
Mar 22 13:24:47.367: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 59.062506ms)
Mar 22 13:24:47.368: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 60.360328ms)
Mar 22 13:24:47.368: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 60.506641ms)
Mar 22 13:24:47.369: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 60.974862ms)
Mar 22 13:24:47.369: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 61.177395ms)
Mar 22 13:24:47.372: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 64.680854ms)
Mar 22 13:24:47.395: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 22.628687ms)
Mar 22 13:24:47.398: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 24.518259ms)
Mar 22 13:24:47.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 26.866598ms)
Mar 22 13:24:47.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 27.024004ms)
Mar 22 13:24:47.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 26.850761ms)
Mar 22 13:24:47.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 26.47159ms)
Mar 22 13:24:47.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 26.755772ms)
Mar 22 13:24:47.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 26.611238ms)
Mar 22 13:24:47.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 26.999586ms)
Mar 22 13:24:47.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 26.572035ms)
Mar 22 13:24:47.400: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 26.842569ms)
Mar 22 13:24:47.410: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 36.842038ms)
Mar 22 13:24:47.413: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 40.064698ms)
Mar 22 13:24:47.413: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 39.803527ms)
Mar 22 13:24:47.414: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 40.726518ms)
Mar 22 13:24:47.415: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 41.707155ms)
Mar 22 13:24:47.433: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 17.30876ms)
Mar 22 13:24:47.433: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 18.363198ms)
Mar 22 13:24:47.433: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 17.732746ms)
Mar 22 13:24:47.433: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 16.900391ms)
Mar 22 13:24:47.433: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 16.902747ms)
Mar 22 13:24:47.433: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 17.94743ms)
Mar 22 13:24:47.435: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 18.422162ms)
Mar 22 13:24:47.435: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 17.960402ms)
Mar 22 13:24:47.435: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 18.90983ms)
Mar 22 13:24:47.435: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 18.743189ms)
Mar 22 13:24:47.444: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 28.340734ms)
Mar 22 13:24:47.451: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 34.509577ms)
Mar 22 13:24:47.458: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 42.062541ms)
Mar 22 13:24:47.458: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 42.922297ms)
Mar 22 13:24:47.458: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 42.022898ms)
Mar 22 13:24:47.458: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 43.095028ms)
Mar 22 13:24:47.467: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 8.314797ms)
Mar 22 13:24:47.470: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 10.947664ms)
Mar 22 13:24:47.470: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 11.154077ms)
Mar 22 13:24:47.470: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 11.367427ms)
Mar 22 13:24:47.470: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 11.302236ms)
Mar 22 13:24:47.472: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 13.307452ms)
Mar 22 13:24:47.472: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 13.618332ms)
Mar 22 13:24:47.473: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 14.244896ms)
Mar 22 13:24:47.473: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 14.35401ms)
Mar 22 13:24:47.473: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 14.945046ms)
Mar 22 13:24:47.476: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 18.040519ms)
Mar 22 13:24:47.476: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 17.740821ms)
Mar 22 13:24:47.481: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 21.797918ms)
Mar 22 13:24:47.481: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 22.141127ms)
Mar 22 13:24:47.481: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 22.30106ms)
Mar 22 13:24:47.481: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 22.629795ms)
Mar 22 13:24:47.490: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 9.096066ms)
Mar 22 13:24:47.494: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 12.266341ms)
Mar 22 13:24:47.494: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 12.307106ms)
Mar 22 13:24:47.494: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 12.242117ms)
Mar 22 13:24:47.494: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 12.237804ms)
Mar 22 13:24:47.494: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 12.060574ms)
Mar 22 13:24:47.494: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 12.180514ms)
Mar 22 13:24:47.494: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 12.584708ms)
Mar 22 13:24:47.494: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 12.805814ms)
Mar 22 13:24:47.494: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 13.2446ms)
Mar 22 13:24:47.495: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 13.228704ms)
Mar 22 13:24:47.502: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 20.836378ms)
Mar 22 13:24:47.503: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 21.594505ms)
Mar 22 13:24:47.503: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 21.853025ms)
Mar 22 13:24:47.504: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 22.041814ms)
Mar 22 13:24:47.504: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 22.221221ms)
Mar 22 13:24:47.518: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 14.443453ms)
Mar 22 13:24:47.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 17.656205ms)
Mar 22 13:24:47.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 17.53646ms)
Mar 22 13:24:47.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 18.127463ms)
Mar 22 13:24:47.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 17.76411ms)
Mar 22 13:24:47.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 18.32448ms)
Mar 22 13:24:47.522: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 18.568456ms)
Mar 22 13:24:47.523: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 19.244673ms)
Mar 22 13:24:47.523: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 18.89025ms)
Mar 22 13:24:47.523: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 19.005699ms)
Mar 22 13:24:47.523: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 19.228487ms)
Mar 22 13:24:47.524: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 20.16955ms)
Mar 22 13:24:47.563: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 58.227935ms)
Mar 22 13:24:47.563: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 58.398691ms)
Mar 22 13:24:47.563: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 58.343792ms)
Mar 22 13:24:47.563: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 58.764905ms)
Mar 22 13:24:47.572: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 8.543733ms)
Mar 22 13:24:47.575: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 11.130703ms)
Mar 22 13:24:47.575: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 11.082111ms)
Mar 22 13:24:47.575: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 11.04093ms)
Mar 22 13:24:47.575: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 11.570716ms)
Mar 22 13:24:47.576: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 12.362372ms)
Mar 22 13:24:47.576: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 12.507665ms)
Mar 22 13:24:47.576: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 12.643645ms)
Mar 22 13:24:47.576: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 12.354895ms)
Mar 22 13:24:47.576: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 12.406275ms)
Mar 22 13:24:47.581: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 17.33687ms)
Mar 22 13:24:47.582: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 18.834485ms)
Mar 22 13:24:47.586: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 22.166685ms)
Mar 22 13:24:47.587: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 22.888875ms)
Mar 22 13:24:47.587: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 23.682844ms)
Mar 22 13:24:47.589: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 24.586228ms)
Mar 22 13:24:47.599: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 10.759035ms)
Mar 22 13:24:47.602: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 12.432814ms)
Mar 22 13:24:47.602: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 12.539345ms)
Mar 22 13:24:47.602: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 12.880722ms)
Mar 22 13:24:47.602: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 12.748345ms)
Mar 22 13:24:47.602: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 12.726163ms)
Mar 22 13:24:47.602: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 13.130011ms)
Mar 22 13:24:47.602: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 13.25172ms)
Mar 22 13:24:47.603: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 13.755022ms)
Mar 22 13:24:47.603: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 14.577177ms)
Mar 22 13:24:47.604: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 15.802984ms)
Mar 22 13:24:47.607: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 17.972614ms)
Mar 22 13:24:47.610: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 20.777786ms)
Mar 22 13:24:47.610: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 20.642204ms)
Mar 22 13:24:47.610: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 20.571285ms)
Mar 22 13:24:47.610: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 20.99402ms)
Mar 22 13:24:47.617: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 6.882188ms)
Mar 22 13:24:47.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 11.204752ms)
Mar 22 13:24:47.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 11.065972ms)
Mar 22 13:24:47.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 11.199082ms)
Mar 22 13:24:47.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 11.288091ms)
Mar 22 13:24:47.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 11.12682ms)
Mar 22 13:24:47.622: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 11.267468ms)
Mar 22 13:24:47.623: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 12.471194ms)
Mar 22 13:24:47.623: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 12.171244ms)
Mar 22 13:24:47.623: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 12.3814ms)
Mar 22 13:24:47.633: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 22.031223ms)
Mar 22 13:24:47.633: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 21.739142ms)
Mar 22 13:24:47.662: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 51.062094ms)
Mar 22 13:24:47.662: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 52.239064ms)
Mar 22 13:24:47.662: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 51.21754ms)
Mar 22 13:24:47.663: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 52.692588ms)
Mar 22 13:24:47.671: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 7.072229ms)
Mar 22 13:24:47.671: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 7.865471ms)
Mar 22 13:24:47.672: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 8.793066ms)
Mar 22 13:24:47.673: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 9.732361ms)
Mar 22 13:24:47.673: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 9.812494ms)
Mar 22 13:24:47.673: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 9.904659ms)
Mar 22 13:24:47.673: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 9.821117ms)
Mar 22 13:24:47.673: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 10.10961ms)
Mar 22 13:24:47.673: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 10.224447ms)
Mar 22 13:24:47.673: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 9.887043ms)
Mar 22 13:24:47.673: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 10.131964ms)
Mar 22 13:24:47.677: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 13.875371ms)
Mar 22 13:24:47.677: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 13.777218ms)
Mar 22 13:24:47.680: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 17.055059ms)
Mar 22 13:24:47.681: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 17.188969ms)
Mar 22 13:24:47.682: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 18.047485ms)
Mar 22 13:24:47.688: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 6.351611ms)
Mar 22 13:24:47.691: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 8.361896ms)
Mar 22 13:24:47.692: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 9.30018ms)
Mar 22 13:24:47.692: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 9.397594ms)
Mar 22 13:24:47.692: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 10.439687ms)
Mar 22 13:24:47.693: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 10.306841ms)
Mar 22 13:24:47.693: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 9.701292ms)
Mar 22 13:24:47.694: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 12.011031ms)
Mar 22 13:24:47.695: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 11.619281ms)
Mar 22 13:24:47.695: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 9.46171ms)
Mar 22 13:24:47.698: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 15.504139ms)
Mar 22 13:24:47.700: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 17.054933ms)
Mar 22 13:24:47.700: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 18.312576ms)
Mar 22 13:24:47.704: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 18.24501ms)
Mar 22 13:24:47.704: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 21.226008ms)
Mar 22 13:24:47.704: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 18.309005ms)
Mar 22 13:24:47.712: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 7.28194ms)
Mar 22 13:24:47.713: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 8.438409ms)
Mar 22 13:24:47.713: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 8.680808ms)
Mar 22 13:24:47.714: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 9.142534ms)
Mar 22 13:24:47.714: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 8.915707ms)
Mar 22 13:24:47.714: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 9.263795ms)
Mar 22 13:24:47.714: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 10.342444ms)
Mar 22 13:24:47.714: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 9.824671ms)
Mar 22 13:24:47.714: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 9.468464ms)
Mar 22 13:24:47.763: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 58.28371ms)
Mar 22 13:24:47.764: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 60.2105ms)
Mar 22 13:24:47.764: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 59.857644ms)
Mar 22 13:24:47.764: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 59.776111ms)
Mar 22 13:24:47.764: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 59.721871ms)
Mar 22 13:24:47.764: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 59.348912ms)
Mar 22 13:24:47.765: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 60.476607ms)
Mar 22 13:24:47.781: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 15.555936ms)
Mar 22 13:24:47.781: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 15.238184ms)
Mar 22 13:24:47.781: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 15.357004ms)
Mar 22 13:24:47.781: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 15.604763ms)
Mar 22 13:24:47.784: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 18.509404ms)
Mar 22 13:24:47.785: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 18.507999ms)
Mar 22 13:24:47.785: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 18.610725ms)
Mar 22 13:24:47.785: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 18.737078ms)
Mar 22 13:24:47.785: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 18.639743ms)
Mar 22 13:24:47.785: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 18.817018ms)
Mar 22 13:24:47.788: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 22.645943ms)
Mar 22 13:24:47.789: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 23.630418ms)
Mar 22 13:24:47.791: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 25.580112ms)
Mar 22 13:24:47.793: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 27.18683ms)
Mar 22 13:24:47.793: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 27.016669ms)
Mar 22 13:24:47.793: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 27.510838ms)
Mar 22 13:24:47.805: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 11.86568ms)
Mar 22 13:24:47.809: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 15.761733ms)
Mar 22 13:24:47.809: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 15.775039ms)
Mar 22 13:24:47.812: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 18.183195ms)
Mar 22 13:24:47.812: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 18.352388ms)
Mar 22 13:24:47.812: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 18.49217ms)
Mar 22 13:24:47.812: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 18.392702ms)
Mar 22 13:24:47.812: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 18.526707ms)
Mar 22 13:24:47.812: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 18.518582ms)
Mar 22 13:24:47.812: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 18.538421ms)
Mar 22 13:24:47.813: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 19.578685ms)
Mar 22 13:24:47.821: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 27.441117ms)
Mar 22 13:24:47.821: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 27.885174ms)
Mar 22 13:24:47.831: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 37.642822ms)
Mar 22 13:24:47.832: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 38.603589ms)
Mar 22 13:24:47.834: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 40.562603ms)
Mar 22 13:24:47.842: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 7.937758ms)
Mar 22 13:24:47.843: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 8.695318ms)
Mar 22 13:24:47.844: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 9.542081ms)
Mar 22 13:24:47.844: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 9.236657ms)
Mar 22 13:24:47.844: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 9.054333ms)
Mar 22 13:24:47.844: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 9.101054ms)
Mar 22 13:24:47.844: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 9.209431ms)
Mar 22 13:24:47.844: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 9.292043ms)
Mar 22 13:24:47.845: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 10.522481ms)
Mar 22 13:24:47.845: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 10.311479ms)
Mar 22 13:24:47.845: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 10.919921ms)
Mar 22 13:24:47.848: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 13.663695ms)
Mar 22 13:24:47.851: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 16.645886ms)
Mar 22 13:24:47.851: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 16.42467ms)
Mar 22 13:24:47.854: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 19.370489ms)
Mar 22 13:24:47.864: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 29.43633ms)
Mar 22 13:24:47.879: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:160/proxy/: foo (200; 14.824086ms)
Mar 22 13:24:47.879: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:162/proxy/: bar (200; 14.393225ms)
Mar 22 13:24:47.879: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:460/proxy/: tls baz (200; 13.938646ms)
Mar 22 13:24:47.879: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:462/proxy/: tls qux (200; 14.36537ms)
Mar 22 13:24:47.883: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:162/proxy/: bar (200; 17.519619ms)
Mar 22 13:24:47.883: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq/proxy/rewriteme"... (200; 17.641979ms)
Mar 22 13:24:47.883: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:160/proxy/: foo (200; 17.675162ms)
Mar 22 13:24:47.883: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/https:proxy-service-v95gp-svgqq:443/proxy/... (200; 17.792435ms)
Mar 22 13:24:47.883: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/proxy-service-v95gp-svgqq:1080/proxy/rewri... (200; 17.853771ms)
Mar 22 13:24:47.883: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-bppqj/pods/http:proxy-service-v95gp-svgqq:1080/proxy/... (200; 17.698084ms)
Mar 22 13:24:47.885: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname1/proxy/: foo (200; 20.458102ms)
Mar 22 13:24:47.892: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname1/proxy/: tls baz (200; 26.952472ms)
Mar 22 13:24:47.893: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/https:proxy-service-v95gp:tlsportname2/proxy/: tls qux (200; 28.476219ms)
Mar 22 13:24:47.893: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname2/proxy/: bar (200; 29.14358ms)
Mar 22 13:24:47.893: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/proxy-service-v95gp:portname2/proxy/: bar (200; 28.378845ms)
Mar 22 13:24:47.894: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-bppqj/services/http:proxy-service-v95gp:portname1/proxy/: foo (200; 28.541633ms)
STEP: deleting { ReplicationController} proxy-service-v95gp in namespace e2e-tests-proxy-bppqj, will wait for the garbage collector to delete the pods
Mar 22 13:24:47.967: INFO: Deleting { ReplicationController} proxy-service-v95gp took: 14.503414ms
Mar 22 13:24:48.067: INFO: Terminating { ReplicationController} proxy-service-v95gp pods took: 100.37094ms
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:25:01.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-bppqj" for this suite.
Mar 22 13:25:07.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:25:07.333: INFO: namespace: e2e-tests-proxy-bppqj, resource: bindings, ignored listing per whitelist
Mar 22 13:25:07.393: INFO: namespace e2e-tests-proxy-bppqj deletion completed in 6.211169728s

â€¢ [SLOW TEST:34.567 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:25:07.394: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-x9pc5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:25:07.610: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 22 13:25:07.622: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 22 13:25:12.635: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 22 13:25:14.647: INFO: Creating deployment "test-rolling-update-deployment"
Mar 22 13:25:14.653: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 22 13:25:14.663: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 22 13:25:16.671: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 22 13:25:16.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 13:25:18.679: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 13:25:20.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688857914, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 13:25:22.687: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 22 13:25:22.699: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-x9pc5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x9pc5/deployments/test-rolling-update-deployment,UID:ed55f513-4ca5-11e9-b0fd-fa163e8e51f5,ResourceVersion:15012,Generation:1,CreationTimestamp:2019-03-22 13:25:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-22 13:25:14 +0000 UTC 2019-03-22 13:25:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-22 13:25:20 +0000 UTC 2019-03-22 13:25:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar 22 13:25:22.702: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-x9pc5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x9pc5/replicasets/test-rolling-update-deployment-65b7695dcf,UID:ed5aea7d-4ca5-11e9-b0fd-fa163e8e51f5,ResourceVersion:15003,Generation:1,CreationTimestamp:2019-03-22 13:25:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ed55f513-4ca5-11e9-b0fd-fa163e8e51f5 0xc4213d95c7 0xc4213d95c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 22 13:25:22.703: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 22 13:25:22.703: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-x9pc5,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x9pc5/replicasets/test-rolling-update-controller,UID:e924377c-4ca5-11e9-b0fd-fa163e8e51f5,ResourceVersion:15011,Generation:2,CreationTimestamp:2019-03-22 13:25:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ed55f513-4ca5-11e9-b0fd-fa163e8e51f5 0xc4213d91ce 0xc4213d91cf}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 22 13:25:22.708: INFO: Pod "test-rolling-update-deployment-65b7695dcf-r7mbp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-r7mbp,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-x9pc5,SelfLink:/api/v1/namespaces/e2e-tests-deployment-x9pc5/pods/test-rolling-update-deployment-65b7695dcf-r7mbp,UID:ed5bf35c-4ca5-11e9-b0fd-fa163e8e51f5,ResourceVersion:15002,Generation:0,CreationTimestamp:2019-03-22 13:25:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf ed5aea7d-4ca5-11e9-b0fd-fa163e8e51f5 0xc4213d9ef7 0xc4213d9ef8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-f96kv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f96kv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-f96kv true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4213d9f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4213d9f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:25:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:25:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:25:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:25:14 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.75,PodIP:10.233.99.144,StartTime:2019-03-22 13:25:14 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-22 13:25:19 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://aa73a06a0900b13d440f658e6b607a1f235f78e02193076cbaeaf621b67b6333}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:25:22.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-x9pc5" for this suite.
Mar 22 13:25:28.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:25:28.850: INFO: namespace: e2e-tests-deployment-x9pc5, resource: bindings, ignored listing per whitelist
Mar 22 13:25:28.999: INFO: namespace e2e-tests-deployment-x9pc5 deletion completed in 6.284160676s

â€¢ [SLOW TEST:21.605 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:25:28.999: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-7ddhs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0322 13:25:39.274623      20 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 22 13:25:39.274: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:25:39.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-7ddhs" for this suite.
Mar 22 13:25:45.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:25:45.430: INFO: namespace: e2e-tests-gc-7ddhs, resource: bindings, ignored listing per whitelist
Mar 22 13:25:45.518: INFO: namespace e2e-tests-gc-7ddhs deletion completed in 6.235904543s

â€¢ [SLOW TEST:16.519 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:25:45.518: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-rzsr9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Mar 22 13:25:45.773: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 22 13:25:45.792: INFO: Waiting for terminating namespaces to be deleted...
Mar 22 13:25:45.797: INFO: 
Logging pods the kubelet thinks is on node metalk8s-01 before test
Mar 22 13:25:45.811: INFO: kube-proxy-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.811: INFO: kube-apiserver-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.811: INFO: nginx-ingress-controller-cxd4p from kube-ingress started at 2019-03-22 11:53:46 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.811: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:25:45.811: INFO: fluent-bit-ps79j from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.811: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:25:45.811: INFO: metrics-server-647b5cf45b-94rmt from kube-system started at 2019-03-22 11:55:38 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.811: INFO: 	Container metrics-server ready: true, restart count 0
Mar 22 13:25:45.811: INFO: elasticsearch-master-2 from kube-ops started at 2019-03-22 11:56:05 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.811: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:25:45.811: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-d92zp from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.811: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:25:45.811: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:25:45.811: INFO: kube-controller-manager-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.811: INFO: calico-node-6x2ch from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.811: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:25:45.811: INFO: nginx-ingress-default-backend-7b98d4dc6c-5ct2x from kube-ingress started at 2019-03-22 11:53:46 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.811: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Mar 22 13:25:45.811: INFO: kube-scheduler-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.811: INFO: elasticsearch-client-6d74f68c8d-9t67x from kube-ops started at 2019-03-22 11:54:09 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.811: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:25:45.811: INFO: 
Logging pods the kubelet thinks is on node metalk8s-02 before test
Mar 22 13:25:45.826: INFO: kube-controller-manager-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.826: INFO: sonobuoy-e2e-job-59a17f0f4a364fed from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container e2e ready: true, restart count 0
Mar 22 13:25:45.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:25:45.826: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-5j5mp from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:25:45.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:25:45.826: INFO: kube-apiserver-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.826: INFO: calico-node-rtkz2 from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:25:45.826: INFO: elasticsearch-client-6d74f68c8d-28qk2 from kube-ops started at 2019-03-22 11:54:09 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:25:45.826: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-22 13:10:19 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 22 13:25:45.826: INFO: kube-scheduler-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.826: INFO: coredns-6bcbc499f7-j6fch from kube-system started at 2019-03-22 11:50:52 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container coredns ready: true, restart count 0
Mar 22 13:25:45.826: INFO: nginx-ingress-controller-7rf9j from kube-ingress started at 2019-03-22 11:53:46 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:25:45.826: INFO: kube-prometheus-exporter-kube-state-68bb849446-r95z2 from kube-ops started at 2019-03-22 11:53:50 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container exporter-kube-state ready: true, restart count 0
Mar 22 13:25:45.826: INFO: 	Container exporter-kube-state-addon-resizer ready: true, restart count 0
Mar 22 13:25:45.826: INFO: fluent-bit-rxwxc from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:25:45.826: INFO: kube-proxy-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.826: INFO: cerebro-78d5fbf859-v8wt6 from kube-ops started at 2019-03-22 11:55:30 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.826: INFO: 	Container cerebro ready: true, restart count 0
Mar 22 13:25:45.826: INFO: 
Logging pods the kubelet thinks is on node metalk8s-03 before test
Mar 22 13:25:45.851: INFO: prometheus-operator-87779759-x9zf7 from kube-ops started at 2019-03-22 11:52:42 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.851: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 22 13:25:45.851: INFO: fluentd-68575dcc69-2hsp4 from kube-ops started at 2019-03-22 11:54:25 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.851: INFO: 	Container fluentd ready: true, restart count 0
Mar 22 13:25:45.851: INFO: fluent-bit-xrslr from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.852: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:25:45.852: INFO: elasticsearch-master-1 from kube-ops started at 2019-03-22 11:55:25 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.852: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:25:45.852: INFO: elasticsearch-data-2 from kube-ops started at 2019-03-22 11:56:08 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.852: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:25:45.852: INFO: kube-proxy-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.852: INFO: kube-controller-manager-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.852: INFO: calico-kube-controllers-5887874ffb-5sb92 from kube-system started at 2019-03-22 11:49:56 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.852: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 22 13:25:45.852: INFO: elasticsearch-client-6d74f68c8d-v47bt from kube-ops started at 2019-03-22 11:54:09 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.852: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:25:45.852: INFO: calico-node-rnssh from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.852: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:25:45.852: INFO: nginx-ingress-controller-7x9tf from kube-ingress started at 2019-03-22 11:53:45 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.852: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:25:45.852: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-79jdm from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.852: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:25:45.852: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:25:45.852: INFO: kube-apiserver-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.852: INFO: kube-scheduler-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.852: INFO: 
Logging pods the kubelet thinks is on node metalk8s-04 before test
Mar 22 13:25:45.872: INFO: nginx-proxy-metalk8s-04 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.872: INFO: kube-prometheus-grafana-f56778dd6-v8hf8 from kube-ops started at 2019-03-22 11:53:34 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container grafana ready: true, restart count 0
Mar 22 13:25:45.872: INFO: 	Container grafana-watcher ready: true, restart count 0
Mar 22 13:25:45.872: INFO: fluentd-68575dcc69-vxd2d from kube-ops started at 2019-03-22 11:54:26 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container fluentd ready: true, restart count 0
Mar 22 13:25:45.872: INFO: prometheus-kube-prometheus-1 from kube-ops started at 2019-03-22 11:54:11 +0000 UTC (3 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container alerting-rule-files-configmap-reloader ready: true, restart count 0
Mar 22 13:25:45.872: INFO: 	Container prometheus ready: true, restart count 1
Mar 22 13:25:45.872: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 22 13:25:45.872: INFO: kibana-index-provisioning-ldhpt from kube-ops started at 2019-03-22 11:55:08 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container provision-index ready: false, restart count 4
Mar 22 13:25:45.872: INFO: kube-proxy-metalk8s-04 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.872: INFO: nginx-ingress-controller-2mvpz from kube-ingress started at 2019-03-22 11:53:45 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:25:45.872: INFO: elasticsearch-data-1 from kube-ops started at 2019-03-22 11:55:27 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:25:45.872: INFO: alertmanager-kube-prometheus-0 from kube-ops started at 2019-03-22 11:53:34 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container alertmanager ready: true, restart count 0
Mar 22 13:25:45.872: INFO: 	Container config-reloader ready: true, restart count 0
Mar 22 13:25:45.872: INFO: coredns-6bcbc499f7-fsrbl from kube-system started at 2019-03-22 11:50:52 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container coredns ready: true, restart count 0
Mar 22 13:25:45.872: INFO: kubernetes-dashboard-68697c45d9-pbvnj from kube-system started at 2019-03-22 11:50:59 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 22 13:25:45.872: INFO: elasticsearch-master-0 from kube-ops started at 2019-03-22 11:54:10 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:25:45.872: INFO: fluent-bit-9bwfp from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:25:45.872: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-mm98f from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:25:45.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:25:45.872: INFO: calico-node-fw5xl from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.872: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:25:45.872: INFO: 
Logging pods the kubelet thinks is on node metalk8s-05 before test
Mar 22 13:25:45.886: INFO: elasticsearch-data-0 from kube-ops started at 2019-03-22 11:54:10 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:25:45.886: INFO: fluent-bit-cjtgw from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:25:45.886: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-fgs2v from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:25:45.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:25:45.886: INFO: nginx-proxy-metalk8s-05 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:25:45.886: INFO: heapster-heapster-869cfbd88b-zps4g from kube-system started at 2019-03-22 11:54:13 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container heapster ready: true, restart count 0
Mar 22 13:25:45.886: INFO: 	Container heapster-nanny ready: true, restart count 0
Mar 22 13:25:45.886: INFO: es-exporter-elasticsearch-exporter-6fdfc4c7d7-6v575 from kube-ops started at 2019-03-22 11:55:14 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
Mar 22 13:25:45.886: INFO: nginx-ingress-controller-p2lw6 from kube-ingress started at 2019-03-22 11:53:45 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:25:45.886: INFO: kibana-d5c76b4dd-76nfw from kube-ops started at 2019-03-22 11:54:52 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container kibana ready: true, restart count 0
Mar 22 13:25:45.886: INFO: prometheus-kube-prometheus-0 from kube-ops started at 2019-03-22 11:53:36 +0000 UTC (3 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container alerting-rule-files-configmap-reloader ready: true, restart count 0
Mar 22 13:25:45.886: INFO: 	Container prometheus ready: true, restart count 1
Mar 22 13:25:45.886: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 22 13:25:45.886: INFO: calico-node-b2zns from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:25:45.886: INFO: tiller-deploy-6f6fd74b68-7b7bc from kube-system started at 2019-03-22 11:52:19 +0000 UTC (1 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container tiller ready: true, restart count 0
Mar 22 13:25:45.886: INFO: alertmanager-kube-prometheus-1 from kube-ops started at 2019-03-22 11:54:08 +0000 UTC (2 container statuses recorded)
Mar 22 13:25:45.886: INFO: 	Container alertmanager ready: true, restart count 0
Mar 22 13:25:45.886: INFO: 	Container config-reloader ready: true, restart count 0
Mar 22 13:25:45.886: INFO: kube-proxy-metalk8s-05 from kube-system started at <nil> (0 container statuses recorded)
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.158e4ac86071e7cb], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:25:46.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-rzsr9" for this suite.
Mar 22 13:25:52.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:25:53.040: INFO: namespace: e2e-tests-sched-pred-rzsr9, resource: bindings, ignored listing per whitelist
Mar 22 13:25:53.209: INFO: namespace e2e-tests-sched-pred-rzsr9 deletion completed in 6.231203941s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

â€¢ [SLOW TEST:7.691 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:25:53.210: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-m8fx2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-046d0162-4ca6-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 13:25:53.421: INFO: Waiting up to 5m0s for pod "pod-configmaps-046dfa39-4ca6-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-configmap-m8fx2" to be "success or failure"
Mar 22 13:25:53.428: INFO: Pod "pod-configmaps-046dfa39-4ca6-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.46115ms
Mar 22 13:25:55.443: INFO: Pod "pod-configmaps-046dfa39-4ca6-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021729915s
STEP: Saw pod success
Mar 22 13:25:55.443: INFO: Pod "pod-configmaps-046dfa39-4ca6-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:25:55.449: INFO: Trying to get logs from node metalk8s-05 pod pod-configmaps-046dfa39-4ca6-11e9-99ae-9e98f636e47c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 13:25:55.492: INFO: Waiting for pod pod-configmaps-046dfa39-4ca6-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:25:55.497: INFO: Pod pod-configmaps-046dfa39-4ca6-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:25:55.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-m8fx2" for this suite.
Mar 22 13:26:01.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:26:01.588: INFO: namespace: e2e-tests-configmap-m8fx2, resource: bindings, ignored listing per whitelist
Mar 22 13:26:01.823: INFO: namespace e2e-tests-configmap-m8fx2 deletion completed in 6.31916942s

â€¢ [SLOW TEST:8.613 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:26:01.823: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-kql7n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-09ae8038-4ca6-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 13:26:02.244: INFO: Waiting up to 5m0s for pod "pod-configmaps-09afabbb-4ca6-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-configmap-kql7n" to be "success or failure"
Mar 22 13:26:02.252: INFO: Pod "pod-configmaps-09afabbb-4ca6-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108916ms
Mar 22 13:26:04.258: INFO: Pod "pod-configmaps-09afabbb-4ca6-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013463694s
Mar 22 13:26:06.273: INFO: Pod "pod-configmaps-09afabbb-4ca6-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02833832s
STEP: Saw pod success
Mar 22 13:26:06.273: INFO: Pod "pod-configmaps-09afabbb-4ca6-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:26:06.278: INFO: Trying to get logs from node metalk8s-02 pod pod-configmaps-09afabbb-4ca6-11e9-99ae-9e98f636e47c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 13:26:06.309: INFO: Waiting for pod pod-configmaps-09afabbb-4ca6-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:26:06.317: INFO: Pod pod-configmaps-09afabbb-4ca6-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:26:06.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-kql7n" for this suite.
Mar 22 13:26:12.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:26:12.514: INFO: namespace: e2e-tests-configmap-kql7n, resource: bindings, ignored listing per whitelist
Mar 22 13:26:12.585: INFO: namespace e2e-tests-configmap-kql7n deletion completed in 6.260627782s

â€¢ [SLOW TEST:10.762 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:26:12.585: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-pjjgn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Mar 22 13:26:12.853: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 22 13:26:12.872: INFO: Waiting for terminating namespaces to be deleted...
Mar 22 13:26:12.877: INFO: 
Logging pods the kubelet thinks is on node metalk8s-01 before test
Mar 22 13:26:12.888: INFO: kube-proxy-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.888: INFO: kube-apiserver-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.888: INFO: nginx-ingress-controller-cxd4p from kube-ingress started at 2019-03-22 11:53:46 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.888: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:26:12.888: INFO: fluent-bit-ps79j from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.888: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:26:12.888: INFO: metrics-server-647b5cf45b-94rmt from kube-system started at 2019-03-22 11:55:38 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.888: INFO: 	Container metrics-server ready: true, restart count 0
Mar 22 13:26:12.888: INFO: elasticsearch-master-2 from kube-ops started at 2019-03-22 11:56:05 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.888: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:26:12.888: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-d92zp from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.888: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:26:12.888: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:26:12.888: INFO: kube-controller-manager-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.888: INFO: calico-node-6x2ch from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.888: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:26:12.888: INFO: nginx-ingress-default-backend-7b98d4dc6c-5ct2x from kube-ingress started at 2019-03-22 11:53:46 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.888: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Mar 22 13:26:12.888: INFO: kube-scheduler-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.889: INFO: elasticsearch-client-6d74f68c8d-9t67x from kube-ops started at 2019-03-22 11:54:09 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.889: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:26:12.889: INFO: 
Logging pods the kubelet thinks is on node metalk8s-02 before test
Mar 22 13:26:12.904: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-5j5mp from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:26:12.904: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:26:12.904: INFO: kube-controller-manager-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.904: INFO: sonobuoy-e2e-job-59a17f0f4a364fed from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container e2e ready: true, restart count 0
Mar 22 13:26:12.904: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:26:12.904: INFO: elasticsearch-client-6d74f68c8d-28qk2 from kube-ops started at 2019-03-22 11:54:09 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:26:12.904: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-22 13:10:19 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 22 13:26:12.904: INFO: kube-apiserver-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.904: INFO: calico-node-rtkz2 from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:26:12.904: INFO: nginx-ingress-controller-7rf9j from kube-ingress started at 2019-03-22 11:53:46 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:26:12.904: INFO: kube-prometheus-exporter-kube-state-68bb849446-r95z2 from kube-ops started at 2019-03-22 11:53:50 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container exporter-kube-state ready: true, restart count 0
Mar 22 13:26:12.904: INFO: 	Container exporter-kube-state-addon-resizer ready: true, restart count 0
Mar 22 13:26:12.904: INFO: fluent-bit-rxwxc from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:26:12.904: INFO: kube-scheduler-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.904: INFO: coredns-6bcbc499f7-j6fch from kube-system started at 2019-03-22 11:50:52 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container coredns ready: true, restart count 0
Mar 22 13:26:12.904: INFO: kube-proxy-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.904: INFO: cerebro-78d5fbf859-v8wt6 from kube-ops started at 2019-03-22 11:55:30 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.904: INFO: 	Container cerebro ready: true, restart count 0
Mar 22 13:26:12.904: INFO: 
Logging pods the kubelet thinks is on node metalk8s-03 before test
Mar 22 13:26:12.919: INFO: nginx-ingress-controller-7x9tf from kube-ingress started at 2019-03-22 11:53:45 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:26:12.919: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-79jdm from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:26:12.919: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:26:12.919: INFO: kube-apiserver-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.919: INFO: kube-scheduler-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.919: INFO: calico-node-rnssh from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:26:12.919: INFO: fluentd-68575dcc69-2hsp4 from kube-ops started at 2019-03-22 11:54:25 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container fluentd ready: true, restart count 0
Mar 22 13:26:12.919: INFO: fluent-bit-xrslr from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:26:12.919: INFO: elasticsearch-master-1 from kube-ops started at 2019-03-22 11:55:25 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:26:12.919: INFO: elasticsearch-data-2 from kube-ops started at 2019-03-22 11:56:08 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:26:12.919: INFO: kube-proxy-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.919: INFO: kube-controller-manager-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.919: INFO: prometheus-operator-87779759-x9zf7 from kube-ops started at 2019-03-22 11:52:42 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 22 13:26:12.919: INFO: calico-kube-controllers-5887874ffb-5sb92 from kube-system started at 2019-03-22 11:49:56 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 22 13:26:12.919: INFO: elasticsearch-client-6d74f68c8d-v47bt from kube-ops started at 2019-03-22 11:54:09 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.919: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:26:12.919: INFO: 
Logging pods the kubelet thinks is on node metalk8s-04 before test
Mar 22 13:26:12.943: INFO: prometheus-kube-prometheus-1 from kube-ops started at 2019-03-22 11:54:11 +0000 UTC (3 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container alerting-rule-files-configmap-reloader ready: true, restart count 0
Mar 22 13:26:12.943: INFO: 	Container prometheus ready: true, restart count 1
Mar 22 13:26:12.943: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 22 13:26:12.943: INFO: kibana-index-provisioning-ldhpt from kube-ops started at 2019-03-22 11:55:08 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container provision-index ready: false, restart count 4
Mar 22 13:26:12.943: INFO: kube-proxy-metalk8s-04 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.943: INFO: nginx-proxy-metalk8s-04 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.943: INFO: kube-prometheus-grafana-f56778dd6-v8hf8 from kube-ops started at 2019-03-22 11:53:34 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container grafana ready: true, restart count 0
Mar 22 13:26:12.943: INFO: 	Container grafana-watcher ready: true, restart count 0
Mar 22 13:26:12.943: INFO: fluentd-68575dcc69-vxd2d from kube-ops started at 2019-03-22 11:54:26 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container fluentd ready: true, restart count 0
Mar 22 13:26:12.943: INFO: alertmanager-kube-prometheus-0 from kube-ops started at 2019-03-22 11:53:34 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container alertmanager ready: true, restart count 0
Mar 22 13:26:12.943: INFO: 	Container config-reloader ready: true, restart count 0
Mar 22 13:26:12.943: INFO: nginx-ingress-controller-2mvpz from kube-ingress started at 2019-03-22 11:53:45 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:26:12.943: INFO: elasticsearch-data-1 from kube-ops started at 2019-03-22 11:55:27 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:26:12.943: INFO: coredns-6bcbc499f7-fsrbl from kube-system started at 2019-03-22 11:50:52 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container coredns ready: true, restart count 0
Mar 22 13:26:12.943: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-mm98f from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:26:12.943: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:26:12.943: INFO: calico-node-fw5xl from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:26:12.943: INFO: kubernetes-dashboard-68697c45d9-pbvnj from kube-system started at 2019-03-22 11:50:59 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 22 13:26:12.943: INFO: elasticsearch-master-0 from kube-ops started at 2019-03-22 11:54:10 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:26:12.943: INFO: fluent-bit-9bwfp from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.943: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:26:12.943: INFO: 
Logging pods the kubelet thinks is on node metalk8s-05 before test
Mar 22 13:26:12.958: INFO: nginx-proxy-metalk8s-05 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.958: INFO: elasticsearch-data-0 from kube-ops started at 2019-03-22 11:54:10 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 13:26:12.958: INFO: fluent-bit-cjtgw from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 13:26:12.958: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-fgs2v from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Mar 22 13:26:12.958: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 13:26:12.958: INFO: nginx-ingress-controller-p2lw6 from kube-ingress started at 2019-03-22 11:53:45 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 13:26:12.958: INFO: heapster-heapster-869cfbd88b-zps4g from kube-system started at 2019-03-22 11:54:13 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container heapster ready: true, restart count 0
Mar 22 13:26:12.958: INFO: 	Container heapster-nanny ready: true, restart count 0
Mar 22 13:26:12.958: INFO: es-exporter-elasticsearch-exporter-6fdfc4c7d7-6v575 from kube-ops started at 2019-03-22 11:55:14 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
Mar 22 13:26:12.958: INFO: calico-node-b2zns from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 13:26:12.958: INFO: kibana-d5c76b4dd-76nfw from kube-ops started at 2019-03-22 11:54:52 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container kibana ready: true, restart count 0
Mar 22 13:26:12.958: INFO: prometheus-kube-prometheus-0 from kube-ops started at 2019-03-22 11:53:36 +0000 UTC (3 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container alerting-rule-files-configmap-reloader ready: true, restart count 0
Mar 22 13:26:12.958: INFO: 	Container prometheus ready: true, restart count 1
Mar 22 13:26:12.958: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 22 13:26:12.958: INFO: kube-proxy-metalk8s-05 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 13:26:12.958: INFO: tiller-deploy-6f6fd74b68-7b7bc from kube-system started at 2019-03-22 11:52:19 +0000 UTC (1 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container tiller ready: true, restart count 0
Mar 22 13:26:12.958: INFO: alertmanager-kube-prometheus-1 from kube-ops started at 2019-03-22 11:54:08 +0000 UTC (2 container statuses recorded)
Mar 22 13:26:12.958: INFO: 	Container alertmanager ready: true, restart count 0
Mar 22 13:26:12.958: INFO: 	Container config-reloader ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node metalk8s-01
STEP: verifying the node has the label node metalk8s-02
STEP: verifying the node has the label node metalk8s-03
STEP: verifying the node has the label node metalk8s-04
STEP: verifying the node has the label node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod sonobuoy requesting resource cpu=0m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod sonobuoy-e2e-job-59a17f0f4a364fed requesting resource cpu=0m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-5j5mp requesting resource cpu=0m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-79jdm requesting resource cpu=0m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-d92zp requesting resource cpu=0m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-fgs2v requesting resource cpu=0m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-mm98f requesting resource cpu=0m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod nginx-ingress-controller-2mvpz requesting resource cpu=0m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod nginx-ingress-controller-7rf9j requesting resource cpu=0m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod nginx-ingress-controller-7x9tf requesting resource cpu=0m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod nginx-ingress-controller-cxd4p requesting resource cpu=0m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod nginx-ingress-controller-p2lw6 requesting resource cpu=0m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod nginx-ingress-default-backend-7b98d4dc6c-5ct2x requesting resource cpu=0m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod alertmanager-kube-prometheus-0 requesting resource cpu=5m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod alertmanager-kube-prometheus-1 requesting resource cpu=5m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod cerebro-78d5fbf859-v8wt6 requesting resource cpu=0m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod elasticsearch-client-6d74f68c8d-28qk2 requesting resource cpu=1000m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod elasticsearch-client-6d74f68c8d-9t67x requesting resource cpu=1000m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod elasticsearch-client-6d74f68c8d-v47bt requesting resource cpu=1000m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod elasticsearch-data-0 requesting resource cpu=1000m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod elasticsearch-data-1 requesting resource cpu=1000m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod elasticsearch-data-2 requesting resource cpu=1000m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod elasticsearch-master-0 requesting resource cpu=500m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod elasticsearch-master-1 requesting resource cpu=500m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod elasticsearch-master-2 requesting resource cpu=500m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod es-exporter-elasticsearch-exporter-6fdfc4c7d7-6v575 requesting resource cpu=0m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod fluent-bit-9bwfp requesting resource cpu=100m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod fluent-bit-cjtgw requesting resource cpu=100m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod fluent-bit-ps79j requesting resource cpu=100m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod fluent-bit-rxwxc requesting resource cpu=100m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod fluent-bit-xrslr requesting resource cpu=100m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod fluentd-68575dcc69-2hsp4 requesting resource cpu=0m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod fluentd-68575dcc69-vxd2d requesting resource cpu=0m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod kibana-d5c76b4dd-76nfw requesting resource cpu=0m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod kube-prometheus-exporter-kube-state-68bb849446-r95z2 requesting resource cpu=204m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod kube-prometheus-grafana-f56778dd6-v8hf8 requesting resource cpu=0m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod prometheus-kube-prometheus-0 requesting resource cpu=15m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod prometheus-kube-prometheus-1 requesting resource cpu=15m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod prometheus-operator-87779759-x9zf7 requesting resource cpu=0m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod calico-kube-controllers-5887874ffb-5sb92 requesting resource cpu=30m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod calico-node-6x2ch requesting resource cpu=150m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod calico-node-b2zns requesting resource cpu=150m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod calico-node-fw5xl requesting resource cpu=150m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod calico-node-rnssh requesting resource cpu=150m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod calico-node-rtkz2 requesting resource cpu=150m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod coredns-6bcbc499f7-fsrbl requesting resource cpu=100m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod coredns-6bcbc499f7-j6fch requesting resource cpu=100m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod heapster-heapster-869cfbd88b-zps4g requesting resource cpu=250m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod kube-apiserver-metalk8s-01 requesting resource cpu=100m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod kube-apiserver-metalk8s-02 requesting resource cpu=100m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod kube-apiserver-metalk8s-03 requesting resource cpu=100m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod kube-controller-manager-metalk8s-01 requesting resource cpu=100m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod kube-controller-manager-metalk8s-02 requesting resource cpu=100m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod kube-controller-manager-metalk8s-03 requesting resource cpu=100m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod kube-proxy-metalk8s-01 requesting resource cpu=150m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod kube-proxy-metalk8s-02 requesting resource cpu=150m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod kube-proxy-metalk8s-03 requesting resource cpu=150m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod kube-proxy-metalk8s-04 requesting resource cpu=150m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod kube-proxy-metalk8s-05 requesting resource cpu=150m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod kube-scheduler-metalk8s-01 requesting resource cpu=80m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod kube-scheduler-metalk8s-02 requesting resource cpu=80m on Node metalk8s-02
Mar 22 13:26:13.168: INFO: Pod kube-scheduler-metalk8s-03 requesting resource cpu=80m on Node metalk8s-03
Mar 22 13:26:13.168: INFO: Pod kubernetes-dashboard-68697c45d9-pbvnj requesting resource cpu=50m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod metrics-server-647b5cf45b-94rmt requesting resource cpu=0m on Node metalk8s-01
Mar 22 13:26:13.168: INFO: Pod nginx-proxy-metalk8s-04 requesting resource cpu=25m on Node metalk8s-04
Mar 22 13:26:13.168: INFO: Pod nginx-proxy-metalk8s-05 requesting resource cpu=25m on Node metalk8s-05
Mar 22 13:26:13.168: INFO: Pod tiller-deploy-6f6fd74b68-7b7bc requesting resource cpu=0m on Node metalk8s-05
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1034bb34-4ca6-11e9-99ae-9e98f636e47c.158e4aceb7de4982], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-pjjgn/filler-pod-1034bb34-4ca6-11e9-99ae-9e98f636e47c to metalk8s-03]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1034bb34-4ca6-11e9-99ae-9e98f636e47c.158e4acef7a98383], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1034bb34-4ca6-11e9-99ae-9e98f636e47c.158e4acf61e7591c], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1034bb34-4ca6-11e9-99ae-9e98f636e47c.158e4acf644edd9c], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1034bb34-4ca6-11e9-99ae-9e98f636e47c.158e4acf6bd8c396], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10385329-4ca6-11e9-99ae-9e98f636e47c.158e4aceb89e094b], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-pjjgn/filler-pod-10385329-4ca6-11e9-99ae-9e98f636e47c to metalk8s-04]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10385329-4ca6-11e9-99ae-9e98f636e47c.158e4acef148a1a2], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10385329-4ca6-11e9-99ae-9e98f636e47c.158e4acf61ed5a3d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10385329-4ca6-11e9-99ae-9e98f636e47c.158e4acf63f872b4], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10385329-4ca6-11e9-99ae-9e98f636e47c.158e4acf6bde12c4], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103991ba-4ca6-11e9-99ae-9e98f636e47c.158e4aceb992c37e], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-pjjgn/filler-pod-103991ba-4ca6-11e9-99ae-9e98f636e47c to metalk8s-05]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103991ba-4ca6-11e9-99ae-9e98f636e47c.158e4acef1c0298f], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103991ba-4ca6-11e9-99ae-9e98f636e47c.158e4acf5c0c3f7e], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103991ba-4ca6-11e9-99ae-9e98f636e47c.158e4acf5f152253], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103991ba-4ca6-11e9-99ae-9e98f636e47c.158e4acf681d17ee], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103b3067-4ca6-11e9-99ae-9e98f636e47c.158e4aceba54148b], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-pjjgn/filler-pod-103b3067-4ca6-11e9-99ae-9e98f636e47c to metalk8s-01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103b3067-4ca6-11e9-99ae-9e98f636e47c.158e4acefcd529e9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103b3067-4ca6-11e9-99ae-9e98f636e47c.158e4aceff1c5ad3], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103b3067-4ca6-11e9-99ae-9e98f636e47c.158e4acf0b7c238a], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103c867e-4ca6-11e9-99ae-9e98f636e47c.158e4acebf73925d], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-pjjgn/filler-pod-103c867e-4ca6-11e9-99ae-9e98f636e47c to metalk8s-02]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103c867e-4ca6-11e9-99ae-9e98f636e47c.158e4acef70b0cf7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103c867e-4ca6-11e9-99ae-9e98f636e47c.158e4acef91a7af4], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-103c867e-4ca6-11e9-99ae-9e98f636e47c.158e4acf024f2d5e], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158e4acfab927a71], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node metalk8s-04
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node metalk8s-05
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node metalk8s-01
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node metalk8s-02
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node metalk8s-03
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:26:18.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-pjjgn" for this suite.
Mar 22 13:26:26.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:26:26.464: INFO: namespace: e2e-tests-sched-pred-pjjgn, resource: bindings, ignored listing per whitelist
Mar 22 13:26:26.613: INFO: namespace e2e-tests-sched-pred-pjjgn deletion completed in 8.190427453s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

â€¢ [SLOW TEST:14.028 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:26:26.613: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bmczg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-185b806c-4ca6-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 13:26:26.861: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-185c7684-4ca6-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-bmczg" to be "success or failure"
Mar 22 13:26:26.867: INFO: Pod "pod-projected-configmaps-185c7684-4ca6-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.816112ms
Mar 22 13:26:28.882: INFO: Pod "pod-projected-configmaps-185c7684-4ca6-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020597881s
Mar 22 13:26:30.888: INFO: Pod "pod-projected-configmaps-185c7684-4ca6-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0267456s
Mar 22 13:26:32.897: INFO: Pod "pod-projected-configmaps-185c7684-4ca6-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035942735s
STEP: Saw pod success
Mar 22 13:26:32.897: INFO: Pod "pod-projected-configmaps-185c7684-4ca6-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:26:32.903: INFO: Trying to get logs from node metalk8s-01 pod pod-projected-configmaps-185c7684-4ca6-11e9-99ae-9e98f636e47c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 13:26:32.939: INFO: Waiting for pod pod-projected-configmaps-185c7684-4ca6-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:26:32.946: INFO: Pod pod-projected-configmaps-185c7684-4ca6-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:26:32.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bmczg" for this suite.
Mar 22 13:26:38.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:26:39.101: INFO: namespace: e2e-tests-projected-bmczg, resource: bindings, ignored listing per whitelist
Mar 22 13:26:39.219: INFO: namespace e2e-tests-projected-bmczg deletion completed in 6.264074833s

â€¢ [SLOW TEST:12.606 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:26:39.219: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fbkt6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar 22 13:26:39.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:39.651: INFO: stderr: ""
Mar 22 13:26:39.651: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 22 13:26:39.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:39.782: INFO: stderr: ""
Mar 22 13:26:39.782: INFO: stdout: "update-demo-nautilus-dmnk2 update-demo-nautilus-zwr4r "
Mar 22 13:26:39.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-dmnk2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:39.913: INFO: stderr: ""
Mar 22 13:26:39.913: INFO: stdout: ""
Mar 22 13:26:39.913: INFO: update-demo-nautilus-dmnk2 is created but not running
Mar 22 13:26:44.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:45.027: INFO: stderr: ""
Mar 22 13:26:45.027: INFO: stdout: "update-demo-nautilus-dmnk2 update-demo-nautilus-zwr4r "
Mar 22 13:26:45.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-dmnk2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:45.132: INFO: stderr: ""
Mar 22 13:26:45.132: INFO: stdout: "true"
Mar 22 13:26:45.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-dmnk2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:45.247: INFO: stderr: ""
Mar 22 13:26:45.247: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:26:45.247: INFO: validating pod update-demo-nautilus-dmnk2
Mar 22 13:26:45.255: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:26:45.255: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:26:45.255: INFO: update-demo-nautilus-dmnk2 is verified up and running
Mar 22 13:26:45.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-zwr4r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:45.363: INFO: stderr: ""
Mar 22 13:26:45.363: INFO: stdout: "true"
Mar 22 13:26:45.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-zwr4r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:45.488: INFO: stderr: ""
Mar 22 13:26:45.488: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:26:45.488: INFO: validating pod update-demo-nautilus-zwr4r
Mar 22 13:26:45.500: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:26:45.500: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:26:45.500: INFO: update-demo-nautilus-zwr4r is verified up and running
STEP: using delete to clean up resources
Mar 22 13:26:45.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:45.605: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 13:26:45.605: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 22 13:26:45.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-fbkt6'
Mar 22 13:26:45.707: INFO: stderr: "No resources found.\n"
Mar 22 13:26:45.707: INFO: stdout: ""
Mar 22 13:26:45.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -l name=update-demo --namespace=e2e-tests-kubectl-fbkt6 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 22 13:26:45.803: INFO: stderr: ""
Mar 22 13:26:45.803: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:26:45.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fbkt6" for this suite.
Mar 22 13:26:53.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:26:53.852: INFO: namespace: e2e-tests-kubectl-fbkt6, resource: bindings, ignored listing per whitelist
Mar 22 13:26:54.028: INFO: namespace e2e-tests-kubectl-fbkt6 deletion completed in 8.208635346s

â€¢ [SLOW TEST:14.809 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:26:54.028: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-kxfl9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:26:54.577: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 22 13:26:54.599: INFO: Number of nodes with available pods: 0
Mar 22 13:26:54.599: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:26:55.630: INFO: Number of nodes with available pods: 0
Mar 22 13:26:55.630: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:26:56.618: INFO: Number of nodes with available pods: 0
Mar 22 13:26:56.618: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:26:57.618: INFO: Number of nodes with available pods: 1
Mar 22 13:26:57.618: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:26:58.612: INFO: Number of nodes with available pods: 1
Mar 22 13:26:58.612: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:26:59.647: INFO: Number of nodes with available pods: 3
Mar 22 13:26:59.647: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:27:00.631: INFO: Number of nodes with available pods: 5
Mar 22 13:27:00.631: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 22 13:27:00.687: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:00.687: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:00.687: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:00.687: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:00.687: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:01.722: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:01.722: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:01.722: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:01.722: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:01.722: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:02.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:02.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:02.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:02.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:02.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:03.715: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:03.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:03.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:03.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:03.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:04.726: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:04.726: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:04.726: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:04.726: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:04.726: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:05.715: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:05.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:05.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:05.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:05.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:06.719: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:06.719: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:06.719: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:06.719: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:06.719: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:07.715: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:07.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:07.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:07.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:07.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:08.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:08.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:08.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:08.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:08.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:09.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:09.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:09.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:09.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:09.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:10.726: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:10.726: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:10.726: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:10.726: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:10.726: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:11.717: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:11.717: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:11.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:11.717: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:11.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:12.713: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:12.713: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:12.713: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:12.713: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:12.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:13.715: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:13.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:13.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:13.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:13.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:14.718: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:14.718: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:14.718: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:14.718: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:14.718: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:15.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:15.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:15.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:15.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:15.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:16.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:16.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:16.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:16.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:16.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:17.713: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:17.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:17.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:17.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:17.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:18.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:18.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:18.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:18.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:18.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:19.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:19.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:19.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:19.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:19.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:20.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:20.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:20.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:20.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:20.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:21.726: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:21.726: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:21.726: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:21.726: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:21.726: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:22.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:22.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:22.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:22.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:22.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:23.715: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:23.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:23.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:23.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:23.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:24.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:24.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:24.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:24.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:24.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:25.719: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:25.719: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:25.719: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:25.719: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:25.719: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:26.715: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:26.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:26.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:26.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:26.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:27.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:27.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:27.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:27.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:27.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:28.714: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:28.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:28.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:28.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:28.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:29.717: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:29.717: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:29.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:29.717: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:29.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:30.715: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:30.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:30.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:30.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:30.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:31.719: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:31.719: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:31.719: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:31.719: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:31.719: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:32.717: INFO: Wrong image for pod: daemon-set-8429p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:32.717: INFO: Pod daemon-set-8429p is not available
Mar 22 13:27:32.717: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:32.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:32.717: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:32.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:33.714: INFO: Pod daemon-set-7q297 is not available
Mar 22 13:27:33.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:33.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:33.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:33.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:34.716: INFO: Pod daemon-set-7q297 is not available
Mar 22 13:27:34.716: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:34.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:34.716: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:34.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:35.715: INFO: Pod daemon-set-7q297 is not available
Mar 22 13:27:35.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:35.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:35.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:35.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:36.717: INFO: Pod daemon-set-7q297 is not available
Mar 22 13:27:36.717: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:36.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:36.717: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:36.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:37.714: INFO: Pod daemon-set-7q297 is not available
Mar 22 13:27:37.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:37.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:37.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:37.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:38.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:38.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:38.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:38.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:39.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:39.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:39.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:39.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:40.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:40.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:40.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:40.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:41.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:41.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:41.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:41.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:42.721: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:42.721: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:42.721: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:42.721: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:43.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:43.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:43.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:43.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:44.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:44.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:44.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:44.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:45.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:45.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:45.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:45.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:46.717: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:46.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:46.717: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:46.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:47.768: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:47.768: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:47.768: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:47.768: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:48.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:48.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:48.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:48.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:49.716: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:49.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:49.716: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:49.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:50.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:50.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:50.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:50.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:51.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:51.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:51.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:51.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:52.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:52.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:52.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:52.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:53.739: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:53.739: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:53.739: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:53.739: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:54.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:54.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:54.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:54.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:55.716: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:55.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:55.716: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:55.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:56.713: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:56.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:56.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:56.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:57.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:57.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:57.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:57.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:58.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:58.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:58.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:58.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:59.717: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:59.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:59.717: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:27:59.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:00.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:00.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:00.715: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:00.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:01.719: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:01.719: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:01.719: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:01.719: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:02.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:02.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:02.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:02.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:03.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:03.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:03.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:03.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:04.732: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:04.732: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:04.732: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:04.732: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:05.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:05.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:05.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:05.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:06.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:06.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:06.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:06.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:07.713: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:07.713: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:07.713: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:07.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:08.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:08.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:08.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:08.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:09.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:09.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:09.714: INFO: Wrong image for pod: daemon-set-jdjzh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:09.714: INFO: Pod daemon-set-jdjzh is not available
Mar 22 13:28:09.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:10.716: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:10.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:10.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:10.716: INFO: Pod daemon-set-pttxp is not available
Mar 22 13:28:11.719: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:11.719: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:11.719: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:11.719: INFO: Pod daemon-set-pttxp is not available
Mar 22 13:28:12.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:12.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:12.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:12.714: INFO: Pod daemon-set-pttxp is not available
Mar 22 13:28:13.720: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:13.720: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:13.720: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:13.720: INFO: Pod daemon-set-pttxp is not available
Mar 22 13:28:14.719: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:14.719: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:14.719: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:14.719: INFO: Pod daemon-set-pttxp is not available
Mar 22 13:28:15.722: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:15.722: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:15.722: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:16.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:16.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:16.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:17.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:17.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:17.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:18.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:18.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:18.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:19.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:19.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:19.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:20.717: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:20.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:20.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:21.716: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:21.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:21.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:22.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:22.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:22.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:23.716: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:23.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:23.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:24.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:24.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:24.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:25.713: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:25.713: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:25.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:26.731: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:26.731: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:26.731: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:27.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:27.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:27.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:28.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:28.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:28.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:29.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:29.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:29.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:30.717: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:30.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:30.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:31.716: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:31.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:31.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:32.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:32.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:32.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:33.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:33.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:33.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:34.731: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:34.731: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:34.731: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:35.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:35.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:35.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:36.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:36.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:36.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:37.721: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:37.721: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:37.721: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:38.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:38.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:38.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:39.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:39.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:39.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:40.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:40.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:40.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:41.717: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:41.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:41.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:42.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:42.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:42.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:43.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:43.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:43.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:44.715: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:44.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:44.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:45.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:45.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:45.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:46.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:46.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:46.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:47.713: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:47.713: INFO: Pod daemon-set-dzxgp is not available
Mar 22 13:28:47.713: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:47.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:48.723: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:48.723: INFO: Pod daemon-set-dzxgp is not available
Mar 22 13:28:48.723: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:48.723: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:49.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:49.714: INFO: Pod daemon-set-dzxgp is not available
Mar 22 13:28:49.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:49.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:50.714: INFO: Wrong image for pod: daemon-set-dzxgp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:50.714: INFO: Pod daemon-set-dzxgp is not available
Mar 22 13:28:50.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:50.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:51.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:51.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:51.715: INFO: Pod daemon-set-xkn69 is not available
Mar 22 13:28:52.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:52.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:52.714: INFO: Pod daemon-set-xkn69 is not available
Mar 22 13:28:53.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:53.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:53.715: INFO: Pod daemon-set-xkn69 is not available
Mar 22 13:28:54.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:54.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:54.716: INFO: Pod daemon-set-xkn69 is not available
Mar 22 13:28:55.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:55.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:55.714: INFO: Pod daemon-set-xkn69 is not available
Mar 22 13:28:56.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:56.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:56.716: INFO: Pod daemon-set-xkn69 is not available
Mar 22 13:28:57.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:57.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:58.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:58.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:59.728: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:28:59.728: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:00.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:00.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:01.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:01.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:02.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:02.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:03.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:03.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:04.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:04.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:05.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:05.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:06.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:06.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:07.713: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:07.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:08.728: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:08.729: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:09.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:09.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:10.749: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:10.749: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:11.719: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:11.719: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:12.713: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:12.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:13.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:13.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:14.721: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:14.721: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:15.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:15.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:16.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:16.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:17.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:17.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:18.717: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:18.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:19.713: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:19.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:20.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:20.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:21.781: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:21.781: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:22.713: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:22.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:23.716: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:23.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:24.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:24.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:25.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:25.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:26.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:26.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:27.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:27.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:28.715: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:28.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:29.714: INFO: Wrong image for pod: daemon-set-fjfrr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:29.714: INFO: Pod daemon-set-fjfrr is not available
Mar 22 13:29:29.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:30.714: INFO: Pod daemon-set-9jv89 is not available
Mar 22 13:29:30.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:31.714: INFO: Pod daemon-set-9jv89 is not available
Mar 22 13:29:31.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:32.721: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:33.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:34.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:35.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:36.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:37.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:38.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:39.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:40.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:41.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:42.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:43.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:44.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:45.713: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:46.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:47.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:48.720: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:49.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:50.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:51.718: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:52.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:53.725: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:54.716: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:55.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:56.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:57.714: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:58.715: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:29:59.717: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:30:00.721: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:30:01.730: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:30:02.732: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:30:03.731: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:30:04.728: INFO: Wrong image for pod: daemon-set-nclb5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Mar 22 13:30:04.728: INFO: Pod daemon-set-nclb5 is not available
Mar 22 13:30:05.727: INFO: Pod daemon-set-t2nhq is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 22 13:30:05.779: INFO: Number of nodes with available pods: 4
Mar 22 13:30:05.779: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:30:06.792: INFO: Number of nodes with available pods: 4
Mar 22 13:30:06.792: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:30:07.793: INFO: Number of nodes with available pods: 4
Mar 22 13:30:07.793: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:30:08.794: INFO: Number of nodes with available pods: 4
Mar 22 13:30:08.794: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:30:09.816: INFO: Number of nodes with available pods: 4
Mar 22 13:30:09.816: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:30:10.804: INFO: Number of nodes with available pods: 4
Mar 22 13:30:10.804: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:30:12.863: INFO: Number of nodes with available pods: 5
Mar 22 13:30:12.863: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-kxfl9, will wait for the garbage collector to delete the pods
Mar 22 13:30:13.026: INFO: Deleting {extensions DaemonSet} daemon-set took: 21.093197ms
Mar 22 13:30:13.126: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.202798ms
Mar 22 13:30:21.939: INFO: Number of nodes with available pods: 0
Mar 22 13:30:21.939: INFO: Number of running nodes: 0, number of available pods: 0
Mar 22 13:30:21.945: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-kxfl9/daemonsets","resourceVersion":"16308"},"items":null}

Mar 22 13:30:21.950: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-kxfl9/pods","resourceVersion":"16308"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:30:21.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-kxfl9" for this suite.
Mar 22 13:30:30.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:30:30.044: INFO: namespace: e2e-tests-daemonsets-kxfl9, resource: bindings, ignored listing per whitelist
Mar 22 13:30:30.275: INFO: namespace e2e-tests-daemonsets-kxfl9 deletion completed in 8.277836718s

â€¢ [SLOW TEST:216.247 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:30:30.275: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-8gc8q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:30:30.485: INFO: Creating ReplicaSet my-hostname-basic-a9943847-4ca6-11e9-99ae-9e98f636e47c
Mar 22 13:30:30.496: INFO: Pod name my-hostname-basic-a9943847-4ca6-11e9-99ae-9e98f636e47c: Found 0 pods out of 1
Mar 22 13:30:35.511: INFO: Pod name my-hostname-basic-a9943847-4ca6-11e9-99ae-9e98f636e47c: Found 1 pods out of 1
Mar 22 13:30:35.511: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a9943847-4ca6-11e9-99ae-9e98f636e47c" is running
Mar 22 13:30:35.516: INFO: Pod "my-hostname-basic-a9943847-4ca6-11e9-99ae-9e98f636e47c-9w2x2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-22 13:30:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-22 13:30:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-22 13:30:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-22 13:30:30 +0000 UTC Reason: Message:}])
Mar 22 13:30:35.516: INFO: Trying to dial the pod
Mar 22 13:30:40.534: INFO: Controller my-hostname-basic-a9943847-4ca6-11e9-99ae-9e98f636e47c: Got expected result from replica 1 [my-hostname-basic-a9943847-4ca6-11e9-99ae-9e98f636e47c-9w2x2]: "my-hostname-basic-a9943847-4ca6-11e9-99ae-9e98f636e47c-9w2x2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:30:40.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-8gc8q" for this suite.
Mar 22 13:30:46.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:30:46.694: INFO: namespace: e2e-tests-replicaset-8gc8q, resource: bindings, ignored listing per whitelist
Mar 22 13:30:46.747: INFO: namespace e2e-tests-replicaset-8gc8q deletion completed in 6.205478332s

â€¢ [SLOW TEST:16.472 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:30:46.748: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-76bbc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-mgtj
STEP: Creating a pod to test atomic-volume-subpath
Mar 22 13:30:46.973: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mgtj" in namespace "e2e-tests-subpath-76bbc" to be "success or failure"
Mar 22 13:30:46.980: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Pending", Reason="", readiness=false. Elapsed: 7.628046ms
Mar 22 13:30:48.995: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022757392s
Mar 22 13:30:51.006: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033312312s
Mar 22 13:30:53.012: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 6.039504696s
Mar 22 13:30:55.019: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 8.046345105s
Mar 22 13:30:57.036: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 10.063045152s
Mar 22 13:30:59.042: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 12.069326751s
Mar 22 13:31:01.048: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 14.074971782s
Mar 22 13:31:03.053: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 16.08000383s
Mar 22 13:31:05.059: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 18.086137399s
Mar 22 13:31:07.072: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 20.099591435s
Mar 22 13:31:09.078: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 22.105335292s
Mar 22 13:31:11.083: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Running", Reason="", readiness=false. Elapsed: 24.110817333s
Mar 22 13:31:13.089: INFO: Pod "pod-subpath-test-configmap-mgtj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.116004742s
STEP: Saw pod success
Mar 22 13:31:13.089: INFO: Pod "pod-subpath-test-configmap-mgtj" satisfied condition "success or failure"
Mar 22 13:31:13.093: INFO: Trying to get logs from node metalk8s-01 pod pod-subpath-test-configmap-mgtj container test-container-subpath-configmap-mgtj: <nil>
STEP: delete the pod
Mar 22 13:31:13.125: INFO: Waiting for pod pod-subpath-test-configmap-mgtj to disappear
Mar 22 13:31:13.130: INFO: Pod pod-subpath-test-configmap-mgtj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mgtj
Mar 22 13:31:13.130: INFO: Deleting pod "pod-subpath-test-configmap-mgtj" in namespace "e2e-tests-subpath-76bbc"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:31:13.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-76bbc" for this suite.
Mar 22 13:31:19.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:31:19.194: INFO: namespace: e2e-tests-subpath-76bbc, resource: bindings, ignored listing per whitelist
Mar 22 13:31:19.331: INFO: namespace e2e-tests-subpath-76bbc deletion completed in 6.188042898s

â€¢ [SLOW TEST:32.583 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:31:19.331: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-2dsfn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Mar 22 13:31:20.066: INFO: Waiting up to 5m0s for pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-jqnj4" in namespace "e2e-tests-svcaccounts-2dsfn" to be "success or failure"
Mar 22 13:31:20.074: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-jqnj4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054885ms
Mar 22 13:31:22.176: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-jqnj4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11046858s
STEP: Saw pod success
Mar 22 13:31:22.176: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-jqnj4" satisfied condition "success or failure"
Mar 22 13:31:22.274: INFO: Trying to get logs from node metalk8s-02 pod pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-jqnj4 container token-test: <nil>
STEP: delete the pod
Mar 22 13:31:22.466: INFO: Waiting for pod pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-jqnj4 to disappear
Mar 22 13:31:22.471: INFO: Pod pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-jqnj4 no longer exists
STEP: Creating a pod to test consume service account root CA
Mar 22 13:31:22.477: INFO: Waiting up to 5m0s for pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-mhdjx" in namespace "e2e-tests-svcaccounts-2dsfn" to be "success or failure"
Mar 22 13:31:22.483: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-mhdjx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.31924ms
Mar 22 13:31:24.489: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-mhdjx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01169036s
Mar 22 13:31:26.497: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-mhdjx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019288489s
STEP: Saw pod success
Mar 22 13:31:26.497: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-mhdjx" satisfied condition "success or failure"
Mar 22 13:31:26.502: INFO: Trying to get logs from node metalk8s-01 pod pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-mhdjx container root-ca-test: <nil>
STEP: delete the pod
Mar 22 13:31:26.572: INFO: Waiting for pod pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-mhdjx to disappear
Mar 22 13:31:26.581: INFO: Pod pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-mhdjx no longer exists
STEP: Creating a pod to test consume service account namespace
Mar 22 13:31:26.613: INFO: Waiting up to 5m0s for pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-zr4lv" in namespace "e2e-tests-svcaccounts-2dsfn" to be "success or failure"
Mar 22 13:31:26.618: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-zr4lv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.528464ms
Mar 22 13:31:28.630: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-zr4lv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016562714s
Mar 22 13:31:30.635: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-zr4lv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021906635s
STEP: Saw pod success
Mar 22 13:31:30.635: INFO: Pod "pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-zr4lv" satisfied condition "success or failure"
Mar 22 13:31:30.641: INFO: Trying to get logs from node metalk8s-05 pod pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-zr4lv container namespace-test: <nil>
STEP: delete the pod
Mar 22 13:31:30.678: INFO: Waiting for pod pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-zr4lv to disappear
Mar 22 13:31:30.683: INFO: Pod pod-service-account-c71f9ef9-4ca6-11e9-99ae-9e98f636e47c-zr4lv no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:31:30.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-2dsfn" for this suite.
Mar 22 13:31:38.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:31:38.732: INFO: namespace: e2e-tests-svcaccounts-2dsfn, resource: bindings, ignored listing per whitelist
Mar 22 13:31:38.916: INFO: namespace e2e-tests-svcaccounts-2dsfn deletion completed in 8.226837916s

â€¢ [SLOW TEST:19.585 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:31:38.916: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-72z9l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-72z9l/secret-test-d285a114-4ca6-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:31:39.207: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2887718-4ca6-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-secrets-72z9l" to be "success or failure"
Mar 22 13:31:39.213: INFO: Pod "pod-configmaps-d2887718-4ca6-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.557641ms
Mar 22 13:31:41.218: INFO: Pod "pod-configmaps-d2887718-4ca6-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01055797s
Mar 22 13:31:43.227: INFO: Pod "pod-configmaps-d2887718-4ca6-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019350814s
Mar 22 13:31:45.237: INFO: Pod "pod-configmaps-d2887718-4ca6-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029087941s
STEP: Saw pod success
Mar 22 13:31:45.237: INFO: Pod "pod-configmaps-d2887718-4ca6-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:31:45.245: INFO: Trying to get logs from node metalk8s-04 pod pod-configmaps-d2887718-4ca6-11e9-99ae-9e98f636e47c container env-test: <nil>
STEP: delete the pod
Mar 22 13:31:45.299: INFO: Waiting for pod pod-configmaps-d2887718-4ca6-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:31:45.305: INFO: Pod pod-configmaps-d2887718-4ca6-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:31:45.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-72z9l" for this suite.
Mar 22 13:31:51.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:31:51.372: INFO: namespace: e2e-tests-secrets-72z9l, resource: bindings, ignored listing per whitelist
Mar 22 13:31:51.594: INFO: namespace e2e-tests-secrets-72z9l deletion completed in 6.282940891s

â€¢ [SLOW TEST:12.678 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:31:51.594: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5vxpr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Mar 22 13:31:51.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 --namespace=e2e-tests-kubectl-5vxpr run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar 22 13:31:54.117: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar 22 13:31:54.117: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:31:56.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5vxpr" for this suite.
Mar 22 13:32:02.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:32:02.187: INFO: namespace: e2e-tests-kubectl-5vxpr, resource: bindings, ignored listing per whitelist
Mar 22 13:32:02.454: INFO: namespace e2e-tests-kubectl-5vxpr deletion completed in 6.320534776s

â€¢ [SLOW TEST:10.860 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:32:02.455: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-m7gg9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0322 13:32:03.824579      20 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 22 13:32:03.824: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:32:03.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-m7gg9" for this suite.
Mar 22 13:32:09.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:32:09.967: INFO: namespace: e2e-tests-gc-m7gg9, resource: bindings, ignored listing per whitelist
Mar 22 13:32:10.050: INFO: namespace e2e-tests-gc-m7gg9 deletion completed in 6.218628563s

â€¢ [SLOW TEST:7.595 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:32:10.051: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-r4dbt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-7bdf
STEP: Creating a pod to test atomic-volume-subpath
Mar 22 13:32:10.409: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7bdf" in namespace "e2e-tests-subpath-r4dbt" to be "success or failure"
Mar 22 13:32:10.415: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.923215ms
Mar 22 13:32:12.421: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012402038s
Mar 22 13:32:14.427: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 4.018633049s
Mar 22 13:32:16.433: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 6.024408302s
Mar 22 13:32:18.440: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 8.031337186s
Mar 22 13:32:20.466: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 10.057659414s
Mar 22 13:32:22.475: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 12.066143195s
Mar 22 13:32:24.481: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 14.071871027s
Mar 22 13:32:26.489: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 16.080134751s
Mar 22 13:32:28.497: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 18.088021609s
Mar 22 13:32:30.514: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 20.105196902s
Mar 22 13:32:32.522: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Running", Reason="", readiness=false. Elapsed: 22.112946434s
Mar 22 13:32:34.530: INFO: Pod "pod-subpath-test-secret-7bdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.121079738s
STEP: Saw pod success
Mar 22 13:32:34.530: INFO: Pod "pod-subpath-test-secret-7bdf" satisfied condition "success or failure"
Mar 22 13:32:34.537: INFO: Trying to get logs from node metalk8s-01 pod pod-subpath-test-secret-7bdf container test-container-subpath-secret-7bdf: <nil>
STEP: delete the pod
Mar 22 13:32:34.633: INFO: Waiting for pod pod-subpath-test-secret-7bdf to disappear
Mar 22 13:32:34.642: INFO: Pod pod-subpath-test-secret-7bdf no longer exists
STEP: Deleting pod pod-subpath-test-secret-7bdf
Mar 22 13:32:34.642: INFO: Deleting pod "pod-subpath-test-secret-7bdf" in namespace "e2e-tests-subpath-r4dbt"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:32:34.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-r4dbt" for this suite.
Mar 22 13:32:40.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:32:40.800: INFO: namespace: e2e-tests-subpath-r4dbt, resource: bindings, ignored listing per whitelist
Mar 22 13:32:40.927: INFO: namespace e2e-tests-subpath-r4dbt deletion completed in 6.260712609s

â€¢ [SLOW TEST:30.877 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:32:40.927: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-nl5zk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 22 13:32:41.175: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-a,UID:f77bd9c9-4ca6-11e9-b0fd-fa163e8e51f5,ResourceVersion:17052,Generation:0,CreationTimestamp:2019-03-22 13:32:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 22 13:32:41.175: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-a,UID:f77bd9c9-4ca6-11e9-b0fd-fa163e8e51f5,ResourceVersion:17052,Generation:0,CreationTimestamp:2019-03-22 13:32:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 22 13:32:51.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-a,UID:f77bd9c9-4ca6-11e9-b0fd-fa163e8e51f5,ResourceVersion:17072,Generation:0,CreationTimestamp:2019-03-22 13:32:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 22 13:32:51.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-a,UID:f77bd9c9-4ca6-11e9-b0fd-fa163e8e51f5,ResourceVersion:17072,Generation:0,CreationTimestamp:2019-03-22 13:32:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 22 13:33:01.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-a,UID:f77bd9c9-4ca6-11e9-b0fd-fa163e8e51f5,ResourceVersion:17093,Generation:0,CreationTimestamp:2019-03-22 13:32:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 22 13:33:01.217: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-a,UID:f77bd9c9-4ca6-11e9-b0fd-fa163e8e51f5,ResourceVersion:17093,Generation:0,CreationTimestamp:2019-03-22 13:32:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 22 13:33:11.236: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-a,UID:f77bd9c9-4ca6-11e9-b0fd-fa163e8e51f5,ResourceVersion:17113,Generation:0,CreationTimestamp:2019-03-22 13:32:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 22 13:33:11.236: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-a,UID:f77bd9c9-4ca6-11e9-b0fd-fa163e8e51f5,ResourceVersion:17113,Generation:0,CreationTimestamp:2019-03-22 13:32:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 22 13:33:21.281: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-b,UID:0f62ccae-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17133,Generation:0,CreationTimestamp:2019-03-22 13:33:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 22 13:33:21.281: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-b,UID:0f62ccae-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17133,Generation:0,CreationTimestamp:2019-03-22 13:33:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 22 13:33:31.303: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-b,UID:0f62ccae-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17154,Generation:0,CreationTimestamp:2019-03-22 13:33:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 22 13:33:31.303: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-nl5zk,SelfLink:/api/v1/namespaces/e2e-tests-watch-nl5zk/configmaps/e2e-watch-test-configmap-b,UID:0f62ccae-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17154,Generation:0,CreationTimestamp:2019-03-22 13:33:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:33:41.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-nl5zk" for this suite.
Mar 22 13:33:47.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:33:47.526: INFO: namespace: e2e-tests-watch-nl5zk, resource: bindings, ignored listing per whitelist
Mar 22 13:33:47.534: INFO: namespace e2e-tests-watch-nl5zk deletion completed in 6.214511948s

â€¢ [SLOW TEST:66.606 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:33:47.534: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-vxx9r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:33:47.748: INFO: Creating deployment "test-recreate-deployment"
Mar 22 13:33:47.755: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 22 13:33:47.763: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar 22 13:33:49.772: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 22 13:33:49.777: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 22 13:33:49.788: INFO: Updating deployment test-recreate-deployment
Mar 22 13:33:49.788: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 22 13:33:50.087: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-vxx9r,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vxx9r/deployments/test-recreate-deployment,UID:1f2b2ed3-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17246,Generation:2,CreationTimestamp:2019-03-22 13:33:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-03-22 13:33:49 +0000 UTC 2019-03-22 13:33:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-22 13:33:50 +0000 UTC 2019-03-22 13:33:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar 22 13:33:50.094: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-vxx9r,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vxx9r/replicasets/test-recreate-deployment-7cf749666b,UID:206cc3bf-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17245,Generation:1,CreationTimestamp:2019-03-22 13:33:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1f2b2ed3-4ca7-11e9-b0fd-fa163e8e51f5 0xc42314e497 0xc42314e498}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 22 13:33:50.094: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 22 13:33:50.094: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-vxx9r,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vxx9r/replicasets/test-recreate-deployment-79f694ff59,UID:1f2d5aaf-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17234,Generation:2,CreationTimestamp:2019-03-22 13:33:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1f2b2ed3-4ca7-11e9-b0fd-fa163e8e51f5 0xc42314e357 0xc42314e358}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 22 13:33:50.101: INFO: Pod "test-recreate-deployment-7cf749666b-slzlg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-slzlg,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-vxx9r,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vxx9r/pods/test-recreate-deployment-7cf749666b-slzlg,UID:206da23e-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17241,Generation:0,CreationTimestamp:2019-03-22 13:33:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b 206cc3bf-4ca7-11e9-b0fd-fa163e8e51f5 0xc42314fb67 0xc42314fb68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-z7mcj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7mcj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7mcj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421fda060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421fda0c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:33:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:33:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:33:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:33:49 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.77,PodIP:,StartTime:2019-03-22 13:33:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:33:50.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-vxx9r" for this suite.
Mar 22 13:33:58.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:33:58.143: INFO: namespace: e2e-tests-deployment-vxx9r, resource: bindings, ignored listing per whitelist
Mar 22 13:33:58.327: INFO: namespace e2e-tests-deployment-vxx9r deletion completed in 8.215104191s

â€¢ [SLOW TEST:10.794 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:33:58.328: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-gp6h9
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-2598d6b0-4ca7-11e9-99ae-9e98f636e47c
STEP: Creating secret with name s-test-opt-upd-2598d713-4ca7-11e9-99ae-9e98f636e47c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2598d6b0-4ca7-11e9-99ae-9e98f636e47c
STEP: Updating secret s-test-opt-upd-2598d713-4ca7-11e9-99ae-9e98f636e47c
STEP: Creating secret with name s-test-opt-create-2598d72d-4ca7-11e9-99ae-9e98f636e47c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:35:23.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-gp6h9" for this suite.
Mar 22 13:35:47.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:35:47.837: INFO: namespace: e2e-tests-secrets-gp6h9, resource: bindings, ignored listing per whitelist
Mar 22 13:35:47.937: INFO: namespace e2e-tests-secrets-gp6h9 deletion completed in 24.2103906s

â€¢ [SLOW TEST:109.610 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:35:47.938: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mwzbj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-66eca28e-4ca7-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:35:48.178: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-66ee1867-4ca7-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-mwzbj" to be "success or failure"
Mar 22 13:35:48.186: INFO: Pod "pod-projected-secrets-66ee1867-4ca7-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.370973ms
Mar 22 13:35:50.193: INFO: Pod "pod-projected-secrets-66ee1867-4ca7-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014316198s
Mar 22 13:35:52.201: INFO: Pod "pod-projected-secrets-66ee1867-4ca7-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022510578s
STEP: Saw pod success
Mar 22 13:35:52.201: INFO: Pod "pod-projected-secrets-66ee1867-4ca7-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:35:52.216: INFO: Trying to get logs from node metalk8s-01 pod pod-projected-secrets-66ee1867-4ca7-11e9-99ae-9e98f636e47c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 22 13:35:52.260: INFO: Waiting for pod pod-projected-secrets-66ee1867-4ca7-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:35:52.275: INFO: Pod pod-projected-secrets-66ee1867-4ca7-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:35:52.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mwzbj" for this suite.
Mar 22 13:35:58.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:35:58.318: INFO: namespace: e2e-tests-projected-mwzbj, resource: bindings, ignored listing per whitelist
Mar 22 13:35:58.505: INFO: namespace e2e-tests-projected-mwzbj deletion completed in 6.219067701s

â€¢ [SLOW TEST:10.567 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:35:58.505: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-255z7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-6d3ce98a-4ca7-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:35:58.775: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6d3f33c0-4ca7-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-255z7" to be "success or failure"
Mar 22 13:35:58.781: INFO: Pod "pod-projected-secrets-6d3f33c0-4ca7-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.99655ms
Mar 22 13:36:00.788: INFO: Pod "pod-projected-secrets-6d3f33c0-4ca7-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012848023s
STEP: Saw pod success
Mar 22 13:36:00.788: INFO: Pod "pod-projected-secrets-6d3f33c0-4ca7-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:36:00.797: INFO: Trying to get logs from node metalk8s-04 pod pod-projected-secrets-6d3f33c0-4ca7-11e9-99ae-9e98f636e47c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 22 13:36:00.830: INFO: Waiting for pod pod-projected-secrets-6d3f33c0-4ca7-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:36:00.835: INFO: Pod pod-projected-secrets-6d3f33c0-4ca7-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:36:00.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-255z7" for this suite.
Mar 22 13:36:07.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:36:07.117: INFO: namespace: e2e-tests-projected-255z7, resource: bindings, ignored listing per whitelist
Mar 22 13:36:07.262: INFO: namespace e2e-tests-projected-255z7 deletion completed in 6.408388008s

â€¢ [SLOW TEST:8.757 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:36:07.262: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-x8xn8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-7270b20d-4ca7-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:36:07.501: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7271ca9c-4ca7-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-x8xn8" to be "success or failure"
Mar 22 13:36:07.510: INFO: Pod "pod-projected-secrets-7271ca9c-4ca7-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.819335ms
Mar 22 13:36:09.524: INFO: Pod "pod-projected-secrets-7271ca9c-4ca7-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022894729s
STEP: Saw pod success
Mar 22 13:36:09.524: INFO: Pod "pod-projected-secrets-7271ca9c-4ca7-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:36:09.529: INFO: Trying to get logs from node metalk8s-05 pod pod-projected-secrets-7271ca9c-4ca7-11e9-99ae-9e98f636e47c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 22 13:36:09.577: INFO: Waiting for pod pod-projected-secrets-7271ca9c-4ca7-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:36:09.581: INFO: Pod pod-projected-secrets-7271ca9c-4ca7-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:36:09.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x8xn8" for this suite.
Mar 22 13:36:15.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:36:15.681: INFO: namespace: e2e-tests-projected-x8xn8, resource: bindings, ignored listing per whitelist
Mar 22 13:36:15.849: INFO: namespace e2e-tests-projected-x8xn8 deletion completed in 6.259954165s

â€¢ [SLOW TEST:8.586 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:36:15.849: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-j49kw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:36:16.111: INFO: Creating deployment "nginx-deployment"
Mar 22 13:36:16.117: INFO: Waiting for observed generation 1
Mar 22 13:36:18.158: INFO: Waiting for all required pods to come up
Mar 22 13:36:18.169: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 22 13:36:24.189: INFO: Waiting for deployment "nginx-deployment" to complete
Mar 22 13:36:24.200: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar 22 13:36:24.215: INFO: Updating deployment nginx-deployment
Mar 22 13:36:24.215: INFO: Waiting for observed generation 2
Mar 22 13:36:26.228: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 22 13:36:26.238: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 22 13:36:26.251: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar 22 13:36:26.267: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 22 13:36:26.268: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 22 13:36:26.273: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar 22 13:36:26.293: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar 22 13:36:26.293: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar 22 13:36:26.304: INFO: Updating deployment nginx-deployment
Mar 22 13:36:26.304: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar 22 13:36:26.314: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 22 13:36:28.390: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 22 13:36:28.441: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-j49kw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j49kw/deployments/nginx-deployment,UID:7799ab33-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18066,Generation:3,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-03-22 13:36:26 +0000 UTC 2019-03-22 13:36:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-22 13:36:26 +0000 UTC 2019-03-22 13:36:16 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar 22 13:36:28.465: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-j49kw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j49kw/replicasets/nginx-deployment-7dc8f79789,UID:7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18051,Generation:3,CreationTimestamp:2019-03-22 13:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 7799ab33-4ca7-11e9-b0fd-fa163e8e51f5 0xc422c094a7 0xc422c094a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 22 13:36:28.465: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar 22 13:36:28.465: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-j49kw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j49kw/replicasets/nginx-deployment-7f9675fb8b,UID:779bd99e-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18047,Generation:3,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 7799ab33-4ca7-11e9-b0fd-fa163e8e51f5 0xc422c09567 0xc422c09568}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar 22 13:36:28.503: INFO: Pod "nginx-deployment-7dc8f79789-2jq95" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-2jq95,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-2jq95,UID:7c6f7a76-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18110,Generation:0,CreationTimestamp:2019-03-22 13:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422be9427 0xc422be9428}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422be94a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422be94c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.75,PodIP:,StartTime:2019-03-22 13:36:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.503: INFO: Pod "nginx-deployment-7dc8f79789-58xfp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-58xfp,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-58xfp,UID:7dbc2a54-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18048,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422be9590 0xc422be9591}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422be9610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422be9630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.71,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.503: INFO: Pod "nginx-deployment-7dc8f79789-5wslw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-5wslw,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-5wslw,UID:7c7136fd-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18117,Generation:0,CreationTimestamp:2019-03-22 13:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422be96f0 0xc422be96f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422be9770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422be9790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.77,PodIP:,StartTime:2019-03-22 13:36:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.503: INFO: Pod "nginx-deployment-7dc8f79789-7j2cs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-7j2cs,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-7j2cs,UID:7c7caaf8-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18119,Generation:0,CreationTimestamp:2019-03-22 13:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422be9860 0xc422be9861}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-04,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422be98e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422be9900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.61,PodIP:,StartTime:2019-03-22 13:36:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.504: INFO: Pod "nginx-deployment-7dc8f79789-7ld5t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-7ld5t,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-7ld5t,UID:7dba0beb-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18052,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422be99d0 0xc422be99d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422be9a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422be9a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.49,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.504: INFO: Pod "nginx-deployment-7dc8f79789-cc842" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-cc842,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-cc842,UID:7c713214-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18192,Generation:0,CreationTimestamp:2019-03-22 13:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422be9b30 0xc422be9b31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422be9bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422be9bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.49,PodIP:,StartTime:2019-03-22 13:36:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.504: INFO: Pod "nginx-deployment-7dc8f79789-gjbxz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-gjbxz,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-gjbxz,UID:7dbc65a4-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18069,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422be9ca0 0xc422be9ca1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422be9d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422be9d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.77,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.504: INFO: Pod "nginx-deployment-7dc8f79789-hc8dd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-hc8dd,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-hc8dd,UID:7dba06e4-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18060,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422be9e00 0xc422be9e01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422be9e80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422be9ea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.77,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.504: INFO: Pod "nginx-deployment-7dc8f79789-jv9xr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-jv9xr,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-jv9xr,UID:7dbc0097-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18055,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422be9f60 0xc422be9f61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-04,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422be9fe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fce010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.61,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.504: INFO: Pod "nginx-deployment-7dc8f79789-kmqgz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-kmqgz,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-kmqgz,UID:7dbc50dc-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18072,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fce0d0 0xc422fce0d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-04,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fce160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fce180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.61,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.505: INFO: Pod "nginx-deployment-7dc8f79789-n9tqm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-n9tqm,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-n9tqm,UID:7dbf1dda-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18100,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fce250 0xc422fce251}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fce2d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fce2f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.49,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.505: INFO: Pod "nginx-deployment-7dc8f79789-qkwbh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-qkwbh,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-qkwbh,UID:7c807470-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18112,Generation:0,CreationTimestamp:2019-03-22 13:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fce3b0 0xc422fce3b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fce430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fce450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:24 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.71,PodIP:,StartTime:2019-03-22 13:36:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.505: INFO: Pod "nginx-deployment-7dc8f79789-sfffj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-sfffj,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7dc8f79789-sfffj,UID:7db4cb69-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18005,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7c6e3686-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fce520 0xc422fce521}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fce5a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fce5c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.75,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.505: INFO: Pod "nginx-deployment-7f9675fb8b-4l8mk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-4l8mk,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-4l8mk,UID:7dba1511-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18046,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fce680 0xc422fce681}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fce6f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fce710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.77,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.505: INFO: Pod "nginx-deployment-7f9675fb8b-4qvv8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-4qvv8,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-4qvv8,UID:77a652ec-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17872,Generation:0,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fce7c0 0xc422fce7c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fce830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fce850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.49,PodIP:10.233.88.28,StartTime:2019-03-22 13:36:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-22 13:36:18 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://04d1b59ae342cfffaf7d3cf728cc0d7c3168510102fdf9eb64f97507829763aa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.505: INFO: Pod "nginx-deployment-7f9675fb8b-527zf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-527zf,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-527zf,UID:7dbc60b0-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18063,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fce910 0xc422fce911}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-04,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fce980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fce9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.61,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.505: INFO: Pod "nginx-deployment-7f9675fb8b-5b72n" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-5b72n,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-5b72n,UID:7db9fca8-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18235,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcea50 0xc422fcea51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fceac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fceae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.71,PodIP:10.233.122.142,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-22 13:36:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://e4c067f7a3806c270f4a6b3faa347b4686da243ed3621843bc18ebafe469d374}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.506: INFO: Pod "nginx-deployment-7f9675fb8b-5ss5m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-5ss5m,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-5ss5m,UID:7dbf88b8-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18083,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fceba0 0xc422fceba1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-04,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcec10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcec30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.61,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.506: INFO: Pod "nginx-deployment-7f9675fb8b-6vw6v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-6vw6v,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-6vw6v,UID:7dbc39c3-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18062,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcece0 0xc422fcece1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fced50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fced70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.49,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.506: INFO: Pod "nginx-deployment-7f9675fb8b-bblsw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-bblsw,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-bblsw,UID:77a0f24e-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17846,Generation:0,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcee20 0xc422fcee21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcee90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fceeb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.75,PodIP:10.233.99.153,StartTime:2019-03-22 13:36:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-22 13:36:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://7759297f5ad1fbf2867e78ab6e7c44f29bf22283fdc98a83afcf0ba7c8e65b2b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.506: INFO: Pod "nginx-deployment-7f9675fb8b-dk4q7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-dk4q7,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-dk4q7,UID:7dbf2928-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18080,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcef70 0xc422fcef71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcefe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcf000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.77,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.506: INFO: Pod "nginx-deployment-7f9675fb8b-fnknb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-fnknb,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-fnknb,UID:77aa58d1-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17857,Generation:0,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcf0b0 0xc422fcf0b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-04,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcf120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcf140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.61,PodIP:10.233.122.92,StartTime:2019-03-22 13:36:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-22 13:36:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://6f8f1e9327711e6f768bb3415d826163c2106a2e6a72f2e700568dbfed0df859}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.506: INFO: Pod "nginx-deployment-7f9675fb8b-g254t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-g254t,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-g254t,UID:7dbf1c60-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18073,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcf200 0xc422fcf201}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcf270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcf290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.49,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.506: INFO: Pod "nginx-deployment-7f9675fb8b-g7864" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-g7864,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-g7864,UID:77a6382d-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17868,Generation:0,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcf340 0xc422fcf341}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcf3b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcf3d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.77,PodIP:10.233.82.96,StartTime:2019-03-22 13:36:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-22 13:36:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://7f7f42bddd0efc68df796a999b79c8fc8b03c0fa7643fede7bd503f2ed9d3b9c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.508: INFO: Pod "nginx-deployment-7f9675fb8b-hksd4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-hksd4,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-hksd4,UID:7dbf8707-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18059,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcf490 0xc422fcf491}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcf500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcf520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.75,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.508: INFO: Pod "nginx-deployment-7f9675fb8b-hpkjr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-hpkjr,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-hpkjr,UID:77a63918-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17865,Generation:0,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcf5d0 0xc422fcf5d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcf640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcf660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.77,PodIP:10.233.82.97,StartTime:2019-03-22 13:36:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-22 13:36:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://83943182b10ada79e7a4e11b47750049225b63b336504c1d3508c56de710e270}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.508: INFO: Pod "nginx-deployment-7f9675fb8b-jrxnv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-jrxnv,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-jrxnv,UID:7db3fc62-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17999,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcf720 0xc422fcf721}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcf790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcf7b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.71,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.508: INFO: Pod "nginx-deployment-7f9675fb8b-k9xbq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-k9xbq,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-k9xbq,UID:7dbf8529-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18088,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcf860 0xc422fcf861}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcf8d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcf8f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.49,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.508: INFO: Pod "nginx-deployment-7f9675fb8b-mbbp2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-mbbp2,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-mbbp2,UID:7dbc4de9-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18057,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcf9a0 0xc422fcf9a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcfa10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcfa30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.71,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.508: INFO: Pod "nginx-deployment-7f9675fb8b-ppw5w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-ppw5w,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-ppw5w,UID:77a2ab0a-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17855,Generation:0,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcfae0 0xc422fcfae1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-04,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcfb50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcfb70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.61,PodIP:10.233.122.91,StartTime:2019-03-22 13:36:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-22 13:36:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://29d6c3f717b58658e2f1ae5d659b6760ef7341cb336f22b47a7107784ac4a76d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.510: INFO: Pod "nginx-deployment-7f9675fb8b-qrm4n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-qrm4n,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-qrm4n,UID:7dbc3f44-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:18050,Generation:0,CreationTimestamp:2019-03-22 13:36:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcfc30 0xc422fcfc31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcfca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcfcc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:26 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.75,PodIP:,StartTime:2019-03-22 13:36:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.531: INFO: Pod "nginx-deployment-7f9675fb8b-qthcd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-qthcd,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-qthcd,UID:77aa6d58-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17844,Generation:0,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcfd70 0xc422fcfd71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcfde0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcfe00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.75,PodIP:10.233.99.154,StartTime:2019-03-22 13:36:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-22 13:36:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://da7bf099f37294846b928fdde06ea2554b9a64e28d823cb4f268d37f02358176}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 22 13:36:28.532: INFO: Pod "nginx-deployment-7f9675fb8b-z5jlt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-z5jlt,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-j49kw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j49kw/pods/nginx-deployment-7f9675fb8b-z5jlt,UID:77a2d31a-4ca7-11e9-b0fd-fa163e8e51f5,ResourceVersion:17875,Generation:0,CreationTimestamp:2019-03-22 13:36:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 779bd99e-4ca7-11e9-b0fd-fa163e8e51f5 0xc422fcfec0 0xc422fcfec1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4g6lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4g6lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4g6lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422fcff30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422fcff50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:36:16 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.49,PodIP:10.233.88.27,StartTime:2019-03-22 13:36:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-22 13:36:18 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://17ca878cbd4c126293f13519e8eddd397715b200cebe3570fe2d130b5e00290b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:36:28.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-j49kw" for this suite.
Mar 22 13:36:38.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:36:38.720: INFO: namespace: e2e-tests-deployment-j49kw, resource: bindings, ignored listing per whitelist
Mar 22 13:36:38.876: INFO: namespace e2e-tests-deployment-j49kw deletion completed in 10.290775733s

â€¢ [SLOW TEST:23.027 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:36:38.877: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-9852v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-8547918c-4ca7-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:36:39.104: INFO: Waiting up to 5m0s for pod "pod-secrets-85490bbb-4ca7-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-secrets-9852v" to be "success or failure"
Mar 22 13:36:39.121: INFO: Pod "pod-secrets-85490bbb-4ca7-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.342603ms
Mar 22 13:36:41.136: INFO: Pod "pod-secrets-85490bbb-4ca7-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031813231s
STEP: Saw pod success
Mar 22 13:36:41.136: INFO: Pod "pod-secrets-85490bbb-4ca7-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:36:41.143: INFO: Trying to get logs from node metalk8s-05 pod pod-secrets-85490bbb-4ca7-11e9-99ae-9e98f636e47c container secret-volume-test: <nil>
STEP: delete the pod
Mar 22 13:36:41.197: INFO: Waiting for pod pod-secrets-85490bbb-4ca7-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:36:41.204: INFO: Pod pod-secrets-85490bbb-4ca7-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:36:41.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-9852v" for this suite.
Mar 22 13:36:47.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:36:47.258: INFO: namespace: e2e-tests-secrets-9852v, resource: bindings, ignored listing per whitelist
Mar 22 13:36:47.403: INFO: namespace e2e-tests-secrets-9852v deletion completed in 6.190828047s

â€¢ [SLOW TEST:8.527 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:36:47.403: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6f827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-ljbb6
STEP: Creating secret with name secret-test-8a5edc62-4ca7-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:36:47.806: INFO: Waiting up to 5m0s for pod "pod-secrets-8a7893eb-4ca7-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-secrets-6f827" to be "success or failure"
Mar 22 13:36:47.813: INFO: Pod "pod-secrets-8a7893eb-4ca7-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.586434ms
Mar 22 13:36:49.843: INFO: Pod "pod-secrets-8a7893eb-4ca7-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037438746s
STEP: Saw pod success
Mar 22 13:36:49.844: INFO: Pod "pod-secrets-8a7893eb-4ca7-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:36:49.853: INFO: Trying to get logs from node metalk8s-02 pod pod-secrets-8a7893eb-4ca7-11e9-99ae-9e98f636e47c container secret-volume-test: <nil>
STEP: delete the pod
Mar 22 13:36:49.886: INFO: Waiting for pod pod-secrets-8a7893eb-4ca7-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:36:49.909: INFO: Pod pod-secrets-8a7893eb-4ca7-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:36:49.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6f827" for this suite.
Mar 22 13:36:55.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:36:56.150: INFO: namespace: e2e-tests-secrets-6f827, resource: bindings, ignored listing per whitelist
Mar 22 13:36:56.165: INFO: namespace e2e-tests-secrets-6f827 deletion completed in 6.248621798s
STEP: Destroying namespace "e2e-tests-secret-namespace-ljbb6" for this suite.
Mar 22 13:37:02.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:37:02.357: INFO: namespace: e2e-tests-secret-namespace-ljbb6, resource: bindings, ignored listing per whitelist
Mar 22 13:37:02.373: INFO: namespace e2e-tests-secret-namespace-ljbb6 deletion completed in 6.207501396s

â€¢ [SLOW TEST:14.970 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:37:02.373: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gw926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
Mar 22 13:37:02.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-gw926'
Mar 22 13:37:02.850: INFO: stderr: ""
Mar 22 13:37:02.850: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Mar 22 13:37:03.862: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 13:37:03.862: INFO: Found 0 / 1
Mar 22 13:37:04.859: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 13:37:04.859: INFO: Found 0 / 1
Mar 22 13:37:05.860: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 13:37:05.861: INFO: Found 1 / 1
Mar 22 13:37:05.861: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 22 13:37:05.867: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 13:37:05.867: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar 22 13:37:05.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 logs redis-master-tqkgp redis-master --namespace=e2e-tests-kubectl-gw926'
Mar 22 13:37:06.030: INFO: stderr: ""
Mar 22 13:37:06.030: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Mar 13:37:04.440 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Mar 13:37:04.441 # Server started, Redis version 3.2.12\n1:M 22 Mar 13:37:04.441 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Mar 13:37:04.441 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar 22 13:37:06.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 log redis-master-tqkgp redis-master --namespace=e2e-tests-kubectl-gw926 --tail=1'
Mar 22 13:37:06.191: INFO: stderr: ""
Mar 22 13:37:06.192: INFO: stdout: "1:M 22 Mar 13:37:04.441 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar 22 13:37:06.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 log redis-master-tqkgp redis-master --namespace=e2e-tests-kubectl-gw926 --limit-bytes=1'
Mar 22 13:37:06.307: INFO: stderr: ""
Mar 22 13:37:06.307: INFO: stdout: " "
STEP: exposing timestamps
Mar 22 13:37:06.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 log redis-master-tqkgp redis-master --namespace=e2e-tests-kubectl-gw926 --tail=1 --timestamps'
Mar 22 13:37:06.418: INFO: stderr: ""
Mar 22 13:37:06.418: INFO: stdout: "2019-03-22T13:37:04.44225107Z 1:M 22 Mar 13:37:04.441 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar 22 13:37:08.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 log redis-master-tqkgp redis-master --namespace=e2e-tests-kubectl-gw926 --since=1s'
Mar 22 13:37:09.037: INFO: stderr: ""
Mar 22 13:37:09.037: INFO: stdout: ""
Mar 22 13:37:09.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 log redis-master-tqkgp redis-master --namespace=e2e-tests-kubectl-gw926 --since=24h'
Mar 22 13:37:09.164: INFO: stderr: ""
Mar 22 13:37:09.164: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Mar 13:37:04.440 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Mar 13:37:04.441 # Server started, Redis version 3.2.12\n1:M 22 Mar 13:37:04.441 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Mar 13:37:04.441 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
Mar 22 13:37:09.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-gw926'
Mar 22 13:37:09.277: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 13:37:09.277: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar 22 13:37:09.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-gw926'
Mar 22 13:37:09.392: INFO: stderr: "No resources found.\n"
Mar 22 13:37:09.392: INFO: stdout: ""
Mar 22 13:37:09.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -l name=nginx --namespace=e2e-tests-kubectl-gw926 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 22 13:37:09.499: INFO: stderr: ""
Mar 22 13:37:09.499: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:37:09.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gw926" for this suite.
Mar 22 13:37:15.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:37:15.637: INFO: namespace: e2e-tests-kubectl-gw926, resource: bindings, ignored listing per whitelist
Mar 22 13:37:15.700: INFO: namespace e2e-tests-kubectl-gw926 deletion completed in 6.193371432s

â€¢ [SLOW TEST:13.327 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:37:15.700: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-pmkbs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0322 13:37:22.001132      20 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 22 13:37:22.001: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:37:22.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-pmkbs" for this suite.
Mar 22 13:37:30.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:37:30.189: INFO: namespace: e2e-tests-gc-pmkbs, resource: bindings, ignored listing per whitelist
Mar 22 13:37:30.242: INFO: namespace e2e-tests-gc-pmkbs deletion completed in 8.22012132s

â€¢ [SLOW TEST:14.542 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:37:30.242: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rnxvp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 22 13:37:30.465: INFO: Waiting up to 5m0s for pod "downward-api-a3e58b13-4ca7-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-rnxvp" to be "success or failure"
Mar 22 13:37:30.470: INFO: Pod "downward-api-a3e58b13-4ca7-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.81967ms
Mar 22 13:37:32.487: INFO: Pod "downward-api-a3e58b13-4ca7-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022078365s
STEP: Saw pod success
Mar 22 13:37:32.487: INFO: Pod "downward-api-a3e58b13-4ca7-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:37:32.492: INFO: Trying to get logs from node metalk8s-02 pod downward-api-a3e58b13-4ca7-11e9-99ae-9e98f636e47c container dapi-container: <nil>
STEP: delete the pod
Mar 22 13:37:32.672: INFO: Waiting for pod downward-api-a3e58b13-4ca7-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:37:32.768: INFO: Pod downward-api-a3e58b13-4ca7-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:37:32.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rnxvp" for this suite.
Mar 22 13:37:38.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:37:38.931: INFO: namespace: e2e-tests-downward-api-rnxvp, resource: bindings, ignored listing per whitelist
Mar 22 13:37:38.981: INFO: namespace e2e-tests-downward-api-rnxvp deletion completed in 6.203929952s

â€¢ [SLOW TEST:8.739 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:37:38.981: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-msjpw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-a920aae4-4ca7-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 13:37:39.247: INFO: Waiting up to 5m0s for pod "pod-configmaps-a921f20d-4ca7-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-configmap-msjpw" to be "success or failure"
Mar 22 13:37:39.258: INFO: Pod "pod-configmaps-a921f20d-4ca7-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.974361ms
Mar 22 13:37:41.268: INFO: Pod "pod-configmaps-a921f20d-4ca7-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020558153s
STEP: Saw pod success
Mar 22 13:37:41.268: INFO: Pod "pod-configmaps-a921f20d-4ca7-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:37:41.274: INFO: Trying to get logs from node metalk8s-01 pod pod-configmaps-a921f20d-4ca7-11e9-99ae-9e98f636e47c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 13:37:41.376: INFO: Waiting for pod pod-configmaps-a921f20d-4ca7-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:37:41.384: INFO: Pod pod-configmaps-a921f20d-4ca7-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:37:41.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-msjpw" for this suite.
Mar 22 13:37:49.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:37:49.575: INFO: namespace: e2e-tests-configmap-msjpw, resource: bindings, ignored listing per whitelist
Mar 22 13:37:49.883: INFO: namespace e2e-tests-configmap-msjpw deletion completed in 8.481339876s

â€¢ [SLOW TEST:10.902 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:37:49.884: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-mflfr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-mflfr
Mar 22 13:37:56.313: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-mflfr
STEP: checking the pod's current state and verifying that restartCount is present
Mar 22 13:37:56.319: INFO: Initial restart count of pod liveness-http is 0
Mar 22 13:38:14.403: INFO: Restart count of pod e2e-tests-container-probe-mflfr/liveness-http is now 1 (18.084679342s elapsed)
Mar 22 13:38:34.482: INFO: Restart count of pod e2e-tests-container-probe-mflfr/liveness-http is now 2 (38.162959888s elapsed)
Mar 22 13:38:54.559: INFO: Restart count of pod e2e-tests-container-probe-mflfr/liveness-http is now 3 (58.240408674s elapsed)
Mar 22 13:39:14.642: INFO: Restart count of pod e2e-tests-container-probe-mflfr/liveness-http is now 4 (1m18.323395205s elapsed)
Mar 22 13:40:14.931: INFO: Restart count of pod e2e-tests-container-probe-mflfr/liveness-http is now 5 (2m18.611932963s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:40:14.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-mflfr" for this suite.
Mar 22 13:40:20.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:40:21.392: INFO: namespace: e2e-tests-container-probe-mflfr, resource: bindings, ignored listing per whitelist
Mar 22 13:40:21.407: INFO: namespace e2e-tests-container-probe-mflfr deletion completed in 6.450948754s

â€¢ [SLOW TEST:151.524 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:40:21.408: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vf4jn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-09ef69d4-4ca8-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 13:40:21.659: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09f08e0e-4ca8-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-vf4jn" to be "success or failure"
Mar 22 13:40:21.667: INFO: Pod "pod-projected-configmaps-09f08e0e-4ca8-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.15897ms
Mar 22 13:40:23.674: INFO: Pod "pod-projected-configmaps-09f08e0e-4ca8-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015296066s
STEP: Saw pod success
Mar 22 13:40:23.674: INFO: Pod "pod-projected-configmaps-09f08e0e-4ca8-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:40:23.678: INFO: Trying to get logs from node metalk8s-05 pod pod-projected-configmaps-09f08e0e-4ca8-11e9-99ae-9e98f636e47c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 13:40:23.710: INFO: Waiting for pod pod-projected-configmaps-09f08e0e-4ca8-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:40:23.723: INFO: Pod pod-projected-configmaps-09f08e0e-4ca8-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:40:23.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vf4jn" for this suite.
Mar 22 13:40:29.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:40:29.842: INFO: namespace: e2e-tests-projected-vf4jn, resource: bindings, ignored listing per whitelist
Mar 22 13:40:29.947: INFO: namespace e2e-tests-projected-vf4jn deletion completed in 6.207112251s

â€¢ [SLOW TEST:8.539 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:40:29.948: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-b4l7q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 13:40:30.154: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f008a1d-4ca8-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-b4l7q" to be "success or failure"
Mar 22 13:40:30.161: INFO: Pod "downwardapi-volume-0f008a1d-4ca8-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.147743ms
Mar 22 13:40:32.166: INFO: Pod "downwardapi-volume-0f008a1d-4ca8-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012683334s
STEP: Saw pod success
Mar 22 13:40:32.166: INFO: Pod "downwardapi-volume-0f008a1d-4ca8-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:40:32.171: INFO: Trying to get logs from node metalk8s-02 pod downwardapi-volume-0f008a1d-4ca8-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 13:40:32.200: INFO: Waiting for pod downwardapi-volume-0f008a1d-4ca8-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:40:32.206: INFO: Pod downwardapi-volume-0f008a1d-4ca8-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:40:32.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-b4l7q" for this suite.
Mar 22 13:40:38.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:40:38.408: INFO: namespace: e2e-tests-downward-api-b4l7q, resource: bindings, ignored listing per whitelist
Mar 22 13:40:38.429: INFO: namespace e2e-tests-downward-api-b4l7q deletion completed in 6.214162185s

â€¢ [SLOW TEST:8.481 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:40:38.430: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-vxt25
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-4xg6
STEP: Creating a pod to test atomic-volume-subpath
Mar 22 13:40:38.727: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4xg6" in namespace "e2e-tests-subpath-vxt25" to be "success or failure"
Mar 22 13:40:38.736: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.598132ms
Mar 22 13:40:40.751: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024304394s
Mar 22 13:40:42.757: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 4.030495051s
Mar 22 13:40:44.763: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 6.036522893s
Mar 22 13:40:46.776: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 8.049210036s
Mar 22 13:40:48.782: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 10.055088492s
Mar 22 13:40:50.792: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 12.064664575s
Mar 22 13:40:52.798: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 14.070806608s
Mar 22 13:40:54.804: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 16.076735793s
Mar 22 13:40:56.817: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 18.089734175s
Mar 22 13:40:58.823: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 20.096307582s
Mar 22 13:41:00.828: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Running", Reason="", readiness=false. Elapsed: 22.101606914s
Mar 22 13:41:02.835: INFO: Pod "pod-subpath-test-configmap-4xg6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.108556508s
STEP: Saw pod success
Mar 22 13:41:02.835: INFO: Pod "pod-subpath-test-configmap-4xg6" satisfied condition "success or failure"
Mar 22 13:41:02.841: INFO: Trying to get logs from node metalk8s-01 pod pod-subpath-test-configmap-4xg6 container test-container-subpath-configmap-4xg6: <nil>
STEP: delete the pod
Mar 22 13:41:02.891: INFO: Waiting for pod pod-subpath-test-configmap-4xg6 to disappear
Mar 22 13:41:02.917: INFO: Pod pod-subpath-test-configmap-4xg6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4xg6
Mar 22 13:41:02.917: INFO: Deleting pod "pod-subpath-test-configmap-4xg6" in namespace "e2e-tests-subpath-vxt25"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:41:02.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-vxt25" for this suite.
Mar 22 13:41:08.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:41:09.061: INFO: namespace: e2e-tests-subpath-vxt25, resource: bindings, ignored listing per whitelist
Mar 22 13:41:09.117: INFO: namespace e2e-tests-subpath-vxt25 deletion completed in 6.179847045s

â€¢ [SLOW TEST:30.688 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:41:09.117: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pnxb2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-265cc609-4ca8-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 13:41:09.350: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-265dbe83-4ca8-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-pnxb2" to be "success or failure"
Mar 22 13:41:09.360: INFO: Pod "pod-projected-secrets-265dbe83-4ca8-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.209825ms
Mar 22 13:41:11.366: INFO: Pod "pod-projected-secrets-265dbe83-4ca8-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015446726s
Mar 22 13:41:13.371: INFO: Pod "pod-projected-secrets-265dbe83-4ca8-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021128859s
STEP: Saw pod success
Mar 22 13:41:13.371: INFO: Pod "pod-projected-secrets-265dbe83-4ca8-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:41:13.377: INFO: Trying to get logs from node metalk8s-05 pod pod-projected-secrets-265dbe83-4ca8-11e9-99ae-9e98f636e47c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 22 13:41:13.408: INFO: Waiting for pod pod-projected-secrets-265dbe83-4ca8-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:41:13.412: INFO: Pod pod-projected-secrets-265dbe83-4ca8-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:41:13.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pnxb2" for this suite.
Mar 22 13:41:19.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:41:19.447: INFO: namespace: e2e-tests-projected-pnxb2, resource: bindings, ignored listing per whitelist
Mar 22 13:41:19.632: INFO: namespace e2e-tests-projected-pnxb2 deletion completed in 6.213922321s

â€¢ [SLOW TEST:10.515 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:41:19.633: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-nqk9h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Mar 22 13:41:20.387: INFO: created pod pod-service-account-defaultsa
Mar 22 13:41:20.387: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 22 13:41:20.396: INFO: created pod pod-service-account-mountsa
Mar 22 13:41:20.397: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 22 13:41:20.403: INFO: created pod pod-service-account-nomountsa
Mar 22 13:41:20.403: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 22 13:41:20.410: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 22 13:41:20.410: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 22 13:41:20.535: INFO: created pod pod-service-account-mountsa-mountspec
Mar 22 13:41:20.535: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 22 13:41:20.546: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 22 13:41:20.546: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 22 13:41:20.555: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 22 13:41:20.555: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 22 13:41:20.575: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 22 13:41:20.575: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 22 13:41:20.582: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 22 13:41:20.582: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:41:20.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-nqk9h" for this suite.
Mar 22 13:41:28.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:41:28.961: INFO: namespace: e2e-tests-svcaccounts-nqk9h, resource: bindings, ignored listing per whitelist
Mar 22 13:41:29.031: INFO: namespace e2e-tests-svcaccounts-nqk9h deletion completed in 8.427272981s

â€¢ [SLOW TEST:9.399 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:41:29.032: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-xb455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 22 13:41:29.360: INFO: Number of nodes with available pods: 0
Mar 22 13:41:29.360: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:41:30.386: INFO: Number of nodes with available pods: 0
Mar 22 13:41:30.386: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:41:31.388: INFO: Number of nodes with available pods: 4
Mar 22 13:41:31.388: INFO: Node metalk8s-05 is running more than one daemon pod
Mar 22 13:41:32.374: INFO: Number of nodes with available pods: 5
Mar 22 13:41:32.374: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 22 13:41:32.408: INFO: Number of nodes with available pods: 4
Mar 22 13:41:32.408: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:33.423: INFO: Number of nodes with available pods: 4
Mar 22 13:41:33.423: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:34.422: INFO: Number of nodes with available pods: 4
Mar 22 13:41:34.422: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:35.423: INFO: Number of nodes with available pods: 4
Mar 22 13:41:35.423: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:36.428: INFO: Number of nodes with available pods: 4
Mar 22 13:41:36.428: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:37.435: INFO: Number of nodes with available pods: 4
Mar 22 13:41:37.436: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:38.430: INFO: Number of nodes with available pods: 4
Mar 22 13:41:38.430: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:39.429: INFO: Number of nodes with available pods: 4
Mar 22 13:41:39.429: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:40.423: INFO: Number of nodes with available pods: 4
Mar 22 13:41:40.423: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:41.422: INFO: Number of nodes with available pods: 4
Mar 22 13:41:41.422: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:42.422: INFO: Number of nodes with available pods: 4
Mar 22 13:41:42.422: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:43.423: INFO: Number of nodes with available pods: 4
Mar 22 13:41:43.423: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:44.425: INFO: Number of nodes with available pods: 4
Mar 22 13:41:44.425: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:45.424: INFO: Number of nodes with available pods: 4
Mar 22 13:41:45.424: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:46.424: INFO: Number of nodes with available pods: 4
Mar 22 13:41:46.424: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:47.423: INFO: Number of nodes with available pods: 4
Mar 22 13:41:47.423: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:48.423: INFO: Number of nodes with available pods: 4
Mar 22 13:41:48.423: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:49.431: INFO: Number of nodes with available pods: 4
Mar 22 13:41:49.431: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:50.425: INFO: Number of nodes with available pods: 4
Mar 22 13:41:50.425: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:51.424: INFO: Number of nodes with available pods: 4
Mar 22 13:41:51.424: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:52.424: INFO: Number of nodes with available pods: 4
Mar 22 13:41:52.424: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:53.425: INFO: Number of nodes with available pods: 4
Mar 22 13:41:53.425: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:54.425: INFO: Number of nodes with available pods: 4
Mar 22 13:41:54.425: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:55.425: INFO: Number of nodes with available pods: 4
Mar 22 13:41:55.425: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:56.427: INFO: Number of nodes with available pods: 4
Mar 22 13:41:56.427: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:57.423: INFO: Number of nodes with available pods: 4
Mar 22 13:41:57.423: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:58.422: INFO: Number of nodes with available pods: 4
Mar 22 13:41:58.422: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:41:59.496: INFO: Number of nodes with available pods: 4
Mar 22 13:41:59.496: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:00.421: INFO: Number of nodes with available pods: 4
Mar 22 13:42:00.421: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:01.425: INFO: Number of nodes with available pods: 4
Mar 22 13:42:01.425: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:02.437: INFO: Number of nodes with available pods: 4
Mar 22 13:42:02.437: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:03.665: INFO: Number of nodes with available pods: 4
Mar 22 13:42:03.665: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:04.424: INFO: Number of nodes with available pods: 4
Mar 22 13:42:04.424: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:05.422: INFO: Number of nodes with available pods: 4
Mar 22 13:42:05.422: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:06.427: INFO: Number of nodes with available pods: 4
Mar 22 13:42:06.427: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:07.422: INFO: Number of nodes with available pods: 4
Mar 22 13:42:07.422: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:08.421: INFO: Number of nodes with available pods: 4
Mar 22 13:42:08.421: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:09.421: INFO: Number of nodes with available pods: 4
Mar 22 13:42:09.421: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:10.431: INFO: Number of nodes with available pods: 4
Mar 22 13:42:10.431: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:11.433: INFO: Number of nodes with available pods: 4
Mar 22 13:42:11.433: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:12.424: INFO: Number of nodes with available pods: 4
Mar 22 13:42:12.424: INFO: Node metalk8s-03 is running more than one daemon pod
Mar 22 13:42:13.441: INFO: Number of nodes with available pods: 5
Mar 22 13:42:13.441: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-xb455, will wait for the garbage collector to delete the pods
Mar 22 13:42:13.535: INFO: Deleting {extensions DaemonSet} daemon-set took: 22.340501ms
Mar 22 13:42:13.635: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.244476ms
Mar 22 13:42:51.949: INFO: Number of nodes with available pods: 0
Mar 22 13:42:51.949: INFO: Number of running nodes: 0, number of available pods: 0
Mar 22 13:42:51.953: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-xb455/daemonsets","resourceVersion":"20440"},"items":null}

Mar 22 13:42:51.957: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-xb455/pods","resourceVersion":"20440"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:42:51.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-xb455" for this suite.
Mar 22 13:42:58.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:42:58.119: INFO: namespace: e2e-tests-daemonsets-xb455, resource: bindings, ignored listing per whitelist
Mar 22 13:42:58.401: INFO: namespace e2e-tests-daemonsets-xb455 deletion completed in 6.401596325s

â€¢ [SLOW TEST:89.369 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:42:58.401: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-r296g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:42:58.654: INFO: (0) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 9.288419ms)
Mar 22 13:42:58.661: INFO: (1) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.215641ms)
Mar 22 13:42:58.667: INFO: (2) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.459734ms)
Mar 22 13:42:58.674: INFO: (3) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.585716ms)
Mar 22 13:42:58.680: INFO: (4) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.003888ms)
Mar 22 13:42:58.686: INFO: (5) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.362024ms)
Mar 22 13:42:58.693: INFO: (6) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.738957ms)
Mar 22 13:42:58.699: INFO: (7) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.175823ms)
Mar 22 13:42:58.707: INFO: (8) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.121395ms)
Mar 22 13:42:58.714: INFO: (9) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.84912ms)
Mar 22 13:42:58.721: INFO: (10) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.828164ms)
Mar 22 13:42:58.729: INFO: (11) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.332038ms)
Mar 22 13:42:58.740: INFO: (12) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 10.947678ms)
Mar 22 13:42:58.748: INFO: (13) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 8.095358ms)
Mar 22 13:42:58.767: INFO: (14) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 19.011993ms)
Mar 22 13:42:58.774: INFO: (15) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.98065ms)
Mar 22 13:42:58.796: INFO: (16) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 21.995891ms)
Mar 22 13:42:58.803: INFO: (17) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.533753ms)
Mar 22 13:42:58.809: INFO: (18) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.688185ms)
Mar 22 13:42:58.815: INFO: (19) /api/v1/nodes/metalk8s-01:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.061734ms)
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:42:58.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-r296g" for this suite.
Mar 22 13:43:04.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:43:04.985: INFO: namespace: e2e-tests-proxy-r296g, resource: bindings, ignored listing per whitelist
Mar 22 13:43:05.078: INFO: namespace e2e-tests-proxy-r296g deletion completed in 6.253526589s

â€¢ [SLOW TEST:6.677 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:43:05.080: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-hfcph
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-6b86c3d2-4ca8-11e9-99ae-9e98f636e47c
STEP: Creating configMap with name cm-test-opt-upd-6b86c421-4ca8-11e9-99ae-9e98f636e47c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6b86c3d2-4ca8-11e9-99ae-9e98f636e47c
STEP: Updating configmap cm-test-opt-upd-6b86c421-4ca8-11e9-99ae-9e98f636e47c
STEP: Creating configMap with name cm-test-opt-create-6b86c43c-4ca8-11e9-99ae-9e98f636e47c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:43:09.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hfcph" for this suite.
Mar 22 13:43:31.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:43:31.774: INFO: namespace: e2e-tests-configmap-hfcph, resource: bindings, ignored listing per whitelist
Mar 22 13:43:31.787: INFO: namespace e2e-tests-configmap-hfcph deletion completed in 22.220024227s

â€¢ [SLOW TEST:26.708 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:43:31.787: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-d8bmp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Mar 22 13:43:32.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:32.383: INFO: stderr: ""
Mar 22 13:43:32.383: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 22 13:43:32.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:32.499: INFO: stderr: ""
Mar 22 13:43:32.499: INFO: stdout: "update-demo-nautilus-6rlb6 update-demo-nautilus-7bb2k "
Mar 22 13:43:32.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-6rlb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:32.608: INFO: stderr: ""
Mar 22 13:43:32.608: INFO: stdout: ""
Mar 22 13:43:32.608: INFO: update-demo-nautilus-6rlb6 is created but not running
Mar 22 13:43:37.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:37.739: INFO: stderr: ""
Mar 22 13:43:37.739: INFO: stdout: "update-demo-nautilus-6rlb6 update-demo-nautilus-7bb2k "
Mar 22 13:43:37.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-6rlb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:37.868: INFO: stderr: ""
Mar 22 13:43:37.868: INFO: stdout: "true"
Mar 22 13:43:37.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-6rlb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:37.989: INFO: stderr: ""
Mar 22 13:43:37.989: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:43:37.989: INFO: validating pod update-demo-nautilus-6rlb6
Mar 22 13:43:37.995: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:43:37.995: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:43:37.995: INFO: update-demo-nautilus-6rlb6 is verified up and running
Mar 22 13:43:37.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-7bb2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:38.092: INFO: stderr: ""
Mar 22 13:43:38.092: INFO: stdout: "true"
Mar 22 13:43:38.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-7bb2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:38.208: INFO: stderr: ""
Mar 22 13:43:38.208: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:43:38.208: INFO: validating pod update-demo-nautilus-7bb2k
Mar 22 13:43:38.215: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:43:38.215: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:43:38.215: INFO: update-demo-nautilus-7bb2k is verified up and running
STEP: scaling down the replication controller
Mar 22 13:43:38.217: INFO: scanned /root for discovery docs: <nil>
Mar 22 13:43:38.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:39.366: INFO: stderr: ""
Mar 22 13:43:39.366: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 22 13:43:39.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:39.464: INFO: stderr: ""
Mar 22 13:43:39.464: INFO: stdout: "update-demo-nautilus-6rlb6 update-demo-nautilus-7bb2k "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 22 13:43:44.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:44.579: INFO: stderr: ""
Mar 22 13:43:44.579: INFO: stdout: "update-demo-nautilus-6rlb6 update-demo-nautilus-7bb2k "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 22 13:43:49.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:49.710: INFO: stderr: ""
Mar 22 13:43:49.710: INFO: stdout: "update-demo-nautilus-6rlb6 update-demo-nautilus-7bb2k "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 22 13:43:54.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:54.829: INFO: stderr: ""
Mar 22 13:43:54.829: INFO: stdout: "update-demo-nautilus-7bb2k "
Mar 22 13:43:54.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-7bb2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:54.945: INFO: stderr: ""
Mar 22 13:43:54.945: INFO: stdout: "true"
Mar 22 13:43:54.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-7bb2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:55.062: INFO: stderr: ""
Mar 22 13:43:55.062: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:43:55.062: INFO: validating pod update-demo-nautilus-7bb2k
Mar 22 13:43:55.069: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:43:55.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:43:55.069: INFO: update-demo-nautilus-7bb2k is verified up and running
STEP: scaling up the replication controller
Mar 22 13:43:55.071: INFO: scanned /root for discovery docs: <nil>
Mar 22 13:43:55.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:56.249: INFO: stderr: ""
Mar 22 13:43:56.249: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 22 13:43:56.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:56.383: INFO: stderr: ""
Mar 22 13:43:56.383: INFO: stdout: "update-demo-nautilus-7bb2k update-demo-nautilus-t6lph "
Mar 22 13:43:56.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-7bb2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:56.504: INFO: stderr: ""
Mar 22 13:43:56.504: INFO: stdout: "true"
Mar 22 13:43:56.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-7bb2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:56.599: INFO: stderr: ""
Mar 22 13:43:56.600: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:43:56.600: INFO: validating pod update-demo-nautilus-7bb2k
Mar 22 13:43:56.610: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:43:56.610: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:43:56.610: INFO: update-demo-nautilus-7bb2k is verified up and running
Mar 22 13:43:56.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-t6lph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:43:56.708: INFO: stderr: ""
Mar 22 13:43:56.708: INFO: stdout: ""
Mar 22 13:43:56.708: INFO: update-demo-nautilus-t6lph is created but not running
Mar 22 13:44:01.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:44:01.813: INFO: stderr: ""
Mar 22 13:44:01.813: INFO: stdout: "update-demo-nautilus-7bb2k update-demo-nautilus-t6lph "
Mar 22 13:44:01.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-7bb2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:44:01.908: INFO: stderr: ""
Mar 22 13:44:01.908: INFO: stdout: "true"
Mar 22 13:44:01.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-7bb2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:44:02.036: INFO: stderr: ""
Mar 22 13:44:02.036: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:44:02.036: INFO: validating pod update-demo-nautilus-7bb2k
Mar 22 13:44:02.043: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:44:02.043: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:44:02.043: INFO: update-demo-nautilus-7bb2k is verified up and running
Mar 22 13:44:02.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-t6lph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:44:02.160: INFO: stderr: ""
Mar 22 13:44:02.160: INFO: stdout: "true"
Mar 22 13:44:02.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods update-demo-nautilus-t6lph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:44:02.276: INFO: stderr: ""
Mar 22 13:44:02.276: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 22 13:44:02.276: INFO: validating pod update-demo-nautilus-t6lph
Mar 22 13:44:02.289: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 13:44:02.289: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 13:44:02.289: INFO: update-demo-nautilus-t6lph is verified up and running
STEP: using delete to clean up resources
Mar 22 13:44:02.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:44:02.411: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 13:44:02.411: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 22 13:44:02.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-d8bmp'
Mar 22 13:44:02.545: INFO: stderr: "No resources found.\n"
Mar 22 13:44:02.545: INFO: stdout: ""
Mar 22 13:44:02.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -l name=update-demo --namespace=e2e-tests-kubectl-d8bmp -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 22 13:44:02.675: INFO: stderr: ""
Mar 22 13:44:02.675: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:44:02.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d8bmp" for this suite.
Mar 22 13:44:26.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:44:26.894: INFO: namespace: e2e-tests-kubectl-d8bmp, resource: bindings, ignored listing per whitelist
Mar 22 13:44:26.942: INFO: namespace e2e-tests-kubectl-d8bmp deletion completed in 24.254073276s

â€¢ [SLOW TEST:55.154 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:44:26.942: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-rqlnh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:44:27.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-rqlnh" for this suite.
Mar 22 13:44:33.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:44:33.306: INFO: namespace: e2e-tests-services-rqlnh, resource: bindings, ignored listing per whitelist
Mar 22 13:44:33.365: INFO: namespace e2e-tests-services-rqlnh deletion completed in 6.203334539s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

â€¢ [SLOW TEST:6.423 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:44:33.365: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-hfw6c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 22 13:44:35.596: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-a01693dc-4ca8-11e9-99ae-9e98f636e47c,GenerateName:,Namespace:e2e-tests-events-hfw6c,SelfLink:/api/v1/namespaces/e2e-tests-events-hfw6c/pods/send-events-a01693dc-4ca8-11e9-99ae-9e98f636e47c,UID:a01a85a2-4ca8-11e9-b0fd-fa163e8e51f5,ResourceVersion:20890,Generation:0,CreationTimestamp:2019-03-22 13:44:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 556175653,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zbrsr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zbrsr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-zbrsr true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421136010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421136090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:44:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:44:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:44:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 13:44:33 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.75,PodIP:10.233.99.173,StartTime:2019-03-22 13:44:33 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-03-22 13:44:34 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://70701b7de41af144ca7d95b4480bffeb7e94b663c7bf45d1d37ba019a29b696f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar 22 13:44:37.602: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 22 13:44:39.615: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:44:39.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-hfw6c" for this suite.
Mar 22 13:45:21.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:45:21.723: INFO: namespace: e2e-tests-events-hfw6c, resource: bindings, ignored listing per whitelist
Mar 22 13:45:21.881: INFO: namespace e2e-tests-events-hfw6c deletion completed in 42.244816458s

â€¢ [SLOW TEST:48.516 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:45:21.881: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wdhtn
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 22 13:45:22.131: INFO: Waiting up to 5m0s for pod "pod-bd08e932-4ca8-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-wdhtn" to be "success or failure"
Mar 22 13:45:22.138: INFO: Pod "pod-bd08e932-4ca8-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.473473ms
Mar 22 13:45:24.154: INFO: Pod "pod-bd08e932-4ca8-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022049427s
STEP: Saw pod success
Mar 22 13:45:24.154: INFO: Pod "pod-bd08e932-4ca8-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:45:24.160: INFO: Trying to get logs from node metalk8s-01 pod pod-bd08e932-4ca8-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:45:24.229: INFO: Waiting for pod pod-bd08e932-4ca8-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:45:24.235: INFO: Pod pod-bd08e932-4ca8-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:45:24.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wdhtn" for this suite.
Mar 22 13:45:30.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:45:30.418: INFO: namespace: e2e-tests-emptydir-wdhtn, resource: bindings, ignored listing per whitelist
Mar 22 13:45:30.431: INFO: namespace e2e-tests-emptydir-wdhtn deletion completed in 6.186511691s

â€¢ [SLOW TEST:8.551 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:45:30.432: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-m5xc6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0322 13:46:01.241075      20 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 22 13:46:01.241: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:46:01.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-m5xc6" for this suite.
Mar 22 13:46:07.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:46:07.498: INFO: namespace: e2e-tests-gc-m5xc6, resource: bindings, ignored listing per whitelist
Mar 22 13:46:07.566: INFO: namespace e2e-tests-gc-m5xc6 deletion completed in 6.312174782s

â€¢ [SLOW TEST:37.135 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:46:07.566: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-h8qnh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 13:46:07.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d84173f8-4ca8-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-h8qnh" to be "success or failure"
Mar 22 13:46:07.807: INFO: Pod "downwardapi-volume-d84173f8-4ca8-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.001323ms
Mar 22 13:46:09.812: INFO: Pod "downwardapi-volume-d84173f8-4ca8-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010317731s
Mar 22 13:46:11.825: INFO: Pod "downwardapi-volume-d84173f8-4ca8-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022748713s
STEP: Saw pod success
Mar 22 13:46:11.825: INFO: Pod "downwardapi-volume-d84173f8-4ca8-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:46:11.830: INFO: Trying to get logs from node metalk8s-02 pod downwardapi-volume-d84173f8-4ca8-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 13:46:11.866: INFO: Waiting for pod downwardapi-volume-d84173f8-4ca8-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:46:11.871: INFO: Pod downwardapi-volume-d84173f8-4ca8-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:46:11.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-h8qnh" for this suite.
Mar 22 13:46:17.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:46:17.935: INFO: namespace: e2e-tests-downward-api-h8qnh, resource: bindings, ignored listing per whitelist
Mar 22 13:46:18.109: INFO: namespace e2e-tests-downward-api-h8qnh deletion completed in 6.229243315s

â€¢ [SLOW TEST:10.543 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:46:18.109: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-f5gnc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 22 13:46:28.406: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 13:46:28.411: INFO: Pod pod-with-poststart-http-hook still exists
Mar 22 13:46:30.411: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 13:46:30.417: INFO: Pod pod-with-poststart-http-hook still exists
Mar 22 13:46:32.411: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 13:46:32.423: INFO: Pod pod-with-poststart-http-hook still exists
Mar 22 13:46:34.411: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 13:46:34.417: INFO: Pod pod-with-poststart-http-hook still exists
Mar 22 13:46:36.411: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 13:46:36.417: INFO: Pod pod-with-poststart-http-hook still exists
Mar 22 13:46:38.411: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 13:46:38.417: INFO: Pod pod-with-poststart-http-hook still exists
Mar 22 13:46:40.411: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 13:46:40.418: INFO: Pod pod-with-poststart-http-hook still exists
Mar 22 13:46:42.411: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 13:46:42.417: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:46:42.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-f5gnc" for this suite.
Mar 22 13:47:04.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:47:04.496: INFO: namespace: e2e-tests-container-lifecycle-hook-f5gnc, resource: bindings, ignored listing per whitelist
Mar 22 13:47:04.745: INFO: namespace e2e-tests-container-lifecycle-hook-f5gnc deletion completed in 22.320176293s

â€¢ [SLOW TEST:46.636 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:47:04.745: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-v2nk9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-v2nk9
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-v2nk9
STEP: Deleting pre-stop pod
Mar 22 13:47:18.214: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:47:18.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-v2nk9" for this suite.
Mar 22 13:47:58.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:47:58.440: INFO: namespace: e2e-tests-prestop-v2nk9, resource: bindings, ignored listing per whitelist
Mar 22 13:47:58.454: INFO: namespace e2e-tests-prestop-v2nk9 deletion completed in 40.213876297s

â€¢ [SLOW TEST:53.708 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:47:58.454: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-45sqr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar 22 13:47:58.673: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:48:02.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-45sqr" for this suite.
Mar 22 13:48:08.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:48:08.358: INFO: namespace: e2e-tests-init-container-45sqr, resource: bindings, ignored listing per whitelist
Mar 22 13:48:08.462: INFO: namespace e2e-tests-init-container-45sqr deletion completed in 6.230431831s

â€¢ [SLOW TEST:10.008 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:48:08.462: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-7ffmv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 22 13:48:08.723: INFO: Waiting up to 5m0s for pod "pod-2053ff51-4ca9-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-7ffmv" to be "success or failure"
Mar 22 13:48:08.733: INFO: Pod "pod-2053ff51-4ca9-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.404172ms
Mar 22 13:48:10.738: INFO: Pod "pod-2053ff51-4ca9-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014400945s
Mar 22 13:48:12.754: INFO: Pod "pod-2053ff51-4ca9-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030808198s
STEP: Saw pod success
Mar 22 13:48:12.754: INFO: Pod "pod-2053ff51-4ca9-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:48:12.760: INFO: Trying to get logs from node metalk8s-04 pod pod-2053ff51-4ca9-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:48:12.788: INFO: Waiting for pod pod-2053ff51-4ca9-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:48:12.792: INFO: Pod pod-2053ff51-4ca9-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:48:12.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-7ffmv" for this suite.
Mar 22 13:48:18.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:48:18.955: INFO: namespace: e2e-tests-emptydir-7ffmv, resource: bindings, ignored listing per whitelist
Mar 22 13:48:18.990: INFO: namespace e2e-tests-emptydir-7ffmv deletion completed in 6.190164034s

â€¢ [SLOW TEST:10.528 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:48:18.990: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-9ps6s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:48:39.235: INFO: Container started at 2019-03-22 13:48:20 +0000 UTC, pod became ready at 2019-03-22 13:48:37 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:48:39.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-9ps6s" for this suite.
Mar 22 13:49:03.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:49:03.286: INFO: namespace: e2e-tests-container-probe-9ps6s, resource: bindings, ignored listing per whitelist
Mar 22 13:49:03.475: INFO: namespace e2e-tests-container-probe-9ps6s deletion completed in 24.23200567s

â€¢ [SLOW TEST:44.485 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:49:03.476: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-fx2wl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Mar 22 13:49:03.745: INFO: Waiting up to 5m0s for pod "var-expansion-41203faa-4ca9-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-var-expansion-fx2wl" to be "success or failure"
Mar 22 13:49:03.753: INFO: Pod "var-expansion-41203faa-4ca9-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.154141ms
Mar 22 13:49:05.759: INFO: Pod "var-expansion-41203faa-4ca9-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013278955s
STEP: Saw pod success
Mar 22 13:49:05.759: INFO: Pod "var-expansion-41203faa-4ca9-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:49:05.764: INFO: Trying to get logs from node metalk8s-02 pod var-expansion-41203faa-4ca9-11e9-99ae-9e98f636e47c container dapi-container: <nil>
STEP: delete the pod
Mar 22 13:49:05.816: INFO: Waiting for pod var-expansion-41203faa-4ca9-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:49:05.824: INFO: Pod var-expansion-41203faa-4ca9-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:49:05.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-fx2wl" for this suite.
Mar 22 13:49:11.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:49:12.087: INFO: namespace: e2e-tests-var-expansion-fx2wl, resource: bindings, ignored listing per whitelist
Mar 22 13:49:12.135: INFO: namespace e2e-tests-var-expansion-fx2wl deletion completed in 6.295963401s

â€¢ [SLOW TEST:8.659 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:49:12.135: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-sm4rv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:49:12.394: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 22 13:49:17.400: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 22 13:49:17.400: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 22 13:49:17.424: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-sm4rv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sm4rv/deployments/test-cleanup-deployment,UID:494a3484-4ca9-11e9-b0fd-fa163e8e51f5,ResourceVersion:21884,Generation:1,CreationTimestamp:2019-03-22 13:49:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar 22 13:49:17.436: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:49:17.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-sm4rv" for this suite.
Mar 22 13:49:23.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:49:23.569: INFO: namespace: e2e-tests-deployment-sm4rv, resource: bindings, ignored listing per whitelist
Mar 22 13:49:23.792: INFO: namespace e2e-tests-deployment-sm4rv deletion completed in 6.332554787s

â€¢ [SLOW TEST:11.657 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:49:23.793: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mjlxj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 22 13:49:24.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-mjlxj'
Mar 22 13:49:24.231: INFO: stderr: ""
Mar 22 13:49:24.231: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
Mar 22 13:49:24.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-mjlxj'
Mar 22 13:49:31.867: INFO: stderr: ""
Mar 22 13:49:31.867: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:49:31.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mjlxj" for this suite.
Mar 22 13:49:37.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:49:37.987: INFO: namespace: e2e-tests-kubectl-mjlxj, resource: bindings, ignored listing per whitelist
Mar 22 13:49:38.111: INFO: namespace e2e-tests-kubectl-mjlxj deletion completed in 6.226368019s

â€¢ [SLOW TEST:14.317 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:49:38.111: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-kzx58
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 22 13:49:38.404: INFO: Number of nodes with available pods: 0
Mar 22 13:49:38.405: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:49:39.430: INFO: Number of nodes with available pods: 0
Mar 22 13:49:39.430: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:49:40.417: INFO: Number of nodes with available pods: 4
Mar 22 13:49:40.417: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 13:49:41.421: INFO: Number of nodes with available pods: 5
Mar 22 13:49:41.421: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 22 13:49:41.463: INFO: Number of nodes with available pods: 5
Mar 22 13:49:41.463: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-kzx58, will wait for the garbage collector to delete the pods
Mar 22 13:49:42.570: INFO: Deleting {extensions DaemonSet} daemon-set took: 26.108938ms
Mar 22 13:49:42.771: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 200.884869ms
Mar 22 13:51:33.985: INFO: Number of nodes with available pods: 0
Mar 22 13:51:33.985: INFO: Number of running nodes: 0, number of available pods: 0
Mar 22 13:51:33.989: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-kzx58/daemonsets","resourceVersion":"22356"},"items":null}

Mar 22 13:51:33.994: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-kzx58/pods","resourceVersion":"22356"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:51:34.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-kzx58" for this suite.
Mar 22 13:51:40.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:51:40.217: INFO: namespace: e2e-tests-daemonsets-kzx58, resource: bindings, ignored listing per whitelist
Mar 22 13:51:40.387: INFO: namespace e2e-tests-daemonsets-kzx58 deletion completed in 6.35192133s

â€¢ [SLOW TEST:122.277 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:51:40.388: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-ns4wj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 22 13:51:40.604: INFO: Waiting up to 5m0s for pod "pod-9e9edb8e-4ca9-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-ns4wj" to be "success or failure"
Mar 22 13:51:40.616: INFO: Pod "pod-9e9edb8e-4ca9-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.956633ms
Mar 22 13:51:42.624: INFO: Pod "pod-9e9edb8e-4ca9-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019647734s
Mar 22 13:51:44.642: INFO: Pod "pod-9e9edb8e-4ca9-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037424304s
Mar 22 13:51:46.648: INFO: Pod "pod-9e9edb8e-4ca9-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04365155s
STEP: Saw pod success
Mar 22 13:51:46.648: INFO: Pod "pod-9e9edb8e-4ca9-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:51:46.654: INFO: Trying to get logs from node metalk8s-02 pod pod-9e9edb8e-4ca9-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:51:46.683: INFO: Waiting for pod pod-9e9edb8e-4ca9-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:51:46.688: INFO: Pod pod-9e9edb8e-4ca9-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:51:46.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ns4wj" for this suite.
Mar 22 13:51:52.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:51:52.844: INFO: namespace: e2e-tests-emptydir-ns4wj, resource: bindings, ignored listing per whitelist
Mar 22 13:51:52.889: INFO: namespace e2e-tests-emptydir-ns4wj deletion completed in 6.190244507s

â€¢ [SLOW TEST:12.501 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:51:52.889: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-jx9fk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar 22 13:51:53.086: INFO: PodSpec: initContainers in spec.initContainers
Mar 22 13:52:37.062: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a611961c-4ca9-11e9-99ae-9e98f636e47c", GenerateName:"", Namespace:"e2e-tests-init-container-jx9fk", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-jx9fk/pods/pod-init-a611961c-4ca9-11e9-99ae-9e98f636e47c", UID:"a61588e1-4ca9-11e9-b0fd-fa163e8e51f5", ResourceVersion:"22597", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63688859513, loc:(*time.Location)(0x6c43b60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"86526418"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hmzjq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc4227ef940), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hmzjq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hmzjq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hmzjq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc423082698), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"metalk8s-01", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc422cc13e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc423082720)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc423082740)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc423082748), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688859513, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688859513, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688859513, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688859513, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.200.6.49", PodIP:"10.233.88.50", StartTime:(*v1.Time)(0xc421e79360), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc420b14e00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc420b14e70)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://1990c0db5822a50340da9771178cec509243788bf52e0a1746abb7647f8a360b"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc421e793a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc421e79380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:52:37.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-jx9fk" for this suite.
Mar 22 13:53:01.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:53:01.215: INFO: namespace: e2e-tests-init-container-jx9fk, resource: bindings, ignored listing per whitelist
Mar 22 13:53:01.319: INFO: namespace e2e-tests-init-container-jx9fk deletion completed in 24.241120144s

â€¢ [SLOW TEST:68.430 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:53:01.319: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xvwnb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 22 13:53:01.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xvwnb'
Mar 22 13:53:01.748: INFO: stderr: ""
Mar 22 13:53:01.748: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar 22 13:53:06.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xvwnb -o json'
Mar 22 13:53:06.891: INFO: stderr: ""
Mar 22 13:53:06.891: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-03-22T13:53:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-xvwnb\",\n        \"resourceVersion\": \"22681\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-xvwnb/pods/e2e-test-nginx-pod\",\n        \"uid\": \"cefdf219-4ca9-11e9-b0fd-fa163e8e51f5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-vtgjc\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"nodeName\": \"metalk8s-05\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-vtgjc\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-vtgjc\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-22T13:53:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-22T13:53:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-22T13:53:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-22T13:53:01Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://86787c90f21cc89aa13d44829945592b90ff9f00450bf4a69457351d77fb2cd8\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-03-22T13:53:03Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.200.6.77\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.82.120\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-03-22T13:53:01Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 22 13:53:06.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 replace -f - --namespace=e2e-tests-kubectl-xvwnb'
Mar 22 13:53:07.127: INFO: stderr: ""
Mar 22 13:53:07.127: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Mar 22 13:53:07.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xvwnb'
Mar 22 13:53:11.863: INFO: stderr: ""
Mar 22 13:53:11.863: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:53:11.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xvwnb" for this suite.
Mar 22 13:53:17.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:53:18.033: INFO: namespace: e2e-tests-kubectl-xvwnb, resource: bindings, ignored listing per whitelist
Mar 22 13:53:18.093: INFO: namespace e2e-tests-kubectl-xvwnb deletion completed in 6.193717163s

â€¢ [SLOW TEST:16.774 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:53:18.093: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-h7pq9
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 13:53:18.344: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:53:24.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-h7pq9" for this suite.
Mar 22 13:53:30.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:53:30.560: INFO: namespace: e2e-tests-custom-resource-definition-h7pq9, resource: bindings, ignored listing per whitelist
Mar 22 13:53:30.668: INFO: namespace e2e-tests-custom-resource-definition-h7pq9 deletion completed in 6.221613901s

â€¢ [SLOW TEST:12.574 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:53:30.668: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-rwd2n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-e05a905e-4ca9-11e9-99ae-9e98f636e47c
Mar 22 13:53:30.884: INFO: Pod name my-hostname-basic-e05a905e-4ca9-11e9-99ae-9e98f636e47c: Found 0 pods out of 1
Mar 22 13:53:35.899: INFO: Pod name my-hostname-basic-e05a905e-4ca9-11e9-99ae-9e98f636e47c: Found 1 pods out of 1
Mar 22 13:53:35.899: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e05a905e-4ca9-11e9-99ae-9e98f636e47c" are running
Mar 22 13:53:35.904: INFO: Pod "my-hostname-basic-e05a905e-4ca9-11e9-99ae-9e98f636e47c-5tmgm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-22 13:53:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-22 13:53:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-22 13:53:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-22 13:53:30 +0000 UTC Reason: Message:}])
Mar 22 13:53:35.904: INFO: Trying to dial the pod
Mar 22 13:53:40.920: INFO: Controller my-hostname-basic-e05a905e-4ca9-11e9-99ae-9e98f636e47c: Got expected result from replica 1 [my-hostname-basic-e05a905e-4ca9-11e9-99ae-9e98f636e47c-5tmgm]: "my-hostname-basic-e05a905e-4ca9-11e9-99ae-9e98f636e47c-5tmgm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:53:40.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-rwd2n" for this suite.
Mar 22 13:53:46.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:53:47.212: INFO: namespace: e2e-tests-replication-controller-rwd2n, resource: bindings, ignored listing per whitelist
Mar 22 13:53:47.238: INFO: namespace e2e-tests-replication-controller-rwd2n deletion completed in 6.310097633s

â€¢ [SLOW TEST:16.570 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:53:47.239: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-ftqnk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 22 13:53:47.525: INFO: Waiting up to 5m0s for pod "pod-ea45de8d-4ca9-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-ftqnk" to be "success or failure"
Mar 22 13:53:47.531: INFO: Pod "pod-ea45de8d-4ca9-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.34876ms
Mar 22 13:53:49.538: INFO: Pod "pod-ea45de8d-4ca9-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012285023s
STEP: Saw pod success
Mar 22 13:53:49.538: INFO: Pod "pod-ea45de8d-4ca9-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:53:49.543: INFO: Trying to get logs from node metalk8s-02 pod pod-ea45de8d-4ca9-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:53:49.576: INFO: Waiting for pod pod-ea45de8d-4ca9-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:53:49.583: INFO: Pod pod-ea45de8d-4ca9-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:53:49.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ftqnk" for this suite.
Mar 22 13:53:55.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:53:55.815: INFO: namespace: e2e-tests-emptydir-ftqnk, resource: bindings, ignored listing per whitelist
Mar 22 13:53:55.902: INFO: namespace e2e-tests-emptydir-ftqnk deletion completed in 6.311545442s

â€¢ [SLOW TEST:8.664 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:53:55.903: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qbbbx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 22 13:53:56.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-qbbbx'
Mar 22 13:53:56.404: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar 22 13:53:56.404: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Mar 22 13:53:56.412: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar 22 13:53:56.426: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar 22 13:53:56.436: INFO: scanned /root for discovery docs: <nil>
Mar 22 13:53:56.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-qbbbx'
Mar 22 13:54:12.343: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 22 13:54:12.343: INFO: stdout: "Created e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7\nScaling up e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar 22 13:54:12.343: INFO: stdout: "Created e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7\nScaling up e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar 22 13:54:12.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qbbbx'
Mar 22 13:54:12.435: INFO: stderr: ""
Mar 22 13:54:12.435: INFO: stdout: "e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7-v84tl "
Mar 22 13:54:12.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7-v84tl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qbbbx'
Mar 22 13:54:12.547: INFO: stderr: ""
Mar 22 13:54:12.548: INFO: stdout: "true"
Mar 22 13:54:12.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7-v84tl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qbbbx'
Mar 22 13:54:12.654: INFO: stderr: ""
Mar 22 13:54:12.654: INFO: stdout: "nginx:1.14-alpine"
Mar 22 13:54:12.654: INFO: e2e-test-nginx-rc-174262ffaef330f47d28a4b20b2371a7-v84tl is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
Mar 22 13:54:12.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qbbbx'
Mar 22 13:54:12.788: INFO: stderr: ""
Mar 22 13:54:12.788: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:54:12.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qbbbx" for this suite.
Mar 22 13:54:18.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:54:19.012: INFO: namespace: e2e-tests-kubectl-qbbbx, resource: bindings, ignored listing per whitelist
Mar 22 13:54:19.012: INFO: namespace e2e-tests-kubectl-qbbbx deletion completed in 6.202890128s

â€¢ [SLOW TEST:23.109 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:54:19.013: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-2h2f8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-2h2f8
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 22 13:54:19.269: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 22 13:54:49.466: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.123:8080/dial?request=hostName&protocol=http&host=10.233.122.153&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-2h2f8 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:54:49.466: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:54:49.630: INFO: Waiting for endpoints: map[]
Mar 22 13:54:49.636: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.123:8080/dial?request=hostName&protocol=http&host=10.233.88.52&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-2h2f8 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:54:49.636: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:54:49.849: INFO: Waiting for endpoints: map[]
Mar 22 13:54:49.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.123:8080/dial?request=hostName&protocol=http&host=10.233.99.181&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-2h2f8 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:54:49.855: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:54:50.075: INFO: Waiting for endpoints: map[]
Mar 22 13:54:50.083: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.123:8080/dial?request=hostName&protocol=http&host=10.233.122.112&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-2h2f8 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:54:50.083: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:54:50.244: INFO: Waiting for endpoints: map[]
Mar 22 13:54:50.251: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.123:8080/dial?request=hostName&protocol=http&host=10.233.82.122&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-2h2f8 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:54:50.251: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:54:50.417: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:54:50.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-2h2f8" for this suite.
Mar 22 13:55:14.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:55:14.521: INFO: namespace: e2e-tests-pod-network-test-2h2f8, resource: bindings, ignored listing per whitelist
Mar 22 13:55:14.703: INFO: namespace e2e-tests-pod-network-test-2h2f8 deletion completed in 24.27478034s

â€¢ [SLOW TEST:55.691 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:55:14.704: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-kdqpt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 22 13:55:14.992: INFO: Waiting up to 5m0s for pod "pod-1e67f3a1-4caa-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-kdqpt" to be "success or failure"
Mar 22 13:55:15.005: INFO: Pod "pod-1e67f3a1-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.542432ms
Mar 22 13:55:17.015: INFO: Pod "pod-1e67f3a1-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022390056s
Mar 22 13:55:19.026: INFO: Pod "pod-1e67f3a1-4caa-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033077264s
STEP: Saw pod success
Mar 22 13:55:19.026: INFO: Pod "pod-1e67f3a1-4caa-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:55:19.030: INFO: Trying to get logs from node metalk8s-01 pod pod-1e67f3a1-4caa-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:55:19.058: INFO: Waiting for pod pod-1e67f3a1-4caa-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:55:19.063: INFO: Pod pod-1e67f3a1-4caa-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:55:19.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kdqpt" for this suite.
Mar 22 13:55:25.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:55:25.385: INFO: namespace: e2e-tests-emptydir-kdqpt, resource: bindings, ignored listing per whitelist
Mar 22 13:55:25.518: INFO: namespace e2e-tests-emptydir-kdqpt deletion completed in 6.448611212s

â€¢ [SLOW TEST:10.815 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:55:25.519: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-89vbd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-24da6ad8-4caa-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 13:55:25.813: INFO: Waiting up to 5m0s for pod "pod-configmaps-24db7246-4caa-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-configmap-89vbd" to be "success or failure"
Mar 22 13:55:25.831: INFO: Pod "pod-configmaps-24db7246-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.299518ms
Mar 22 13:55:27.838: INFO: Pod "pod-configmaps-24db7246-4caa-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024612115s
STEP: Saw pod success
Mar 22 13:55:27.838: INFO: Pod "pod-configmaps-24db7246-4caa-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:55:27.843: INFO: Trying to get logs from node metalk8s-04 pod pod-configmaps-24db7246-4caa-11e9-99ae-9e98f636e47c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 13:55:27.874: INFO: Waiting for pod pod-configmaps-24db7246-4caa-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:55:27.879: INFO: Pod pod-configmaps-24db7246-4caa-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:55:27.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-89vbd" for this suite.
Mar 22 13:55:33.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:55:33.916: INFO: namespace: e2e-tests-configmap-89vbd, resource: bindings, ignored listing per whitelist
Mar 22 13:55:34.099: INFO: namespace e2e-tests-configmap-89vbd deletion completed in 6.213431249s

â€¢ [SLOW TEST:8.580 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:55:34.100: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-dpkqq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 22 13:55:34.309: INFO: Waiting up to 5m0s for pod "downward-api-29ebf41a-4caa-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-dpkqq" to be "success or failure"
Mar 22 13:55:34.315: INFO: Pod "downward-api-29ebf41a-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.364926ms
Mar 22 13:55:36.329: INFO: Pod "downward-api-29ebf41a-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019706022s
Mar 22 13:55:38.339: INFO: Pod "downward-api-29ebf41a-4caa-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029939888s
STEP: Saw pod success
Mar 22 13:55:38.339: INFO: Pod "downward-api-29ebf41a-4caa-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:55:38.344: INFO: Trying to get logs from node metalk8s-02 pod downward-api-29ebf41a-4caa-11e9-99ae-9e98f636e47c container dapi-container: <nil>
STEP: delete the pod
Mar 22 13:55:38.390: INFO: Waiting for pod downward-api-29ebf41a-4caa-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:55:38.397: INFO: Pod downward-api-29ebf41a-4caa-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:55:38.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dpkqq" for this suite.
Mar 22 13:55:44.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:55:44.636: INFO: namespace: e2e-tests-downward-api-dpkqq, resource: bindings, ignored listing per whitelist
Mar 22 13:55:44.658: INFO: namespace e2e-tests-downward-api-dpkqq deletion completed in 6.254573636s

â€¢ [SLOW TEST:10.559 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:55:44.659: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-9jrd8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-9jrd8/configmap-test-3042bcaf-4caa-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 13:55:44.950: INFO: Waiting up to 5m0s for pod "pod-configmaps-304390ad-4caa-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-configmap-9jrd8" to be "success or failure"
Mar 22 13:55:44.955: INFO: Pod "pod-configmaps-304390ad-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.930043ms
Mar 22 13:55:46.974: INFO: Pod "pod-configmaps-304390ad-4caa-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023744841s
STEP: Saw pod success
Mar 22 13:55:46.974: INFO: Pod "pod-configmaps-304390ad-4caa-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:55:46.979: INFO: Trying to get logs from node metalk8s-02 pod pod-configmaps-304390ad-4caa-11e9-99ae-9e98f636e47c container env-test: <nil>
STEP: delete the pod
Mar 22 13:55:47.012: INFO: Waiting for pod pod-configmaps-304390ad-4caa-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:55:47.016: INFO: Pod pod-configmaps-304390ad-4caa-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:55:47.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9jrd8" for this suite.
Mar 22 13:55:53.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:55:53.251: INFO: namespace: e2e-tests-configmap-9jrd8, resource: bindings, ignored listing per whitelist
Mar 22 13:55:53.251: INFO: namespace e2e-tests-configmap-9jrd8 deletion completed in 6.223599888s

â€¢ [SLOW TEST:8.592 seconds]
[sig-api-machinery] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:55:53.251: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ghc9v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 22 13:55:53.453: INFO: Waiting up to 5m0s for pod "downward-api-3554b524-4caa-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-ghc9v" to be "success or failure"
Mar 22 13:55:53.458: INFO: Pod "downward-api-3554b524-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.891103ms
Mar 22 13:55:55.464: INFO: Pod "downward-api-3554b524-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010824732s
Mar 22 13:55:57.488: INFO: Pod "downward-api-3554b524-4caa-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035202149s
STEP: Saw pod success
Mar 22 13:55:57.488: INFO: Pod "downward-api-3554b524-4caa-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:55:57.495: INFO: Trying to get logs from node metalk8s-01 pod downward-api-3554b524-4caa-11e9-99ae-9e98f636e47c container dapi-container: <nil>
STEP: delete the pod
Mar 22 13:55:57.539: INFO: Waiting for pod downward-api-3554b524-4caa-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:55:57.543: INFO: Pod downward-api-3554b524-4caa-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:55:57.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ghc9v" for this suite.
Mar 22 13:56:03.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:56:03.633: INFO: namespace: e2e-tests-downward-api-ghc9v, resource: bindings, ignored listing per whitelist
Mar 22 13:56:03.837: INFO: namespace e2e-tests-downward-api-ghc9v deletion completed in 6.285104203s

â€¢ [SLOW TEST:10.586 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:56:03.837: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-nq8z9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Mar 22 13:56:08.096: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-3ba5605e-4caa-11e9-99ae-9e98f636e47c", GenerateName:"", Namespace:"e2e-tests-pods-nq8z9", SelfLink:"/api/v1/namespaces/e2e-tests-pods-nq8z9/pods/pod-submit-remove-3ba5605e-4caa-11e9-99ae-9e98f636e47c", UID:"3babeb31-4caa-11e9-b0fd-fa163e8e51f5", ResourceVersion:"23622", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63688859764, loc:(*time.Location)(0x6c43b60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"35600596"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xkw72", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc422a2ce00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xkw72", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc422a34ba8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"metalk8s-01", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc422a662a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc422a34bf0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc422a34c10)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc422a34c18), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688859764, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688859766, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688859766, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688859764, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.200.6.49", PodIP:"10.233.88.55", StartTime:(*v1.Time)(0xc422a40f00), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc422a40f20), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d", ContainerID:"docker://04e0a6cb6bde2c83f089ce9dfc835d38a90fdc9bd488db540af490459f6c2764"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:56:21.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nq8z9" for this suite.
Mar 22 13:56:27.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:56:27.412: INFO: namespace: e2e-tests-pods-nq8z9, resource: bindings, ignored listing per whitelist
Mar 22 13:56:27.558: INFO: namespace e2e-tests-pods-nq8z9 deletion completed in 6.275989383s

â€¢ [SLOW TEST:23.721 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:56:27.559: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-n86vg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Mar 22 13:56:27.841: INFO: Waiting up to 5m0s for pod "client-containers-49d3fd39-4caa-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-containers-n86vg" to be "success or failure"
Mar 22 13:56:27.846: INFO: Pod "client-containers-49d3fd39-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.604808ms
Mar 22 13:56:29.852: INFO: Pod "client-containers-49d3fd39-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011391285s
Mar 22 13:56:31.864: INFO: Pod "client-containers-49d3fd39-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02317667s
Mar 22 13:56:33.877: INFO: Pod "client-containers-49d3fd39-4caa-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036104464s
STEP: Saw pod success
Mar 22 13:56:33.877: INFO: Pod "client-containers-49d3fd39-4caa-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:56:33.883: INFO: Trying to get logs from node metalk8s-05 pod client-containers-49d3fd39-4caa-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:56:33.917: INFO: Waiting for pod client-containers-49d3fd39-4caa-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:56:33.922: INFO: Pod client-containers-49d3fd39-4caa-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:56:33.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-n86vg" for this suite.
Mar 22 13:56:39.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:56:39.975: INFO: namespace: e2e-tests-containers-n86vg, resource: bindings, ignored listing per whitelist
Mar 22 13:56:40.313: INFO: namespace e2e-tests-containers-n86vg deletion completed in 6.381548804s

â€¢ [SLOW TEST:12.755 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:56:40.314: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-9tqs9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Mar 22 13:56:40.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 cluster-info'
Mar 22 13:56:40.746: INFO: stderr: ""
Mar 22 13:56:40.746: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:56:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9tqs9" for this suite.
Mar 22 13:56:46.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:56:46.830: INFO: namespace: e2e-tests-kubectl-9tqs9, resource: bindings, ignored listing per whitelist
Mar 22 13:56:47.002: INFO: namespace e2e-tests-kubectl-9tqs9 deletion completed in 6.242779515s

â€¢ [SLOW TEST:6.688 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:56:47.002: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-sxln5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 22 13:56:47.363: INFO: Waiting up to 5m0s for pod "pod-55770a1f-4caa-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-sxln5" to be "success or failure"
Mar 22 13:56:47.367: INFO: Pod "pod-55770a1f-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074961ms
Mar 22 13:56:49.374: INFO: Pod "pod-55770a1f-4caa-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011380879s
STEP: Saw pod success
Mar 22 13:56:49.375: INFO: Pod "pod-55770a1f-4caa-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:56:49.380: INFO: Trying to get logs from node metalk8s-01 pod pod-55770a1f-4caa-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:56:49.415: INFO: Waiting for pod pod-55770a1f-4caa-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:56:49.422: INFO: Pod pod-55770a1f-4caa-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:56:49.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sxln5" for this suite.
Mar 22 13:56:55.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:56:55.698: INFO: namespace: e2e-tests-emptydir-sxln5, resource: bindings, ignored listing per whitelist
Mar 22 13:56:55.714: INFO: namespace e2e-tests-emptydir-sxln5 deletion completed in 6.282456222s

â€¢ [SLOW TEST:8.712 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:56:55.714: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-vh8pn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Mar 22 13:56:55.942: INFO: Waiting up to 5m0s for pod "client-containers-5a92843f-4caa-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-containers-vh8pn" to be "success or failure"
Mar 22 13:56:55.950: INFO: Pod "client-containers-5a92843f-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.535062ms
Mar 22 13:56:57.966: INFO: Pod "client-containers-5a92843f-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023750643s
Mar 22 13:56:59.976: INFO: Pod "client-containers-5a92843f-4caa-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034113587s
Mar 22 13:57:01.984: INFO: Pod "client-containers-5a92843f-4caa-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042317254s
STEP: Saw pod success
Mar 22 13:57:01.984: INFO: Pod "client-containers-5a92843f-4caa-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 13:57:01.990: INFO: Trying to get logs from node metalk8s-01 pod client-containers-5a92843f-4caa-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 13:57:02.036: INFO: Waiting for pod client-containers-5a92843f-4caa-11e9-99ae-9e98f636e47c to disappear
Mar 22 13:57:02.045: INFO: Pod client-containers-5a92843f-4caa-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:57:02.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-vh8pn" for this suite.
Mar 22 13:57:08.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:57:08.092: INFO: namespace: e2e-tests-containers-vh8pn, resource: bindings, ignored listing per whitelist
Mar 22 13:57:08.258: INFO: namespace e2e-tests-containers-vh8pn deletion completed in 6.200740881s

â€¢ [SLOW TEST:12.543 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:57:08.258: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-tc694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:58:08.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-tc694" for this suite.
Mar 22 13:58:30.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:58:30.682: INFO: namespace: e2e-tests-container-probe-tc694, resource: bindings, ignored listing per whitelist
Mar 22 13:58:30.736: INFO: namespace e2e-tests-container-probe-tc694 deletion completed in 22.237561693s

â€¢ [SLOW TEST:82.479 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:58:30.737: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-8sjj6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 22 13:58:35.037: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:35.037: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:35.173: INFO: Exec stderr: ""
Mar 22 13:58:35.173: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:35.173: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:35.335: INFO: Exec stderr: ""
Mar 22 13:58:35.335: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:35.335: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:35.556: INFO: Exec stderr: ""
Mar 22 13:58:35.556: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:35.556: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:35.760: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 22 13:58:35.760: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:35.760: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:35.887: INFO: Exec stderr: ""
Mar 22 13:58:35.887: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:35.887: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:36.015: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 22 13:58:36.015: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:36.015: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:36.272: INFO: Exec stderr: ""
Mar 22 13:58:36.272: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:36.272: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:36.528: INFO: Exec stderr: ""
Mar 22 13:58:36.528: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:36.528: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:36.754: INFO: Exec stderr: ""
Mar 22 13:58:36.754: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-8sjj6 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 13:58:36.754: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 13:58:36.941: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 13:58:36.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-8sjj6" for this suite.
Mar 22 13:59:22.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 13:59:23.044: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-8sjj6, resource: bindings, ignored listing per whitelist
Mar 22 13:59:23.140: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-8sjj6 deletion completed in 46.189412832s

â€¢ [SLOW TEST:52.403 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 13:59:23.141: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-jq8s7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 22 13:59:33.465: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:33.471: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:35.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:35.477: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:37.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:37.478: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:39.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:39.479: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:41.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:41.478: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:43.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:43.485: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:45.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:45.477: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:47.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:47.481: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:49.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:49.483: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:51.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:51.477: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:53.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:53.477: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:55.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:55.486: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:57.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:57.479: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 13:59:59.471: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 13:59:59.477: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 14:00:01.473: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 14:00:01.488: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:00:01.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-jq8s7" for this suite.
Mar 22 14:00:25.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:00:25.556: INFO: namespace: e2e-tests-container-lifecycle-hook-jq8s7, resource: bindings, ignored listing per whitelist
Mar 22 14:00:25.776: INFO: namespace e2e-tests-container-lifecycle-hook-jq8s7 deletion completed in 24.271500964s

â€¢ [SLOW TEST:62.636 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:00:25.776: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-g29nx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-g29nx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-g29nx to expose endpoints map[]
Mar 22 14:00:26.115: INFO: Get endpoints failed (14.33544ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar 22 14:00:27.121: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-g29nx exposes endpoints map[] (1.020589253s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-g29nx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-g29nx to expose endpoints map[pod1:[100]]
Mar 22 14:00:30.196: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-g29nx exposes endpoints map[pod1:[100]] (3.061008462s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-g29nx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-g29nx to expose endpoints map[pod1:[100] pod2:[101]]
Mar 22 14:00:32.260: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-g29nx exposes endpoints map[pod1:[100] pod2:[101]] (2.056371265s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-g29nx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-g29nx to expose endpoints map[pod2:[101]]
Mar 22 14:00:33.291: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-g29nx exposes endpoints map[pod2:[101]] (1.021594131s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-g29nx
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-g29nx to expose endpoints map[]
Mar 22 14:00:33.305: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-g29nx exposes endpoints map[] (6.213687ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:00:33.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-g29nx" for this suite.
Mar 22 14:00:57.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:00:57.551: INFO: namespace: e2e-tests-services-g29nx, resource: bindings, ignored listing per whitelist
Mar 22 14:00:57.574: INFO: namespace e2e-tests-services-g29nx deletion completed in 24.224609318s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

â€¢ [SLOW TEST:31.797 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:00:57.574: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ff4ts
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 22 14:00:57.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-ff4ts'
Mar 22 14:00:58.020: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar 22 14:00:58.020: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
Mar 22 14:00:58.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-ff4ts'
Mar 22 14:00:58.164: INFO: stderr: ""
Mar 22 14:00:58.164: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:00:58.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ff4ts" for this suite.
Mar 22 14:01:04.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:01:04.243: INFO: namespace: e2e-tests-kubectl-ff4ts, resource: bindings, ignored listing per whitelist
Mar 22 14:01:04.494: INFO: namespace e2e-tests-kubectl-ff4ts deletion completed in 6.319560426s

â€¢ [SLOW TEST:6.921 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:01:04.494: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hfkgp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-eefb1280-4caa-11e9-99ae-9e98f636e47c
STEP: Creating configMap with name cm-test-opt-upd-eefb13aa-4caa-11e9-99ae-9e98f636e47c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-eefb1280-4caa-11e9-99ae-9e98f636e47c
STEP: Updating configmap cm-test-opt-upd-eefb13aa-4caa-11e9-99ae-9e98f636e47c
STEP: Creating configMap with name cm-test-opt-create-eefb13bf-4caa-11e9-99ae-9e98f636e47c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:01:11.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hfkgp" for this suite.
Mar 22 14:01:35.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:01:35.498: INFO: namespace: e2e-tests-projected-hfkgp, resource: bindings, ignored listing per whitelist
Mar 22 14:01:35.599: INFO: namespace e2e-tests-projected-hfkgp deletion completed in 24.371682768s

â€¢ [SLOW TEST:31.105 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:01:35.600: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-w9bbb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 22 14:01:35.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-w9bbb'
Mar 22 14:01:36.034: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar 22 14:01:36.034: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
Mar 22 14:01:38.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-w9bbb'
Mar 22 14:01:38.181: INFO: stderr: ""
Mar 22 14:01:38.182: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:01:38.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w9bbb" for this suite.
Mar 22 14:02:02.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:02:02.397: INFO: namespace: e2e-tests-kubectl-w9bbb, resource: bindings, ignored listing per whitelist
Mar 22 14:02:02.452: INFO: namespace e2e-tests-kubectl-w9bbb deletion completed in 24.260707288s

â€¢ [SLOW TEST:26.853 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:02:02.453: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xm5lf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
Mar 22 14:02:02.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-xm5lf'
Mar 22 14:02:03.004: INFO: stderr: ""
Mar 22 14:02:03.004: INFO: stdout: "pod/pause created\n"
Mar 22 14:02:03.004: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 22 14:02:03.005: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-xm5lf" to be "running and ready"
Mar 22 14:02:03.012: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.544033ms
Mar 22 14:02:05.021: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016395746s
Mar 22 14:02:07.027: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.022145449s
Mar 22 14:02:07.027: INFO: Pod "pause" satisfied condition "running and ready"
Mar 22 14:02:07.027: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 22 14:02:07.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-xm5lf'
Mar 22 14:02:07.172: INFO: stderr: ""
Mar 22 14:02:07.172: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 22 14:02:07.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pod pause -L testing-label --namespace=e2e-tests-kubectl-xm5lf'
Mar 22 14:02:07.284: INFO: stderr: ""
Mar 22 14:02:07.284: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 22 14:02:07.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 label pods pause testing-label- --namespace=e2e-tests-kubectl-xm5lf'
Mar 22 14:02:07.389: INFO: stderr: ""
Mar 22 14:02:07.389: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 22 14:02:07.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pod pause -L testing-label --namespace=e2e-tests-kubectl-xm5lf'
Mar 22 14:02:07.512: INFO: stderr: ""
Mar 22 14:02:07.512: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
Mar 22 14:02:07.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xm5lf'
Mar 22 14:02:07.644: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 14:02:07.644: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 22 14:02:07.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-xm5lf'
Mar 22 14:02:07.774: INFO: stderr: "No resources found.\n"
Mar 22 14:02:07.774: INFO: stdout: ""
Mar 22 14:02:07.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 get pods -l name=pause --namespace=e2e-tests-kubectl-xm5lf -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 22 14:02:07.904: INFO: stderr: ""
Mar 22 14:02:07.904: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:02:07.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xm5lf" for this suite.
Mar 22 14:02:13.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:02:14.017: INFO: namespace: e2e-tests-kubectl-xm5lf, resource: bindings, ignored listing per whitelist
Mar 22 14:02:14.107: INFO: namespace e2e-tests-kubectl-xm5lf deletion completed in 6.193305551s

â€¢ [SLOW TEST:11.654 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:02:14.107: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-qf48w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-qf48w
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-qf48w
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-qf48w
Mar 22 14:02:14.357: INFO: Found 0 stateful pods, waiting for 1
Mar 22 14:02:24.371: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 22 14:02:24.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-qf48w ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:02:24.653: INFO: stderr: ""
Mar 22 14:02:24.653: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:02:24.653: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 22 14:02:24.658: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 22 14:02:34.681: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 14:02:34.681: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 14:02:34.723: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999405s
Mar 22 14:02:35.731: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.986299185s
Mar 22 14:02:36.738: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.979727065s
Mar 22 14:02:37.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.972860803s
Mar 22 14:02:38.750: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.967066585s
Mar 22 14:02:39.756: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.960837661s
Mar 22 14:02:40.762: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954561203s
Mar 22 14:02:41.771: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.948298639s
Mar 22 14:02:42.777: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.939803374s
Mar 22 14:02:43.785: INFO: Verifying statefulset ss doesn't scale past 1 for another 933.626065ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-qf48w
Mar 22 14:02:44.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-qf48w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 22 14:02:45.231: INFO: stderr: ""
Mar 22 14:02:45.231: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 22 14:02:45.231: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 22 14:02:45.262: INFO: Found 1 stateful pods, waiting for 3
Mar 22 14:02:55.299: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 14:02:55.299: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 14:02:55.300: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 22 14:02:55.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-qf48w ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:02:55.664: INFO: stderr: ""
Mar 22 14:02:55.664: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:02:55.664: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 22 14:02:55.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-qf48w ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:02:55.938: INFO: stderr: ""
Mar 22 14:02:55.938: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:02:55.938: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 22 14:02:55.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-qf48w ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:02:56.166: INFO: stderr: ""
Mar 22 14:02:56.166: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:02:56.166: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 22 14:02:56.166: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 14:02:56.173: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 22 14:03:06.201: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 14:03:06.201: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 14:03:06.201: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 14:03:06.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999306s
Mar 22 14:03:07.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.978024519s
Mar 22 14:03:08.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968404401s
Mar 22 14:03:09.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.961737297s
Mar 22 14:03:10.311: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.954868066s
Mar 22 14:03:11.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.947149585s
Mar 22 14:03:12.331: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.93460485s
Mar 22 14:03:13.339: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.926469136s
Mar 22 14:03:14.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.918402561s
Mar 22 14:03:15.354: INFO: Verifying statefulset ss doesn't scale past 3 for another 910.998761ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-qf48w
Mar 22 14:03:16.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-qf48w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 22 14:03:16.659: INFO: stderr: ""
Mar 22 14:03:16.659: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 22 14:03:16.659: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 22 14:03:16.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-qf48w ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 22 14:03:16.983: INFO: stderr: ""
Mar 22 14:03:16.983: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 22 14:03:16.983: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 22 14:03:16.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-qf48w ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 22 14:03:17.235: INFO: stderr: ""
Mar 22 14:03:17.235: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 22 14:03:17.235: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 22 14:03:17.235: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 22 14:03:47.275: INFO: Deleting all statefulset in ns e2e-tests-statefulset-qf48w
Mar 22 14:03:47.280: INFO: Scaling statefulset ss to 0
Mar 22 14:03:47.295: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 14:03:47.300: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:03:47.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-qf48w" for this suite.
Mar 22 14:03:53.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:03:53.409: INFO: namespace: e2e-tests-statefulset-qf48w, resource: bindings, ignored listing per whitelist
Mar 22 14:03:53.604: INFO: namespace e2e-tests-statefulset-qf48w deletion completed in 6.270623092s

â€¢ [SLOW TEST:99.497 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:03:53.604: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-szh2c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:03:53.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53b89e91-4cab-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-szh2c" to be "success or failure"
Mar 22 14:03:53.947: INFO: Pod "downwardapi-volume-53b89e91-4cab-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00633ms
Mar 22 14:03:55.953: INFO: Pod "downwardapi-volume-53b89e91-4cab-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013710036s
STEP: Saw pod success
Mar 22 14:03:55.953: INFO: Pod "downwardapi-volume-53b89e91-4cab-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:03:55.958: INFO: Trying to get logs from node metalk8s-02 pod downwardapi-volume-53b89e91-4cab-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:03:55.988: INFO: Waiting for pod downwardapi-volume-53b89e91-4cab-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:03:55.993: INFO: Pod downwardapi-volume-53b89e91-4cab-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:03:55.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-szh2c" for this suite.
Mar 22 14:04:02.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:04:02.092: INFO: namespace: e2e-tests-projected-szh2c, resource: bindings, ignored listing per whitelist
Mar 22 14:04:02.533: INFO: namespace e2e-tests-projected-szh2c deletion completed in 6.52371833s

â€¢ [SLOW TEST:8.929 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:04:02.533: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-qm5hv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-g6q6
STEP: Creating a pod to test atomic-volume-subpath
Mar 22 14:04:02.845: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-g6q6" in namespace "e2e-tests-subpath-qm5hv" to be "success or failure"
Mar 22 14:04:02.852: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.056769ms
Mar 22 14:04:04.872: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026565576s
Mar 22 14:04:06.878: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 4.03322249s
Mar 22 14:04:08.893: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 6.047782282s
Mar 22 14:04:10.900: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 8.054629127s
Mar 22 14:04:12.907: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 10.06156071s
Mar 22 14:04:14.916: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 12.070450927s
Mar 22 14:04:16.921: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 14.076117927s
Mar 22 14:04:18.937: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 16.091616423s
Mar 22 14:04:20.952: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 18.106930785s
Mar 22 14:04:22.958: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 20.112470351s
Mar 22 14:04:24.963: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Running", Reason="", readiness=false. Elapsed: 22.117970942s
Mar 22 14:04:26.969: INFO: Pod "pod-subpath-test-projected-g6q6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.124146665s
STEP: Saw pod success
Mar 22 14:04:26.969: INFO: Pod "pod-subpath-test-projected-g6q6" satisfied condition "success or failure"
Mar 22 14:04:26.977: INFO: Trying to get logs from node metalk8s-01 pod pod-subpath-test-projected-g6q6 container test-container-subpath-projected-g6q6: <nil>
STEP: delete the pod
Mar 22 14:04:27.038: INFO: Waiting for pod pod-subpath-test-projected-g6q6 to disappear
Mar 22 14:04:27.054: INFO: Pod pod-subpath-test-projected-g6q6 no longer exists
STEP: Deleting pod pod-subpath-test-projected-g6q6
Mar 22 14:04:27.054: INFO: Deleting pod "pod-subpath-test-projected-g6q6" in namespace "e2e-tests-subpath-qm5hv"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:04:27.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-qm5hv" for this suite.
Mar 22 14:04:33.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:04:33.110: INFO: namespace: e2e-tests-subpath-qm5hv, resource: bindings, ignored listing per whitelist
Mar 22 14:04:33.255: INFO: namespace e2e-tests-subpath-qm5hv deletion completed in 6.185679292s

â€¢ [SLOW TEST:30.722 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:04:33.256: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-wmtjt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-wmtjt
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 22 14:04:33.475: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 22 14:04:59.724: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.233.82.127:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wmtjt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:04:59.724: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:04:59.912: INFO: Found all expected endpoints: [netserver-0]
Mar 22 14:04:59.918: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.233.122.120:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wmtjt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:04:59.918: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:05:00.060: INFO: Found all expected endpoints: [netserver-1]
Mar 22 14:05:00.065: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.233.99.189:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wmtjt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:05:00.065: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:05:00.226: INFO: Found all expected endpoints: [netserver-2]
Mar 22 14:05:00.233: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.233.88.61:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wmtjt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:05:00.233: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:05:00.397: INFO: Found all expected endpoints: [netserver-3]
Mar 22 14:05:00.403: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.233.122.154:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-wmtjt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:05:00.403: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:05:00.555: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:05:00.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-wmtjt" for this suite.
Mar 22 14:05:24.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:05:24.750: INFO: namespace: e2e-tests-pod-network-test-wmtjt, resource: bindings, ignored listing per whitelist
Mar 22 14:05:24.841: INFO: namespace e2e-tests-pod-network-test-wmtjt deletion completed in 24.275125853s

â€¢ [SLOW TEST:51.585 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:05:24.841: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-cxzhk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:05:25.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-cxzhk" for this suite.
Mar 22 14:05:47.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:05:47.371: INFO: namespace: e2e-tests-pods-cxzhk, resource: bindings, ignored listing per whitelist
Mar 22 14:05:47.424: INFO: namespace e2e-tests-pods-cxzhk deletion completed in 22.247682766s

â€¢ [SLOW TEST:22.583 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:05:47.424: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-6rmlh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 22 14:05:50.225: INFO: Successfully updated pod "pod-update-activedeadlineseconds-977ff3d7-4cab-11e9-99ae-9e98f636e47c"
Mar 22 14:05:50.226: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-977ff3d7-4cab-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-pods-6rmlh" to be "terminated due to deadline exceeded"
Mar 22 14:05:50.232: INFO: Pod "pod-update-activedeadlineseconds-977ff3d7-4cab-11e9-99ae-9e98f636e47c": Phase="Running", Reason="", readiness=true. Elapsed: 6.060271ms
Mar 22 14:05:52.237: INFO: Pod "pod-update-activedeadlineseconds-977ff3d7-4cab-11e9-99ae-9e98f636e47c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011606029s
Mar 22 14:05:54.243: INFO: Pod "pod-update-activedeadlineseconds-977ff3d7-4cab-11e9-99ae-9e98f636e47c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.017262839s
Mar 22 14:05:54.243: INFO: Pod "pod-update-activedeadlineseconds-977ff3d7-4cab-11e9-99ae-9e98f636e47c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:05:54.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6rmlh" for this suite.
Mar 22 14:06:02.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:06:02.438: INFO: namespace: e2e-tests-pods-6rmlh, resource: bindings, ignored listing per whitelist
Mar 22 14:06:02.548: INFO: namespace e2e-tests-pods-6rmlh deletion completed in 8.298552835s

â€¢ [SLOW TEST:15.124 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:06:02.548: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-bbcw9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:06:02.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a08f43ea-4cab-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-bbcw9" to be "success or failure"
Mar 22 14:06:02.855: INFO: Pod "downwardapi-volume-a08f43ea-4cab-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.293439ms
Mar 22 14:06:04.860: INFO: Pod "downwardapi-volume-a08f43ea-4cab-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011560009s
Mar 22 14:06:06.876: INFO: Pod "downwardapi-volume-a08f43ea-4cab-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026738915s
STEP: Saw pod success
Mar 22 14:06:06.876: INFO: Pod "downwardapi-volume-a08f43ea-4cab-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:06:06.882: INFO: Trying to get logs from node metalk8s-04 pod downwardapi-volume-a08f43ea-4cab-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:06:06.936: INFO: Waiting for pod downwardapi-volume-a08f43ea-4cab-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:06:06.941: INFO: Pod downwardapi-volume-a08f43ea-4cab-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:06:06.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bbcw9" for this suite.
Mar 22 14:06:12.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:06:13.022: INFO: namespace: e2e-tests-downward-api-bbcw9, resource: bindings, ignored listing per whitelist
Mar 22 14:06:13.139: INFO: namespace e2e-tests-downward-api-bbcw9 deletion completed in 6.183586039s

â€¢ [SLOW TEST:10.591 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:06:13.140: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-959wm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Mar 22 14:06:13.386: INFO: Waiting up to 5m0s for pod "downward-api-a6d711d8-4cab-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-959wm" to be "success or failure"
Mar 22 14:06:13.392: INFO: Pod "downward-api-a6d711d8-4cab-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.64892ms
Mar 22 14:06:15.398: INFO: Pod "downward-api-a6d711d8-4cab-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012424112s
STEP: Saw pod success
Mar 22 14:06:15.398: INFO: Pod "downward-api-a6d711d8-4cab-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:06:15.404: INFO: Trying to get logs from node metalk8s-05 pod downward-api-a6d711d8-4cab-11e9-99ae-9e98f636e47c container dapi-container: <nil>
STEP: delete the pod
Mar 22 14:06:15.436: INFO: Waiting for pod downward-api-a6d711d8-4cab-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:06:15.441: INFO: Pod downward-api-a6d711d8-4cab-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:06:15.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-959wm" for this suite.
Mar 22 14:06:21.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:06:21.578: INFO: namespace: e2e-tests-downward-api-959wm, resource: bindings, ignored listing per whitelist
Mar 22 14:06:21.679: INFO: namespace e2e-tests-downward-api-959wm deletion completed in 6.230681879s

â€¢ [SLOW TEST:8.539 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:06:21.679: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-8k47m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-abed8cd5-4cab-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 14:06:21.929: INFO: Waiting up to 5m0s for pod "pod-secrets-abeeae6e-4cab-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-secrets-8k47m" to be "success or failure"
Mar 22 14:06:21.936: INFO: Pod "pod-secrets-abeeae6e-4cab-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685562ms
Mar 22 14:06:23.942: INFO: Pod "pod-secrets-abeeae6e-4cab-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012880819s
STEP: Saw pod success
Mar 22 14:06:23.942: INFO: Pod "pod-secrets-abeeae6e-4cab-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:06:23.949: INFO: Trying to get logs from node metalk8s-02 pod pod-secrets-abeeae6e-4cab-11e9-99ae-9e98f636e47c container secret-volume-test: <nil>
STEP: delete the pod
Mar 22 14:06:23.981: INFO: Waiting for pod pod-secrets-abeeae6e-4cab-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:06:23.985: INFO: Pod pod-secrets-abeeae6e-4cab-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:06:23.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8k47m" for this suite.
Mar 22 14:06:30.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:06:30.068: INFO: namespace: e2e-tests-secrets-8k47m, resource: bindings, ignored listing per whitelist
Mar 22 14:06:30.261: INFO: namespace e2e-tests-secrets-8k47m deletion completed in 6.268719833s

â€¢ [SLOW TEST:8.582 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:06:30.261: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-g42lj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-g42lj
I0322 14:06:30.543192      20 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-g42lj, replica count: 1
I0322 14:06:31.593706      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0322 14:06:32.593882      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 14:06:32.709: INFO: Created: latency-svc-rcnl5
Mar 22 14:06:32.726: INFO: Got endpoints: latency-svc-rcnl5 [31.866876ms]
Mar 22 14:06:32.744: INFO: Created: latency-svc-sbwkz
Mar 22 14:06:32.753: INFO: Got endpoints: latency-svc-sbwkz [27.592739ms]
Mar 22 14:06:32.757: INFO: Created: latency-svc-5mdhk
Mar 22 14:06:32.763: INFO: Got endpoints: latency-svc-5mdhk [37.315787ms]
Mar 22 14:06:32.767: INFO: Created: latency-svc-mzwt9
Mar 22 14:06:32.773: INFO: Got endpoints: latency-svc-mzwt9 [46.800206ms]
Mar 22 14:06:32.776: INFO: Created: latency-svc-qmrjv
Mar 22 14:06:32.784: INFO: Got endpoints: latency-svc-qmrjv [57.509073ms]
Mar 22 14:06:32.786: INFO: Created: latency-svc-pz58g
Mar 22 14:06:32.795: INFO: Got endpoints: latency-svc-pz58g [68.799922ms]
Mar 22 14:06:32.798: INFO: Created: latency-svc-dtllg
Mar 22 14:06:32.803: INFO: Got endpoints: latency-svc-dtllg [76.107813ms]
Mar 22 14:06:32.809: INFO: Created: latency-svc-jvngp
Mar 22 14:06:32.814: INFO: Created: latency-svc-wsl2z
Mar 22 14:06:32.815: INFO: Got endpoints: latency-svc-jvngp [88.233826ms]
Mar 22 14:06:32.822: INFO: Created: latency-svc-z8sd8
Mar 22 14:06:32.823: INFO: Got endpoints: latency-svc-wsl2z [96.133659ms]
Mar 22 14:06:32.828: INFO: Got endpoints: latency-svc-z8sd8 [101.358949ms]
Mar 22 14:06:32.829: INFO: Created: latency-svc-dh5wl
Mar 22 14:06:32.837: INFO: Got endpoints: latency-svc-dh5wl [22.12761ms]
Mar 22 14:06:32.856: INFO: Created: latency-svc-2nslf
Mar 22 14:06:32.856: INFO: Got endpoints: latency-svc-2nslf [128.175207ms]
Mar 22 14:06:32.859: INFO: Created: latency-svc-kt6fz
Mar 22 14:06:32.866: INFO: Got endpoints: latency-svc-kt6fz [138.134772ms]
Mar 22 14:06:32.870: INFO: Created: latency-svc-jv5rd
Mar 22 14:06:32.875: INFO: Got endpoints: latency-svc-jv5rd [146.762669ms]
Mar 22 14:06:32.885: INFO: Created: latency-svc-9kqm9
Mar 22 14:06:32.895: INFO: Created: latency-svc-l9zcl
Mar 22 14:06:32.896: INFO: Got endpoints: latency-svc-9kqm9 [168.086189ms]
Mar 22 14:06:32.904: INFO: Got endpoints: latency-svc-l9zcl [176.310057ms]
Mar 22 14:06:32.906: INFO: Created: latency-svc-87srj
Mar 22 14:06:32.911: INFO: Got endpoints: latency-svc-87srj [183.999009ms]
Mar 22 14:06:32.917: INFO: Created: latency-svc-k52sv
Mar 22 14:06:32.922: INFO: Got endpoints: latency-svc-k52sv [168.937897ms]
Mar 22 14:06:32.930: INFO: Created: latency-svc-m6pg9
Mar 22 14:06:32.957: INFO: Got endpoints: latency-svc-m6pg9 [193.032262ms]
Mar 22 14:06:32.966: INFO: Created: latency-svc-sz47p
Mar 22 14:06:32.976: INFO: Got endpoints: latency-svc-sz47p [202.428654ms]
Mar 22 14:06:32.976: INFO: Created: latency-svc-pcxkp
Mar 22 14:06:32.984: INFO: Created: latency-svc-zsxl8
Mar 22 14:06:32.985: INFO: Got endpoints: latency-svc-pcxkp [200.67921ms]
Mar 22 14:06:32.991: INFO: Got endpoints: latency-svc-zsxl8 [195.569714ms]
Mar 22 14:06:32.993: INFO: Created: latency-svc-5pdbr
Mar 22 14:06:32.999: INFO: Got endpoints: latency-svc-5pdbr [195.243927ms]
Mar 22 14:06:33.003: INFO: Created: latency-svc-7dd9b
Mar 22 14:06:33.009: INFO: Got endpoints: latency-svc-7dd9b [185.895973ms]
Mar 22 14:06:33.014: INFO: Created: latency-svc-8j879
Mar 22 14:06:33.030: INFO: Created: latency-svc-hd9nq
Mar 22 14:06:33.033: INFO: Got endpoints: latency-svc-8j879 [205.258146ms]
Mar 22 14:06:33.040: INFO: Created: latency-svc-nv5mr
Mar 22 14:06:33.042: INFO: Got endpoints: latency-svc-hd9nq [204.320691ms]
Mar 22 14:06:33.049: INFO: Got endpoints: latency-svc-nv5mr [192.663918ms]
Mar 22 14:06:33.050: INFO: Created: latency-svc-ptmz6
Mar 22 14:06:33.056: INFO: Got endpoints: latency-svc-ptmz6 [189.657062ms]
Mar 22 14:06:33.063: INFO: Created: latency-svc-tlz4s
Mar 22 14:06:33.069: INFO: Got endpoints: latency-svc-tlz4s [194.807748ms]
Mar 22 14:06:33.072: INFO: Created: latency-svc-vjt95
Mar 22 14:06:33.079: INFO: Got endpoints: latency-svc-vjt95 [183.384814ms]
Mar 22 14:06:33.083: INFO: Created: latency-svc-r4vgv
Mar 22 14:06:33.099: INFO: Got endpoints: latency-svc-r4vgv [194.808179ms]
Mar 22 14:06:33.114: INFO: Created: latency-svc-22n8w
Mar 22 14:06:33.117: INFO: Got endpoints: latency-svc-22n8w [205.939615ms]
Mar 22 14:06:33.123: INFO: Created: latency-svc-p5zst
Mar 22 14:06:33.126: INFO: Got endpoints: latency-svc-p5zst [203.999088ms]
Mar 22 14:06:33.128: INFO: Created: latency-svc-z28nx
Mar 22 14:06:33.156: INFO: Created: latency-svc-sm6qg
Mar 22 14:06:33.156: INFO: Created: latency-svc-7xpr2
Mar 22 14:06:33.156: INFO: Got endpoints: latency-svc-z28nx [199.309154ms]
Mar 22 14:06:33.157: INFO: Created: latency-svc-zq2jj
Mar 22 14:06:33.170: INFO: Created: latency-svc-xg4p4
Mar 22 14:06:33.174: INFO: Got endpoints: latency-svc-sm6qg [189.496574ms]
Mar 22 14:06:33.175: INFO: Got endpoints: latency-svc-7xpr2 [199.267919ms]
Mar 22 14:06:33.178: INFO: Got endpoints: latency-svc-zq2jj [186.802093ms]
Mar 22 14:06:33.191: INFO: Got endpoints: latency-svc-xg4p4 [192.222069ms]
Mar 22 14:06:33.197: INFO: Created: latency-svc-6ftk6
Mar 22 14:06:33.216: INFO: Got endpoints: latency-svc-6ftk6 [206.853168ms]
Mar 22 14:06:33.222: INFO: Created: latency-svc-czvrl
Mar 22 14:06:33.228: INFO: Got endpoints: latency-svc-czvrl [194.717066ms]
Mar 22 14:06:33.232: INFO: Created: latency-svc-rqcmh
Mar 22 14:06:33.243: INFO: Created: latency-svc-54cjd
Mar 22 14:06:33.251: INFO: Created: latency-svc-57pzn
Mar 22 14:06:33.258: INFO: Created: latency-svc-fppwj
Mar 22 14:06:33.267: INFO: Got endpoints: latency-svc-rqcmh [225.405758ms]
Mar 22 14:06:33.268: INFO: Created: latency-svc-ptc54
Mar 22 14:06:33.275: INFO: Created: latency-svc-5ps49
Mar 22 14:06:33.282: INFO: Created: latency-svc-7rnx9
Mar 22 14:06:33.291: INFO: Created: latency-svc-kmrdj
Mar 22 14:06:33.297: INFO: Created: latency-svc-h6vrd
Mar 22 14:06:33.316: INFO: Created: latency-svc-b9glf
Mar 22 14:06:33.323: INFO: Created: latency-svc-pvqth
Mar 22 14:06:33.323: INFO: Created: latency-svc-npl7j
Mar 22 14:06:33.328: INFO: Created: latency-svc-mxf9b
Mar 22 14:06:33.334: INFO: Created: latency-svc-9fnjl
Mar 22 14:06:33.357: INFO: Created: latency-svc-v4nwx
Mar 22 14:06:33.372: INFO: Created: latency-svc-tspgx
Mar 22 14:06:33.375: INFO: Got endpoints: latency-svc-54cjd [326.179217ms]
Mar 22 14:06:33.378: INFO: Got endpoints: latency-svc-57pzn [321.916709ms]
Mar 22 14:06:33.386: INFO: Created: latency-svc-wrw5f
Mar 22 14:06:33.393: INFO: Created: latency-svc-tpg5l
Mar 22 14:06:33.417: INFO: Got endpoints: latency-svc-fppwj [347.232133ms]
Mar 22 14:06:33.432: INFO: Created: latency-svc-5ctlp
Mar 22 14:06:33.468: INFO: Got endpoints: latency-svc-ptc54 [388.610827ms]
Mar 22 14:06:33.487: INFO: Created: latency-svc-nkhj5
Mar 22 14:06:33.524: INFO: Got endpoints: latency-svc-5ps49 [424.873136ms]
Mar 22 14:06:33.558: INFO: Created: latency-svc-cf4dk
Mar 22 14:06:33.580: INFO: Got endpoints: latency-svc-7rnx9 [462.965786ms]
Mar 22 14:06:33.595: INFO: Created: latency-svc-2jgxx
Mar 22 14:06:33.617: INFO: Got endpoints: latency-svc-kmrdj [490.80576ms]
Mar 22 14:06:33.657: INFO: Created: latency-svc-z6jc6
Mar 22 14:06:33.668: INFO: Got endpoints: latency-svc-h6vrd [512.347818ms]
Mar 22 14:06:33.696: INFO: Created: latency-svc-hs9vx
Mar 22 14:06:33.718: INFO: Got endpoints: latency-svc-b9glf [543.505182ms]
Mar 22 14:06:33.742: INFO: Created: latency-svc-m6btk
Mar 22 14:06:33.767: INFO: Got endpoints: latency-svc-pvqth [592.023796ms]
Mar 22 14:06:33.783: INFO: Created: latency-svc-nzw8d
Mar 22 14:06:33.817: INFO: Got endpoints: latency-svc-npl7j [638.395965ms]
Mar 22 14:06:33.830: INFO: Created: latency-svc-qt2ff
Mar 22 14:06:33.867: INFO: Got endpoints: latency-svc-mxf9b [675.939464ms]
Mar 22 14:06:33.881: INFO: Created: latency-svc-xvvfg
Mar 22 14:06:33.916: INFO: Got endpoints: latency-svc-9fnjl [700.164826ms]
Mar 22 14:06:33.933: INFO: Created: latency-svc-lrw6v
Mar 22 14:06:33.966: INFO: Got endpoints: latency-svc-v4nwx [737.677594ms]
Mar 22 14:06:33.985: INFO: Created: latency-svc-7x9v6
Mar 22 14:06:34.016: INFO: Got endpoints: latency-svc-tspgx [749.070048ms]
Mar 22 14:06:34.029: INFO: Created: latency-svc-dlzhh
Mar 22 14:06:34.066: INFO: Got endpoints: latency-svc-wrw5f [691.358397ms]
Mar 22 14:06:34.078: INFO: Created: latency-svc-65dpg
Mar 22 14:06:34.116: INFO: Got endpoints: latency-svc-tpg5l [737.92239ms]
Mar 22 14:06:34.128: INFO: Created: latency-svc-czjtx
Mar 22 14:06:34.167: INFO: Got endpoints: latency-svc-5ctlp [750.077623ms]
Mar 22 14:06:34.180: INFO: Created: latency-svc-s9h44
Mar 22 14:06:34.218: INFO: Got endpoints: latency-svc-nkhj5 [749.722665ms]
Mar 22 14:06:34.233: INFO: Created: latency-svc-lx2kc
Mar 22 14:06:34.267: INFO: Got endpoints: latency-svc-cf4dk [742.392914ms]
Mar 22 14:06:34.280: INFO: Created: latency-svc-tc589
Mar 22 14:06:34.316: INFO: Got endpoints: latency-svc-2jgxx [735.704813ms]
Mar 22 14:06:34.328: INFO: Created: latency-svc-gz48c
Mar 22 14:06:34.373: INFO: Got endpoints: latency-svc-z6jc6 [756.02915ms]
Mar 22 14:06:34.390: INFO: Created: latency-svc-2277g
Mar 22 14:06:34.417: INFO: Got endpoints: latency-svc-hs9vx [748.251693ms]
Mar 22 14:06:34.430: INFO: Created: latency-svc-djgck
Mar 22 14:06:34.472: INFO: Got endpoints: latency-svc-m6btk [753.286943ms]
Mar 22 14:06:34.519: INFO: Created: latency-svc-rm87x
Mar 22 14:06:34.524: INFO: Got endpoints: latency-svc-nzw8d [756.959813ms]
Mar 22 14:06:34.543: INFO: Created: latency-svc-d8cj8
Mar 22 14:06:34.569: INFO: Got endpoints: latency-svc-qt2ff [752.302955ms]
Mar 22 14:06:34.608: INFO: Created: latency-svc-d9z2q
Mar 22 14:06:34.616: INFO: Got endpoints: latency-svc-xvvfg [748.978647ms]
Mar 22 14:06:34.650: INFO: Created: latency-svc-t9vq2
Mar 22 14:06:34.667: INFO: Got endpoints: latency-svc-lrw6v [751.022714ms]
Mar 22 14:06:34.696: INFO: Created: latency-svc-mcpv4
Mar 22 14:06:34.730: INFO: Got endpoints: latency-svc-7x9v6 [763.915146ms]
Mar 22 14:06:34.760: INFO: Created: latency-svc-vfc8n
Mar 22 14:06:34.772: INFO: Got endpoints: latency-svc-dlzhh [755.931319ms]
Mar 22 14:06:34.818: INFO: Created: latency-svc-s9hfg
Mar 22 14:06:34.820: INFO: Got endpoints: latency-svc-65dpg [753.805546ms]
Mar 22 14:06:34.838: INFO: Created: latency-svc-nrpkt
Mar 22 14:06:34.882: INFO: Got endpoints: latency-svc-czjtx [766.406025ms]
Mar 22 14:06:34.908: INFO: Created: latency-svc-nh4tr
Mar 22 14:06:34.927: INFO: Got endpoints: latency-svc-s9h44 [760.153501ms]
Mar 22 14:06:34.948: INFO: Created: latency-svc-xt9lh
Mar 22 14:06:34.973: INFO: Got endpoints: latency-svc-lx2kc [755.299544ms]
Mar 22 14:06:35.002: INFO: Created: latency-svc-dq22v
Mar 22 14:06:35.034: INFO: Got endpoints: latency-svc-tc589 [767.584629ms]
Mar 22 14:06:35.057: INFO: Created: latency-svc-5rj87
Mar 22 14:06:35.067: INFO: Got endpoints: latency-svc-gz48c [750.449587ms]
Mar 22 14:06:35.084: INFO: Created: latency-svc-484b6
Mar 22 14:06:35.118: INFO: Got endpoints: latency-svc-2277g [744.329079ms]
Mar 22 14:06:35.258: INFO: Created: latency-svc-l8btw
Mar 22 14:06:35.258: INFO: Got endpoints: latency-svc-djgck [841.186863ms]
Mar 22 14:06:35.272: INFO: Got endpoints: latency-svc-rm87x [800.331371ms]
Mar 22 14:06:35.357: INFO: Got endpoints: latency-svc-d8cj8 [833.759214ms]
Mar 22 14:06:35.382: INFO: Got endpoints: latency-svc-d9z2q [813.270526ms]
Mar 22 14:06:35.456: INFO: Got endpoints: latency-svc-t9vq2 [839.832538ms]
Mar 22 14:06:35.468: INFO: Created: latency-svc-hv25j
Mar 22 14:06:35.471: INFO: Got endpoints: latency-svc-mcpv4 [804.273024ms]
Mar 22 14:06:35.472: INFO: Got endpoints: latency-svc-vfc8n [742.20642ms]
Mar 22 14:06:35.494: INFO: Created: latency-svc-wfxkp
Mar 22 14:06:35.506: INFO: Created: latency-svc-nr86p
Mar 22 14:06:35.513: INFO: Created: latency-svc-8v46w
Mar 22 14:06:35.563: INFO: Got endpoints: latency-svc-s9hfg [790.30513ms]
Mar 22 14:06:35.570: INFO: Got endpoints: latency-svc-nrpkt [750.065619ms]
Mar 22 14:06:35.571: INFO: Created: latency-svc-g25m5
Mar 22 14:06:35.578: INFO: Created: latency-svc-hbrll
Mar 22 14:06:35.587: INFO: Created: latency-svc-h8v7v
Mar 22 14:06:35.598: INFO: Created: latency-svc-2td4p
Mar 22 14:06:35.613: INFO: Created: latency-svc-txww4
Mar 22 14:06:35.631: INFO: Got endpoints: latency-svc-nh4tr [748.771028ms]
Mar 22 14:06:35.655: INFO: Created: latency-svc-jmtqf
Mar 22 14:06:35.685: INFO: Got endpoints: latency-svc-xt9lh [758.302891ms]
Mar 22 14:06:35.709: INFO: Created: latency-svc-b56mc
Mar 22 14:06:35.729: INFO: Got endpoints: latency-svc-dq22v [756.057456ms]
Mar 22 14:06:35.755: INFO: Created: latency-svc-cs7d6
Mar 22 14:06:35.767: INFO: Got endpoints: latency-svc-5rj87 [733.123696ms]
Mar 22 14:06:35.787: INFO: Created: latency-svc-w7k7f
Mar 22 14:06:35.828: INFO: Got endpoints: latency-svc-484b6 [760.916542ms]
Mar 22 14:06:35.874: INFO: Got endpoints: latency-svc-l8btw [756.481602ms]
Mar 22 14:06:35.886: INFO: Created: latency-svc-rvlsp
Mar 22 14:06:35.903: INFO: Created: latency-svc-hqrs7
Mar 22 14:06:35.917: INFO: Got endpoints: latency-svc-hv25j [659.263587ms]
Mar 22 14:06:35.936: INFO: Created: latency-svc-8jfnq
Mar 22 14:06:35.969: INFO: Got endpoints: latency-svc-wfxkp [697.242936ms]
Mar 22 14:06:35.987: INFO: Created: latency-svc-z4s58
Mar 22 14:06:36.059: INFO: Got endpoints: latency-svc-nr86p [701.319831ms]
Mar 22 14:06:36.070: INFO: Got endpoints: latency-svc-8v46w [687.475932ms]
Mar 22 14:06:36.082: INFO: Created: latency-svc-g5kqc
Mar 22 14:06:36.090: INFO: Created: latency-svc-vs7tm
Mar 22 14:06:36.156: INFO: Got endpoints: latency-svc-g25m5 [700.054555ms]
Mar 22 14:06:36.185: INFO: Created: latency-svc-xbhv4
Mar 22 14:06:36.186: INFO: Got endpoints: latency-svc-hbrll [713.99279ms]
Mar 22 14:06:36.237: INFO: Got endpoints: latency-svc-h8v7v [666.93282ms]
Mar 22 14:06:36.266: INFO: Created: latency-svc-nrglk
Mar 22 14:06:36.296: INFO: Got endpoints: latency-svc-2td4p [824.2477ms]
Mar 22 14:06:36.301: INFO: Created: latency-svc-8sjk8
Mar 22 14:06:36.311: INFO: Created: latency-svc-cvprb
Mar 22 14:06:36.315: INFO: Got endpoints: latency-svc-txww4 [752.45501ms]
Mar 22 14:06:36.359: INFO: Created: latency-svc-jqtmm
Mar 22 14:06:36.366: INFO: Got endpoints: latency-svc-jmtqf [735.397897ms]
Mar 22 14:06:36.395: INFO: Created: latency-svc-6lt2t
Mar 22 14:06:36.466: INFO: Got endpoints: latency-svc-b56mc [780.137637ms]
Mar 22 14:06:36.471: INFO: Got endpoints: latency-svc-cs7d6 [741.506117ms]
Mar 22 14:06:36.482: INFO: Created: latency-svc-9twfw
Mar 22 14:06:36.493: INFO: Created: latency-svc-d9nqm
Mar 22 14:06:36.516: INFO: Got endpoints: latency-svc-w7k7f [748.713332ms]
Mar 22 14:06:36.530: INFO: Created: latency-svc-4995b
Mar 22 14:06:36.566: INFO: Got endpoints: latency-svc-rvlsp [738.215747ms]
Mar 22 14:06:36.585: INFO: Created: latency-svc-wtfg7
Mar 22 14:06:36.617: INFO: Got endpoints: latency-svc-hqrs7 [742.751683ms]
Mar 22 14:06:36.632: INFO: Created: latency-svc-vxbl5
Mar 22 14:06:36.669: INFO: Got endpoints: latency-svc-8jfnq [752.20735ms]
Mar 22 14:06:36.689: INFO: Created: latency-svc-c8vlx
Mar 22 14:06:36.720: INFO: Got endpoints: latency-svc-z4s58 [750.05828ms]
Mar 22 14:06:36.744: INFO: Created: latency-svc-wv8wv
Mar 22 14:06:36.766: INFO: Got endpoints: latency-svc-g5kqc [707.529286ms]
Mar 22 14:06:36.801: INFO: Created: latency-svc-2rtjm
Mar 22 14:06:36.815: INFO: Got endpoints: latency-svc-vs7tm [745.210095ms]
Mar 22 14:06:36.829: INFO: Created: latency-svc-tcpwh
Mar 22 14:06:36.866: INFO: Got endpoints: latency-svc-xbhv4 [710.131302ms]
Mar 22 14:06:36.880: INFO: Created: latency-svc-k8ddl
Mar 22 14:06:36.916: INFO: Got endpoints: latency-svc-nrglk [730.790324ms]
Mar 22 14:06:36.938: INFO: Created: latency-svc-r9p5d
Mar 22 14:06:36.965: INFO: Got endpoints: latency-svc-8sjk8 [728.008316ms]
Mar 22 14:06:36.977: INFO: Created: latency-svc-6wqpc
Mar 22 14:06:37.017: INFO: Got endpoints: latency-svc-cvprb [720.565094ms]
Mar 22 14:06:37.042: INFO: Created: latency-svc-7kk7m
Mar 22 14:06:37.066: INFO: Got endpoints: latency-svc-jqtmm [750.178535ms]
Mar 22 14:06:37.076: INFO: Created: latency-svc-pdqhf
Mar 22 14:06:37.116: INFO: Got endpoints: latency-svc-6lt2t [749.174861ms]
Mar 22 14:06:37.128: INFO: Created: latency-svc-76t2x
Mar 22 14:06:37.166: INFO: Got endpoints: latency-svc-9twfw [700.788662ms]
Mar 22 14:06:37.179: INFO: Created: latency-svc-l58gp
Mar 22 14:06:37.217: INFO: Got endpoints: latency-svc-d9nqm [745.540645ms]
Mar 22 14:06:37.234: INFO: Created: latency-svc-2fg82
Mar 22 14:06:37.268: INFO: Got endpoints: latency-svc-4995b [751.585273ms]
Mar 22 14:06:37.286: INFO: Created: latency-svc-f7mq2
Mar 22 14:06:37.319: INFO: Got endpoints: latency-svc-wtfg7 [752.65437ms]
Mar 22 14:06:37.336: INFO: Created: latency-svc-w2szv
Mar 22 14:06:37.371: INFO: Got endpoints: latency-svc-vxbl5 [754.432256ms]
Mar 22 14:06:37.385: INFO: Created: latency-svc-8z4lh
Mar 22 14:06:37.420: INFO: Got endpoints: latency-svc-c8vlx [750.98896ms]
Mar 22 14:06:37.437: INFO: Created: latency-svc-hxwq9
Mar 22 14:06:37.467: INFO: Got endpoints: latency-svc-wv8wv [747.448393ms]
Mar 22 14:06:37.480: INFO: Created: latency-svc-9qprq
Mar 22 14:06:37.515: INFO: Got endpoints: latency-svc-2rtjm [748.579481ms]
Mar 22 14:06:37.526: INFO: Created: latency-svc-8wk8n
Mar 22 14:06:37.566: INFO: Got endpoints: latency-svc-tcpwh [750.859165ms]
Mar 22 14:06:37.579: INFO: Created: latency-svc-wnkdh
Mar 22 14:06:37.617: INFO: Got endpoints: latency-svc-k8ddl [750.355487ms]
Mar 22 14:06:37.632: INFO: Created: latency-svc-9kc5s
Mar 22 14:06:37.666: INFO: Got endpoints: latency-svc-r9p5d [750.042036ms]
Mar 22 14:06:37.680: INFO: Created: latency-svc-wwc6c
Mar 22 14:06:37.716: INFO: Got endpoints: latency-svc-6wqpc [750.307994ms]
Mar 22 14:06:37.729: INFO: Created: latency-svc-w9xgm
Mar 22 14:06:37.765: INFO: Got endpoints: latency-svc-7kk7m [747.657919ms]
Mar 22 14:06:37.777: INFO: Created: latency-svc-g4pwr
Mar 22 14:06:37.824: INFO: Got endpoints: latency-svc-pdqhf [758.653036ms]
Mar 22 14:06:37.839: INFO: Created: latency-svc-qqpfz
Mar 22 14:06:37.865: INFO: Got endpoints: latency-svc-76t2x [749.463079ms]
Mar 22 14:06:37.879: INFO: Created: latency-svc-kfp86
Mar 22 14:06:37.915: INFO: Got endpoints: latency-svc-l58gp [748.986992ms]
Mar 22 14:06:37.927: INFO: Created: latency-svc-9qrtd
Mar 22 14:06:37.967: INFO: Got endpoints: latency-svc-2fg82 [750.317773ms]
Mar 22 14:06:37.980: INFO: Created: latency-svc-trdpd
Mar 22 14:06:38.016: INFO: Got endpoints: latency-svc-f7mq2 [747.998599ms]
Mar 22 14:06:38.027: INFO: Created: latency-svc-nkwvq
Mar 22 14:06:38.065: INFO: Got endpoints: latency-svc-w2szv [746.18488ms]
Mar 22 14:06:38.094: INFO: Created: latency-svc-p9gjr
Mar 22 14:06:38.116: INFO: Got endpoints: latency-svc-8z4lh [744.487445ms]
Mar 22 14:06:38.127: INFO: Created: latency-svc-l75gx
Mar 22 14:06:38.166: INFO: Got endpoints: latency-svc-hxwq9 [745.228996ms]
Mar 22 14:06:38.184: INFO: Created: latency-svc-wrzwv
Mar 22 14:06:38.220: INFO: Got endpoints: latency-svc-9qprq [752.647344ms]
Mar 22 14:06:38.235: INFO: Created: latency-svc-x4fd5
Mar 22 14:06:38.267: INFO: Got endpoints: latency-svc-8wk8n [751.983407ms]
Mar 22 14:06:38.301: INFO: Created: latency-svc-bb2d2
Mar 22 14:06:38.317: INFO: Got endpoints: latency-svc-wnkdh [751.186532ms]
Mar 22 14:06:38.332: INFO: Created: latency-svc-vxrgj
Mar 22 14:06:38.396: INFO: Got endpoints: latency-svc-9kc5s [779.014804ms]
Mar 22 14:06:38.418: INFO: Created: latency-svc-xxbcx
Mar 22 14:06:38.420: INFO: Got endpoints: latency-svc-wwc6c [753.210139ms]
Mar 22 14:06:38.438: INFO: Created: latency-svc-n4v64
Mar 22 14:06:38.468: INFO: Got endpoints: latency-svc-w9xgm [752.261756ms]
Mar 22 14:06:38.489: INFO: Created: latency-svc-rp7q4
Mar 22 14:06:38.520: INFO: Got endpoints: latency-svc-g4pwr [755.283583ms]
Mar 22 14:06:38.536: INFO: Created: latency-svc-btfxw
Mar 22 14:06:38.572: INFO: Got endpoints: latency-svc-qqpfz [747.793218ms]
Mar 22 14:06:38.595: INFO: Created: latency-svc-5k5r6
Mar 22 14:06:38.619: INFO: Got endpoints: latency-svc-kfp86 [754.153295ms]
Mar 22 14:06:38.636: INFO: Created: latency-svc-f6nc5
Mar 22 14:06:38.674: INFO: Got endpoints: latency-svc-9qrtd [758.658271ms]
Mar 22 14:06:38.716: INFO: Created: latency-svc-9lx5b
Mar 22 14:06:38.723: INFO: Got endpoints: latency-svc-trdpd [756.072114ms]
Mar 22 14:06:38.754: INFO: Created: latency-svc-8mqdt
Mar 22 14:06:38.768: INFO: Got endpoints: latency-svc-nkwvq [752.191154ms]
Mar 22 14:06:38.808: INFO: Created: latency-svc-4d8fq
Mar 22 14:06:38.846: INFO: Got endpoints: latency-svc-p9gjr [781.233275ms]
Mar 22 14:06:38.868: INFO: Created: latency-svc-9kxcb
Mar 22 14:06:38.870: INFO: Got endpoints: latency-svc-l75gx [753.75305ms]
Mar 22 14:06:38.904: INFO: Created: latency-svc-zdjlw
Mar 22 14:06:38.916: INFO: Got endpoints: latency-svc-wrzwv [750.568706ms]
Mar 22 14:06:38.930: INFO: Created: latency-svc-dl29x
Mar 22 14:06:38.967: INFO: Got endpoints: latency-svc-x4fd5 [746.710488ms]
Mar 22 14:06:38.992: INFO: Created: latency-svc-fh96j
Mar 22 14:06:39.026: INFO: Got endpoints: latency-svc-bb2d2 [758.956314ms]
Mar 22 14:06:39.043: INFO: Created: latency-svc-f789t
Mar 22 14:06:39.066: INFO: Got endpoints: latency-svc-vxrgj [748.440725ms]
Mar 22 14:06:39.091: INFO: Created: latency-svc-58sdn
Mar 22 14:06:39.124: INFO: Got endpoints: latency-svc-xxbcx [727.904177ms]
Mar 22 14:06:39.142: INFO: Created: latency-svc-tchfn
Mar 22 14:06:39.170: INFO: Got endpoints: latency-svc-n4v64 [750.043273ms]
Mar 22 14:06:39.191: INFO: Created: latency-svc-tmcjx
Mar 22 14:06:39.218: INFO: Got endpoints: latency-svc-rp7q4 [749.899057ms]
Mar 22 14:06:39.233: INFO: Created: latency-svc-dxtw8
Mar 22 14:06:39.266: INFO: Got endpoints: latency-svc-btfxw [746.056169ms]
Mar 22 14:06:39.280: INFO: Created: latency-svc-t6ccd
Mar 22 14:06:39.317: INFO: Got endpoints: latency-svc-5k5r6 [744.430796ms]
Mar 22 14:06:39.330: INFO: Created: latency-svc-b4sd7
Mar 22 14:06:39.365: INFO: Got endpoints: latency-svc-f6nc5 [745.294926ms]
Mar 22 14:06:39.376: INFO: Created: latency-svc-8tg8s
Mar 22 14:06:39.418: INFO: Got endpoints: latency-svc-9lx5b [743.473695ms]
Mar 22 14:06:39.430: INFO: Created: latency-svc-pj7g7
Mar 22 14:06:39.486: INFO: Got endpoints: latency-svc-8mqdt [763.330833ms]
Mar 22 14:06:39.505: INFO: Created: latency-svc-zgrjl
Mar 22 14:06:39.524: INFO: Got endpoints: latency-svc-4d8fq [755.376015ms]
Mar 22 14:06:39.543: INFO: Created: latency-svc-sr4zh
Mar 22 14:06:39.569: INFO: Got endpoints: latency-svc-9kxcb [722.585765ms]
Mar 22 14:06:39.590: INFO: Created: latency-svc-kzmwl
Mar 22 14:06:39.622: INFO: Got endpoints: latency-svc-zdjlw [751.839121ms]
Mar 22 14:06:39.643: INFO: Created: latency-svc-wn26m
Mar 22 14:06:39.669: INFO: Got endpoints: latency-svc-dl29x [752.609671ms]
Mar 22 14:06:39.691: INFO: Created: latency-svc-6jjm4
Mar 22 14:06:39.715: INFO: Got endpoints: latency-svc-fh96j [748.821558ms]
Mar 22 14:06:39.731: INFO: Created: latency-svc-frpc4
Mar 22 14:06:39.770: INFO: Got endpoints: latency-svc-f789t [744.366864ms]
Mar 22 14:06:39.797: INFO: Created: latency-svc-pkt7n
Mar 22 14:06:39.817: INFO: Got endpoints: latency-svc-58sdn [751.109503ms]
Mar 22 14:06:39.857: INFO: Created: latency-svc-9hrdl
Mar 22 14:06:39.876: INFO: Got endpoints: latency-svc-tchfn [752.386648ms]
Mar 22 14:06:39.903: INFO: Created: latency-svc-wgdsq
Mar 22 14:06:39.925: INFO: Got endpoints: latency-svc-tmcjx [754.7149ms]
Mar 22 14:06:39.959: INFO: Created: latency-svc-sm7wt
Mar 22 14:06:39.970: INFO: Got endpoints: latency-svc-dxtw8 [752.270578ms]
Mar 22 14:06:40.002: INFO: Created: latency-svc-gcqns
Mar 22 14:06:40.055: INFO: Got endpoints: latency-svc-t6ccd [788.494817ms]
Mar 22 14:06:40.070: INFO: Got endpoints: latency-svc-b4sd7 [753.018393ms]
Mar 22 14:06:40.084: INFO: Created: latency-svc-tkrf5
Mar 22 14:06:40.117: INFO: Got endpoints: latency-svc-8tg8s [752.467215ms]
Mar 22 14:06:40.117: INFO: Created: latency-svc-cqt56
Mar 22 14:06:40.177: INFO: Got endpoints: latency-svc-pj7g7 [759.05093ms]
Mar 22 14:06:40.199: INFO: Created: latency-svc-xll2k
Mar 22 14:06:40.242: INFO: Got endpoints: latency-svc-zgrjl [755.03552ms]
Mar 22 14:06:40.258: INFO: Created: latency-svc-mshvq
Mar 22 14:06:40.268: INFO: Got endpoints: latency-svc-sr4zh [744.686902ms]
Mar 22 14:06:40.278: INFO: Created: latency-svc-vwrqz
Mar 22 14:06:40.293: INFO: Created: latency-svc-4lm8j
Mar 22 14:06:40.361: INFO: Got endpoints: latency-svc-kzmwl [791.661958ms]
Mar 22 14:06:40.366: INFO: Got endpoints: latency-svc-wn26m [744.371697ms]
Mar 22 14:06:40.380: INFO: Created: latency-svc-2cnkz
Mar 22 14:06:40.394: INFO: Created: latency-svc-pww7g
Mar 22 14:06:40.469: INFO: Got endpoints: latency-svc-6jjm4 [800.453997ms]
Mar 22 14:06:40.473: INFO: Got endpoints: latency-svc-frpc4 [757.201482ms]
Mar 22 14:06:40.496: INFO: Created: latency-svc-rbvxx
Mar 22 14:06:40.508: INFO: Created: latency-svc-824zb
Mar 22 14:06:40.517: INFO: Got endpoints: latency-svc-pkt7n [746.847804ms]
Mar 22 14:06:40.530: INFO: Created: latency-svc-2vgkm
Mar 22 14:06:40.567: INFO: Got endpoints: latency-svc-9hrdl [750.273355ms]
Mar 22 14:06:40.617: INFO: Got endpoints: latency-svc-wgdsq [740.513185ms]
Mar 22 14:06:40.673: INFO: Got endpoints: latency-svc-sm7wt [748.158329ms]
Mar 22 14:06:40.719: INFO: Got endpoints: latency-svc-gcqns [748.464025ms]
Mar 22 14:06:40.766: INFO: Got endpoints: latency-svc-tkrf5 [710.823666ms]
Mar 22 14:06:40.816: INFO: Got endpoints: latency-svc-cqt56 [746.6675ms]
Mar 22 14:06:40.867: INFO: Got endpoints: latency-svc-xll2k [749.389219ms]
Mar 22 14:06:40.916: INFO: Got endpoints: latency-svc-mshvq [739.071826ms]
Mar 22 14:06:40.976: INFO: Got endpoints: latency-svc-vwrqz [734.538201ms]
Mar 22 14:06:41.018: INFO: Got endpoints: latency-svc-4lm8j [749.552381ms]
Mar 22 14:06:41.067: INFO: Got endpoints: latency-svc-2cnkz [706.45201ms]
Mar 22 14:06:41.117: INFO: Got endpoints: latency-svc-pww7g [751.032076ms]
Mar 22 14:06:41.173: INFO: Got endpoints: latency-svc-rbvxx [703.056449ms]
Mar 22 14:06:41.216: INFO: Got endpoints: latency-svc-824zb [743.152777ms]
Mar 22 14:06:41.266: INFO: Got endpoints: latency-svc-2vgkm [748.649574ms]
Mar 22 14:06:41.266: INFO: Latencies: [22.12761ms 27.592739ms 37.315787ms 46.800206ms 57.509073ms 68.799922ms 76.107813ms 88.233826ms 96.133659ms 101.358949ms 128.175207ms 138.134772ms 146.762669ms 168.086189ms 168.937897ms 176.310057ms 183.384814ms 183.999009ms 185.895973ms 186.802093ms 189.496574ms 189.657062ms 192.222069ms 192.663918ms 193.032262ms 194.717066ms 194.807748ms 194.808179ms 195.243927ms 195.569714ms 199.267919ms 199.309154ms 200.67921ms 202.428654ms 203.999088ms 204.320691ms 205.258146ms 205.939615ms 206.853168ms 225.405758ms 321.916709ms 326.179217ms 347.232133ms 388.610827ms 424.873136ms 462.965786ms 490.80576ms 512.347818ms 543.505182ms 592.023796ms 638.395965ms 659.263587ms 666.93282ms 675.939464ms 687.475932ms 691.358397ms 697.242936ms 700.054555ms 700.164826ms 700.788662ms 701.319831ms 703.056449ms 706.45201ms 707.529286ms 710.131302ms 710.823666ms 713.99279ms 720.565094ms 722.585765ms 727.904177ms 728.008316ms 730.790324ms 733.123696ms 734.538201ms 735.397897ms 735.704813ms 737.677594ms 737.92239ms 738.215747ms 739.071826ms 740.513185ms 741.506117ms 742.20642ms 742.392914ms 742.751683ms 743.152777ms 743.473695ms 744.329079ms 744.366864ms 744.371697ms 744.430796ms 744.487445ms 744.686902ms 745.210095ms 745.228996ms 745.294926ms 745.540645ms 746.056169ms 746.18488ms 746.6675ms 746.710488ms 746.847804ms 747.448393ms 747.657919ms 747.793218ms 747.998599ms 748.158329ms 748.251693ms 748.440725ms 748.464025ms 748.579481ms 748.649574ms 748.713332ms 748.771028ms 748.821558ms 748.978647ms 748.986992ms 749.070048ms 749.174861ms 749.389219ms 749.463079ms 749.552381ms 749.722665ms 749.899057ms 750.042036ms 750.043273ms 750.05828ms 750.065619ms 750.077623ms 750.178535ms 750.273355ms 750.307994ms 750.317773ms 750.355487ms 750.449587ms 750.568706ms 750.859165ms 750.98896ms 751.022714ms 751.032076ms 751.109503ms 751.186532ms 751.585273ms 751.839121ms 751.983407ms 752.191154ms 752.20735ms 752.261756ms 752.270578ms 752.302955ms 752.386648ms 752.45501ms 752.467215ms 752.609671ms 752.647344ms 752.65437ms 753.018393ms 753.210139ms 753.286943ms 753.75305ms 753.805546ms 754.153295ms 754.432256ms 754.7149ms 755.03552ms 755.283583ms 755.299544ms 755.376015ms 755.931319ms 756.02915ms 756.057456ms 756.072114ms 756.481602ms 756.959813ms 757.201482ms 758.302891ms 758.653036ms 758.658271ms 758.956314ms 759.05093ms 760.153501ms 760.916542ms 763.330833ms 763.915146ms 766.406025ms 767.584629ms 779.014804ms 780.137637ms 781.233275ms 788.494817ms 790.30513ms 791.661958ms 800.331371ms 800.453997ms 804.273024ms 813.270526ms 824.2477ms 833.759214ms 839.832538ms 841.186863ms]
Mar 22 14:06:41.266: INFO: 50 %ile: 746.710488ms
Mar 22 14:06:41.266: INFO: 90 %ile: 760.153501ms
Mar 22 14:06:41.266: INFO: 99 %ile: 839.832538ms
Mar 22 14:06:41.266: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:06:41.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-g42lj" for this suite.
Mar 22 14:07:01.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:07:01.466: INFO: namespace: e2e-tests-svc-latency-g42lj, resource: bindings, ignored listing per whitelist
Mar 22 14:07:01.609: INFO: namespace e2e-tests-svc-latency-g42lj deletion completed in 20.334293411s

â€¢ [SLOW TEST:31.347 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:07:01.609: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6zpr5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-c3bc11bb-4cab-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 14:07:01.884: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c3be7c75-4cab-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-6zpr5" to be "success or failure"
Mar 22 14:07:01.893: INFO: Pod "pod-projected-secrets-c3be7c75-4cab-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.003059ms
Mar 22 14:07:03.907: INFO: Pod "pod-projected-secrets-c3be7c75-4cab-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023283166s
STEP: Saw pod success
Mar 22 14:07:03.908: INFO: Pod "pod-projected-secrets-c3be7c75-4cab-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:07:03.914: INFO: Trying to get logs from node metalk8s-04 pod pod-projected-secrets-c3be7c75-4cab-11e9-99ae-9e98f636e47c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 22 14:07:03.958: INFO: Waiting for pod pod-projected-secrets-c3be7c75-4cab-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:07:03.962: INFO: Pod pod-projected-secrets-c3be7c75-4cab-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:07:03.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6zpr5" for this suite.
Mar 22 14:07:09.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:07:10.021: INFO: namespace: e2e-tests-projected-6zpr5, resource: bindings, ignored listing per whitelist
Mar 22 14:07:10.772: INFO: namespace e2e-tests-projected-6zpr5 deletion completed in 6.803131699s

â€¢ [SLOW TEST:9.163 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:07:10.772: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-txmkl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-c934e9f5-4cab-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 14:07:11.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-c935fdb7-4cab-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-configmap-txmkl" to be "success or failure"
Mar 22 14:07:11.056: INFO: Pod "pod-configmaps-c935fdb7-4cab-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.401168ms
Mar 22 14:07:13.071: INFO: Pod "pod-configmaps-c935fdb7-4cab-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020964236s
STEP: Saw pod success
Mar 22 14:07:13.071: INFO: Pod "pod-configmaps-c935fdb7-4cab-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:07:13.075: INFO: Trying to get logs from node metalk8s-05 pod pod-configmaps-c935fdb7-4cab-11e9-99ae-9e98f636e47c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 14:07:13.104: INFO: Waiting for pod pod-configmaps-c935fdb7-4cab-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:07:13.110: INFO: Pod pod-configmaps-c935fdb7-4cab-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:07:13.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-txmkl" for this suite.
Mar 22 14:07:19.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:07:19.258: INFO: namespace: e2e-tests-configmap-txmkl, resource: bindings, ignored listing per whitelist
Mar 22 14:07:19.332: INFO: namespace e2e-tests-configmap-txmkl deletion completed in 6.211591111s

â€¢ [SLOW TEST:8.560 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:07:19.333: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-zgp4s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 14:07:19.578: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
Mar 22 14:07:19.589: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-zgp4s/daemonsets","resourceVersion":"27443"},"items":null}

Mar 22 14:07:19.612: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-zgp4s/pods","resourceVersion":"27443"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:07:19.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-zgp4s" for this suite.
Mar 22 14:07:25.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:07:25.897: INFO: namespace: e2e-tests-daemonsets-zgp4s, resource: bindings, ignored listing per whitelist
Mar 22 14:07:25.909: INFO: namespace e2e-tests-daemonsets-zgp4s deletion completed in 6.236591386s

S [SKIPPING] [6.576 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Mar 22 14:07:19.578: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:07:25.909: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-kkfw7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 22 14:07:26.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-kkfw7'
Mar 22 14:07:26.399: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar 22 14:07:26.399: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar 22 14:07:26.410: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-thbc7]
Mar 22 14:07:26.411: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-thbc7" in namespace "e2e-tests-kubectl-kkfw7" to be "running and ready"
Mar 22 14:07:26.416: INFO: Pod "e2e-test-nginx-rc-thbc7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.332202ms
Mar 22 14:07:28.422: INFO: Pod "e2e-test-nginx-rc-thbc7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01177615s
Mar 22 14:07:28.422: INFO: Pod "e2e-test-nginx-rc-thbc7" satisfied condition "running and ready"
Mar 22 14:07:28.422: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-thbc7]
Mar 22 14:07:28.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-kkfw7'
Mar 22 14:07:28.558: INFO: stderr: ""
Mar 22 14:07:28.558: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
Mar 22 14:07:28.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-kkfw7'
Mar 22 14:07:28.706: INFO: stderr: ""
Mar 22 14:07:28.706: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:07:28.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kkfw7" for this suite.
Mar 22 14:07:52.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:07:52.817: INFO: namespace: e2e-tests-kubectl-kkfw7, resource: bindings, ignored listing per whitelist
Mar 22 14:07:52.923: INFO: namespace e2e-tests-kubectl-kkfw7 deletion completed in 24.209858699s

â€¢ [SLOW TEST:27.014 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:07:52.923: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-nkgs9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 22 14:07:53.185: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-nkgs9,SelfLink:/api/v1/namespaces/e2e-tests-watch-nkgs9/configmaps/e2e-watch-test-watch-closed,UID:e2540ef7-4cab-11e9-b0fd-fa163e8e51f5,ResourceVersion:27568,Generation:0,CreationTimestamp:2019-03-22 14:07:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 22 14:07:53.185: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-nkgs9,SelfLink:/api/v1/namespaces/e2e-tests-watch-nkgs9/configmaps/e2e-watch-test-watch-closed,UID:e2540ef7-4cab-11e9-b0fd-fa163e8e51f5,ResourceVersion:27569,Generation:0,CreationTimestamp:2019-03-22 14:07:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 22 14:07:53.212: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-nkgs9,SelfLink:/api/v1/namespaces/e2e-tests-watch-nkgs9/configmaps/e2e-watch-test-watch-closed,UID:e2540ef7-4cab-11e9-b0fd-fa163e8e51f5,ResourceVersion:27570,Generation:0,CreationTimestamp:2019-03-22 14:07:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 22 14:07:53.212: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-nkgs9,SelfLink:/api/v1/namespaces/e2e-tests-watch-nkgs9/configmaps/e2e-watch-test-watch-closed,UID:e2540ef7-4cab-11e9-b0fd-fa163e8e51f5,ResourceVersion:27571,Generation:0,CreationTimestamp:2019-03-22 14:07:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:07:53.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-nkgs9" for this suite.
Mar 22 14:07:59.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:07:59.471: INFO: namespace: e2e-tests-watch-nkgs9, resource: bindings, ignored listing per whitelist
Mar 22 14:07:59.551: INFO: namespace e2e-tests-watch-nkgs9 deletion completed in 6.328897411s

â€¢ [SLOW TEST:6.628 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:07:59.551: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qqs78
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:07:59.907: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6526bd2-4cab-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-qqs78" to be "success or failure"
Mar 22 14:07:59.934: INFO: Pod "downwardapi-volume-e6526bd2-4cab-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 27.460575ms
Mar 22 14:08:01.953: INFO: Pod "downwardapi-volume-e6526bd2-4cab-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045806855s
STEP: Saw pod success
Mar 22 14:08:01.953: INFO: Pod "downwardapi-volume-e6526bd2-4cab-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:08:01.963: INFO: Trying to get logs from node metalk8s-01 pod downwardapi-volume-e6526bd2-4cab-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:08:02.059: INFO: Waiting for pod downwardapi-volume-e6526bd2-4cab-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:08:02.069: INFO: Pod downwardapi-volume-e6526bd2-4cab-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:08:02.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qqs78" for this suite.
Mar 22 14:08:08.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:08:08.137: INFO: namespace: e2e-tests-projected-qqs78, resource: bindings, ignored listing per whitelist
Mar 22 14:08:08.335: INFO: namespace e2e-tests-projected-qqs78 deletion completed in 6.246607353s

â€¢ [SLOW TEST:8.783 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:08:08.335: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-pqwf6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0322 14:08:18.744220      20 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 22 14:08:18.744: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:08:18.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-pqwf6" for this suite.
Mar 22 14:08:26.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:08:26.899: INFO: namespace: e2e-tests-gc-pqwf6, resource: bindings, ignored listing per whitelist
Mar 22 14:08:26.942: INFO: namespace e2e-tests-gc-pqwf6 deletion completed in 8.190243625s

â€¢ [SLOW TEST:18.607 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:08:26.942: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-zrrdr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-f6972791-4cab-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 14:08:27.200: INFO: Waiting up to 5m0s for pod "pod-configmaps-f699b0ea-4cab-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-configmap-zrrdr" to be "success or failure"
Mar 22 14:08:27.206: INFO: Pod "pod-configmaps-f699b0ea-4cab-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.088459ms
Mar 22 14:08:29.220: INFO: Pod "pod-configmaps-f699b0ea-4cab-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019643781s
STEP: Saw pod success
Mar 22 14:08:29.220: INFO: Pod "pod-configmaps-f699b0ea-4cab-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:08:29.232: INFO: Trying to get logs from node metalk8s-02 pod pod-configmaps-f699b0ea-4cab-11e9-99ae-9e98f636e47c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 14:08:29.272: INFO: Waiting for pod pod-configmaps-f699b0ea-4cab-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:08:29.279: INFO: Pod pod-configmaps-f699b0ea-4cab-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:08:29.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zrrdr" for this suite.
Mar 22 14:08:35.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:08:35.480: INFO: namespace: e2e-tests-configmap-zrrdr, resource: bindings, ignored listing per whitelist
Mar 22 14:08:35.504: INFO: namespace e2e-tests-configmap-zrrdr deletion completed in 6.215673144s

â€¢ [SLOW TEST:8.562 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:08:35.504: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-8k2vl
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-fbb23b7f-4cab-11e9-99ae-9e98f636e47c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-fbb23b7f-4cab-11e9-99ae-9e98f636e47c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:08:39.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8k2vl" for this suite.
Mar 22 14:09:03.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:09:04.086: INFO: namespace: e2e-tests-configmap-8k2vl, resource: bindings, ignored listing per whitelist
Mar 22 14:09:04.089: INFO: namespace e2e-tests-configmap-8k2vl deletion completed in 24.254145469s

â€¢ [SLOW TEST:28.585 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:09:04.090: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-h5zbv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-h5zbv A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-h5zbv;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-h5zbv A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-h5zbv;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-h5zbv.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-h5zbv.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-h5zbv.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-h5zbv.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-h5zbv.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-h5zbv.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 24.5.233.10.in-addr.arpa. PTR)" && echo OK > /results/10.233.5.24_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 24.5.233.10.in-addr.arpa. PTR)" && echo OK > /results/10.233.5.24_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-h5zbv A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-h5zbv;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-h5zbv A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-h5zbv;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-h5zbv.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-h5zbv.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-h5zbv.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-h5zbv.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-h5zbv.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-h5zbv.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-h5zbv.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 24.5.233.10.in-addr.arpa. PTR)" && echo OK > /results/10.233.5.24_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 24.5.233.10.in-addr.arpa. PTR)" && echo OK > /results/10.233.5.24_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 22 14:09:34.389: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.397: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.413: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-h5zbv from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.433: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-h5zbv.svc from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.442: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.454: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.503: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.525: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.533: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-h5zbv from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.540: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-h5zbv from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.547: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-h5zbv.svc from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.567: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-h5zbv.svc from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.574: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.580: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc from pod e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c: the server could not find the requested resource (get pods dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c)
Mar 22 14:09:34.617: INFO: Lookups using e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-h5zbv wheezy_tcp@dns-test-service.e2e-tests-dns-h5zbv.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-h5zbv jessie_tcp@dns-test-service.e2e-tests-dns-h5zbv jessie_udp@dns-test-service.e2e-tests-dns-h5zbv.svc jessie_tcp@dns-test-service.e2e-tests-dns-h5zbv.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-h5zbv.svc]

Mar 22 14:09:44.654: INFO: DNS probes using e2e-tests-dns-h5zbv/dns-test-0cbba006-4cac-11e9-99ae-9e98f636e47c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:09:44.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-h5zbv" for this suite.
Mar 22 14:09:50.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:09:51.110: INFO: namespace: e2e-tests-dns-h5zbv, resource: bindings, ignored listing per whitelist
Mar 22 14:09:51.261: INFO: namespace e2e-tests-dns-h5zbv deletion completed in 6.433164159s

â€¢ [SLOW TEST:47.171 seconds]
[sig-network] DNS
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:09:51.261: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ww66f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:09:51.650: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28edc754-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-ww66f" to be "success or failure"
Mar 22 14:09:51.657: INFO: Pod "downwardapi-volume-28edc754-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.658613ms
Mar 22 14:09:53.663: INFO: Pod "downwardapi-volume-28edc754-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012364136s
Mar 22 14:09:55.676: INFO: Pod "downwardapi-volume-28edc754-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025600908s
STEP: Saw pod success
Mar 22 14:09:55.676: INFO: Pod "downwardapi-volume-28edc754-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:09:55.681: INFO: Trying to get logs from node metalk8s-05 pod downwardapi-volume-28edc754-4cac-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:09:55.715: INFO: Waiting for pod downwardapi-volume-28edc754-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:09:55.720: INFO: Pod downwardapi-volume-28edc754-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:09:55.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ww66f" for this suite.
Mar 22 14:10:01.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:10:02.023: INFO: namespace: e2e-tests-projected-ww66f, resource: bindings, ignored listing per whitelist
Mar 22 14:10:02.601: INFO: namespace e2e-tests-projected-ww66f deletion completed in 6.873206264s

â€¢ [SLOW TEST:11.339 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:10:02.601: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mxb6w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Mar 22 14:10:03.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 api-versions'
Mar 22 14:10:03.572: INFO: stderr: ""
Mar 22 14:10:03.572: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:10:03.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mxb6w" for this suite.
Mar 22 14:10:09.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:10:09.823: INFO: namespace: e2e-tests-kubectl-mxb6w, resource: bindings, ignored listing per whitelist
Mar 22 14:10:09.919: INFO: namespace e2e-tests-kubectl-mxb6w deletion completed in 6.321734659s

â€¢ [SLOW TEST:7.318 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:10:09.919: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-g5znt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:10:10.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34064bac-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-g5znt" to be "success or failure"
Mar 22 14:10:10.273: INFO: Pod "downwardapi-volume-34064bac-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.187164ms
Mar 22 14:10:12.279: INFO: Pod "downwardapi-volume-34064bac-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012674753s
Mar 22 14:10:14.287: INFO: Pod "downwardapi-volume-34064bac-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020316519s
STEP: Saw pod success
Mar 22 14:10:14.287: INFO: Pod "downwardapi-volume-34064bac-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:10:14.291: INFO: Trying to get logs from node metalk8s-02 pod downwardapi-volume-34064bac-4cac-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:10:14.322: INFO: Waiting for pod downwardapi-volume-34064bac-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:10:14.326: INFO: Pod downwardapi-volume-34064bac-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:10:14.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-g5znt" for this suite.
Mar 22 14:10:20.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:10:20.473: INFO: namespace: e2e-tests-projected-g5znt, resource: bindings, ignored listing per whitelist
Mar 22 14:10:20.565: INFO: namespace e2e-tests-projected-g5znt deletion completed in 6.230405481s

â€¢ [SLOW TEST:10.646 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:10:20.565: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-gjl67
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-gjl67
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-gjl67
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-gjl67
Mar 22 14:10:20.809: INFO: Found 0 stateful pods, waiting for 1
Mar 22 14:10:30.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 22 14:10:30.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-gjl67 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:10:31.089: INFO: stderr: ""
Mar 22 14:10:31.089: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:10:31.089: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 22 14:10:31.097: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 22 14:10:41.115: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 14:10:41.115: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 14:10:41.153: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:10:41.153: INFO: ss-0  metalk8s-01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  }]
Mar 22 14:10:41.153: INFO: 
Mar 22 14:10:41.153: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 22 14:10:42.161: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989825049s
Mar 22 14:10:43.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983223946s
Mar 22 14:10:44.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976771805s
Mar 22 14:10:45.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969566509s
Mar 22 14:10:46.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963134234s
Mar 22 14:10:47.205: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.955176283s
Mar 22 14:10:48.210: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.939782931s
Mar 22 14:10:49.216: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.934538932s
Mar 22 14:10:50.225: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.235669ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-gjl67
Mar 22 14:10:51.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-gjl67 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 22 14:10:51.594: INFO: stderr: ""
Mar 22 14:10:51.594: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 22 14:10:51.594: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 22 14:10:51.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-gjl67 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 22 14:10:51.914: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar 22 14:10:51.914: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 22 14:10:51.914: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 22 14:10:51.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-gjl67 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 22 14:10:52.139: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Mar 22 14:10:52.139: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 22 14:10:52.139: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 22 14:10:52.144: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 22 14:11:02.159: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 14:11:02.159: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 14:11:02.159: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 22 14:11:02.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-gjl67 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:11:02.501: INFO: stderr: ""
Mar 22 14:11:02.501: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:11:02.501: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 22 14:11:02.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-gjl67 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:11:02.908: INFO: stderr: ""
Mar 22 14:11:02.908: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:11:02.908: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 22 14:11:02.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-gjl67 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:11:03.234: INFO: stderr: ""
Mar 22 14:11:03.234: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:11:03.234: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 22 14:11:03.234: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 14:11:03.238: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 22 14:11:13.260: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 14:11:13.260: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 14:11:13.260: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 14:11:13.276: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:11:13.276: INFO: ss-0  metalk8s-01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  }]
Mar 22 14:11:13.276: INFO: ss-1  metalk8s-04  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:13.276: INFO: ss-2  metalk8s-05  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:13.277: INFO: 
Mar 22 14:11:13.277: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 22 14:11:14.286: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:11:14.286: INFO: ss-0  metalk8s-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  }]
Mar 22 14:11:14.286: INFO: ss-1  metalk8s-04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:14.286: INFO: ss-2  metalk8s-05  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:14.286: INFO: 
Mar 22 14:11:14.286: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 22 14:11:15.294: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:11:15.294: INFO: ss-0  metalk8s-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  }]
Mar 22 14:11:15.294: INFO: ss-1  metalk8s-04  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:15.294: INFO: ss-2  metalk8s-05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:15.294: INFO: 
Mar 22 14:11:15.294: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 22 14:11:16.299: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:11:16.299: INFO: ss-0  metalk8s-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  }]
Mar 22 14:11:16.299: INFO: ss-2  metalk8s-05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:16.299: INFO: 
Mar 22 14:11:16.299: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 22 14:11:17.311: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:11:17.311: INFO: ss-0  metalk8s-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  }]
Mar 22 14:11:17.311: INFO: ss-2  metalk8s-05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:17.311: INFO: 
Mar 22 14:11:17.311: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 22 14:11:18.323: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:11:18.323: INFO: ss-0  metalk8s-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  }]
Mar 22 14:11:18.323: INFO: ss-2  metalk8s-05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:18.323: INFO: 
Mar 22 14:11:18.323: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 22 14:11:19.331: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:11:19.331: INFO: ss-0  metalk8s-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  }]
Mar 22 14:11:19.331: INFO: ss-2  metalk8s-05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:19.331: INFO: 
Mar 22 14:11:19.331: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 22 14:11:20.337: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:11:20.337: INFO: ss-0  metalk8s-01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:20 +0000 UTC  }]
Mar 22 14:11:20.337: INFO: ss-2  metalk8s-05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:20.337: INFO: 
Mar 22 14:11:20.337: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 22 14:11:21.365: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 22 14:11:21.365: INFO: ss-2  metalk8s-05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:11:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:10:41 +0000 UTC  }]
Mar 22 14:11:21.365: INFO: 
Mar 22 14:11:21.365: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 22 14:11:22.371: INFO: Verifying statefulset ss doesn't scale past 0 for another 905.41726ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-gjl67
Mar 22 14:11:23.385: INFO: Scaling statefulset ss to 0
Mar 22 14:11:23.399: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 22 14:11:23.404: INFO: Deleting all statefulset in ns e2e-tests-statefulset-gjl67
Mar 22 14:11:23.409: INFO: Scaling statefulset ss to 0
Mar 22 14:11:23.424: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 14:11:23.429: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:11:23.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-gjl67" for this suite.
Mar 22 14:11:29.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:11:29.508: INFO: namespace: e2e-tests-statefulset-gjl67, resource: bindings, ignored listing per whitelist
Mar 22 14:11:29.749: INFO: namespace e2e-tests-statefulset-gjl67 deletion completed in 6.290387491s

â€¢ [SLOW TEST:69.184 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:11:29.750: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-p2p7q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-63a97925-4cac-11e9-99ae-9e98f636e47c
STEP: Creating secret with name secret-projected-all-test-volume-63a9790b-4cac-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 22 14:11:30.267: INFO: Waiting up to 5m0s for pod "projected-volume-63a978c7-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-p2p7q" to be "success or failure"
Mar 22 14:11:30.272: INFO: Pod "projected-volume-63a978c7-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.767784ms
Mar 22 14:11:32.277: INFO: Pod "projected-volume-63a978c7-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010287754s
STEP: Saw pod success
Mar 22 14:11:32.277: INFO: Pod "projected-volume-63a978c7-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:11:32.284: INFO: Trying to get logs from node metalk8s-02 pod projected-volume-63a978c7-4cac-11e9-99ae-9e98f636e47c container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 22 14:11:32.319: INFO: Waiting for pod projected-volume-63a978c7-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:11:32.325: INFO: Pod projected-volume-63a978c7-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:11:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p2p7q" for this suite.
Mar 22 14:11:38.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:11:38.420: INFO: namespace: e2e-tests-projected-p2p7q, resource: bindings, ignored listing per whitelist
Mar 22 14:11:38.647: INFO: namespace e2e-tests-projected-p2p7q deletion completed in 6.312904185s

â€¢ [SLOW TEST:8.897 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:11:38.647: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4s5xz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar 22 14:11:41.527: INFO: Successfully updated pod "labelsupdate68e630aa-4cac-11e9-99ae-9e98f636e47c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:11:43.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4s5xz" for this suite.
Mar 22 14:12:05.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:12:05.673: INFO: namespace: e2e-tests-downward-api-4s5xz, resource: bindings, ignored listing per whitelist
Mar 22 14:12:05.844: INFO: namespace e2e-tests-downward-api-4s5xz deletion completed in 22.284876515s

â€¢ [SLOW TEST:27.197 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:12:05.845: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-cfczf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 14:12:06.287: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7929dba0-4cac-11e9-b0fd-fa163e8e51f5", Controller:(*bool)(0xc42286c462), BlockOwnerDeletion:(*bool)(0xc42286c463)}}
Mar 22 14:12:06.308: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"792405fd-4cac-11e9-b0fd-fa163e8e51f5", Controller:(*bool)(0xc4228d300e), BlockOwnerDeletion:(*bool)(0xc4228d300f)}}
Mar 22 14:12:06.317: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"79264e0f-4cac-11e9-b0fd-fa163e8e51f5", Controller:(*bool)(0xc4230d53ae), BlockOwnerDeletion:(*bool)(0xc4230d53af)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:12:11.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-cfczf" for this suite.
Mar 22 14:12:17.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:12:17.456: INFO: namespace: e2e-tests-gc-cfczf, resource: bindings, ignored listing per whitelist
Mar 22 14:12:17.630: INFO: namespace e2e-tests-gc-cfczf deletion completed in 6.232285253s

â€¢ [SLOW TEST:11.785 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:12:17.630: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-nkqvt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 22 14:12:23.920: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:23.925: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 14:12:25.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:25.934: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 14:12:27.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:27.933: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 14:12:29.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:29.939: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 14:12:31.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:31.945: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 14:12:33.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:33.932: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 14:12:35.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:35.934: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 14:12:37.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:37.931: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 14:12:39.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:39.932: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 14:12:41.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 14:12:41.937: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:12:41.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-nkqvt" for this suite.
Mar 22 14:13:06.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:13:06.188: INFO: namespace: e2e-tests-container-lifecycle-hook-nkqvt, resource: bindings, ignored listing per whitelist
Mar 22 14:13:06.396: INFO: namespace e2e-tests-container-lifecycle-hook-nkqvt deletion completed in 24.420584725s

â€¢ [SLOW TEST:48.766 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:13:06.396: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hb5kz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:13:06.789: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d3ec9c0-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-hb5kz" to be "success or failure"
Mar 22 14:13:06.797: INFO: Pod "downwardapi-volume-9d3ec9c0-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049421ms
Mar 22 14:13:08.805: INFO: Pod "downwardapi-volume-9d3ec9c0-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01618009s
STEP: Saw pod success
Mar 22 14:13:08.806: INFO: Pod "downwardapi-volume-9d3ec9c0-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:13:08.812: INFO: Trying to get logs from node metalk8s-05 pod downwardapi-volume-9d3ec9c0-4cac-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:13:08.858: INFO: Waiting for pod downwardapi-volume-9d3ec9c0-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:13:08.865: INFO: Pod downwardapi-volume-9d3ec9c0-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:13:08.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hb5kz" for this suite.
Mar 22 14:13:14.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:13:14.945: INFO: namespace: e2e-tests-projected-hb5kz, resource: bindings, ignored listing per whitelist
Mar 22 14:13:15.187: INFO: namespace e2e-tests-projected-hb5kz deletion completed in 6.306268447s

â€¢ [SLOW TEST:8.791 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:13:15.188: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-gjm99
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-a27d6574-4cac-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 14:13:15.597: INFO: Waiting up to 5m0s for pod "pod-configmaps-a27e7b72-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-configmap-gjm99" to be "success or failure"
Mar 22 14:13:15.617: INFO: Pod "pod-configmaps-a27e7b72-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.851964ms
Mar 22 14:13:17.632: INFO: Pod "pod-configmaps-a27e7b72-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034419098s
Mar 22 14:13:19.642: INFO: Pod "pod-configmaps-a27e7b72-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044467068s
STEP: Saw pod success
Mar 22 14:13:19.642: INFO: Pod "pod-configmaps-a27e7b72-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:13:19.670: INFO: Trying to get logs from node metalk8s-01 pod pod-configmaps-a27e7b72-4cac-11e9-99ae-9e98f636e47c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 14:13:19.744: INFO: Waiting for pod pod-configmaps-a27e7b72-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:13:19.754: INFO: Pod pod-configmaps-a27e7b72-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:13:19.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gjm99" for this suite.
Mar 22 14:13:27.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:13:27.887: INFO: namespace: e2e-tests-configmap-gjm99, resource: bindings, ignored listing per whitelist
Mar 22 14:13:28.273: INFO: namespace e2e-tests-configmap-gjm99 deletion completed in 8.504514548s

â€¢ [SLOW TEST:13.085 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:13:28.273: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6x2v2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-aa44059d-4cac-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 14:13:28.644: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aa4581a0-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-6x2v2" to be "success or failure"
Mar 22 14:13:28.659: INFO: Pod "pod-projected-configmaps-aa4581a0-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.26379ms
Mar 22 14:13:30.666: INFO: Pod "pod-projected-configmaps-aa4581a0-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022245327s
Mar 22 14:13:32.672: INFO: Pod "pod-projected-configmaps-aa4581a0-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028257858s
STEP: Saw pod success
Mar 22 14:13:32.672: INFO: Pod "pod-projected-configmaps-aa4581a0-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:13:32.676: INFO: Trying to get logs from node metalk8s-01 pod pod-projected-configmaps-aa4581a0-4cac-11e9-99ae-9e98f636e47c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 14:13:32.708: INFO: Waiting for pod pod-projected-configmaps-aa4581a0-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:13:32.714: INFO: Pod pod-projected-configmaps-aa4581a0-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:13:32.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6x2v2" for this suite.
Mar 22 14:13:38.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:13:39.037: INFO: namespace: e2e-tests-projected-6x2v2, resource: bindings, ignored listing per whitelist
Mar 22 14:13:39.205: INFO: namespace e2e-tests-projected-6x2v2 deletion completed in 6.477294736s

â€¢ [SLOW TEST:10.932 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:13:39.205: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qmxf2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-b0bde064-4cac-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 14:13:39.512: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b0bfea66-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-qmxf2" to be "success or failure"
Mar 22 14:13:39.520: INFO: Pod "pod-projected-configmaps-b0bfea66-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024627ms
Mar 22 14:13:41.526: INFO: Pod "pod-projected-configmaps-b0bfea66-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014254728s
Mar 22 14:13:43.532: INFO: Pod "pod-projected-configmaps-b0bfea66-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019780399s
STEP: Saw pod success
Mar 22 14:13:43.532: INFO: Pod "pod-projected-configmaps-b0bfea66-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:13:43.537: INFO: Trying to get logs from node metalk8s-04 pod pod-projected-configmaps-b0bfea66-4cac-11e9-99ae-9e98f636e47c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 14:13:43.573: INFO: Waiting for pod pod-projected-configmaps-b0bfea66-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:13:43.577: INFO: Pod pod-projected-configmaps-b0bfea66-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:13:43.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qmxf2" for this suite.
Mar 22 14:13:49.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:13:49.671: INFO: namespace: e2e-tests-projected-qmxf2, resource: bindings, ignored listing per whitelist
Mar 22 14:13:49.905: INFO: namespace e2e-tests-projected-qmxf2 deletion completed in 6.316667067s

â€¢ [SLOW TEST:10.700 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:13:49.906: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-z8shs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 22 14:13:50.187: INFO: Waiting up to 5m0s for pod "pod-b71ce3cc-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-z8shs" to be "success or failure"
Mar 22 14:13:50.194: INFO: Pod "pod-b71ce3cc-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.882119ms
Mar 22 14:13:52.200: INFO: Pod "pod-b71ce3cc-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012784742s
Mar 22 14:13:54.206: INFO: Pod "pod-b71ce3cc-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018643038s
Mar 22 14:13:56.211: INFO: Pod "pod-b71ce3cc-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024357115s
STEP: Saw pod success
Mar 22 14:13:56.211: INFO: Pod "pod-b71ce3cc-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:13:56.218: INFO: Trying to get logs from node metalk8s-05 pod pod-b71ce3cc-4cac-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 14:13:56.248: INFO: Waiting for pod pod-b71ce3cc-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:13:56.254: INFO: Pod pod-b71ce3cc-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:13:56.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z8shs" for this suite.
Mar 22 14:14:02.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:14:02.298: INFO: namespace: e2e-tests-emptydir-z8shs, resource: bindings, ignored listing per whitelist
Mar 22 14:14:02.551: INFO: namespace e2e-tests-emptydir-z8shs deletion completed in 6.286182826s

â€¢ [SLOW TEST:12.645 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:14:02.551: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-vf5j9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 22 14:14:02.812: INFO: Waiting up to 5m0s for pod "pod-bea34a20-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-vf5j9" to be "success or failure"
Mar 22 14:14:02.828: INFO: Pod "pod-bea34a20-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.975657ms
Mar 22 14:14:04.834: INFO: Pod "pod-bea34a20-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022608225s
STEP: Saw pod success
Mar 22 14:14:04.834: INFO: Pod "pod-bea34a20-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:14:04.841: INFO: Trying to get logs from node metalk8s-02 pod pod-bea34a20-4cac-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 14:14:04.887: INFO: Waiting for pod pod-bea34a20-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:14:04.898: INFO: Pod pod-bea34a20-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:14:04.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vf5j9" for this suite.
Mar 22 14:14:10.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:14:10.968: INFO: namespace: e2e-tests-emptydir-vf5j9, resource: bindings, ignored listing per whitelist
Mar 22 14:14:11.166: INFO: namespace e2e-tests-emptydir-vf5j9 deletion completed in 6.258992258s

â€¢ [SLOW TEST:8.615 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:14:11.167: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-p8rtl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-c3c94498-4cac-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 14:14:11.476: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c3ccba5c-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-p8rtl" to be "success or failure"
Mar 22 14:14:11.484: INFO: Pod "pod-projected-configmaps-c3ccba5c-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.141864ms
Mar 22 14:14:13.490: INFO: Pod "pod-projected-configmaps-c3ccba5c-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01409523s
STEP: Saw pod success
Mar 22 14:14:13.490: INFO: Pod "pod-projected-configmaps-c3ccba5c-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:14:13.496: INFO: Trying to get logs from node metalk8s-01 pod pod-projected-configmaps-c3ccba5c-4cac-11e9-99ae-9e98f636e47c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 14:14:13.547: INFO: Waiting for pod pod-projected-configmaps-c3ccba5c-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:14:13.555: INFO: Pod pod-projected-configmaps-c3ccba5c-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:14:13.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p8rtl" for this suite.
Mar 22 14:14:19.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:14:19.808: INFO: namespace: e2e-tests-projected-p8rtl, resource: bindings, ignored listing per whitelist
Mar 22 14:14:19.865: INFO: namespace e2e-tests-projected-p8rtl deletion completed in 6.294775279s

â€¢ [SLOW TEST:8.699 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:14:19.865: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-vjjfn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:14:20.187: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8fe41f9-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-vjjfn" to be "success or failure"
Mar 22 14:14:20.193: INFO: Pod "downwardapi-volume-c8fe41f9-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.996474ms
Mar 22 14:14:22.198: INFO: Pod "downwardapi-volume-c8fe41f9-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011215695s
Mar 22 14:14:24.204: INFO: Pod "downwardapi-volume-c8fe41f9-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017363803s
STEP: Saw pod success
Mar 22 14:14:24.204: INFO: Pod "downwardapi-volume-c8fe41f9-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:14:24.209: INFO: Trying to get logs from node metalk8s-04 pod downwardapi-volume-c8fe41f9-4cac-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:14:24.244: INFO: Waiting for pod downwardapi-volume-c8fe41f9-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:14:24.249: INFO: Pod downwardapi-volume-c8fe41f9-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:14:24.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vjjfn" for this suite.
Mar 22 14:14:30.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:14:30.449: INFO: namespace: e2e-tests-downward-api-vjjfn, resource: bindings, ignored listing per whitelist
Mar 22 14:14:30.491: INFO: namespace e2e-tests-downward-api-vjjfn deletion completed in 6.232986517s

â€¢ [SLOW TEST:10.625 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:14:30.491: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-k5cnx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-cf46ff1d-4cac-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 14:14:30.761: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cf485534-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-k5cnx" to be "success or failure"
Mar 22 14:14:30.771: INFO: Pod "pod-projected-configmaps-cf485534-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.338449ms
Mar 22 14:14:32.777: INFO: Pod "pod-projected-configmaps-cf485534-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015349229s
Mar 22 14:14:34.785: INFO: Pod "pod-projected-configmaps-cf485534-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023968983s
STEP: Saw pod success
Mar 22 14:14:34.786: INFO: Pod "pod-projected-configmaps-cf485534-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:14:34.807: INFO: Trying to get logs from node metalk8s-05 pod pod-projected-configmaps-cf485534-4cac-11e9-99ae-9e98f636e47c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 14:14:34.847: INFO: Waiting for pod pod-projected-configmaps-cf485534-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:14:34.853: INFO: Pod pod-projected-configmaps-cf485534-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:14:34.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k5cnx" for this suite.
Mar 22 14:14:42.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:14:42.972: INFO: namespace: e2e-tests-projected-k5cnx, resource: bindings, ignored listing per whitelist
Mar 22 14:14:43.102: INFO: namespace e2e-tests-projected-k5cnx deletion completed in 8.237675747s

â€¢ [SLOW TEST:12.611 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:14:43.102: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-7sc9b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-d6ccd32b-4cac-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 14:14:43.353: INFO: Waiting up to 5m0s for pod "pod-secrets-d6ce0b70-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-secrets-7sc9b" to be "success or failure"
Mar 22 14:14:43.359: INFO: Pod "pod-secrets-d6ce0b70-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.365565ms
Mar 22 14:14:45.366: INFO: Pod "pod-secrets-d6ce0b70-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012978699s
STEP: Saw pod success
Mar 22 14:14:45.366: INFO: Pod "pod-secrets-d6ce0b70-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:14:45.375: INFO: Trying to get logs from node metalk8s-02 pod pod-secrets-d6ce0b70-4cac-11e9-99ae-9e98f636e47c container secret-volume-test: <nil>
STEP: delete the pod
Mar 22 14:14:45.431: INFO: Waiting for pod pod-secrets-d6ce0b70-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:14:45.443: INFO: Pod pod-secrets-d6ce0b70-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:14:45.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7sc9b" for this suite.
Mar 22 14:14:51.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:14:51.664: INFO: namespace: e2e-tests-secrets-7sc9b, resource: bindings, ignored listing per whitelist
Mar 22 14:14:51.851: INFO: namespace e2e-tests-secrets-7sc9b deletion completed in 6.388935542s

â€¢ [SLOW TEST:8.749 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:14:51.851: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-xwz4n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0322 14:15:32.205105      20 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 22 14:15:32.205: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:15:32.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-xwz4n" for this suite.
Mar 22 14:15:40.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:15:40.520: INFO: namespace: e2e-tests-gc-xwz4n, resource: bindings, ignored listing per whitelist
Mar 22 14:15:40.630: INFO: namespace e2e-tests-gc-xwz4n deletion completed in 8.416981579s

â€¢ [SLOW TEST:48.779 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:15:40.631: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-5jb8g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Mar 22 14:15:40.950: INFO: Waiting up to 5m0s for pod "client-containers-f921e48c-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-containers-5jb8g" to be "success or failure"
Mar 22 14:15:40.957: INFO: Pod "client-containers-f921e48c-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.227779ms
Mar 22 14:15:42.977: INFO: Pod "client-containers-f921e48c-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02654984s
STEP: Saw pod success
Mar 22 14:15:42.977: INFO: Pod "client-containers-f921e48c-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:15:42.982: INFO: Trying to get logs from node metalk8s-05 pod client-containers-f921e48c-4cac-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 14:15:43.025: INFO: Waiting for pod client-containers-f921e48c-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:15:43.030: INFO: Pod client-containers-f921e48c-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:15:43.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-5jb8g" for this suite.
Mar 22 14:15:49.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:15:49.300: INFO: namespace: e2e-tests-containers-5jb8g, resource: bindings, ignored listing per whitelist
Mar 22 14:15:49.312: INFO: namespace e2e-tests-containers-5jb8g deletion completed in 6.267225185s

â€¢ [SLOW TEST:8.681 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:15:49.313: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-9sgmh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:15:49.546: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe4045e0-4cac-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-9sgmh" to be "success or failure"
Mar 22 14:15:49.554: INFO: Pod "downwardapi-volume-fe4045e0-4cac-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.255375ms
Mar 22 14:15:51.564: INFO: Pod "downwardapi-volume-fe4045e0-4cac-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017856342s
STEP: Saw pod success
Mar 22 14:15:51.564: INFO: Pod "downwardapi-volume-fe4045e0-4cac-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:15:51.573: INFO: Trying to get logs from node metalk8s-02 pod downwardapi-volume-fe4045e0-4cac-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:15:51.615: INFO: Waiting for pod downwardapi-volume-fe4045e0-4cac-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:15:51.623: INFO: Pod downwardapi-volume-fe4045e0-4cac-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:15:51.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-9sgmh" for this suite.
Mar 22 14:15:57.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:15:57.798: INFO: namespace: e2e-tests-downward-api-9sgmh, resource: bindings, ignored listing per whitelist
Mar 22 14:15:57.876: INFO: namespace e2e-tests-downward-api-9sgmh deletion completed in 6.238182718s

â€¢ [SLOW TEST:8.563 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:15:57.876: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-cbn7h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Mar 22 14:16:00.761: INFO: Successfully updated pod "annotationupdate036851c2-4cad-11e9-99ae-9e98f636e47c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:16:02.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cbn7h" for this suite.
Mar 22 14:16:26.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:16:27.106: INFO: namespace: e2e-tests-downward-api-cbn7h, resource: bindings, ignored listing per whitelist
Mar 22 14:16:27.119: INFO: namespace e2e-tests-downward-api-cbn7h deletion completed in 24.311484735s

â€¢ [SLOW TEST:29.243 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:16:27.119: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xdh4k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Mar 22 14:16:27.349: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar 22 14:16:27.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:16:27.611: INFO: stderr: ""
Mar 22 14:16:27.611: INFO: stdout: "service/redis-slave created\n"
Mar 22 14:16:27.612: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar 22 14:16:27.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:16:27.868: INFO: stderr: ""
Mar 22 14:16:27.868: INFO: stdout: "service/redis-master created\n"
Mar 22 14:16:27.868: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 22 14:16:27.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:16:28.130: INFO: stderr: ""
Mar 22 14:16:28.130: INFO: stdout: "service/frontend created\n"
Mar 22 14:16:28.130: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar 22 14:16:28.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:16:28.310: INFO: stderr: ""
Mar 22 14:16:28.311: INFO: stdout: "deployment.extensions/frontend created\n"
Mar 22 14:16:28.311: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 22 14:16:28.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:16:28.580: INFO: stderr: ""
Mar 22 14:16:28.580: INFO: stdout: "deployment.extensions/redis-master created\n"
Mar 22 14:16:28.580: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar 22 14:16:28.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:16:29.257: INFO: stderr: ""
Mar 22 14:16:29.257: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Mar 22 14:16:29.257: INFO: Waiting for all frontend pods to be Running.
Mar 22 14:16:54.308: INFO: Waiting for frontend to serve content.
Mar 22 14:16:54.347: INFO: Trying to add a new entry to the guestbook.
Mar 22 14:16:54.375: INFO: Verifying that added entry can be retrieved.
Mar 22 14:16:54.416: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

STEP: using delete to clean up resources
Mar 22 14:16:59.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:16:59.642: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 14:16:59.642: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 22 14:16:59.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:16:59.802: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 14:16:59.803: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 22 14:16:59.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:16:59.937: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 14:16:59.937: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 22 14:16:59.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:17:00.057: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 14:17:00.057: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 22 14:17:00.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:17:00.171: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 14:17:00.171: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 22 14:17:00.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xdh4k'
Mar 22 14:17:00.312: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 14:17:00.312: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:17:00.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xdh4k" for this suite.
Mar 22 14:17:40.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:17:40.704: INFO: namespace: e2e-tests-kubectl-xdh4k, resource: bindings, ignored listing per whitelist
Mar 22 14:17:40.796: INFO: namespace e2e-tests-kubectl-xdh4k deletion completed in 40.423201563s

â€¢ [SLOW TEST:73.677 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:17:40.796: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-gwqcm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 22 14:17:41.121: INFO: Waiting up to 5m0s for pod "pod-40c1e3fb-4cad-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-gwqcm" to be "success or failure"
Mar 22 14:17:41.133: INFO: Pod "pod-40c1e3fb-4cad-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.950922ms
Mar 22 14:17:43.143: INFO: Pod "pod-40c1e3fb-4cad-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02204454s
STEP: Saw pod success
Mar 22 14:17:43.143: INFO: Pod "pod-40c1e3fb-4cad-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:17:43.152: INFO: Trying to get logs from node metalk8s-02 pod pod-40c1e3fb-4cad-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 14:17:43.184: INFO: Waiting for pod pod-40c1e3fb-4cad-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:17:43.189: INFO: Pod pod-40c1e3fb-4cad-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:17:43.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gwqcm" for this suite.
Mar 22 14:17:51.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:17:51.506: INFO: namespace: e2e-tests-emptydir-gwqcm, resource: bindings, ignored listing per whitelist
Mar 22 14:17:51.627: INFO: namespace e2e-tests-emptydir-gwqcm deletion completed in 8.425678518s

â€¢ [SLOW TEST:10.831 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:17:51.627: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-gb2h7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:17:51.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4737e7ed-4cad-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-gb2h7" to be "success or failure"
Mar 22 14:17:51.960: INFO: Pod "downwardapi-volume-4737e7ed-4cad-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.123243ms
Mar 22 14:17:53.967: INFO: Pod "downwardapi-volume-4737e7ed-4cad-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012702614s
Mar 22 14:17:55.981: INFO: Pod "downwardapi-volume-4737e7ed-4cad-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027011026s
STEP: Saw pod success
Mar 22 14:17:55.981: INFO: Pod "downwardapi-volume-4737e7ed-4cad-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:17:56.034: INFO: Trying to get logs from node metalk8s-01 pod downwardapi-volume-4737e7ed-4cad-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:17:56.204: INFO: Waiting for pod downwardapi-volume-4737e7ed-4cad-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:17:56.215: INFO: Pod downwardapi-volume-4737e7ed-4cad-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:17:56.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gb2h7" for this suite.
Mar 22 14:18:04.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:18:04.261: INFO: namespace: e2e-tests-downward-api-gb2h7, resource: bindings, ignored listing per whitelist
Mar 22 14:18:04.438: INFO: namespace e2e-tests-downward-api-gb2h7 deletion completed in 8.21170808s

â€¢ [SLOW TEST:12.811 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:18:04.438: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-ffv8j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 14:18:04.680: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 22 14:18:09.687: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 22 14:18:09.687: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 22 14:18:11.692: INFO: Creating deployment "test-rollover-deployment"
Mar 22 14:18:11.724: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 22 14:18:13.745: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 22 14:18:13.756: INFO: Ensure that both replica sets have 1 created replica
Mar 22 14:18:13.765: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 22 14:18:13.775: INFO: Updating deployment test-rollover-deployment
Mar 22 14:18:13.775: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 22 14:18:15.788: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 22 14:18:15.801: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 22 14:18:15.811: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 14:18:15.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861095, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 14:18:17.827: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 14:18:17.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861095, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 14:18:19.825: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 14:18:19.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861095, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 14:18:21.830: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 14:18:21.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861095, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 14:18:23.833: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 14:18:23.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861095, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63688861091, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 14:18:25.822: INFO: 
Mar 22 14:18:25.822: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 22 14:18:25.835: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-ffv8j,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-ffv8j/deployments/test-rollover-deployment,UID:5301d2ed-4cad-11e9-b0fd-fa163e8e51f5,ResourceVersion:30934,Generation:2,CreationTimestamp:2019-03-22 14:18:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-22 14:18:11 +0000 UTC 2019-03-22 14:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-22 14:18:25 +0000 UTC 2019-03-22 14:18:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar 22 14:18:25.850: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-ffv8j,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-ffv8j/replicasets/test-rollover-deployment-5b76ff8c4,UID:543fa1f3-4cad-11e9-b0fd-fa163e8e51f5,ResourceVersion:30924,Generation:2,CreationTimestamp:2019-03-22 14:18:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 5301d2ed-4cad-11e9-b0fd-fa163e8e51f5 0xc421d3e4c7 0xc421d3e4c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 22 14:18:25.850: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 22 14:18:25.850: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-ffv8j,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-ffv8j/replicasets/test-rollover-controller,UID:4ed1b97e-4cad-11e9-b0fd-fa163e8e51f5,ResourceVersion:30933,Generation:2,CreationTimestamp:2019-03-22 14:18:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 5301d2ed-4cad-11e9-b0fd-fa163e8e51f5 0xc421d3e3fe 0xc421d3e3ff}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 22 14:18:25.850: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-ffv8j,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-ffv8j/replicasets/test-rollover-deployment-6975f4fb87,UID:53093c02-4cad-11e9-b0fd-fa163e8e51f5,ResourceVersion:30881,Generation:2,CreationTimestamp:2019-03-22 14:18:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 5301d2ed-4cad-11e9-b0fd-fa163e8e51f5 0xc421d3e587 0xc421d3e588}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 22 14:18:25.858: INFO: Pod "test-rollover-deployment-5b76ff8c4-gmsfb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-gmsfb,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-ffv8j,SelfLink:/api/v1/namespaces/e2e-tests-deployment-ffv8j/pods/test-rollover-deployment-5b76ff8c4-gmsfb,UID:54466e06-4cad-11e9-b0fd-fa163e8e51f5,ResourceVersion:30902,Generation:0,CreationTimestamp:2019-03-22 14:18:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 543fa1f3-4cad-11e9-b0fd-fa163e8e51f5 0xc421d3f0a0 0xc421d3f0a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-mfxm8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mfxm8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-mfxm8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421d3f110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421d3f130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:18:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:18:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:18:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-22 14:18:13 +0000 UTC  }],Message:,Reason:,HostIP:10.200.6.75,PodIP:10.233.99.149,StartTime:2019-03-22 14:18:13 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-22 14:18:15 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://92e93847a2c0d8cefc13da9079b5b001aab17398f4e9200fd0fa23401c0d8211}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:18:25.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-ffv8j" for this suite.
Mar 22 14:18:31.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:18:32.326: INFO: namespace: e2e-tests-deployment-ffv8j, resource: bindings, ignored listing per whitelist
Mar 22 14:18:32.326: INFO: namespace e2e-tests-deployment-ffv8j deletion completed in 6.456176093s

â€¢ [SLOW TEST:27.888 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:18:32.326: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-87j7t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 14:18:32.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 version'
Mar 22 14:18:32.761: INFO: stderr: ""
Mar 22 14:18:32.762: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.1\", GitCommit:\"4ed3216f3ec431b140b1d899130a69fc671678f4\", GitTreeState:\"clean\", BuildDate:\"2018-10-05T16:46:06Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.3\", GitCommit:\"435f92c719f279a3a67808c80521ea17d5715c66\", GitTreeState:\"clean\", BuildDate:\"2018-11-26T12:46:57Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:18:32.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-87j7t" for this suite.
Mar 22 14:18:38.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:18:38.873: INFO: namespace: e2e-tests-kubectl-87j7t, resource: bindings, ignored listing per whitelist
Mar 22 14:18:39.043: INFO: namespace e2e-tests-kubectl-87j7t deletion completed in 6.262200759s

â€¢ [SLOW TEST:6.717 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:18:39.043: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ztrgf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 22 14:18:39.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-ztrgf'
Mar 22 14:18:39.593: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Mar 22 14:18:39.593: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
Mar 22 14:18:43.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-ztrgf'
Mar 22 14:18:43.780: INFO: stderr: ""
Mar 22 14:18:43.780: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:18:43.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ztrgf" for this suite.
Mar 22 14:19:07.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:19:07.845: INFO: namespace: e2e-tests-kubectl-ztrgf, resource: bindings, ignored listing per whitelist
Mar 22 14:19:08.034: INFO: namespace e2e-tests-kubectl-ztrgf deletion completed in 24.238338168s

â€¢ [SLOW TEST:28.991 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:19:08.034: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-j26fz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 22 14:19:12.368: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 22 14:19:12.375: INFO: Pod pod-with-prestop-http-hook still exists
Mar 22 14:19:14.375: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 22 14:19:14.382: INFO: Pod pod-with-prestop-http-hook still exists
Mar 22 14:19:16.375: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 22 14:19:16.381: INFO: Pod pod-with-prestop-http-hook still exists
Mar 22 14:19:18.375: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 22 14:19:18.391: INFO: Pod pod-with-prestop-http-hook still exists
Mar 22 14:19:20.375: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 22 14:19:20.382: INFO: Pod pod-with-prestop-http-hook still exists
Mar 22 14:19:22.375: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 22 14:19:22.384: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:19:22.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-j26fz" for this suite.
Mar 22 14:19:46.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:19:46.544: INFO: namespace: e2e-tests-container-lifecycle-hook-j26fz, resource: bindings, ignored listing per whitelist
Mar 22 14:19:46.631: INFO: namespace e2e-tests-container-lifecycle-hook-j26fz deletion completed in 24.221746352s

â€¢ [SLOW TEST:38.596 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:19:46.631: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-72vg9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Mar 22 14:19:46.850: INFO: Waiting up to 5m0s for pod "var-expansion-8bb41e4d-4cad-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-var-expansion-72vg9" to be "success or failure"
Mar 22 14:19:46.855: INFO: Pod "var-expansion-8bb41e4d-4cad-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.100621ms
Mar 22 14:19:48.865: INFO: Pod "var-expansion-8bb41e4d-4cad-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0148316s
STEP: Saw pod success
Mar 22 14:19:48.865: INFO: Pod "var-expansion-8bb41e4d-4cad-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:19:48.871: INFO: Trying to get logs from node metalk8s-02 pod var-expansion-8bb41e4d-4cad-11e9-99ae-9e98f636e47c container dapi-container: <nil>
STEP: delete the pod
Mar 22 14:19:48.911: INFO: Waiting for pod var-expansion-8bb41e4d-4cad-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:19:48.920: INFO: Pod var-expansion-8bb41e4d-4cad-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:19:48.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-72vg9" for this suite.
Mar 22 14:19:54.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:19:55.120: INFO: namespace: e2e-tests-var-expansion-72vg9, resource: bindings, ignored listing per whitelist
Mar 22 14:19:55.390: INFO: namespace e2e-tests-var-expansion-72vg9 deletion completed in 6.455371439s

â€¢ [SLOW TEST:8.759 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:19:55.390: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2pgmp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar 22 14:19:55.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-2pgmp'
Mar 22 14:19:55.933: INFO: stderr: ""
Mar 22 14:19:55.933: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 22 14:19:56.947: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:19:56.947: INFO: Found 0 / 1
Mar 22 14:19:57.945: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:19:57.945: INFO: Found 1 / 1
Mar 22 14:19:57.945: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 22 14:19:57.954: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:19:57.954: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 22 14:19:57.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 patch pod redis-master-tqjq8 --namespace=e2e-tests-kubectl-2pgmp -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 22 14:19:58.094: INFO: stderr: ""
Mar 22 14:19:58.094: INFO: stdout: "pod/redis-master-tqjq8 patched\n"
STEP: checking annotations
Mar 22 14:19:58.101: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:19:58.101: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:19:58.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2pgmp" for this suite.
Mar 22 14:20:22.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:20:22.290: INFO: namespace: e2e-tests-kubectl-2pgmp, resource: bindings, ignored listing per whitelist
Mar 22 14:20:22.339: INFO: namespace e2e-tests-kubectl-2pgmp deletion completed in 24.22127585s

â€¢ [SLOW TEST:26.949 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:20:22.339: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-tsz4x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 14:20:22.602: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 22 14:20:22.615: INFO: Number of nodes with available pods: 0
Mar 22 14:20:22.615: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 22 14:20:22.647: INFO: Number of nodes with available pods: 0
Mar 22 14:20:22.647: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:23.659: INFO: Number of nodes with available pods: 0
Mar 22 14:20:23.659: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:24.675: INFO: Number of nodes with available pods: 0
Mar 22 14:20:24.676: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:25.653: INFO: Number of nodes with available pods: 1
Mar 22 14:20:25.653: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 22 14:20:25.689: INFO: Number of nodes with available pods: 1
Mar 22 14:20:25.689: INFO: Number of running nodes: 0, number of available pods: 1
Mar 22 14:20:26.695: INFO: Number of nodes with available pods: 0
Mar 22 14:20:26.695: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 22 14:20:26.716: INFO: Number of nodes with available pods: 0
Mar 22 14:20:26.716: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:27.724: INFO: Number of nodes with available pods: 0
Mar 22 14:20:27.724: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:28.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:28.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:29.723: INFO: Number of nodes with available pods: 0
Mar 22 14:20:29.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:30.759: INFO: Number of nodes with available pods: 0
Mar 22 14:20:30.759: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:31.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:31.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:32.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:32.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:33.723: INFO: Number of nodes with available pods: 0
Mar 22 14:20:33.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:34.738: INFO: Number of nodes with available pods: 0
Mar 22 14:20:34.738: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:35.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:35.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:36.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:36.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:37.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:37.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:38.726: INFO: Number of nodes with available pods: 0
Mar 22 14:20:38.726: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:39.723: INFO: Number of nodes with available pods: 0
Mar 22 14:20:39.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:40.725: INFO: Number of nodes with available pods: 0
Mar 22 14:20:40.725: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:41.723: INFO: Number of nodes with available pods: 0
Mar 22 14:20:41.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:42.723: INFO: Number of nodes with available pods: 0
Mar 22 14:20:42.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:43.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:43.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:44.724: INFO: Number of nodes with available pods: 0
Mar 22 14:20:44.724: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:45.734: INFO: Number of nodes with available pods: 0
Mar 22 14:20:45.734: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:46.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:46.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:47.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:47.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:48.723: INFO: Number of nodes with available pods: 0
Mar 22 14:20:48.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:49.723: INFO: Number of nodes with available pods: 0
Mar 22 14:20:49.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:50.726: INFO: Number of nodes with available pods: 0
Mar 22 14:20:50.726: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:51.723: INFO: Number of nodes with available pods: 0
Mar 22 14:20:51.723: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:52.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:52.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:53.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:53.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:54.731: INFO: Number of nodes with available pods: 0
Mar 22 14:20:54.731: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:55.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:55.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:56.755: INFO: Number of nodes with available pods: 0
Mar 22 14:20:56.755: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:57.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:57.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:58.722: INFO: Number of nodes with available pods: 0
Mar 22 14:20:58.722: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:20:59.736: INFO: Number of nodes with available pods: 0
Mar 22 14:20:59.736: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:21:00.731: INFO: Number of nodes with available pods: 0
Mar 22 14:21:00.731: INFO: Node metalk8s-01 is running more than one daemon pod
Mar 22 14:21:01.725: INFO: Number of nodes with available pods: 1
Mar 22 14:21:01.725: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-tsz4x, will wait for the garbage collector to delete the pods
Mar 22 14:21:01.844: INFO: Deleting {extensions DaemonSet} daemon-set took: 27.10576ms
Mar 22 14:21:01.945: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.305437ms
Mar 22 14:21:41.261: INFO: Number of nodes with available pods: 0
Mar 22 14:21:41.261: INFO: Number of running nodes: 0, number of available pods: 0
Mar 22 14:21:41.265: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tsz4x/daemonsets","resourceVersion":"31613"},"items":null}

Mar 22 14:21:41.271: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tsz4x/pods","resourceVersion":"31613"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:21:41.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tsz4x" for this suite.
Mar 22 14:21:47.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:21:47.444: INFO: namespace: e2e-tests-daemonsets-tsz4x, resource: bindings, ignored listing per whitelist
Mar 22 14:21:47.529: INFO: namespace e2e-tests-daemonsets-tsz4x deletion completed in 6.202468468s

â€¢ [SLOW TEST:85.190 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:21:47.529: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4cgzf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-d3c710f0-4cad-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 14:21:48.054: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3e4b8ab-4cad-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-4cgzf" to be "success or failure"
Mar 22 14:21:48.060: INFO: Pod "pod-projected-configmaps-d3e4b8ab-4cad-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073921ms
Mar 22 14:21:50.067: INFO: Pod "pod-projected-configmaps-d3e4b8ab-4cad-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012329449s
STEP: Saw pod success
Mar 22 14:21:50.067: INFO: Pod "pod-projected-configmaps-d3e4b8ab-4cad-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:21:50.074: INFO: Trying to get logs from node metalk8s-04 pod pod-projected-configmaps-d3e4b8ab-4cad-11e9-99ae-9e98f636e47c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 22 14:21:50.125: INFO: Waiting for pod pod-projected-configmaps-d3e4b8ab-4cad-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:21:50.131: INFO: Pod pod-projected-configmaps-d3e4b8ab-4cad-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:21:50.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4cgzf" for this suite.
Mar 22 14:21:56.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:21:56.207: INFO: namespace: e2e-tests-projected-4cgzf, resource: bindings, ignored listing per whitelist
Mar 22 14:21:56.381: INFO: namespace e2e-tests-projected-4cgzf deletion completed in 6.24079996s

â€¢ [SLOW TEST:8.852 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:21:56.381: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-25g58
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-4b7nk
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Mar 22 14:22:00.805: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-q2qq4
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:22:26.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-25g58" for this suite.
Mar 22 14:22:32.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:22:32.196: INFO: namespace: e2e-tests-namespaces-25g58, resource: bindings, ignored listing per whitelist
Mar 22 14:22:32.271: INFO: namespace e2e-tests-namespaces-25g58 deletion completed in 6.22352484s
STEP: Destroying namespace "e2e-tests-nsdeletetest-4b7nk" for this suite.
Mar 22 14:22:32.276: INFO: Namespace e2e-tests-nsdeletetest-4b7nk was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-q2qq4" for this suite.
Mar 22 14:22:38.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:22:38.407: INFO: namespace: e2e-tests-nsdeletetest-q2qq4, resource: bindings, ignored listing per whitelist
Mar 22 14:22:38.486: INFO: namespace e2e-tests-nsdeletetest-q2qq4 deletion completed in 6.209520463s

â€¢ [SLOW TEST:42.105 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:22:38.486: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-lkkck
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-lkkck
Mar 22 14:22:46.757: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-lkkck
STEP: checking the pod's current state and verifying that restartCount is present
Mar 22 14:22:46.762: INFO: Initial restart count of pod liveness-http is 0
Mar 22 14:23:06.848: INFO: Restart count of pod e2e-tests-container-probe-lkkck/liveness-http is now 1 (20.086305424s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:23:06.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-lkkck" for this suite.
Mar 22 14:23:12.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:23:13.110: INFO: namespace: e2e-tests-container-probe-lkkck, resource: bindings, ignored listing per whitelist
Mar 22 14:23:13.138: INFO: namespace e2e-tests-container-probe-lkkck deletion completed in 6.25294922s

â€¢ [SLOW TEST:34.652 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:23:13.138: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7cnk5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 14:23:13.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 version --client'
Mar 22 14:23:13.447: INFO: stderr: ""
Mar 22 14:23:13.447: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.1\", GitCommit:\"4ed3216f3ec431b140b1d899130a69fc671678f4\", GitTreeState:\"clean\", BuildDate:\"2018-10-05T16:46:06Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Mar 22 14:23:13.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-7cnk5'
Mar 22 14:23:13.675: INFO: stderr: ""
Mar 22 14:23:13.675: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar 22 14:23:13.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-7cnk5'
Mar 22 14:23:13.887: INFO: stderr: ""
Mar 22 14:23:13.887: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 22 14:23:14.900: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:23:14.900: INFO: Found 0 / 1
Mar 22 14:23:15.964: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:23:15.964: INFO: Found 1 / 1
Mar 22 14:23:15.964: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 22 14:23:15.971: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:23:15.971: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 22 14:23:15.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 describe pod redis-master-47ksg --namespace=e2e-tests-kubectl-7cnk5'
Mar 22 14:23:16.127: INFO: stderr: ""
Mar 22 14:23:16.127: INFO: stdout: "Name:               redis-master-47ksg\nNamespace:          e2e-tests-kubectl-7cnk5\nPriority:           0\nPriorityClassName:  <none>\nNode:               metalk8s-04/10.200.6.61\nStart Time:         Fri, 22 Mar 2019 14:23:13 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.233.122.87\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://30061fb47f24ae3e3243850f01c82c5c8dac52e95d50852dcb53a817b2c528ca\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 22 Mar 2019 14:23:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-kn7b5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-kn7b5:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-kn7b5\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                  Message\n  ----    ------     ----  ----                  -------\n  Normal  Scheduled  3s    default-scheduler     Successfully assigned e2e-tests-kubectl-7cnk5/redis-master-47ksg to metalk8s-04\n  Normal  Pulled     2s    kubelet, metalk8s-04  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, metalk8s-04  Created container\n  Normal  Started    2s    kubelet, metalk8s-04  Started container\n"
Mar 22 14:23:16.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 describe rc redis-master --namespace=e2e-tests-kubectl-7cnk5'
Mar 22 14:23:16.265: INFO: stderr: ""
Mar 22 14:23:16.265: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-7cnk5\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-47ksg\n"
Mar 22 14:23:16.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 describe service redis-master --namespace=e2e-tests-kubectl-7cnk5'
Mar 22 14:23:16.398: INFO: stderr: ""
Mar 22 14:23:16.398: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-7cnk5\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.23.121\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.122.87:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 22 14:23:16.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 describe node metalk8s-01'
Mar 22 14:23:16.556: INFO: stderr: ""
Mar 22 14:23:16.556: INFO: stdout: "Name:               metalk8s-01\nRoles:              master,node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=metalk8s-01\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/node=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 22 Mar 2019 11:47:45 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  OutOfDisk        False   Fri, 22 Mar 2019 14:23:14 +0000   Fri, 22 Mar 2019 11:47:45 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure   False   Fri, 22 Mar 2019 14:23:14 +0000   Fri, 22 Mar 2019 11:47:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 22 Mar 2019 14:23:14 +0000   Fri, 22 Mar 2019 11:47:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 22 Mar 2019 14:23:14 +0000   Fri, 22 Mar 2019 11:47:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 22 Mar 2019 14:23:14 +0000   Fri, 22 Mar 2019 11:48:55 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.200.6.49\n  Hostname:    metalk8s-01\nCapacity:\n cpu:                8\n ephemeral-storage:  41931756Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             32780540Ki\n pods:               110\nAllocatable:\n cpu:                7800m\n ephemeral-storage:  38644306266\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             32178140Ki\n pods:               110\nSystem Info:\n Machine ID:                 5003025f93c1a84914ea5ae66519c100\n System UUID:                2597C10C-4A43-4CF3-9D52-2A22E91DC4E1\n Boot ID:                    42a0123c-b8e1-4b24-b3bf-5c74af2e4c3c\n Kernel Version:             3.10.0-862.14.4.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.12.3\n Kube-Proxy Version:         v1.12.3\nNon-terminated Pods:         (12 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-d92zp    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-ingress               nginx-ingress-controller-cxd4p                             0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-ingress               nginx-ingress-default-backend-7b98d4dc6c-5ct2x             0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-ops                   elasticsearch-client-6d74f68c8d-9t67x                      1 (12%)       4 (51%)     2Gi (6%)         0 (0%)\n  kube-ops                   elasticsearch-master-2                                     500m (6%)     2 (25%)     512Mi (1%)       0 (0%)\n  kube-ops                   fluent-bit-ps79j                                           100m (1%)     0 (0%)      100Mi (0%)       100Mi (0%)\n  kube-system                calico-node-6x2ch                                          150m (1%)     300m (3%)   64M (0%)         500M (1%)\n  kube-system                kube-apiserver-metalk8s-01                                 100m (1%)     800m (10%)  256M (0%)        2G (6%)\n  kube-system                kube-controller-manager-metalk8s-01                        100m (1%)     250m (3%)   100M (0%)        512M (1%)\n  kube-system                kube-proxy-metalk8s-01                                     150m (1%)     500m (6%)   64M (0%)         2G (6%)\n  kube-system                kube-scheduler-metalk8s-01                                 80m (1%)      250m (3%)   170M (0%)        512M (1%)\n  kube-system                metrics-server-647b5cf45b-94rmt                            0 (0%)        0 (0%)      0 (0%)           0 (0%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource  Requests          Limits\n  --------  --------          ------\n  cpu       2180m (27%)       8100m (103%)\n  memory    3443212160 (10%)  5628857600 (17%)\nEvents:     <none>\n"
Mar 22 14:23:16.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 describe namespace e2e-tests-kubectl-7cnk5'
Mar 22 14:23:16.670: INFO: stderr: ""
Mar 22 14:23:16.671: INFO: stdout: "Name:         e2e-tests-kubectl-7cnk5\nLabels:       e2e-framework=kubectl\n              e2e-run=ee994162-4ca3-11e9-99ae-9e98f636e47c\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:23:16.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7cnk5" for this suite.
Mar 22 14:23:30.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:23:30.855: INFO: namespace: e2e-tests-kubectl-7cnk5, resource: bindings, ignored listing per whitelist
Mar 22 14:23:30.914: INFO: namespace e2e-tests-kubectl-7cnk5 deletion completed in 14.235012301s

â€¢ [SLOW TEST:17.776 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:23:30.914: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-hqg8v
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-11699b01-4cae-11e9-99ae-9e98f636e47c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:23:35.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hqg8v" for this suite.
Mar 22 14:23:59.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:23:59.616: INFO: namespace: e2e-tests-configmap-hqg8v, resource: bindings, ignored listing per whitelist
Mar 22 14:23:59.634: INFO: namespace e2e-tests-configmap-hqg8v deletion completed in 24.382249738s

â€¢ [SLOW TEST:28.720 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:23:59.635: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8fg59
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-228aa6d9-4cae-11e9-99ae-9e98f636e47c
STEP: Creating secret with name s-test-opt-upd-228aa773-4cae-11e9-99ae-9e98f636e47c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-228aa6d9-4cae-11e9-99ae-9e98f636e47c
STEP: Updating secret s-test-opt-upd-228aa773-4cae-11e9-99ae-9e98f636e47c
STEP: Creating secret with name s-test-opt-create-228aa789-4cae-11e9-99ae-9e98f636e47c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:25:17.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8fg59" for this suite.
Mar 22 14:25:41.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:25:41.429: INFO: namespace: e2e-tests-projected-8fg59, resource: bindings, ignored listing per whitelist
Mar 22 14:25:41.532: INFO: namespace e2e-tests-projected-8fg59 deletion completed in 24.224135993s

â€¢ [SLOW TEST:101.898 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:25:41.532: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-lsntg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-lsntg
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Mar 22 14:25:41.833: INFO: Found 0 stateful pods, waiting for 3
Mar 22 14:25:51.853: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 14:25:51.853: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 14:25:51.853: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 14:25:51.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-lsntg ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:25:52.106: INFO: stderr: ""
Mar 22 14:25:52.106: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:25:52.106: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar 22 14:26:02.204: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 22 14:26:12.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-lsntg ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 22 14:26:12.591: INFO: stderr: ""
Mar 22 14:26:12.591: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 22 14:26:12.591: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 22 14:26:22.631: INFO: Waiting for StatefulSet e2e-tests-statefulset-lsntg/ss2 to complete update
Mar 22 14:26:22.631: INFO: Waiting for Pod e2e-tests-statefulset-lsntg/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar 22 14:26:22.631: INFO: Waiting for Pod e2e-tests-statefulset-lsntg/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar 22 14:26:32.650: INFO: Waiting for StatefulSet e2e-tests-statefulset-lsntg/ss2 to complete update
Mar 22 14:26:32.650: INFO: Waiting for Pod e2e-tests-statefulset-lsntg/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Mar 22 14:26:42.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-lsntg ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 22 14:26:42.946: INFO: stderr: ""
Mar 22 14:26:42.946: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 22 14:26:42.946: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 22 14:26:53.010: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 22 14:27:03.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 exec --namespace=e2e-tests-statefulset-lsntg ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 22 14:27:03.416: INFO: stderr: ""
Mar 22 14:27:03.416: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 22 14:27:03.416: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 22 14:27:13.462: INFO: Waiting for StatefulSet e2e-tests-statefulset-lsntg/ss2 to complete update
Mar 22 14:27:13.462: INFO: Waiting for Pod e2e-tests-statefulset-lsntg/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar 22 14:27:13.462: INFO: Waiting for Pod e2e-tests-statefulset-lsntg/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar 22 14:27:23.484: INFO: Waiting for StatefulSet e2e-tests-statefulset-lsntg/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 22 14:27:33.473: INFO: Deleting all statefulset in ns e2e-tests-statefulset-lsntg
Mar 22 14:27:33.488: INFO: Scaling statefulset ss2 to 0
Mar 22 14:27:53.513: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 14:27:53.518: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:27:53.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-lsntg" for this suite.
Mar 22 14:28:01.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:28:01.861: INFO: namespace: e2e-tests-statefulset-lsntg, resource: bindings, ignored listing per whitelist
Mar 22 14:28:01.942: INFO: namespace e2e-tests-statefulset-lsntg deletion completed in 8.383708669s

â€¢ [SLOW TEST:140.409 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:28:01.942: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-czfbp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Mar 22 14:28:04.341: INFO: Pod pod-hostip-b304d1c9-4cae-11e9-99ae-9e98f636e47c has hostIP: 10.200.6.77
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:28:04.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-czfbp" for this suite.
Mar 22 14:28:26.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:28:26.509: INFO: namespace: e2e-tests-pods-czfbp, resource: bindings, ignored listing per whitelist
Mar 22 14:28:26.635: INFO: namespace e2e-tests-pods-czfbp deletion completed in 22.284488077s

â€¢ [SLOW TEST:24.693 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:28:26.636: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-gmh8q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-gmh8q/configmap-test-c1b309b9-4cae-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume configMaps
Mar 22 14:28:26.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1b48778-4cae-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-configmap-gmh8q" to be "success or failure"
Mar 22 14:28:26.953: INFO: Pod "pod-configmaps-c1b48778-4cae-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.597885ms
Mar 22 14:28:28.959: INFO: Pod "pod-configmaps-c1b48778-4cae-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011323503s
STEP: Saw pod success
Mar 22 14:28:28.959: INFO: Pod "pod-configmaps-c1b48778-4cae-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:28:28.964: INFO: Trying to get logs from node metalk8s-05 pod pod-configmaps-c1b48778-4cae-11e9-99ae-9e98f636e47c container env-test: <nil>
STEP: delete the pod
Mar 22 14:28:29.001: INFO: Waiting for pod pod-configmaps-c1b48778-4cae-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:28:29.007: INFO: Pod pod-configmaps-c1b48778-4cae-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:28:29.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gmh8q" for this suite.
Mar 22 14:28:35.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:28:35.141: INFO: namespace: e2e-tests-configmap-gmh8q, resource: bindings, ignored listing per whitelist
Mar 22 14:28:35.334: INFO: namespace e2e-tests-configmap-gmh8q deletion completed in 6.315916314s

â€¢ [SLOW TEST:8.698 seconds]
[sig-api-machinery] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:28:35.335: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mzjpr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:28:35.570: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6d7cb18-4cae-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-mzjpr" to be "success or failure"
Mar 22 14:28:35.581: INFO: Pod "downwardapi-volume-c6d7cb18-4cae-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.633399ms
Mar 22 14:28:37.595: INFO: Pod "downwardapi-volume-c6d7cb18-4cae-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02432362s
STEP: Saw pod success
Mar 22 14:28:37.595: INFO: Pod "downwardapi-volume-c6d7cb18-4cae-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:28:37.602: INFO: Trying to get logs from node metalk8s-02 pod downwardapi-volume-c6d7cb18-4cae-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:28:37.644: INFO: Waiting for pod downwardapi-volume-c6d7cb18-4cae-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:28:37.654: INFO: Pod downwardapi-volume-c6d7cb18-4cae-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:28:37.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mzjpr" for this suite.
Mar 22 14:28:43.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:28:43.883: INFO: namespace: e2e-tests-projected-mzjpr, resource: bindings, ignored listing per whitelist
Mar 22 14:28:43.935: INFO: namespace e2e-tests-projected-mzjpr deletion completed in 6.25605412s

â€¢ [SLOW TEST:8.601 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:28:43.935: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vmbts
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cbf93657-4cae-11e9-99ae-9e98f636e47c
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-cbf93657-4cae-11e9-99ae-9e98f636e47c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:30:13.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vmbts" for this suite.
Mar 22 14:30:37.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:30:38.043: INFO: namespace: e2e-tests-projected-vmbts, resource: bindings, ignored listing per whitelist
Mar 22 14:30:38.137: INFO: namespace e2e-tests-projected-vmbts deletion completed in 25.061579396s

â€¢ [SLOW TEST:114.202 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:30:38.137: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-8pmgf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Mar 22 14:30:38.373: INFO: (0) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.607741ms)
Mar 22 14:30:38.380: INFO: (1) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.409875ms)
Mar 22 14:30:38.387: INFO: (2) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.509902ms)
Mar 22 14:30:38.397: INFO: (3) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 9.92891ms)
Mar 22 14:30:38.404: INFO: (4) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.536697ms)
Mar 22 14:30:38.410: INFO: (5) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 5.918771ms)
Mar 22 14:30:38.418: INFO: (6) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.531318ms)
Mar 22 14:30:38.424: INFO: (7) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.699045ms)
Mar 22 14:30:38.431: INFO: (8) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.396083ms)
Mar 22 14:30:38.438: INFO: (9) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.29009ms)
Mar 22 14:30:38.446: INFO: (10) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.78171ms)
Mar 22 14:30:38.457: INFO: (11) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 10.47761ms)
Mar 22 14:30:38.462: INFO: (12) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 5.872847ms)
Mar 22 14:30:38.473: INFO: (13) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 10.560874ms)
Mar 22 14:30:38.480: INFO: (14) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.025753ms)
Mar 22 14:30:38.487: INFO: (15) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.692386ms)
Mar 22 14:30:38.493: INFO: (16) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.270369ms)
Mar 22 14:30:38.499: INFO: (17) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 5.490789ms)
Mar 22 14:30:38.506: INFO: (18) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.833601ms)
Mar 22 14:30:38.513: INFO: (19) /api/v1/nodes/metalk8s-01/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.081891ms)
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:30:38.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-8pmgf" for this suite.
Mar 22 14:30:44.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:30:44.709: INFO: namespace: e2e-tests-proxy-8pmgf, resource: bindings, ignored listing per whitelist
Mar 22 14:30:44.786: INFO: namespace e2e-tests-proxy-8pmgf deletion completed in 6.261843113s

â€¢ [SLOW TEST:6.649 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:30:44.786: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-ldxsk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-14013c1f-4caf-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 14:30:45.034: INFO: Waiting up to 5m0s for pod "pod-secrets-1402c81c-4caf-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-secrets-ldxsk" to be "success or failure"
Mar 22 14:30:45.040: INFO: Pod "pod-secrets-1402c81c-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764786ms
Mar 22 14:30:47.047: INFO: Pod "pod-secrets-1402c81c-4caf-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012607647s
STEP: Saw pod success
Mar 22 14:30:47.047: INFO: Pod "pod-secrets-1402c81c-4caf-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:30:47.052: INFO: Trying to get logs from node metalk8s-05 pod pod-secrets-1402c81c-4caf-11e9-99ae-9e98f636e47c container secret-volume-test: <nil>
STEP: delete the pod
Mar 22 14:30:47.087: INFO: Waiting for pod pod-secrets-1402c81c-4caf-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:30:47.092: INFO: Pod pod-secrets-1402c81c-4caf-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:30:47.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-ldxsk" for this suite.
Mar 22 14:30:53.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:30:53.203: INFO: namespace: e2e-tests-secrets-ldxsk, resource: bindings, ignored listing per whitelist
Mar 22 14:30:53.345: INFO: namespace e2e-tests-secrets-ldxsk deletion completed in 6.236928353s

â€¢ [SLOW TEST:8.559 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:30:53.346: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-fd5jv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 22 14:30:53.651: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-fd5jv,SelfLink:/api/v1/namespaces/e2e-tests-watch-fd5jv/configmaps/e2e-watch-test-resource-version,UID:192474d6-4caf-11e9-b0fd-fa163e8e51f5,ResourceVersion:33616,Generation:0,CreationTimestamp:2019-03-22 14:30:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 22 14:30:53.651: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-fd5jv,SelfLink:/api/v1/namespaces/e2e-tests-watch-fd5jv/configmaps/e2e-watch-test-resource-version,UID:192474d6-4caf-11e9-b0fd-fa163e8e51f5,ResourceVersion:33617,Generation:0,CreationTimestamp:2019-03-22 14:30:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:30:53.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-fd5jv" for this suite.
Mar 22 14:30:59.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:30:59.914: INFO: namespace: e2e-tests-watch-fd5jv, resource: bindings, ignored listing per whitelist
Mar 22 14:30:59.965: INFO: namespace e2e-tests-watch-fd5jv deletion completed in 6.303274363s

â€¢ [SLOW TEST:6.619 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:30:59.965: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4q7z8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:31:00.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d0ff986-4caf-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-4q7z8" to be "success or failure"
Mar 22 14:31:00.226: INFO: Pod "downwardapi-volume-1d0ff986-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.275811ms
Mar 22 14:31:02.232: INFO: Pod "downwardapi-volume-1d0ff986-4caf-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01278186s
STEP: Saw pod success
Mar 22 14:31:02.232: INFO: Pod "downwardapi-volume-1d0ff986-4caf-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:31:02.238: INFO: Trying to get logs from node metalk8s-05 pod downwardapi-volume-1d0ff986-4caf-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:31:02.275: INFO: Waiting for pod downwardapi-volume-1d0ff986-4caf-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:31:02.280: INFO: Pod downwardapi-volume-1d0ff986-4caf-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:31:02.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4q7z8" for this suite.
Mar 22 14:31:08.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:31:08.454: INFO: namespace: e2e-tests-projected-4q7z8, resource: bindings, ignored listing per whitelist
Mar 22 14:31:08.471: INFO: namespace e2e-tests-projected-4q7z8 deletion completed in 6.182243904s

â€¢ [SLOW TEST:8.506 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:31:08.471: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-gf84q
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 22 14:31:08.693: INFO: Waiting up to 5m0s for pod "pod-221cd3ae-4caf-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-gf84q" to be "success or failure"
Mar 22 14:31:08.698: INFO: Pod "pod-221cd3ae-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.549596ms
Mar 22 14:31:10.712: INFO: Pod "pod-221cd3ae-4caf-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018687041s
STEP: Saw pod success
Mar 22 14:31:10.712: INFO: Pod "pod-221cd3ae-4caf-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:31:10.718: INFO: Trying to get logs from node metalk8s-02 pod pod-221cd3ae-4caf-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 14:31:10.759: INFO: Waiting for pod pod-221cd3ae-4caf-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:31:10.766: INFO: Pod pod-221cd3ae-4caf-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:31:10.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gf84q" for this suite.
Mar 22 14:31:16.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:31:16.887: INFO: namespace: e2e-tests-emptydir-gf84q, resource: bindings, ignored listing per whitelist
Mar 22 14:31:17.033: INFO: namespace e2e-tests-emptydir-gf84q deletion completed in 6.246047282s

â€¢ [SLOW TEST:8.562 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:31:17.034: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-d66x7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 22 14:31:17.332: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-d66x7,SelfLink:/api/v1/namespaces/e2e-tests-watch-d66x7/configmaps/e2e-watch-test-label-changed,UID:27414757-4caf-11e9-b0fd-fa163e8e51f5,ResourceVersion:33746,Generation:0,CreationTimestamp:2019-03-22 14:31:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 22 14:31:17.333: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-d66x7,SelfLink:/api/v1/namespaces/e2e-tests-watch-d66x7/configmaps/e2e-watch-test-label-changed,UID:27414757-4caf-11e9-b0fd-fa163e8e51f5,ResourceVersion:33747,Generation:0,CreationTimestamp:2019-03-22 14:31:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 22 14:31:17.333: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-d66x7,SelfLink:/api/v1/namespaces/e2e-tests-watch-d66x7/configmaps/e2e-watch-test-label-changed,UID:27414757-4caf-11e9-b0fd-fa163e8e51f5,ResourceVersion:33748,Generation:0,CreationTimestamp:2019-03-22 14:31:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 22 14:31:27.408: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-d66x7,SelfLink:/api/v1/namespaces/e2e-tests-watch-d66x7/configmaps/e2e-watch-test-label-changed,UID:27414757-4caf-11e9-b0fd-fa163e8e51f5,ResourceVersion:33769,Generation:0,CreationTimestamp:2019-03-22 14:31:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 22 14:31:27.408: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-d66x7,SelfLink:/api/v1/namespaces/e2e-tests-watch-d66x7/configmaps/e2e-watch-test-label-changed,UID:27414757-4caf-11e9-b0fd-fa163e8e51f5,ResourceVersion:33770,Generation:0,CreationTimestamp:2019-03-22 14:31:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar 22 14:31:27.409: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-d66x7,SelfLink:/api/v1/namespaces/e2e-tests-watch-d66x7/configmaps/e2e-watch-test-label-changed,UID:27414757-4caf-11e9-b0fd-fa163e8e51f5,ResourceVersion:33771,Generation:0,CreationTimestamp:2019-03-22 14:31:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:31:27.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-d66x7" for this suite.
Mar 22 14:31:33.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:31:33.519: INFO: namespace: e2e-tests-watch-d66x7, resource: bindings, ignored listing per whitelist
Mar 22 14:31:33.637: INFO: namespace e2e-tests-watch-d66x7 deletion completed in 6.214711082s

â€¢ [SLOW TEST:16.603 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:31:33.637: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-k5gv9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Mar 22 14:31:33.852: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-171814416 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:31:33.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-k5gv9" for this suite.
Mar 22 14:31:40.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:31:40.171: INFO: namespace: e2e-tests-kubectl-k5gv9, resource: bindings, ignored listing per whitelist
Mar 22 14:31:40.453: INFO: namespace e2e-tests-kubectl-k5gv9 deletion completed in 6.477901955s

â€¢ [SLOW TEST:6.816 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:31:40.453: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-7sq4v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Mar 22 14:31:40.791: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 22 14:31:40.805: INFO: Waiting for terminating namespaces to be deleted...
Mar 22 14:31:40.809: INFO: 
Logging pods the kubelet thinks is on node metalk8s-01 before test
Mar 22 14:31:40.821: INFO: nginx-ingress-default-backend-7b98d4dc6c-5ct2x from kube-ingress started at 2019-03-22 11:53:46 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.821: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Mar 22 14:31:40.821: INFO: kube-controller-manager-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.821: INFO: calico-node-6x2ch from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.821: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 14:31:40.821: INFO: kube-scheduler-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.821: INFO: elasticsearch-client-6d74f68c8d-9t67x from kube-ops started at 2019-03-22 11:54:09 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.821: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 14:31:40.821: INFO: kube-proxy-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.821: INFO: kube-apiserver-metalk8s-01 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.821: INFO: metrics-server-647b5cf45b-94rmt from kube-system started at 2019-03-22 11:55:38 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.821: INFO: 	Container metrics-server ready: true, restart count 0
Mar 22 14:31:40.821: INFO: elasticsearch-master-2 from kube-ops started at 2019-03-22 11:56:05 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.821: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 14:31:40.821: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-d92zp from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.821: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar 22 14:31:40.821: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 22 14:31:40.821: INFO: nginx-ingress-controller-cxd4p from kube-ingress started at 2019-03-22 11:53:46 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.821: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 14:31:40.821: INFO: fluent-bit-ps79j from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.821: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 14:31:40.821: INFO: 
Logging pods the kubelet thinks is on node metalk8s-02 before test
Mar 22 14:31:40.835: INFO: kube-apiserver-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.835: INFO: calico-node-rtkz2 from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 14:31:40.835: INFO: elasticsearch-client-6d74f68c8d-28qk2 from kube-ops started at 2019-03-22 11:54:09 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 14:31:40.835: INFO: sonobuoy from heptio-sonobuoy started at 2019-03-22 13:10:19 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 22 14:31:40.835: INFO: kube-scheduler-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.835: INFO: coredns-6bcbc499f7-j6fch from kube-system started at 2019-03-22 11:50:52 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container coredns ready: true, restart count 0
Mar 22 14:31:40.835: INFO: nginx-ingress-controller-7rf9j from kube-ingress started at 2019-03-22 11:53:46 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 14:31:40.835: INFO: kube-prometheus-exporter-kube-state-68bb849446-r95z2 from kube-ops started at 2019-03-22 11:53:50 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container exporter-kube-state ready: true, restart count 0
Mar 22 14:31:40.835: INFO: 	Container exporter-kube-state-addon-resizer ready: true, restart count 0
Mar 22 14:31:40.835: INFO: fluent-bit-rxwxc from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 14:31:40.835: INFO: kube-proxy-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.835: INFO: cerebro-78d5fbf859-v8wt6 from kube-ops started at 2019-03-22 11:55:30 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container cerebro ready: true, restart count 0
Mar 22 14:31:40.835: INFO: kube-controller-manager-metalk8s-02 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.835: INFO: sonobuoy-e2e-job-59a17f0f4a364fed from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container e2e ready: true, restart count 0
Mar 22 14:31:40.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 14:31:40.835: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-5j5mp from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.835: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar 22 14:31:40.835: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 22 14:31:40.835: INFO: 
Logging pods the kubelet thinks is on node metalk8s-03 before test
Mar 22 14:31:40.857: INFO: kube-apiserver-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.858: INFO: kube-scheduler-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.858: INFO: calico-node-rnssh from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 14:31:40.858: INFO: nginx-ingress-controller-7x9tf from kube-ingress started at 2019-03-22 11:53:45 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 14:31:40.858: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-79jdm from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar 22 14:31:40.858: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 22 14:31:40.858: INFO: kube-proxy-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.858: INFO: kube-controller-manager-metalk8s-03 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.858: INFO: prometheus-operator-87779759-x9zf7 from kube-ops started at 2019-03-22 11:52:42 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 22 14:31:40.858: INFO: fluentd-68575dcc69-2hsp4 from kube-ops started at 2019-03-22 11:54:25 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container fluentd ready: true, restart count 0
Mar 22 14:31:40.858: INFO: fluent-bit-xrslr from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 14:31:40.858: INFO: elasticsearch-master-1 from kube-ops started at 2019-03-22 11:55:25 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 14:31:40.858: INFO: elasticsearch-data-2 from kube-ops started at 2019-03-22 11:56:08 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 14:31:40.858: INFO: calico-kube-controllers-5887874ffb-5sb92 from kube-system started at 2019-03-22 11:49:56 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 22 14:31:40.858: INFO: elasticsearch-client-6d74f68c8d-v47bt from kube-ops started at 2019-03-22 11:54:09 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.858: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 14:31:40.858: INFO: 
Logging pods the kubelet thinks is on node metalk8s-04 before test
Mar 22 14:31:40.871: INFO: elasticsearch-data-1 from kube-ops started at 2019-03-22 11:55:27 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 14:31:40.871: INFO: alertmanager-kube-prometheus-0 from kube-ops started at 2019-03-22 11:53:34 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container alertmanager ready: true, restart count 0
Mar 22 14:31:40.871: INFO: 	Container config-reloader ready: true, restart count 0
Mar 22 14:31:40.871: INFO: nginx-ingress-controller-2mvpz from kube-ingress started at 2019-03-22 11:53:45 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 14:31:40.871: INFO: coredns-6bcbc499f7-fsrbl from kube-system started at 2019-03-22 11:50:52 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container coredns ready: true, restart count 0
Mar 22 14:31:40.871: INFO: elasticsearch-master-0 from kube-ops started at 2019-03-22 11:54:10 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 14:31:40.871: INFO: fluent-bit-9bwfp from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 14:31:40.871: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-mm98f from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar 22 14:31:40.871: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar 22 14:31:40.871: INFO: calico-node-fw5xl from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 14:31:40.871: INFO: kubernetes-dashboard-68697c45d9-pbvnj from kube-system started at 2019-03-22 11:50:59 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 22 14:31:40.871: INFO: kube-prometheus-grafana-f56778dd6-v8hf8 from kube-ops started at 2019-03-22 11:53:34 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container grafana ready: true, restart count 0
Mar 22 14:31:40.871: INFO: 	Container grafana-watcher ready: true, restart count 0
Mar 22 14:31:40.871: INFO: fluentd-68575dcc69-vxd2d from kube-ops started at 2019-03-22 11:54:26 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container fluentd ready: true, restart count 0
Mar 22 14:31:40.871: INFO: prometheus-kube-prometheus-1 from kube-ops started at 2019-03-22 11:54:11 +0000 UTC (3 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container alerting-rule-files-configmap-reloader ready: true, restart count 0
Mar 22 14:31:40.871: INFO: 	Container prometheus ready: true, restart count 1
Mar 22 14:31:40.871: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 22 14:31:40.871: INFO: kibana-index-provisioning-ldhpt from kube-ops started at 2019-03-22 11:55:08 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.871: INFO: 	Container provision-index ready: false, restart count 4
Mar 22 14:31:40.871: INFO: kube-proxy-metalk8s-04 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.871: INFO: nginx-proxy-metalk8s-04 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.871: INFO: 
Logging pods the kubelet thinks is on node metalk8s-05 before test
Mar 22 14:31:40.884: INFO: nginx-ingress-controller-p2lw6 from kube-ingress started at 2019-03-22 11:53:45 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 22 14:31:40.884: INFO: heapster-heapster-869cfbd88b-zps4g from kube-system started at 2019-03-22 11:54:13 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container heapster ready: true, restart count 0
Mar 22 14:31:40.884: INFO: 	Container heapster-nanny ready: true, restart count 0
Mar 22 14:31:40.884: INFO: es-exporter-elasticsearch-exporter-6fdfc4c7d7-6v575 from kube-ops started at 2019-03-22 11:55:14 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container elasticsearch-exporter ready: true, restart count 0
Mar 22 14:31:40.884: INFO: calico-node-b2zns from kube-system started at 2019-03-22 11:49:41 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container calico-node ready: true, restart count 0
Mar 22 14:31:40.884: INFO: kibana-d5c76b4dd-76nfw from kube-ops started at 2019-03-22 11:54:52 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container kibana ready: true, restart count 0
Mar 22 14:31:40.884: INFO: prometheus-kube-prometheus-0 from kube-ops started at 2019-03-22 11:53:36 +0000 UTC (3 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container alerting-rule-files-configmap-reloader ready: true, restart count 0
Mar 22 14:31:40.884: INFO: 	Container prometheus ready: true, restart count 1
Mar 22 14:31:40.884: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 22 14:31:40.884: INFO: kube-proxy-metalk8s-05 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.884: INFO: tiller-deploy-6f6fd74b68-7b7bc from kube-system started at 2019-03-22 11:52:19 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container tiller ready: true, restart count 0
Mar 22 14:31:40.884: INFO: alertmanager-kube-prometheus-1 from kube-ops started at 2019-03-22 11:54:08 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container alertmanager ready: true, restart count 0
Mar 22 14:31:40.884: INFO: 	Container config-reloader ready: true, restart count 0
Mar 22 14:31:40.884: INFO: nginx-proxy-metalk8s-05 from kube-system started at <nil> (0 container statuses recorded)
Mar 22 14:31:40.884: INFO: elasticsearch-data-0 from kube-ops started at 2019-03-22 11:54:10 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container elasticsearch ready: true, restart count 0
Mar 22 14:31:40.884: INFO: fluent-bit-cjtgw from kube-ops started at 2019-03-22 11:54:37 +0000 UTC (1 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container fluent-bit ready: true, restart count 0
Mar 22 14:31:40.884: INFO: sonobuoy-systemd-logs-daemon-set-99f1075eee60464d-fgs2v from heptio-sonobuoy started at 2019-03-22 13:10:26 +0000 UTC (2 container statuses recorded)
Mar 22 14:31:40.884: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Mar 22 14:31:40.884: INFO: 	Container sonobuoy-worker ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-37b8510c-4caf-11e9-99ae-9e98f636e47c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-37b8510c-4caf-11e9-99ae-9e98f636e47c off the node metalk8s-01
STEP: verifying the node doesn't have the label kubernetes.io/e2e-37b8510c-4caf-11e9-99ae-9e98f636e47c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:31:49.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-7sq4v" for this suite.
Mar 22 14:32:03.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:32:03.205: INFO: namespace: e2e-tests-sched-pred-7sq4v, resource: bindings, ignored listing per whitelist
Mar 22 14:32:03.320: INFO: namespace e2e-tests-sched-pred-7sq4v deletion completed in 14.291176541s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

â€¢ [SLOW TEST:22.867 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:32:03.320: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-r5kgb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Mar 22 14:32:03.602: INFO: namespace e2e-tests-kubectl-r5kgb
Mar 22 14:32:03.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 create -f - --namespace=e2e-tests-kubectl-r5kgb'
Mar 22 14:32:03.975: INFO: stderr: ""
Mar 22 14:32:03.975: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 22 14:32:04.982: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:32:04.982: INFO: Found 0 / 1
Mar 22 14:32:05.985: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:32:05.985: INFO: Found 1 / 1
Mar 22 14:32:05.985: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 22 14:32:06.002: INFO: Selector matched 1 pods for map[app:redis]
Mar 22 14:32:06.002: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 22 14:32:06.002: INFO: wait on redis-master startup in e2e-tests-kubectl-r5kgb 
Mar 22 14:32:06.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 logs redis-master-g9s6n redis-master --namespace=e2e-tests-kubectl-r5kgb'
Mar 22 14:32:06.196: INFO: stderr: ""
Mar 22 14:32:06.196: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Mar 14:32:05.260 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Mar 14:32:05.260 # Server started, Redis version 3.2.12\n1:M 22 Mar 14:32:05.261 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Mar 14:32:05.261 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar 22 14:32:06.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-r5kgb'
Mar 22 14:32:06.325: INFO: stderr: ""
Mar 22 14:32:06.325: INFO: stdout: "service/rm2 exposed\n"
Mar 22 14:32:06.361: INFO: Service rm2 in namespace e2e-tests-kubectl-r5kgb found.
STEP: exposing service
Mar 22 14:32:08.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-171814416 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-r5kgb'
Mar 22 14:32:08.494: INFO: stderr: ""
Mar 22 14:32:08.494: INFO: stdout: "service/rm3 exposed\n"
Mar 22 14:32:08.499: INFO: Service rm3 in namespace e2e-tests-kubectl-r5kgb found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:32:10.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r5kgb" for this suite.
Mar 22 14:32:34.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:32:34.726: INFO: namespace: e2e-tests-kubectl-r5kgb, resource: bindings, ignored listing per whitelist
Mar 22 14:32:34.726: INFO: namespace e2e-tests-kubectl-r5kgb deletion completed in 24.199812304s

â€¢ [SLOW TEST:31.406 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:32:34.727: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xsbml
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:32:34.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5589a79c-4caf-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-xsbml" to be "success or failure"
Mar 22 14:32:34.973: INFO: Pod "downwardapi-volume-5589a79c-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.959342ms
Mar 22 14:32:36.990: INFO: Pod "downwardapi-volume-5589a79c-4caf-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020853413s
STEP: Saw pod success
Mar 22 14:32:36.990: INFO: Pod "downwardapi-volume-5589a79c-4caf-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:32:37.000: INFO: Trying to get logs from node metalk8s-05 pod downwardapi-volume-5589a79c-4caf-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:32:37.039: INFO: Waiting for pod downwardapi-volume-5589a79c-4caf-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:32:37.043: INFO: Pod downwardapi-volume-5589a79c-4caf-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:32:37.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xsbml" for this suite.
Mar 22 14:32:43.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:32:43.111: INFO: namespace: e2e-tests-projected-xsbml, resource: bindings, ignored listing per whitelist
Mar 22 14:32:43.397: INFO: namespace e2e-tests-projected-xsbml deletion completed in 6.332264206s

â€¢ [SLOW TEST:8.670 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:32:43.398: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-fnqb2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 22 14:32:46.259: INFO: Successfully updated pod "pod-update-5ab6e979-4caf-11e9-99ae-9e98f636e47c"
STEP: verifying the updated pod is in kubernetes
Mar 22 14:32:46.272: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:32:46.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fnqb2" for this suite.
Mar 22 14:33:08.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:33:08.409: INFO: namespace: e2e-tests-pods-fnqb2, resource: bindings, ignored listing per whitelist
Mar 22 14:33:08.546: INFO: namespace e2e-tests-pods-fnqb2 deletion completed in 22.251070861s

â€¢ [SLOW TEST:25.149 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:33:08.546: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-9chch
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:33:08.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69b11bfb-4caf-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-projected-9chch" to be "success or failure"
Mar 22 14:33:08.796: INFO: Pod "downwardapi-volume-69b11bfb-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.872253ms
Mar 22 14:33:10.818: INFO: Pod "downwardapi-volume-69b11bfb-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033738607s
Mar 22 14:33:12.827: INFO: Pod "downwardapi-volume-69b11bfb-4caf-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042285265s
STEP: Saw pod success
Mar 22 14:33:12.827: INFO: Pod "downwardapi-volume-69b11bfb-4caf-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:33:12.832: INFO: Trying to get logs from node metalk8s-01 pod downwardapi-volume-69b11bfb-4caf-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:33:12.865: INFO: Waiting for pod downwardapi-volume-69b11bfb-4caf-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:33:12.870: INFO: Pod downwardapi-volume-69b11bfb-4caf-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:33:12.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9chch" for this suite.
Mar 22 14:33:18.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:33:18.908: INFO: namespace: e2e-tests-projected-9chch, resource: bindings, ignored listing per whitelist
Mar 22 14:33:19.067: INFO: namespace e2e-tests-projected-9chch deletion completed in 6.187815151s

â€¢ [SLOW TEST:10.521 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:33:19.067: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-dzs8p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-dzs8p
Mar 22 14:33:21.315: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-dzs8p
STEP: checking the pod's current state and verifying that restartCount is present
Mar 22 14:33:21.321: INFO: Initial restart count of pod liveness-exec is 0
Mar 22 14:34:09.564: INFO: Restart count of pod e2e-tests-container-probe-dzs8p/liveness-exec is now 1 (48.243152316s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:34:09.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-dzs8p" for this suite.
Mar 22 14:34:15.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:34:15.794: INFO: namespace: e2e-tests-container-probe-dzs8p, resource: bindings, ignored listing per whitelist
Mar 22 14:34:15.827: INFO: namespace e2e-tests-container-probe-dzs8p deletion completed in 6.232619661s

â€¢ [SLOW TEST:56.760 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:34:15.827: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-kqwzc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 22 14:34:16.043: INFO: Waiting up to 5m0s for pod "pod-91c814b7-4caf-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-emptydir-kqwzc" to be "success or failure"
Mar 22 14:34:16.048: INFO: Pod "pod-91c814b7-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.250149ms
Mar 22 14:34:18.061: INFO: Pod "pod-91c814b7-4caf-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018477678s
STEP: Saw pod success
Mar 22 14:34:18.061: INFO: Pod "pod-91c814b7-4caf-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:34:18.066: INFO: Trying to get logs from node metalk8s-05 pod pod-91c814b7-4caf-11e9-99ae-9e98f636e47c container test-container: <nil>
STEP: delete the pod
Mar 22 14:34:18.093: INFO: Waiting for pod pod-91c814b7-4caf-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:34:18.097: INFO: Pod pod-91c814b7-4caf-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:34:18.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kqwzc" for this suite.
Mar 22 14:34:24.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:34:24.238: INFO: namespace: e2e-tests-emptydir-kqwzc, resource: bindings, ignored listing per whitelist
Mar 22 14:34:24.416: INFO: namespace e2e-tests-emptydir-kqwzc deletion completed in 6.312002542s

â€¢ [SLOW TEST:8.589 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:34:24.416: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-p5ctz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:34:24.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96eb1dde-4caf-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-p5ctz" to be "success or failure"
Mar 22 14:34:24.706: INFO: Pod "downwardapi-volume-96eb1dde-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.951203ms
Mar 22 14:34:26.717: INFO: Pod "downwardapi-volume-96eb1dde-4caf-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023780992s
STEP: Saw pod success
Mar 22 14:34:26.717: INFO: Pod "downwardapi-volume-96eb1dde-4caf-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:34:26.725: INFO: Trying to get logs from node metalk8s-02 pod downwardapi-volume-96eb1dde-4caf-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:34:26.775: INFO: Waiting for pod downwardapi-volume-96eb1dde-4caf-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:34:26.780: INFO: Pod downwardapi-volume-96eb1dde-4caf-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:34:26.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-p5ctz" for this suite.
Mar 22 14:34:32.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:34:33.008: INFO: namespace: e2e-tests-downward-api-p5ctz, resource: bindings, ignored listing per whitelist
Mar 22 14:34:33.014: INFO: namespace e2e-tests-downward-api-p5ctz deletion completed in 6.225663529s

â€¢ [SLOW TEST:8.598 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:34:33.014: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-bg5fq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-9c08da8c-4caf-11e9-99ae-9e98f636e47c
STEP: Creating a pod to test consume secrets
Mar 22 14:34:33.248: INFO: Waiting up to 5m0s for pod "pod-secrets-9c09dda5-4caf-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-secrets-bg5fq" to be "success or failure"
Mar 22 14:34:33.252: INFO: Pod "pod-secrets-9c09dda5-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.137283ms
Mar 22 14:34:35.270: INFO: Pod "pod-secrets-9c09dda5-4caf-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021389868s
STEP: Saw pod success
Mar 22 14:34:35.270: INFO: Pod "pod-secrets-9c09dda5-4caf-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:34:35.275: INFO: Trying to get logs from node metalk8s-01 pod pod-secrets-9c09dda5-4caf-11e9-99ae-9e98f636e47c container secret-volume-test: <nil>
STEP: delete the pod
Mar 22 14:34:35.349: INFO: Waiting for pod pod-secrets-9c09dda5-4caf-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:34:35.354: INFO: Pod pod-secrets-9c09dda5-4caf-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:34:35.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bg5fq" for this suite.
Mar 22 14:34:41.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:34:41.505: INFO: namespace: e2e-tests-secrets-bg5fq, resource: bindings, ignored listing per whitelist
Mar 22 14:34:41.610: INFO: namespace e2e-tests-secrets-bg5fq deletion completed in 6.229256447s

â€¢ [SLOW TEST:8.596 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:34:41.610: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-5nnzx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-5nnzx
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 22 14:34:41.895: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 22 14:35:02.130: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.103:8080/dial?request=hostName&protocol=udp&host=10.233.99.158&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-5nnzx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:35:02.130: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:35:02.301: INFO: Waiting for endpoints: map[]
Mar 22 14:35:02.307: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.103:8080/dial?request=hostName&protocol=udp&host=10.233.122.159&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-5nnzx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:35:02.307: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:35:02.461: INFO: Waiting for endpoints: map[]
Mar 22 14:35:02.467: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.103:8080/dial?request=hostName&protocol=udp&host=10.233.122.90&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-5nnzx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:35:02.467: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:35:02.616: INFO: Waiting for endpoints: map[]
Mar 22 14:35:02.621: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.103:8080/dial?request=hostName&protocol=udp&host=10.233.88.27&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-5nnzx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:35:02.621: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:35:02.757: INFO: Waiting for endpoints: map[]
Mar 22 14:35:02.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.82.103:8080/dial?request=hostName&protocol=udp&host=10.233.82.105&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-5nnzx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:35:02.764: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:35:02.967: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:35:02.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-5nnzx" for this suite.
Mar 22 14:35:27.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:35:27.187: INFO: namespace: e2e-tests-pod-network-test-5nnzx, resource: bindings, ignored listing per whitelist
Mar 22 14:35:27.192: INFO: namespace e2e-tests-pod-network-test-5nnzx deletion completed in 24.200560114s

â€¢ [SLOW TEST:45.582 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:35:27.193: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-k4dgt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Mar 22 14:35:27.467: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-k4dgt" to be "success or failure"
Mar 22 14:35:27.472: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.706269ms
Mar 22 14:35:29.479: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01266755s
Mar 22 14:35:31.488: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021326437s
STEP: Saw pod success
Mar 22 14:35:31.488: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar 22 14:35:31.499: INFO: Trying to get logs from node metalk8s-02 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 22 14:35:31.556: INFO: Waiting for pod pod-host-path-test to disappear
Mar 22 14:35:31.564: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:35:31.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-k4dgt" for this suite.
Mar 22 14:35:37.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:35:37.794: INFO: namespace: e2e-tests-hostpath-k4dgt, resource: bindings, ignored listing per whitelist
Mar 22 14:35:37.800: INFO: namespace e2e-tests-hostpath-k4dgt deletion completed in 6.222864367s

â€¢ [SLOW TEST:10.607 seconds]
[sig-storage] HostPath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:35:37.800: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4d8ss
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:35:38.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2a46036-4caf-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-4d8ss" to be "success or failure"
Mar 22 14:35:38.023: INFO: Pod "downwardapi-volume-c2a46036-4caf-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.92021ms
Mar 22 14:35:40.033: INFO: Pod "downwardapi-volume-c2a46036-4caf-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016950184s
STEP: Saw pod success
Mar 22 14:35:40.033: INFO: Pod "downwardapi-volume-c2a46036-4caf-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:35:40.039: INFO: Trying to get logs from node metalk8s-01 pod downwardapi-volume-c2a46036-4caf-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:35:40.122: INFO: Waiting for pod downwardapi-volume-c2a46036-4caf-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:35:40.159: INFO: Pod downwardapi-volume-c2a46036-4caf-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:35:40.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4d8ss" for this suite.
Mar 22 14:35:46.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:35:46.246: INFO: namespace: e2e-tests-downward-api-4d8ss, resource: bindings, ignored listing per whitelist
Mar 22 14:35:46.462: INFO: namespace e2e-tests-downward-api-4d8ss deletion completed in 6.282824459s

â€¢ [SLOW TEST:8.662 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:35:46.462: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-xbqk9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-xbqk9
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 22 14:35:46.706: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 22 14:36:08.954: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.233.82.101 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xbqk9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:36:08.955: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:36:10.075: INFO: Found all expected endpoints: [netserver-0]
Mar 22 14:36:10.082: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.233.122.93 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xbqk9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:36:10.082: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:36:11.215: INFO: Found all expected endpoints: [netserver-1]
Mar 22 14:36:11.221: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.233.88.38 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xbqk9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:36:11.221: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:36:12.351: INFO: Found all expected endpoints: [netserver-2]
Mar 22 14:36:12.362: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.233.99.163 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xbqk9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:36:12.362: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:36:13.482: INFO: Found all expected endpoints: [netserver-3]
Mar 22 14:36:13.488: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.233.122.160 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-xbqk9 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 22 14:36:13.488: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
Mar 22 14:36:14.610: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:36:14.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-xbqk9" for this suite.
Mar 22 14:36:38.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:36:38.701: INFO: namespace: e2e-tests-pod-network-test-xbqk9, resource: bindings, ignored listing per whitelist
Mar 22 14:36:38.865: INFO: namespace e2e-tests-pod-network-test-xbqk9 deletion completed in 24.240239179s

â€¢ [SLOW TEST:52.403 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:36:38.865: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-2qf4q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-2qf4q
Mar 22 14:36:41.113: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-2qf4q
STEP: checking the pod's current state and verifying that restartCount is present
Mar 22 14:36:41.119: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:40:42.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-2qf4q" for this suite.
Mar 22 14:40:48.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:40:48.631: INFO: namespace: e2e-tests-container-probe-2qf4q, resource: bindings, ignored listing per whitelist
Mar 22 14:40:48.740: INFO: namespace e2e-tests-container-probe-2qf4q deletion completed in 6.195384599s

â€¢ [SLOW TEST:249.875 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:40:48.740: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-h2mzq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar 22 14:40:48.993: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:40:53.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-h2mzq" for this suite.
Mar 22 14:40:59.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:40:59.671: INFO: namespace: e2e-tests-init-container-h2mzq, resource: bindings, ignored listing per whitelist
Mar 22 14:40:59.871: INFO: namespace e2e-tests-init-container-h2mzq deletion completed in 6.26209336s

â€¢ [SLOW TEST:11.131 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:40:59.872: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rnn8g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Mar 22 14:41:00.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-829f4b9d-4cb0-11e9-99ae-9e98f636e47c" in namespace "e2e-tests-downward-api-rnn8g" to be "success or failure"
Mar 22 14:41:00.114: INFO: Pod "downwardapi-volume-829f4b9d-4cb0-11e9-99ae-9e98f636e47c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.958843ms
Mar 22 14:41:02.121: INFO: Pod "downwardapi-volume-829f4b9d-4cb0-11e9-99ae-9e98f636e47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015665061s
STEP: Saw pod success
Mar 22 14:41:02.121: INFO: Pod "downwardapi-volume-829f4b9d-4cb0-11e9-99ae-9e98f636e47c" satisfied condition "success or failure"
Mar 22 14:41:02.132: INFO: Trying to get logs from node metalk8s-04 pod downwardapi-volume-829f4b9d-4cb0-11e9-99ae-9e98f636e47c container client-container: <nil>
STEP: delete the pod
Mar 22 14:41:02.165: INFO: Waiting for pod downwardapi-volume-829f4b9d-4cb0-11e9-99ae-9e98f636e47c to disappear
Mar 22 14:41:02.169: INFO: Pod downwardapi-volume-829f4b9d-4cb0-11e9-99ae-9e98f636e47c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:41:02.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rnn8g" for this suite.
Mar 22 14:41:08.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:41:08.245: INFO: namespace: e2e-tests-downward-api-rnn8g, resource: bindings, ignored listing per whitelist
Mar 22 14:41:08.374: INFO: namespace e2e-tests-downward-api-rnn8g deletion completed in 6.194670034s

â€¢ [SLOW TEST:8.502 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Mar 22 14:41:08.374: INFO: >>> kubeConfig: /tmp/kubeconfig-171814416
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-5dc26
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Mar 22 14:41:08.601: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Mar 22 14:41:12.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-5dc26" for this suite.
Mar 22 14:41:34.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 22 14:41:34.959: INFO: namespace: e2e-tests-init-container-5dc26, resource: bindings, ignored listing per whitelist
Mar 22 14:41:35.092: INFO: namespace e2e-tests-init-container-5dc26 deletion completed in 22.261579905s

â€¢ [SLOW TEST:26.718 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
Mar 22 14:41:35.092: INFO: Running AfterSuite actions on all node
Mar 22 14:41:35.092: INFO: Running AfterSuite actions on node 1
Mar 22 14:41:35.093: INFO: Skipping dumping logs from cluster

Ran 187 of 1814 Specs in 5436.564 seconds
SUCCESS! -- 187 Passed | 0 Failed | 0 Pending | 1627 Skipped PASS

Ginkgo ran 1 suite in 1h30m37.460291025s
Test Suite Passed
