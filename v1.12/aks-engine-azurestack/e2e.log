May 31 00:11:56.194: INFO: Overriding default scale value of zero to 1
May 31 00:11:56.194: INFO: Overriding default milliseconds value of zero to 5000
I0531 00:11:57.381345      21 test_context.go:385] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-190461910
I0531 00:11:57.381511      21 e2e.go:304] Starting e2e run "b34b421a-8338-11e9-88d7-16679dc4b203" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1559261515 - Will randomize all specs
Will run 188 of 1814 specs

May 31 00:11:57.528: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 00:11:57.630: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 31 00:11:57.674: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 31 00:11:57.705: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 31 00:11:57.705: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
May 31 00:11:57.705: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 31 00:11:57.713: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
May 31 00:11:57.713: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
May 31 00:11:57.713: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 31 00:11:57.713: INFO: e2e test version: v1.12.1
May 31 00:11:57.714: INFO: kube-apiserver version: v1.12.8
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:11:57.714: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
May 31 00:11:57.830: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-b453e7e8-8338-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:11:57.838: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b4544bef-8338-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-hsjxw" to be "success or failure"
May 31 00:11:57.850: INFO: Pod "pod-projected-secrets-b4544bef-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 11.974203ms
May 31 00:11:59.855: INFO: Pod "pod-projected-secrets-b4544bef-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017030231s
May 31 00:12:01.861: INFO: Pod "pod-projected-secrets-b4544bef-8338-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023414167s
STEP: Saw pod success
May 31 00:12:01.861: INFO: Pod "pod-projected-secrets-b4544bef-8338-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:12:01.864: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-projected-secrets-b4544bef-8338-11e9-88d7-16679dc4b203 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 00:12:01.941: INFO: Waiting for pod pod-projected-secrets-b4544bef-8338-11e9-88d7-16679dc4b203 to disappear
May 31 00:12:01.943: INFO: Pod pod-projected-secrets-b4544bef-8338-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:12:01.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hsjxw" for this suite.
May 31 00:12:08.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:12:08.050: INFO: namespace: e2e-tests-projected-hsjxw, resource: bindings, ignored listing per whitelist
May 31 00:12:08.084: INFO: namespace e2e-tests-projected-hsjxw deletion completed in 6.138324526s

• [SLOW TEST:10.370 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:12:08.085: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:12:08.176: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba7d7bed-8338-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-rb6lk" to be "success or failure"
May 31 00:12:08.204: INFO: Pod "downwardapi-volume-ba7d7bed-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 28.798548ms
May 31 00:12:10.209: INFO: Pod "downwardapi-volume-ba7d7bed-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032956657s
May 31 00:12:12.212: INFO: Pod "downwardapi-volume-ba7d7bed-8338-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036275055s
STEP: Saw pod success
May 31 00:12:12.212: INFO: Pod "downwardapi-volume-ba7d7bed-8338-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:12:12.214: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod downwardapi-volume-ba7d7bed-8338-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:12:12.262: INFO: Waiting for pod downwardapi-volume-ba7d7bed-8338-11e9-88d7-16679dc4b203 to disappear
May 31 00:12:12.264: INFO: Pod downwardapi-volume-ba7d7bed-8338-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:12:12.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rb6lk" for this suite.
May 31 00:12:18.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:12:18.464: INFO: namespace: e2e-tests-downward-api-rb6lk, resource: bindings, ignored listing per whitelist
May 31 00:12:18.471: INFO: namespace e2e-tests-downward-api-rb6lk deletion completed in 6.203705657s

• [SLOW TEST:10.386 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:12:18.471: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:12:18.658: INFO: Creating deployment "test-recreate-deployment"
May 31 00:12:18.663: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 31 00:12:18.722: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
May 31 00:12:20.729: INFO: Waiting deployment "test-recreate-deployment" to complete
May 31 00:12:20.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:12:22.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:12:24.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:12:26.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694858338, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:12:28.734: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 31 00:12:28.739: INFO: Updating deployment test-recreate-deployment
May 31 00:12:28.739: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 00:12:28.960: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-4zxqj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4zxqj/deployments/test-recreate-deployment,UID:c0be3b8e-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1371,Generation:2,CreationTimestamp:2019-05-31 00:12:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-31 00:12:28 +0000 UTC 2019-05-31 00:12:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-31 00:12:28 +0000 UTC 2019-05-31 00:12:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 31 00:12:28.986: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-4zxqj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4zxqj/replicasets/test-recreate-deployment-7cf749666b,UID:c6d623ff-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1368,Generation:1,CreationTimestamp:2019-05-31 00:12:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment c0be3b8e-8338-11e9-b3f2-001dd80c0014 0xc421540bd7 0xc421540bd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 00:12:28.986: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 31 00:12:28.986: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-4zxqj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4zxqj/replicasets/test-recreate-deployment-79f694ff59,UID:c0c7e6b8-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1360,Generation:2,CreationTimestamp:2019-05-31 00:12:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment c0be3b8e-8338-11e9-b3f2-001dd80c0014 0xc421540b27 0xc421540b28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 00:12:28.989: INFO: Pod "test-recreate-deployment-7cf749666b-w4557" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-w4557,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-4zxqj,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4zxqj/pods/test-recreate-deployment-7cf749666b-w4557,UID:c6d714ff-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1372,Generation:0,CreationTimestamp:2019-05-31 00:12:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b c6d623ff-8338-11e9-b3f2-001dd80c0014 0xc421541747 0xc421541748}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-md88m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-md88m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-md88m true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421541800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421541820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:12:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:12:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:12:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:12:28 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2019-05-31 00:12:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:12:28.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-4zxqj" for this suite.
May 31 00:12:35.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:12:35.138: INFO: namespace: e2e-tests-deployment-4zxqj, resource: bindings, ignored listing per whitelist
May 31 00:12:35.138: INFO: namespace e2e-tests-deployment-4zxqj deletion completed in 6.145822708s

• [SLOW TEST:16.667 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:12:35.140: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
May 31 00:12:35.303: INFO: Waiting up to 5m0s for pod "client-containers-caa870aa-8338-11e9-88d7-16679dc4b203" in namespace "e2e-tests-containers-6gwxh" to be "success or failure"
May 31 00:12:35.307: INFO: Pod "client-containers-caa870aa-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.161235ms
May 31 00:12:37.310: INFO: Pod "client-containers-caa870aa-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00738711s
May 31 00:12:39.314: INFO: Pod "client-containers-caa870aa-8338-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010895685s
STEP: Saw pod success
May 31 00:12:39.314: INFO: Pod "client-containers-caa870aa-8338-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:12:39.317: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod client-containers-caa870aa-8338-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:12:39.329: INFO: Waiting for pod client-containers-caa870aa-8338-11e9-88d7-16679dc4b203 to disappear
May 31 00:12:39.332: INFO: Pod client-containers-caa870aa-8338-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:12:39.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-6gwxh" for this suite.
May 31 00:12:45.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:12:45.395: INFO: namespace: e2e-tests-containers-6gwxh, resource: bindings, ignored listing per whitelist
May 31 00:12:45.432: INFO: namespace e2e-tests-containers-6gwxh deletion completed in 6.096947161s

• [SLOW TEST:10.293 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:12:45.435: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-dmgl
STEP: Creating a pod to test atomic-volume-subpath
May 31 00:12:45.591: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-dmgl" in namespace "e2e-tests-subpath-97t87" to be "success or failure"
May 31 00:12:45.595: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071835ms
May 31 00:12:47.653: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062427976s
May 31 00:12:49.656: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065165737s
May 31 00:12:51.660: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 6.069142806s
May 31 00:12:53.663: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 8.072296567s
May 31 00:12:55.667: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 10.075595527s
May 31 00:12:57.670: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 12.078698384s
May 31 00:12:59.673: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 14.081912141s
May 31 00:13:01.676: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 16.084753293s
May 31 00:13:03.679: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 18.087534443s
May 31 00:13:05.681: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 20.090201291s
May 31 00:13:07.684: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 22.092987838s
May 31 00:13:09.729: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Running", Reason="", readiness=false. Elapsed: 24.13830295s
May 31 00:13:11.732: INFO: Pod "pod-subpath-test-secret-dmgl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.141419998s
STEP: Saw pod success
May 31 00:13:11.733: INFO: Pod "pod-subpath-test-secret-dmgl" satisfied condition "success or failure"
May 31 00:13:11.735: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-subpath-test-secret-dmgl container test-container-subpath-secret-dmgl: <nil>
STEP: delete the pod
May 31 00:13:11.752: INFO: Waiting for pod pod-subpath-test-secret-dmgl to disappear
May 31 00:13:11.755: INFO: Pod pod-subpath-test-secret-dmgl no longer exists
STEP: Deleting pod pod-subpath-test-secret-dmgl
May 31 00:13:11.755: INFO: Deleting pod "pod-subpath-test-secret-dmgl" in namespace "e2e-tests-subpath-97t87"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:13:11.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-97t87" for this suite.
May 31 00:13:17.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:13:17.794: INFO: namespace: e2e-tests-subpath-97t87, resource: bindings, ignored listing per whitelist
May 31 00:13:17.861: INFO: namespace e2e-tests-subpath-97t87 deletion completed in 6.096560685s

• [SLOW TEST:32.427 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:13:17.862: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
May 31 00:13:17.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 cluster-info'
May 31 00:13:18.358: INFO: stderr: ""
May 31 00:13:18.358: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mHeapster\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/heapster/proxy\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mtiller-deploy\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/tiller-deploy:tiller/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:13:18.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-854fr" for this suite.
May 31 00:13:24.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:13:24.438: INFO: namespace: e2e-tests-kubectl-854fr, resource: bindings, ignored listing per whitelist
May 31 00:13:24.491: INFO: namespace e2e-tests-kubectl-854fr deletion completed in 6.128318247s

• [SLOW TEST:6.629 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:13:24.492: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:13:24.577: INFO: Creating deployment "nginx-deployment"
May 31 00:13:24.583: INFO: Waiting for observed generation 1
May 31 00:13:26.594: INFO: Waiting for all required pods to come up
May 31 00:13:26.599: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 31 00:13:34.615: INFO: Waiting for deployment "nginx-deployment" to complete
May 31 00:13:34.620: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 31 00:13:34.625: INFO: Updating deployment nginx-deployment
May 31 00:13:34.625: INFO: Waiting for observed generation 2
May 31 00:13:36.635: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 31 00:13:36.640: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 31 00:13:36.648: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 31 00:13:36.655: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 31 00:13:36.655: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 31 00:13:36.659: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 31 00:13:36.663: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 31 00:13:36.663: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 31 00:13:36.672: INFO: Updating deployment nginx-deployment
May 31 00:13:36.672: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 31 00:13:36.684: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 31 00:13:36.708: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 00:13:36.759: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-w6tfc/deployments/nginx-deployment,UID:e808beef-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1791,Generation:3,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-05-31 00:13:34 +0000 UTC 2019-05-31 00:13:24 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.} {Available False 2019-05-31 00:13:36 +0000 UTC 2019-05-31 00:13:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

May 31 00:13:36.779: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-w6tfc/replicasets/nginx-deployment-7dc8f79789,UID:ee0740e4-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1772,Generation:3,CreationTimestamp:2019-05-31 00:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment e808beef-8338-11e9-b3f2-001dd80c0014 0xc4218158f7 0xc4218158f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 00:13:36.779: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 31 00:13:36.780: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-w6tfc/replicasets/nginx-deployment-7f9675fb8b,UID:e80a9d01-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1771,Generation:3,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment e808beef-8338-11e9-b3f2-001dd80c0014 0xc421815a07 0xc421815a08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 31 00:13:36.804: INFO: Pod "nginx-deployment-7dc8f79789-2b46p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-2b46p,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-2b46p,UID:ef44e6a7-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1809,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc421315490 0xc421315491}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421315500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421315520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.805: INFO: Pod "nginx-deployment-7dc8f79789-99ph8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-99ph8,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-99ph8,UID:ef44c55e-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1813,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc421315600 0xc421315601}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421315670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421315690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.805: INFO: Pod "nginx-deployment-7dc8f79789-ff7xb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-ff7xb,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-ff7xb,UID:ef44da4f-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1818,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc421315700 0xc421315701}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421315770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421315790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.805: INFO: Pod "nginx-deployment-7dc8f79789-gvs2l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-gvs2l,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-gvs2l,UID:ef4051a9-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1800,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc421315870 0xc421315871}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4213158e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421315910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2019-05-31 00:13:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.805: INFO: Pod "nginx-deployment-7dc8f79789-h2s52" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-h2s52,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-h2s52,UID:ee0c13ac-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1742,Generation:0,CreationTimestamp:2019-05-31 00:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc421315b30 0xc421315b31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421315bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421315bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2019-05-31 00:13:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.805: INFO: Pod "nginx-deployment-7dc8f79789-hxtqb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-hxtqb,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-hxtqb,UID:ee16855c-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1756,Generation:0,CreationTimestamp:2019-05-31 00:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc421315d10 0xc421315d11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421315d80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421315db0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2019-05-31 00:13:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.805: INFO: Pod "nginx-deployment-7dc8f79789-j8k9h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-j8k9h,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-j8k9h,UID:ee1a72b5-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1758,Generation:0,CreationTimestamp:2019-05-31 00:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc421315f00 0xc421315f01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421315f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421315f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2019-05-31 00:13:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.806: INFO: Pod "nginx-deployment-7dc8f79789-jfg4n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-jfg4n,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-jfg4n,UID:ef4909d5-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1821,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc420efe060 0xc420efe061}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420efe0d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420efe0f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.806: INFO: Pod "nginx-deployment-7dc8f79789-kgt49" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-kgt49,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-kgt49,UID:ef412b00-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1789,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc420efe180 0xc420efe181}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420efe1f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420efe210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.806: INFO: Pod "nginx-deployment-7dc8f79789-mtgkz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-mtgkz,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-mtgkz,UID:ee0c19ca-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1744,Generation:0,CreationTimestamp:2019-05-31 00:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc420efe2e0 0xc420efe2e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420efe3c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420efe410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-31 00:13:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.806: INFO: Pod "nginx-deployment-7dc8f79789-qm7tn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-qm7tn,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-qm7tn,UID:ef44d08e-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1816,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc420efe5e0 0xc420efe5e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420efe690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420efe6b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.806: INFO: Pod "nginx-deployment-7dc8f79789-vwl7x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-vwl7x,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-vwl7x,UID:ef413801-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1787,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc420efe820 0xc420efe821}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420efe890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420efe8b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.806: INFO: Pod "nginx-deployment-7dc8f79789-zhbg2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-zhbg2,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7dc8f79789-zhbg2,UID:ee0a73ff-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1736,Generation:0,CreationTimestamp:2019-05-31 00:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 ee0740e4-8338-11e9-b3f2-001dd80c0014 0xc420efea20 0xc420efea21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420efeaf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420efeb10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2019-05-31 00:13:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.807: INFO: Pod "nginx-deployment-7f9675fb8b-557gr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-557gr,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-557gr,UID:ef446784-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1812,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420efec90 0xc420efec91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420efedb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420efede0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.807: INFO: Pod "nginx-deployment-7f9675fb8b-5c48r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-5c48r,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-5c48r,UID:ef44d149-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1814,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420efeec0 0xc420efeec1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420efef60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420efefa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.807: INFO: Pod "nginx-deployment-7f9675fb8b-5cxpz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-5cxpz,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-5cxpz,UID:ef3f15e5-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1803,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420eff040 0xc420eff041}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420eff150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420eff170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2019-05-31 00:13:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.807: INFO: Pod "nginx-deployment-7f9675fb8b-5kbn5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-5kbn5,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-5kbn5,UID:e8113025-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1692,Generation:0,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420eff2f0 0xc420eff2f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420eff390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420eff3d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.3.12,StartTime:2019-05-31 00:13:24 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 00:13:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://cecd0369ea5fe0295886aef81fdbcd0142457bb1a62238461eb67b8d89458364}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.807: INFO: Pod "nginx-deployment-7f9675fb8b-7znl8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-7znl8,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-7znl8,UID:ef4915a4-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1822,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420eff4f0 0xc420eff4f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420eff5a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420eff670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.807: INFO: Pod "nginx-deployment-7f9675fb8b-c479s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-c479s,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-c479s,UID:e8129e01-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1664,Generation:0,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420eff6e0 0xc420eff6e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420eff7a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420eff7c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.3.10,StartTime:2019-05-31 00:13:24 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 00:13:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1bc6d3db706951d4fecb4cec0a1028c6a54522c960bdd7b4eec71f6797f93f52}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.807: INFO: Pod "nginx-deployment-7f9675fb8b-chn75" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-chn75,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-chn75,UID:ef44c6b4-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1815,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420eff880 0xc420eff881}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420eff8e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420eff900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.808: INFO: Pod "nginx-deployment-7f9675fb8b-ddwlx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-ddwlx,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-ddwlx,UID:ef412b22-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1788,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420eff970 0xc420eff971}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420eff9d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420eff9f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.808: INFO: Pod "nginx-deployment-7f9675fb8b-f6hrx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-f6hrx,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-f6hrx,UID:ef495ce9-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1826,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420effa60 0xc420effa61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420effac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420effae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.808: INFO: Pod "nginx-deployment-7f9675fb8b-fdngn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-fdngn,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-fdngn,UID:ef4136dd-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1817,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420effb50 0xc420effb51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420effbb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420effbd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-31 00:13:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.808: INFO: Pod "nginx-deployment-7f9675fb8b-fv845" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-fv845,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-fv845,UID:ef48fa7d-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1825,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420effc80 0xc420effc81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420effce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420effd00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.808: INFO: Pod "nginx-deployment-7f9675fb8b-gkxb8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-gkxb8,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-gkxb8,UID:e80ec28d-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1672,Generation:0,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420effd70 0xc420effd71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420effdd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420effdf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.3.13,StartTime:2019-05-31 00:13:24 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 00:13:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://dbfadb97ee4aeb3a2ab048f72bd9962be768d998a224620aa16d73c465a181a5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.809: INFO: Pod "nginx-deployment-7f9675fb8b-jkbmx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-jkbmx,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-jkbmx,UID:ef44df18-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1810,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc420efff60 0xc420efff61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420effff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cf2010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.809: INFO: Pod "nginx-deployment-7f9675fb8b-jqp8k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-jqp8k,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-jqp8k,UID:ef48ceab-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1823,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc421cf2080 0xc421cf2081}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cf20e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cf2100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.809: INFO: Pod "nginx-deployment-7f9675fb8b-lx4nn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-lx4nn,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-lx4nn,UID:ef496b3c-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1827,Generation:0,CreationTimestamp:2019-05-31 00:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc421cf2170 0xc421cf2171}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cf21d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cf21f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.809: INFO: Pod "nginx-deployment-7f9675fb8b-nfmdw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-nfmdw,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-nfmdw,UID:e8154825-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1689,Generation:0,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc421cf2260 0xc421cf2261}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cf22c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cf22e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.0.6,StartTime:2019-05-31 00:13:24 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 00:13:30 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a244bcac341618ae6e46913b449dd58b7a44549bd799c3a6dffa1c9258110aea}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.809: INFO: Pod "nginx-deployment-7f9675fb8b-rd7zf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-rd7zf,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-rd7zf,UID:e81557fd-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1675,Generation:0,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc421cf23a0 0xc421cf23a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cf2400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cf2420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:10.244.2.6,StartTime:2019-05-31 00:13:24 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 00:13:30 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f4e156e7208c595346a6c327d25c5730bda56b482f938908d46fb8c5e14f7359}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.809: INFO: Pod "nginx-deployment-7f9675fb8b-sqt8r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-sqt8r,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-sqt8r,UID:e81117f0-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1686,Generation:0,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc421cf24e0 0xc421cf24e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cf2540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cf2560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.0.5,StartTime:2019-05-31 00:13:24 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 00:13:30 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://809f0bea71dd11ac00394a6fb3cdaf3dc24bd2cc0d8ca8ad4b143ffbaa489252}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.810: INFO: Pod "nginx-deployment-7f9675fb8b-vb4vc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-vb4vc,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-vb4vc,UID:e8156365-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1667,Generation:0,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc421cf2620 0xc421cf2621}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cf2680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cf26a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.3.9,StartTime:2019-05-31 00:13:24 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 00:13:27 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://63defe26617deb9b12d7cbe46690757284228cd5a7470ea88261b9bc24adb330}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 00:13:36.810: INFO: Pod "nginx-deployment-7f9675fb8b-xzshl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-xzshl,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-w6tfc,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w6tfc/pods/nginx-deployment-7f9675fb8b-xzshl,UID:e81337d3-8338-11e9-b3f2-001dd80c0014,ResourceVersion:1661,Generation:0,CreationTimestamp:2019-05-31 00:13:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b e80a9d01-8338-11e9-b3f2-001dd80c0014 0xc421cf2760 0xc421cf2761}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-tkx2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tkx2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tkx2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cf27c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cf27e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:13:24 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.3.11,StartTime:2019-05-31 00:13:24 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 00:13:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://acfcc7438e3b6e814cd1099c0d92176d883a8d257f9d38538db4aa5cc9e93173}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:13:36.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-w6tfc" for this suite.
May 31 00:13:42.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:13:42.955: INFO: namespace: e2e-tests-deployment-w6tfc, resource: bindings, ignored listing per whitelist
May 31 00:13:42.961: INFO: namespace e2e-tests-deployment-w6tfc deletion completed in 6.113539688s

• [SLOW TEST:18.469 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:13:42.961: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-f309594a-8338-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:13:43.048: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-f2kjh" to be "success or failure"
May 31 00:13:43.053: INFO: Pod "pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.322637ms
May 31 00:13:45.056: INFO: Pod "pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007424865s
May 31 00:13:47.058: INFO: Pod "pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009985988s
May 31 00:13:49.061: INFO: Pod "pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012951013s
May 31 00:13:51.064: INFO: Pod "pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016010838s
STEP: Saw pod success
May 31 00:13:51.065: INFO: Pod "pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:13:51.067: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203 container secret-volume-test: <nil>
STEP: delete the pod
May 31 00:13:51.083: INFO: Waiting for pod pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203 to disappear
May 31 00:13:51.087: INFO: Pod pod-projected-secrets-f309ca71-8338-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:13:51.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-f2kjh" for this suite.
May 31 00:13:57.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:13:57.131: INFO: namespace: e2e-tests-projected-f2kjh, resource: bindings, ignored listing per whitelist
May 31 00:13:57.196: INFO: namespace e2e-tests-projected-f2kjh deletion completed in 6.106409005s

• [SLOW TEST:14.235 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:13:57.196: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-m9x68
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
May 31 00:13:57.300: INFO: Found 0 stateful pods, waiting for 3
May 31 00:14:07.303: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 00:14:07.303: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 00:14:07.303: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 31 00:14:07.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-m9x68 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 00:14:07.566: INFO: stderr: ""
May 31 00:14:07.566: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 00:14:07.566: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 31 00:14:17.593: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 31 00:14:27.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-m9x68 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 00:14:27.875: INFO: stderr: ""
May 31 00:14:27.875: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 00:14:27.875: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 00:14:37.890: INFO: Waiting for StatefulSet e2e-tests-statefulset-m9x68/ss2 to complete update
May 31 00:14:37.890: INFO: Waiting for Pod e2e-tests-statefulset-m9x68/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 31 00:14:37.890: INFO: Waiting for Pod e2e-tests-statefulset-m9x68/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 31 00:14:47.896: INFO: Waiting for StatefulSet e2e-tests-statefulset-m9x68/ss2 to complete update
May 31 00:14:47.896: INFO: Waiting for Pod e2e-tests-statefulset-m9x68/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 31 00:14:47.896: INFO: Waiting for Pod e2e-tests-statefulset-m9x68/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 31 00:14:57.897: INFO: Waiting for StatefulSet e2e-tests-statefulset-m9x68/ss2 to complete update
May 31 00:14:57.897: INFO: Waiting for Pod e2e-tests-statefulset-m9x68/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 31 00:15:07.896: INFO: Waiting for StatefulSet e2e-tests-statefulset-m9x68/ss2 to complete update
STEP: Rolling back to a previous revision
May 31 00:15:17.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-m9x68 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 00:15:18.174: INFO: stderr: ""
May 31 00:15:18.174: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 00:15:18.174: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 00:15:28.201: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 31 00:15:38.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-m9x68 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 00:15:38.476: INFO: stderr: ""
May 31 00:15:38.476: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 00:15:38.476: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 00:16:08.490: INFO: Deleting all statefulset in ns e2e-tests-statefulset-m9x68
May 31 00:16:08.492: INFO: Scaling statefulset ss2 to 0
May 31 00:16:38.505: INFO: Waiting for statefulset status.replicas updated to 0
May 31 00:16:38.508: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:16:38.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-m9x68" for this suite.
May 31 00:16:44.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:16:44.610: INFO: namespace: e2e-tests-statefulset-m9x68, resource: bindings, ignored listing per whitelist
May 31 00:16:44.610: INFO: namespace e2e-tests-statefulset-m9x68 deletion completed in 6.087484411s

• [SLOW TEST:167.414 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:16:44.612: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:16:44.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f4f3cb7-8339-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-f6nbw" to be "success or failure"
May 31 00:16:44.699: INFO: Pod "downwardapi-volume-5f4f3cb7-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.878724ms
May 31 00:16:46.702: INFO: Pod "downwardapi-volume-5f4f3cb7-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005801402s
May 31 00:16:48.706: INFO: Pod "downwardapi-volume-5f4f3cb7-8339-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009074983s
STEP: Saw pod success
May 31 00:16:48.706: INFO: Pod "downwardapi-volume-5f4f3cb7-8339-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:16:48.708: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-5f4f3cb7-8339-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:16:48.722: INFO: Waiting for pod downwardapi-volume-5f4f3cb7-8339-11e9-88d7-16679dc4b203 to disappear
May 31 00:16:48.724: INFO: Pod downwardapi-volume-5f4f3cb7-8339-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:16:48.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-f6nbw" for this suite.
May 31 00:16:54.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:16:54.787: INFO: namespace: e2e-tests-downward-api-f6nbw, resource: bindings, ignored listing per whitelist
May 31 00:16:54.837: INFO: namespace e2e-tests-downward-api-f6nbw deletion completed in 6.109063092s

• [SLOW TEST:10.225 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:16:54.837: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 31 00:16:54.934: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-vpcdw,SelfLink:/api/v1/namespaces/e2e-tests-watch-vpcdw/configmaps/e2e-watch-test-resource-version,UID:6567b97e-8339-11e9-b3f2-001dd80c0014,ResourceVersion:2773,Generation:0,CreationTimestamp:2019-05-31 00:16:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 00:16:54.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-vpcdw,SelfLink:/api/v1/namespaces/e2e-tests-watch-vpcdw/configmaps/e2e-watch-test-resource-version,UID:6567b97e-8339-11e9-b3f2-001dd80c0014,ResourceVersion:2774,Generation:0,CreationTimestamp:2019-05-31 00:16:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:16:54.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-vpcdw" for this suite.
May 31 00:17:00.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:17:01.032: INFO: namespace: e2e-tests-watch-vpcdw, resource: bindings, ignored listing per whitelist
May 31 00:17:01.090: INFO: namespace e2e-tests-watch-vpcdw deletion completed in 6.153156668s

• [SLOW TEST:6.253 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:17:01.093: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-787ht
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
May 31 00:17:01.191: INFO: Found 0 stateful pods, waiting for 3
May 31 00:17:11.195: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 00:17:11.195: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 00:17:11.195: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 31 00:17:11.220: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 31 00:17:21.262: INFO: Updating stateful set ss2
May 31 00:17:21.269: INFO: Waiting for Pod e2e-tests-statefulset-787ht/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 31 00:17:31.302: INFO: Found 1 stateful pods, waiting for 3
May 31 00:17:41.325: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 00:17:41.325: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 00:17:41.325: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 31 00:17:41.347: INFO: Updating stateful set ss2
May 31 00:17:41.353: INFO: Waiting for Pod e2e-tests-statefulset-787ht/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 31 00:17:51.378: INFO: Updating stateful set ss2
May 31 00:17:51.384: INFO: Waiting for StatefulSet e2e-tests-statefulset-787ht/ss2 to complete update
May 31 00:17:51.384: INFO: Waiting for Pod e2e-tests-statefulset-787ht/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 00:18:01.390: INFO: Deleting all statefulset in ns e2e-tests-statefulset-787ht
May 31 00:18:01.392: INFO: Scaling statefulset ss2 to 0
May 31 00:18:21.405: INFO: Waiting for statefulset status.replicas updated to 0
May 31 00:18:21.407: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:18:21.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-787ht" for this suite.
May 31 00:18:27.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:18:27.510: INFO: namespace: e2e-tests-statefulset-787ht, resource: bindings, ignored listing per whitelist
May 31 00:18:27.535: INFO: namespace e2e-tests-statefulset-787ht deletion completed in 6.111902233s

• [SLOW TEST:86.443 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:18:27.538: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:18:27.646: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 31 00:18:32.649: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 31 00:18:32.649: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 00:18:32.674: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-9szjv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9szjv/deployments/test-cleanup-deployment,UID:9fa9842e-8339-11e9-b3f2-001dd80c0014,ResourceVersion:3205,Generation:1,CreationTimestamp:2019-05-31 00:18:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 31 00:18:32.679: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May 31 00:18:32.679: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 31 00:18:32.679: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-9szjv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9szjv/replicasets/test-cleanup-controller,UID:9cac7146-8339-11e9-b3f2-001dd80c0014,ResourceVersion:3206,Generation:1,CreationTimestamp:2019-05-31 00:18:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 9fa9842e-8339-11e9-b3f2-001dd80c0014 0xc421314c4f 0xc421314c60}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 31 00:18:32.706: INFO: Pod "test-cleanup-controller-g7lrt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-g7lrt,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-9szjv,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9szjv/pods/test-cleanup-controller-g7lrt,UID:9cadf955-8339-11e9-b3f2-001dd80c0014,ResourceVersion:3198,Generation:0,CreationTimestamp:2019-05-31 00:18:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 9cac7146-8339-11e9-b3f2-001dd80c0014 0xc4210104ef 0xc421010500}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-crdz9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-crdz9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-crdz9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421010560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421010580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:18:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:18:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:18:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:18:27 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.3.35,StartTime:2019-05-31 00:18:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 00:18:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://02f789cd5cb36fd60fa8368b7da1ac10a9420925fe7f486ce68ca3b89a29ead0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:18:32.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-9szjv" for this suite.
May 31 00:18:38.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:18:38.823: INFO: namespace: e2e-tests-deployment-9szjv, resource: bindings, ignored listing per whitelist
May 31 00:18:38.832: INFO: namespace e2e-tests-deployment-9szjv deletion completed in 6.114008548s

• [SLOW TEST:11.295 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:18:38.833: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 31 00:18:43.441: INFO: Successfully updated pod "pod-update-a364e44b-8339-11e9-88d7-16679dc4b203"
STEP: verifying the updated pod is in kubernetes
May 31 00:18:43.446: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:18:43.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-rfb2k" for this suite.
May 31 00:19:05.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:19:05.496: INFO: namespace: e2e-tests-pods-rfb2k, resource: bindings, ignored listing per whitelist
May 31 00:19:05.538: INFO: namespace e2e-tests-pods-rfb2k deletion completed in 22.088510824s

• [SLOW TEST:26.705 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:19:05.539: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-b34e5cbc-8339-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 00:19:05.625: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b34ee99a-8339-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-dn45n" to be "success or failure"
May 31 00:19:05.629: INFO: Pod "pod-projected-configmaps-b34ee99a-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.874257ms
May 31 00:19:07.632: INFO: Pod "pod-projected-configmaps-b34ee99a-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007048441s
May 31 00:19:09.635: INFO: Pod "pod-projected-configmaps-b34ee99a-8339-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009787639s
STEP: Saw pod success
May 31 00:19:09.635: INFO: Pod "pod-projected-configmaps-b34ee99a-8339-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:19:09.637: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-projected-configmaps-b34ee99a-8339-11e9-88d7-16679dc4b203 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 00:19:09.658: INFO: Waiting for pod pod-projected-configmaps-b34ee99a-8339-11e9-88d7-16679dc4b203 to disappear
May 31 00:19:09.660: INFO: Pod pod-projected-configmaps-b34ee99a-8339-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:19:09.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dn45n" for this suite.
May 31 00:19:15.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:19:15.691: INFO: namespace: e2e-tests-projected-dn45n, resource: bindings, ignored listing per whitelist
May 31 00:19:15.762: INFO: namespace e2e-tests-projected-dn45n deletion completed in 6.095985571s

• [SLOW TEST:10.223 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:19:15.762: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-7qkz6 in namespace e2e-tests-proxy-q6trf
I0531 00:19:15.855929      21 runners.go:180] Created replication controller with name: proxy-service-7qkz6, namespace: e2e-tests-proxy-q6trf, replica count: 1
I0531 00:19:16.906284      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 00:19:17.906533      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 00:19:18.906771      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 00:19:19.907004      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 00:19:20.907247      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 00:19:21.907579      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 00:19:22.907815      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 00:19:23.908032      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 00:19:24.908262      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 00:19:25.908471      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 00:19:26.909308      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 00:19:27.909799      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 00:19:28.912943      21 runners.go:180] proxy-service-7qkz6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 00:19:28.919: INFO: setup took 13.079951319s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 31 00:19:28.932: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 13.624193ms)
May 31 00:19:28.933: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 13.659294ms)
May 31 00:19:28.937: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 17.00384ms)
May 31 00:19:28.941: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 21.718407ms)
May 31 00:19:28.942: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 22.795423ms)
May 31 00:19:28.942: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 22.521218ms)
May 31 00:19:28.942: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 22.572819ms)
May 31 00:19:28.943: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 23.965539ms)
May 31 00:19:28.944: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 24.836152ms)
May 31 00:19:28.945: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 25.280258ms)
May 31 00:19:28.945: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 25.178457ms)
May 31 00:19:28.945: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 25.829666ms)
May 31 00:19:28.945: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 25.942967ms)
May 31 00:19:28.945: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 25.659763ms)
May 31 00:19:28.946: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 26.493675ms)
May 31 00:19:28.946: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 27.483089ms)
May 31 00:19:28.958: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 11.32346ms)
May 31 00:19:28.958: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 11.30366ms)
May 31 00:19:28.958: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 11.716865ms)
May 31 00:19:28.958: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 11.33636ms)
May 31 00:19:28.961: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 14.0785ms)
May 31 00:19:28.962: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 14.085999ms)
May 31 00:19:28.963: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 15.907325ms)
May 31 00:19:28.963: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 15.942425ms)
May 31 00:19:28.964: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 16.189829ms)
May 31 00:19:28.964: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 16.135528ms)
May 31 00:19:28.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 20.931496ms)
May 31 00:19:28.970: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 22.951824ms)
May 31 00:19:28.970: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 22.895024ms)
May 31 00:19:28.971: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 24.459946ms)
May 31 00:19:28.971: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 24.266043ms)
May 31 00:19:28.971: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 23.973439ms)
May 31 00:19:28.979: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 7.991213ms)
May 31 00:19:28.990: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 18.096156ms)
May 31 00:19:28.990: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 18.276158ms)
May 31 00:19:28.991: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 19.493076ms)
May 31 00:19:28.992: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 19.365074ms)
May 31 00:19:28.992: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 20.238186ms)
May 31 00:19:28.992: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 19.980682ms)
May 31 00:19:28.992: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 20.215686ms)
May 31 00:19:28.992: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 19.979282ms)
May 31 00:19:28.992: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 20.50339ms)
May 31 00:19:28.992: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 20.982097ms)
May 31 00:19:28.993: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 21.443103ms)
May 31 00:19:28.993: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 21.285801ms)
May 31 00:19:28.993: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 20.559791ms)
May 31 00:19:28.993: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 21.2207ms)
May 31 00:19:28.993: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 21.467003ms)
May 31 00:19:29.005: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 12.363875ms)
May 31 00:19:29.012: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 18.856966ms)
May 31 00:19:29.012: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 18.519662ms)
May 31 00:19:29.013: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 19.051269ms)
May 31 00:19:29.013: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 19.249672ms)
May 31 00:19:29.013: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 19.587377ms)
May 31 00:19:29.013: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 19.322973ms)
May 31 00:19:29.014: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 20.331487ms)
May 31 00:19:29.014: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 21.270501ms)
May 31 00:19:29.014: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 20.990197ms)
May 31 00:19:29.016: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 21.368402ms)
May 31 00:19:29.016: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 21.839309ms)
May 31 00:19:29.016: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 22.695321ms)
May 31 00:19:29.016: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 22.079512ms)
May 31 00:19:29.016: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 22.905224ms)
May 31 00:19:29.016: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 22.875024ms)
May 31 00:19:29.022: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 5.391477ms)
May 31 00:19:29.025: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 8.081915ms)
May 31 00:19:29.025: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 8.51922ms)
May 31 00:19:29.028: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 10.996256ms)
May 31 00:19:29.028: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 10.954355ms)
May 31 00:19:29.029: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 11.445162ms)
May 31 00:19:29.031: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 13.43749ms)
May 31 00:19:29.035: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 17.493647ms)
May 31 00:19:29.035: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 17.973953ms)
May 31 00:19:29.038: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 20.731193ms)
May 31 00:19:29.038: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 20.665292ms)
May 31 00:19:29.039: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 21.713707ms)
May 31 00:19:29.040: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 22.343015ms)
May 31 00:19:29.040: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 22.511418ms)
May 31 00:19:29.040: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 22.506718ms)
May 31 00:19:29.040: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 23.435231ms)
May 31 00:19:29.049: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 8.461619ms)
May 31 00:19:29.051: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 9.842639ms)
May 31 00:19:29.054: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 12.916682ms)
May 31 00:19:29.055: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 13.912297ms)
May 31 00:19:29.055: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 13.301788ms)
May 31 00:19:29.055: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 13.271088ms)
May 31 00:19:29.056: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 14.345802ms)
May 31 00:19:29.056: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 15.278816ms)
May 31 00:19:29.056: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 15.556419ms)
May 31 00:19:29.056: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 14.946611ms)
May 31 00:19:29.057: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 16.27123ms)
May 31 00:19:29.057: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 16.598234ms)
May 31 00:19:29.057: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 15.917525ms)
May 31 00:19:29.057: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 16.563133ms)
May 31 00:19:29.059: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 17.765051ms)
May 31 00:19:29.060: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 18.714264ms)
May 31 00:19:29.069: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 9.728337ms)
May 31 00:19:29.069: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 9.357632ms)
May 31 00:19:29.070: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 9.95494ms)
May 31 00:19:29.070: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 10.344946ms)
May 31 00:19:29.071: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 9.782838ms)
May 31 00:19:29.071: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 10.324246ms)
May 31 00:19:29.071: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 9.994241ms)
May 31 00:19:29.075: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 14.075199ms)
May 31 00:19:29.075: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 13.652593ms)
May 31 00:19:29.075: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 14.658107ms)
May 31 00:19:29.075: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 14.374403ms)
May 31 00:19:29.075: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 14.465204ms)
May 31 00:19:29.075: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 14.316702ms)
May 31 00:19:29.075: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 13.952497ms)
May 31 00:19:29.075: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 13.824395ms)
May 31 00:19:29.075: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 14.253201ms)
May 31 00:19:29.083: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 7.231902ms)
May 31 00:19:29.084: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 8.51602ms)
May 31 00:19:29.084: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 8.48032ms)
May 31 00:19:29.085: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 10.081343ms)
May 31 00:19:29.086: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 10.542149ms)
May 31 00:19:29.086: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 10.461048ms)
May 31 00:19:29.086: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 10.403447ms)
May 31 00:19:29.086: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 11.198658ms)
May 31 00:19:29.087: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 11.29446ms)
May 31 00:19:29.087: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 11.761566ms)
May 31 00:19:29.088: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 12.171072ms)
May 31 00:19:29.089: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 13.217386ms)
May 31 00:19:29.090: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 14.921311ms)
May 31 00:19:29.090: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 14.899011ms)
May 31 00:19:29.090: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 14.82261ms)
May 31 00:19:29.092: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 15.955626ms)
May 31 00:19:29.098: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 6.37319ms)
May 31 00:19:29.099: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 6.697094ms)
May 31 00:19:29.099: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 6.820896ms)
May 31 00:19:29.099: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 6.927998ms)
May 31 00:19:29.099: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 7.231002ms)
May 31 00:19:29.100: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 7.439005ms)
May 31 00:19:29.100: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 7.547506ms)
May 31 00:19:29.100: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 7.645908ms)
May 31 00:19:29.100: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 7.611907ms)
May 31 00:19:29.103: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 10.228745ms)
May 31 00:19:29.103: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 10.864454ms)
May 31 00:19:29.103: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 10.542549ms)
May 31 00:19:29.103: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 10.63405ms)
May 31 00:19:29.103: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 11.823867ms)
May 31 00:19:29.104: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 11.128558ms)
May 31 00:19:29.104: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 11.726966ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 16.486833ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 16.24623ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 16.727036ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 16.710836ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 16.544133ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 16.782538ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 16.676236ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 16.769137ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 16.890338ms)
May 31 00:19:29.121: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 16.688936ms)
May 31 00:19:29.122: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 16.94214ms)
May 31 00:19:29.122: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 17.024341ms)
May 31 00:19:29.122: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 17.439646ms)
May 31 00:19:29.122: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 17.346745ms)
May 31 00:19:29.122: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 17.495947ms)
May 31 00:19:29.122: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 18.005454ms)
May 31 00:19:29.133: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 10.207244ms)
May 31 00:19:29.133: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 9.656837ms)
May 31 00:19:29.135: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 12.141072ms)
May 31 00:19:29.136: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 12.537377ms)
May 31 00:19:29.136: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 12.431576ms)
May 31 00:19:29.136: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 12.154372ms)
May 31 00:19:29.136: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 12.851382ms)
May 31 00:19:29.136: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 13.336189ms)
May 31 00:19:29.136: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 12.558078ms)
May 31 00:19:29.136: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 12.75168ms)
May 31 00:19:29.136: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 13.704393ms)
May 31 00:19:29.136: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 13.552492ms)
May 31 00:19:29.137: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 13.121886ms)
May 31 00:19:29.137: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 13.639893ms)
May 31 00:19:29.137: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 14.520705ms)
May 31 00:19:29.137: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 14.467504ms)
May 31 00:19:29.145: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 7.183601ms)
May 31 00:19:29.145: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 7.597407ms)
May 31 00:19:29.146: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 8.014813ms)
May 31 00:19:29.146: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 8.223016ms)
May 31 00:19:29.146: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 7.960113ms)
May 31 00:19:29.146: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 8.565421ms)
May 31 00:19:29.147: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 9.525334ms)
May 31 00:19:29.147: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 9.496835ms)
May 31 00:19:29.148: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 10.854954ms)
May 31 00:19:29.149: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 11.837967ms)
May 31 00:19:29.150: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 12.325975ms)
May 31 00:19:29.150: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 12.336474ms)
May 31 00:19:29.150: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 12.632579ms)
May 31 00:19:29.150: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 12.678079ms)
May 31 00:19:29.150: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 12.680779ms)
May 31 00:19:29.151: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 13.251087ms)
May 31 00:19:29.160: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 8.913626ms)
May 31 00:19:29.161: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 10.061943ms)
May 31 00:19:29.161: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 9.89924ms)
May 31 00:19:29.161: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 9.999641ms)
May 31 00:19:29.162: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 11.424961ms)
May 31 00:19:29.162: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 10.56555ms)
May 31 00:19:29.163: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 10.978155ms)
May 31 00:19:29.163: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 11.29476ms)
May 31 00:19:29.164: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 12.816381ms)
May 31 00:19:29.165: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 13.205286ms)
May 31 00:19:29.165: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 13.767094ms)
May 31 00:19:29.166: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 13.860796ms)
May 31 00:19:29.166: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 13.840196ms)
May 31 00:19:29.166: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 13.779295ms)
May 31 00:19:29.166: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 14.053998ms)
May 31 00:19:29.166: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 13.983097ms)
May 31 00:19:29.170: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 4.135158ms)
May 31 00:19:29.173: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 7.416905ms)
May 31 00:19:29.174: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 7.293503ms)
May 31 00:19:29.174: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 7.346104ms)
May 31 00:19:29.174: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 7.946612ms)
May 31 00:19:29.175: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 8.723224ms)
May 31 00:19:29.178: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 11.514863ms)
May 31 00:19:29.178: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 11.911369ms)
May 31 00:19:29.179: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 12.395075ms)
May 31 00:19:29.179: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 12.399175ms)
May 31 00:19:29.179: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 12.919282ms)
May 31 00:19:29.179: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 12.999083ms)
May 31 00:19:29.180: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 13.733294ms)
May 31 00:19:29.180: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 13.945997ms)
May 31 00:19:29.181: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 14.1755ms)
May 31 00:19:29.181: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 13.989997ms)
May 31 00:19:29.189: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 7.555207ms)
May 31 00:19:29.191: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 10.292345ms)
May 31 00:19:29.191: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 9.517335ms)
May 31 00:19:29.191: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 9.694837ms)
May 31 00:19:29.191: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 10.129243ms)
May 31 00:19:29.192: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 10.869753ms)
May 31 00:19:29.192: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 11.046856ms)
May 31 00:19:29.192: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 10.483248ms)
May 31 00:19:29.192: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 10.950055ms)
May 31 00:19:29.192: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 10.222045ms)
May 31 00:19:29.192: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 10.888354ms)
May 31 00:19:29.193: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 12.056171ms)
May 31 00:19:29.194: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 12.463476ms)
May 31 00:19:29.194: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 12.280474ms)
May 31 00:19:29.194: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 13.077385ms)
May 31 00:19:29.196: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 15.024013ms)
May 31 00:19:29.204: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 8.326017ms)
May 31 00:19:29.209: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 12.71288ms)
May 31 00:19:29.209: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 12.684179ms)
May 31 00:19:29.210: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 14.276901ms)
May 31 00:19:29.213: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 17.04094ms)
May 31 00:19:29.217: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 21.186999ms)
May 31 00:19:29.218: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 21.389602ms)
May 31 00:19:29.218: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 21.547804ms)
May 31 00:19:29.218: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 21.724206ms)
May 31 00:19:29.218: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 21.90141ms)
May 31 00:19:29.218: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 22.300015ms)
May 31 00:19:29.218: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 22.001511ms)
May 31 00:19:29.218: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 22.353515ms)
May 31 00:19:29.219: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 22.596919ms)
May 31 00:19:29.219: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 22.421517ms)
May 31 00:19:29.221: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 24.574147ms)
May 31 00:19:29.233: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 11.804966ms)
May 31 00:19:29.233: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 12.151071ms)
May 31 00:19:29.234: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 12.269473ms)
May 31 00:19:29.234: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 13.027284ms)
May 31 00:19:29.234: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 13.073385ms)
May 31 00:19:29.234: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 13.227886ms)
May 31 00:19:29.235: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 13.668493ms)
May 31 00:19:29.235: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 13.880496ms)
May 31 00:19:29.235: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 13.944197ms)
May 31 00:19:29.235: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 14.576406ms)
May 31 00:19:29.236: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 14.522705ms)
May 31 00:19:29.236: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 15.128813ms)
May 31 00:19:29.236: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 15.068412ms)
May 31 00:19:29.236: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 15.337317ms)
May 31 00:19:29.237: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 15.394817ms)
May 31 00:19:29.237: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 15.860124ms)
May 31 00:19:29.244: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 6.753495ms)
May 31 00:19:29.245: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 6.651094ms)
May 31 00:19:29.245: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 7.118401ms)
May 31 00:19:29.245: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 7.680908ms)
May 31 00:19:29.246: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 8.044514ms)
May 31 00:19:29.250: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 12.72138ms)
May 31 00:19:29.250: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 12.267873ms)
May 31 00:19:29.251: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 13.192286ms)
May 31 00:19:29.251: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 12.970184ms)
May 31 00:19:29.251: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 12.616978ms)
May 31 00:19:29.251: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 12.301274ms)
May 31 00:19:29.251: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 13.492391ms)
May 31 00:19:29.251: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 13.328389ms)
May 31 00:19:29.251: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 13.48599ms)
May 31 00:19:29.251: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 14.117499ms)
May 31 00:19:29.253: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 15.859224ms)
May 31 00:19:29.263: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 9.331932ms)
May 31 00:19:29.265: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 11.982869ms)
May 31 00:19:29.265: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 11.124257ms)
May 31 00:19:29.265: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 11.593963ms)
May 31 00:19:29.265: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 11.268559ms)
May 31 00:19:29.266: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 12.714479ms)
May 31 00:19:29.266: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 12.582278ms)
May 31 00:19:29.266: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 13.048984ms)
May 31 00:19:29.267: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 13.214787ms)
May 31 00:19:29.267: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 12.989084ms)
May 31 00:19:29.267: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 13.874196ms)
May 31 00:19:29.267: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 13.599292ms)
May 31 00:19:29.269: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 14.568806ms)
May 31 00:19:29.271: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 16.92854ms)
May 31 00:19:29.271: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 17.155643ms)
May 31 00:19:29.271: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 17.647049ms)
May 31 00:19:29.279: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:462/proxy/: tls qux (200; 7.947113ms)
May 31 00:19:29.282: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:460/proxy/: tls baz (200; 10.839953ms)
May 31 00:19:29.284: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w/proxy/rewriteme"... (200; 12.07507ms)
May 31 00:19:29.285: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/https:proxy-service-7qkz6-lkx6w:443/proxy/... (200; 12.811981ms)
May 31 00:19:29.285: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:1080/proxy/... (200; 13.821795ms)
May 31 00:19:29.285: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:1080/proxy/rewri... (200; 13.967497ms)
May 31 00:19:29.285: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 13.846296ms)
May 31 00:19:29.286: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname2/proxy/: bar (200; 14.061798ms)
May 31 00:19:29.286: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 13.933997ms)
May 31 00:19:29.286: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/proxy-service-7qkz6-lkx6w:160/proxy/: foo (200; 14.114399ms)
May 31 00:19:29.286: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/pods/http:proxy-service-7qkz6-lkx6w:162/proxy/: bar (200; 14.411504ms)
May 31 00:19:29.286: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname1/proxy/: tls baz (200; 14.741208ms)
May 31 00:19:29.286: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname1/proxy/: foo (200; 14.87021ms)
May 31 00:19:29.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/http:proxy-service-7qkz6:portname1/proxy/: foo (200; 15.240916ms)
May 31 00:19:29.288: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/proxy-service-7qkz6:portname2/proxy/: bar (200; 16.888338ms)
May 31 00:19:29.288: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-q6trf/services/https:proxy-service-7qkz6:tlsportname2/proxy/: tls qux (200; 16.862138ms)
STEP: deleting { ReplicationController} proxy-service-7qkz6 in namespace e2e-tests-proxy-q6trf, will wait for the garbage collector to delete the pods
May 31 00:19:29.345: INFO: Deleting { ReplicationController} proxy-service-7qkz6 took: 4.310061ms
May 31 00:19:29.446: INFO: Terminating { ReplicationController} proxy-service-7qkz6 pods took: 100.346617ms
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:19:36.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-q6trf" for this suite.
May 31 00:19:42.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:19:42.586: INFO: namespace: e2e-tests-proxy-q6trf, resource: bindings, ignored listing per whitelist
May 31 00:19:42.660: INFO: namespace e2e-tests-proxy-q6trf deletion completed in 6.110728093s

• [SLOW TEST:26.898 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:19:42.660: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-c96f4acf-8339-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:19:42.747: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c96fc0a3-8339-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-wvs89" to be "success or failure"
May 31 00:19:42.755: INFO: Pod "pod-projected-secrets-c96fc0a3-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 8.221314ms
May 31 00:19:44.758: INFO: Pod "pod-projected-secrets-c96fc0a3-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010942987s
May 31 00:19:46.761: INFO: Pod "pod-projected-secrets-c96fc0a3-8339-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013839993s
STEP: Saw pod success
May 31 00:19:46.761: INFO: Pod "pod-projected-secrets-c96fc0a3-8339-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:19:46.763: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-projected-secrets-c96fc0a3-8339-11e9-88d7-16679dc4b203 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 00:19:46.781: INFO: Waiting for pod pod-projected-secrets-c96fc0a3-8339-11e9-88d7-16679dc4b203 to disappear
May 31 00:19:46.783: INFO: Pod pod-projected-secrets-c96fc0a3-8339-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:19:46.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wvs89" for this suite.
May 31 00:19:52.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:19:52.858: INFO: namespace: e2e-tests-projected-wvs89, resource: bindings, ignored listing per whitelist
May 31 00:19:52.886: INFO: namespace e2e-tests-projected-wvs89 deletion completed in 6.099362743s

• [SLOW TEST:10.225 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:19:52.889: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:19:52.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf885e74-8339-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-lcwgp" to be "success or failure"
May 31 00:19:52.980: INFO: Pod "downwardapi-volume-cf885e74-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826666ms
May 31 00:19:54.984: INFO: Pod "downwardapi-volume-cf885e74-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008155096s
May 31 00:19:56.986: INFO: Pod "downwardapi-volume-cf885e74-8339-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01088585s
STEP: Saw pod success
May 31 00:19:56.986: INFO: Pod "downwardapi-volume-cf885e74-8339-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:19:56.989: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-cf885e74-8339-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:19:57.007: INFO: Waiting for pod downwardapi-volume-cf885e74-8339-11e9-88d7-16679dc4b203 to disappear
May 31 00:19:57.012: INFO: Pod downwardapi-volume-cf885e74-8339-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:19:57.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lcwgp" for this suite.
May 31 00:20:03.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:20:03.059: INFO: namespace: e2e-tests-downward-api-lcwgp, resource: bindings, ignored listing per whitelist
May 31 00:20:03.118: INFO: namespace e2e-tests-downward-api-lcwgp deletion completed in 6.101948729s

• [SLOW TEST:10.229 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:20:03.118: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
May 31 00:20:03.198: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-190461910 proxy --unix-socket=/tmp/kubectl-proxy-unix117194557/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:20:03.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ndhp2" for this suite.
May 31 00:20:09.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:20:09.355: INFO: namespace: e2e-tests-kubectl-ndhp2, resource: bindings, ignored listing per whitelist
May 31 00:20:09.361: INFO: namespace e2e-tests-kubectl-ndhp2 deletion completed in 6.090398153s

• [SLOW TEST:6.243 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:20:09.364: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
May 31 00:20:09.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:09.739: INFO: stderr: ""
May 31 00:20:09.739: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 00:20:09.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:09.855: INFO: stderr: ""
May 31 00:20:09.856: INFO: stdout: "update-demo-nautilus-87lmv update-demo-nautilus-b5dzq "
May 31 00:20:09.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-87lmv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:09.979: INFO: stderr: ""
May 31 00:20:09.979: INFO: stdout: ""
May 31 00:20:09.979: INFO: update-demo-nautilus-87lmv is created but not running
May 31 00:20:14.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:15.098: INFO: stderr: ""
May 31 00:20:15.098: INFO: stdout: "update-demo-nautilus-87lmv update-demo-nautilus-b5dzq "
May 31 00:20:15.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-87lmv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:15.211: INFO: stderr: ""
May 31 00:20:15.211: INFO: stdout: ""
May 31 00:20:15.211: INFO: update-demo-nautilus-87lmv is created but not running
May 31 00:20:20.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:20.329: INFO: stderr: ""
May 31 00:20:20.329: INFO: stdout: "update-demo-nautilus-87lmv update-demo-nautilus-b5dzq "
May 31 00:20:20.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-87lmv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:20.444: INFO: stderr: ""
May 31 00:20:20.444: INFO: stdout: "true"
May 31 00:20:20.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-87lmv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:20.577: INFO: stderr: ""
May 31 00:20:20.577: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 00:20:20.577: INFO: validating pod update-demo-nautilus-87lmv
May 31 00:20:20.583: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 00:20:20.583: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 00:20:20.583: INFO: update-demo-nautilus-87lmv is verified up and running
May 31 00:20:20.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-b5dzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:20.706: INFO: stderr: ""
May 31 00:20:20.706: INFO: stdout: "true"
May 31 00:20:20.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-b5dzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:20.847: INFO: stderr: ""
May 31 00:20:20.847: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 00:20:20.847: INFO: validating pod update-demo-nautilus-b5dzq
May 31 00:20:20.853: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 00:20:20.853: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 00:20:20.853: INFO: update-demo-nautilus-b5dzq is verified up and running
STEP: scaling down the replication controller
May 31 00:20:20.855: INFO: scanned /root for discovery docs: <nil>
May 31 00:20:20.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:21.995: INFO: stderr: ""
May 31 00:20:21.995: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 00:20:21.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:22.102: INFO: stderr: ""
May 31 00:20:22.103: INFO: stdout: "update-demo-nautilus-87lmv update-demo-nautilus-b5dzq "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 31 00:20:27.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:27.225: INFO: stderr: ""
May 31 00:20:27.225: INFO: stdout: "update-demo-nautilus-b5dzq "
May 31 00:20:27.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-b5dzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:27.342: INFO: stderr: ""
May 31 00:20:27.342: INFO: stdout: "true"
May 31 00:20:27.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-b5dzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:27.455: INFO: stderr: ""
May 31 00:20:27.455: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 00:20:27.455: INFO: validating pod update-demo-nautilus-b5dzq
May 31 00:20:27.459: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 00:20:27.459: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 00:20:27.459: INFO: update-demo-nautilus-b5dzq is verified up and running
STEP: scaling up the replication controller
May 31 00:20:27.460: INFO: scanned /root for discovery docs: <nil>
May 31 00:20:27.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:28.592: INFO: stderr: ""
May 31 00:20:28.592: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 00:20:28.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:28.707: INFO: stderr: ""
May 31 00:20:28.707: INFO: stdout: "update-demo-nautilus-b5dzq update-demo-nautilus-svrdr "
May 31 00:20:28.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-b5dzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:28.879: INFO: stderr: ""
May 31 00:20:28.879: INFO: stdout: "true"
May 31 00:20:28.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-b5dzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:28.995: INFO: stderr: ""
May 31 00:20:28.995: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 00:20:28.995: INFO: validating pod update-demo-nautilus-b5dzq
May 31 00:20:28.999: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 00:20:28.999: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 00:20:28.999: INFO: update-demo-nautilus-b5dzq is verified up and running
May 31 00:20:28.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-svrdr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:29.117: INFO: stderr: ""
May 31 00:20:29.117: INFO: stdout: ""
May 31 00:20:29.117: INFO: update-demo-nautilus-svrdr is created but not running
May 31 00:20:34.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:34.234: INFO: stderr: ""
May 31 00:20:34.234: INFO: stdout: "update-demo-nautilus-b5dzq update-demo-nautilus-svrdr "
May 31 00:20:34.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-b5dzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:34.351: INFO: stderr: ""
May 31 00:20:34.351: INFO: stdout: "true"
May 31 00:20:34.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-b5dzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:34.462: INFO: stderr: ""
May 31 00:20:34.462: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 00:20:34.462: INFO: validating pod update-demo-nautilus-b5dzq
May 31 00:20:34.466: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 00:20:34.466: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 00:20:34.466: INFO: update-demo-nautilus-b5dzq is verified up and running
May 31 00:20:34.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-svrdr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:34.583: INFO: stderr: ""
May 31 00:20:34.583: INFO: stdout: "true"
May 31 00:20:34.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-svrdr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:34.701: INFO: stderr: ""
May 31 00:20:34.701: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 00:20:34.701: INFO: validating pod update-demo-nautilus-svrdr
May 31 00:20:34.707: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 00:20:34.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 00:20:34.707: INFO: update-demo-nautilus-svrdr is verified up and running
STEP: using delete to clean up resources
May 31 00:20:34.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:34.830: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 00:20:34.830: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 31 00:20:34.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-tl7gk'
May 31 00:20:34.958: INFO: stderr: "No resources found.\n"
May 31 00:20:34.958: INFO: stdout: ""
May 31 00:20:34.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -l name=update-demo --namespace=e2e-tests-kubectl-tl7gk -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 00:20:35.085: INFO: stderr: ""
May 31 00:20:35.085: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:20:35.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tl7gk" for this suite.
May 31 00:20:57.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:20:57.158: INFO: namespace: e2e-tests-kubectl-tl7gk, resource: bindings, ignored listing per whitelist
May 31 00:20:57.181: INFO: namespace e2e-tests-kubectl-tl7gk deletion completed in 22.092268631s

• [SLOW TEST:47.817 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:20:57.183: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:21:01.298: INFO: Waiting up to 5m0s for pod "client-envvars-f8414075-8339-11e9-88d7-16679dc4b203" in namespace "e2e-tests-pods-9lkhs" to be "success or failure"
May 31 00:21:01.307: INFO: Pod "client-envvars-f8414075-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 9.031315ms
May 31 00:21:03.310: INFO: Pod "client-envvars-f8414075-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012206317s
May 31 00:21:05.313: INFO: Pod "client-envvars-f8414075-8339-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015122763s
May 31 00:21:07.316: INFO: Pod "client-envvars-f8414075-8339-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018503965s
STEP: Saw pod success
May 31 00:21:07.317: INFO: Pod "client-envvars-f8414075-8339-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:21:07.319: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod client-envvars-f8414075-8339-11e9-88d7-16679dc4b203 container env3cont: <nil>
STEP: delete the pod
May 31 00:21:07.333: INFO: Waiting for pod client-envvars-f8414075-8339-11e9-88d7-16679dc4b203 to disappear
May 31 00:21:07.338: INFO: Pod client-envvars-f8414075-8339-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:21:07.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-9lkhs" for this suite.
May 31 00:21:45.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:21:45.396: INFO: namespace: e2e-tests-pods-9lkhs, resource: bindings, ignored listing per whitelist
May 31 00:21:45.426: INFO: namespace e2e-tests-pods-9lkhs deletion completed in 38.084045946s

• [SLOW TEST:48.243 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:21:45.426: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-wgsl2
May 31 00:21:55.515: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-wgsl2
STEP: checking the pod's current state and verifying that restartCount is present
May 31 00:21:55.517: INFO: Initial restart count of pod liveness-http is 0
May 31 00:22:19.565: INFO: Restart count of pod e2e-tests-container-probe-wgsl2/liveness-http is now 1 (24.047924098s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:22:19.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wgsl2" for this suite.
May 31 00:22:25.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:22:25.631: INFO: namespace: e2e-tests-container-probe-wgsl2, resource: bindings, ignored listing per whitelist
May 31 00:22:25.688: INFO: namespace e2e-tests-container-probe-wgsl2 deletion completed in 6.103169763s

• [SLOW TEST:40.263 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:22:25.689: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-2a9b1909-833a-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 00:22:25.780: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a9b8cae-833a-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-qxwz2" to be "success or failure"
May 31 00:22:25.782: INFO: Pod "pod-projected-configmaps-2a9b8cae-833a-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.254526ms
May 31 00:22:27.785: INFO: Pod "pod-projected-configmaps-2a9b8cae-833a-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004986063s
May 31 00:22:29.787: INFO: Pod "pod-projected-configmaps-2a9b8cae-833a-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007345758s
STEP: Saw pod success
May 31 00:22:29.787: INFO: Pod "pod-projected-configmaps-2a9b8cae-833a-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:22:29.790: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-projected-configmaps-2a9b8cae-833a-11e9-88d7-16679dc4b203 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 00:22:29.810: INFO: Waiting for pod pod-projected-configmaps-2a9b8cae-833a-11e9-88d7-16679dc4b203 to disappear
May 31 00:22:29.812: INFO: Pod pod-projected-configmaps-2a9b8cae-833a-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:22:29.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qxwz2" for this suite.
May 31 00:22:35.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:22:35.857: INFO: namespace: e2e-tests-projected-qxwz2, resource: bindings, ignored listing per whitelist
May 31 00:22:35.926: INFO: namespace e2e-tests-projected-qxwz2 deletion completed in 6.108996555s

• [SLOW TEST:10.237 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:22:35.927: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:22:36.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 version'
May 31 00:22:36.162: INFO: stderr: ""
May 31 00:22:36.162: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.1\", GitCommit:\"4ed3216f3ec431b140b1d899130a69fc671678f4\", GitTreeState:\"clean\", BuildDate:\"2018-10-05T16:46:06Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.8\", GitCommit:\"742489962129bdc0b63b4303f5ac20899b923fc3\", GitTreeState:\"clean\", BuildDate:\"2019-05-06T21:20:33Z\", GoVersion:\"go1.10.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:22:36.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-q8tbq" for this suite.
May 31 00:22:42.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:22:42.274: INFO: namespace: e2e-tests-kubectl-q8tbq, resource: bindings, ignored listing per whitelist
May 31 00:22:42.283: INFO: namespace e2e-tests-kubectl-q8tbq deletion completed in 6.116321792s

• [SLOW TEST:6.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:22:42.283: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-347fe143-833a-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:22:42.375: INFO: Waiting up to 5m0s for pod "pod-secrets-34806750-833a-11e9-88d7-16679dc4b203" in namespace "e2e-tests-secrets-pjdvj" to be "success or failure"
May 31 00:22:42.379: INFO: Pod "pod-secrets-34806750-833a-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.826044ms
May 31 00:22:44.382: INFO: Pod "pod-secrets-34806750-833a-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00723179s
May 31 00:22:46.385: INFO: Pod "pod-secrets-34806750-833a-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009863992s
STEP: Saw pod success
May 31 00:22:46.385: INFO: Pod "pod-secrets-34806750-833a-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:22:46.387: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-secrets-34806750-833a-11e9-88d7-16679dc4b203 container secret-volume-test: <nil>
STEP: delete the pod
May 31 00:22:46.403: INFO: Waiting for pod pod-secrets-34806750-833a-11e9-88d7-16679dc4b203 to disappear
May 31 00:22:46.405: INFO: Pod pod-secrets-34806750-833a-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:22:46.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pjdvj" for this suite.
May 31 00:22:52.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:22:52.476: INFO: namespace: e2e-tests-secrets-pjdvj, resource: bindings, ignored listing per whitelist
May 31 00:22:52.505: INFO: namespace e2e-tests-secrets-pjdvj deletion completed in 6.097921238s

• [SLOW TEST:10.223 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:22:52.507: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 31 00:22:52.596: INFO: Waiting up to 5m0s for pod "pod-3a97a36f-833a-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-sv27n" to be "success or failure"
May 31 00:22:52.600: INFO: Pod "pod-3a97a36f-833a-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.517452ms
May 31 00:22:54.603: INFO: Pod "pod-3a97a36f-833a-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007251717s
May 31 00:22:56.606: INFO: Pod "pod-3a97a36f-833a-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010609655s
STEP: Saw pod success
May 31 00:22:56.606: INFO: Pod "pod-3a97a36f-833a-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:22:56.608: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-3a97a36f-833a-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:22:56.622: INFO: Waiting for pod pod-3a97a36f-833a-11e9-88d7-16679dc4b203 to disappear
May 31 00:22:56.624: INFO: Pod pod-3a97a36f-833a-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:22:56.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sv27n" for this suite.
May 31 00:23:02.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:23:02.707: INFO: namespace: e2e-tests-emptydir-sv27n, resource: bindings, ignored listing per whitelist
May 31 00:23:02.720: INFO: namespace e2e-tests-emptydir-sv27n deletion completed in 6.092000152s

• [SLOW TEST:10.213 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:23:02.721: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 31 00:23:02.816: INFO: Waiting up to 5m0s for pod "pod-40ae6215-833a-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-6mmz5" to be "success or failure"
May 31 00:23:02.821: INFO: Pod "pod-40ae6215-833a-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.770566ms
May 31 00:23:04.824: INFO: Pod "pod-40ae6215-833a-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008924067s
May 31 00:23:06.828: INFO: Pod "pod-40ae6215-833a-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012029935s
STEP: Saw pod success
May 31 00:23:06.828: INFO: Pod "pod-40ae6215-833a-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:23:06.830: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-40ae6215-833a-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:23:06.844: INFO: Waiting for pod pod-40ae6215-833a-11e9-88d7-16679dc4b203 to disappear
May 31 00:23:06.848: INFO: Pod pod-40ae6215-833a-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:23:06.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-6mmz5" for this suite.
May 31 00:23:12.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:23:12.878: INFO: namespace: e2e-tests-emptydir-6mmz5, resource: bindings, ignored listing per whitelist
May 31 00:23:12.946: INFO: namespace e2e-tests-emptydir-6mmz5 deletion completed in 6.094992889s

• [SLOW TEST:10.225 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:23:12.947: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0531 00:23:23.169486      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 00:23:23.169: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:23:23.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-vltl7" for this suite.
May 31 00:23:29.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:23:29.206: INFO: namespace: e2e-tests-gc-vltl7, resource: bindings, ignored listing per whitelist
May 31 00:23:29.260: INFO: namespace e2e-tests-gc-vltl7 deletion completed in 6.08856696s

• [SLOW TEST:16.313 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:23:29.261: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
May 31 00:23:29.867: INFO: created pod pod-service-account-defaultsa
May 31 00:23:29.867: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 31 00:23:29.872: INFO: created pod pod-service-account-mountsa
May 31 00:23:29.872: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 31 00:23:29.878: INFO: created pod pod-service-account-nomountsa
May 31 00:23:29.878: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 31 00:23:29.889: INFO: created pod pod-service-account-defaultsa-mountspec
May 31 00:23:29.889: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 31 00:23:29.901: INFO: created pod pod-service-account-mountsa-mountspec
May 31 00:23:29.901: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 31 00:23:29.909: INFO: created pod pod-service-account-nomountsa-mountspec
May 31 00:23:29.910: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 31 00:23:29.914: INFO: created pod pod-service-account-defaultsa-nomountspec
May 31 00:23:29.914: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 31 00:23:29.919: INFO: created pod pod-service-account-mountsa-nomountspec
May 31 00:23:29.919: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 31 00:23:29.932: INFO: created pod pod-service-account-nomountsa-nomountspec
May 31 00:23:29.932: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:23:29.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-8mlb7" for this suite.
May 31 00:23:51.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:23:52.030: INFO: namespace: e2e-tests-svcaccounts-8mlb7, resource: bindings, ignored listing per whitelist
May 31 00:23:52.032: INFO: namespace e2e-tests-svcaccounts-8mlb7 deletion completed in 22.094281977s

• [SLOW TEST:22.771 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:23:52.032: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
May 31 00:23:56.175: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:24:20.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-vqhh2" for this suite.
May 31 00:24:26.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:24:26.304: INFO: namespace: e2e-tests-namespaces-vqhh2, resource: bindings, ignored listing per whitelist
May 31 00:24:26.332: INFO: namespace e2e-tests-namespaces-vqhh2 deletion completed in 6.082393504s
STEP: Destroying namespace "e2e-tests-nsdeletetest-2lbg9" for this suite.
May 31 00:24:26.335: INFO: Namespace e2e-tests-nsdeletetest-2lbg9 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-cr5qh" for this suite.
May 31 00:24:32.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:24:32.408: INFO: namespace: e2e-tests-nsdeletetest-cr5qh, resource: bindings, ignored listing per whitelist
May 31 00:24:32.424: INFO: namespace e2e-tests-nsdeletetest-cr5qh deletion completed in 6.08890086s

• [SLOW TEST:40.392 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:24:32.426: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 31 00:24:32.513: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-a,UID:76273ea2-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4613,Generation:0,CreationTimestamp:2019-05-31 00:24:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 00:24:32.513: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-a,UID:76273ea2-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4613,Generation:0,CreationTimestamp:2019-05-31 00:24:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 31 00:24:42.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-a,UID:76273ea2-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4629,Generation:0,CreationTimestamp:2019-05-31 00:24:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 31 00:24:42.520: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-a,UID:76273ea2-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4629,Generation:0,CreationTimestamp:2019-05-31 00:24:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 31 00:24:52.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-a,UID:76273ea2-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4645,Generation:0,CreationTimestamp:2019-05-31 00:24:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 00:24:52.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-a,UID:76273ea2-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4645,Generation:0,CreationTimestamp:2019-05-31 00:24:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 31 00:25:02.532: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-a,UID:76273ea2-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4661,Generation:0,CreationTimestamp:2019-05-31 00:24:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 00:25:02.532: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-a,UID:76273ea2-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4661,Generation:0,CreationTimestamp:2019-05-31 00:24:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 31 00:25:12.537: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-b,UID:8e023cdb-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4677,Generation:0,CreationTimestamp:2019-05-31 00:25:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 00:25:12.538: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-b,UID:8e023cdb-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4677,Generation:0,CreationTimestamp:2019-05-31 00:25:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 31 00:25:22.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-b,UID:8e023cdb-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4697,Generation:0,CreationTimestamp:2019-05-31 00:25:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 00:25:22.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-96zjt,SelfLink:/api/v1/namespaces/e2e-tests-watch-96zjt/configmaps/e2e-watch-test-configmap-b,UID:8e023cdb-833a-11e9-b3f2-001dd80c0014,ResourceVersion:4697,Generation:0,CreationTimestamp:2019-05-31 00:25:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:25:32.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-96zjt" for this suite.
May 31 00:25:38.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:25:38.611: INFO: namespace: e2e-tests-watch-96zjt, resource: bindings, ignored listing per whitelist
May 31 00:25:38.639: INFO: namespace e2e-tests-watch-96zjt deletion completed in 6.092752563s

• [SLOW TEST:66.213 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:25:38.640: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-qrwwl
May 31 00:25:42.738: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-qrwwl
STEP: checking the pod's current state and verifying that restartCount is present
May 31 00:25:42.740: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:29:43.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qrwwl" for this suite.
May 31 00:29:49.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:29:49.275: INFO: namespace: e2e-tests-container-probe-qrwwl, resource: bindings, ignored listing per whitelist
May 31 00:29:49.307: INFO: namespace e2e-tests-container-probe-qrwwl deletion completed in 6.098397607s

• [SLOW TEST:250.667 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:29:49.307: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-33090d11-833b-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 00:29:49.417: INFO: Waiting up to 5m0s for pod "pod-configmaps-33097d0f-833b-11e9-88d7-16679dc4b203" in namespace "e2e-tests-configmap-p2xsm" to be "success or failure"
May 31 00:29:49.422: INFO: Pod "pod-configmaps-33097d0f-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.01045ms
May 31 00:29:51.425: INFO: Pod "pod-configmaps-33097d0f-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007886478s
May 31 00:29:53.428: INFO: Pod "pod-configmaps-33097d0f-833b-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011159504s
STEP: Saw pod success
May 31 00:29:53.428: INFO: Pod "pod-configmaps-33097d0f-833b-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:29:53.431: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-configmaps-33097d0f-833b-11e9-88d7-16679dc4b203 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 00:29:53.453: INFO: Waiting for pod pod-configmaps-33097d0f-833b-11e9-88d7-16679dc4b203 to disappear
May 31 00:29:53.456: INFO: Pod pod-configmaps-33097d0f-833b-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:29:53.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-p2xsm" for this suite.
May 31 00:29:59.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:29:59.517: INFO: namespace: e2e-tests-configmap-p2xsm, resource: bindings, ignored listing per whitelist
May 31 00:29:59.549: INFO: namespace e2e-tests-configmap-p2xsm deletion completed in 6.089753321s

• [SLOW TEST:10.242 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:29:59.550: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
May 31 00:29:59.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:00.291: INFO: stderr: ""
May 31 00:30:00.291: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 00:30:00.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:00.419: INFO: stderr: ""
May 31 00:30:00.419: INFO: stdout: "update-demo-nautilus-5xtmd update-demo-nautilus-75nlw "
May 31 00:30:00.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-5xtmd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:00.540: INFO: stderr: ""
May 31 00:30:00.540: INFO: stdout: ""
May 31 00:30:00.540: INFO: update-demo-nautilus-5xtmd is created but not running
May 31 00:30:05.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:05.662: INFO: stderr: ""
May 31 00:30:05.662: INFO: stdout: "update-demo-nautilus-5xtmd update-demo-nautilus-75nlw "
May 31 00:30:05.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-5xtmd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:05.787: INFO: stderr: ""
May 31 00:30:05.787: INFO: stdout: "true"
May 31 00:30:05.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-5xtmd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:05.900: INFO: stderr: ""
May 31 00:30:05.900: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 00:30:05.900: INFO: validating pod update-demo-nautilus-5xtmd
May 31 00:30:05.904: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 00:30:05.904: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 00:30:05.904: INFO: update-demo-nautilus-5xtmd is verified up and running
May 31 00:30:05.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-75nlw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:06.005: INFO: stderr: ""
May 31 00:30:06.005: INFO: stdout: "true"
May 31 00:30:06.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-75nlw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:06.119: INFO: stderr: ""
May 31 00:30:06.119: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 00:30:06.119: INFO: validating pod update-demo-nautilus-75nlw
May 31 00:30:06.123: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 00:30:06.123: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 00:30:06.123: INFO: update-demo-nautilus-75nlw is verified up and running
STEP: rolling-update to new replication controller
May 31 00:30:06.125: INFO: scanned /root for discovery docs: <nil>
May 31 00:30:06.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:33.605: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 31 00:30:33.605: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 00:30:33.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:33.735: INFO: stderr: ""
May 31 00:30:33.735: INFO: stdout: "update-demo-kitten-hxsgr update-demo-kitten-vwgm5 update-demo-nautilus-5xtmd "
STEP: Replicas for name=update-demo: expected=2 actual=3
May 31 00:30:38.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:38.865: INFO: stderr: ""
May 31 00:30:38.865: INFO: stdout: "update-demo-kitten-hxsgr update-demo-kitten-vwgm5 "
May 31 00:30:38.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-kitten-hxsgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:38.986: INFO: stderr: ""
May 31 00:30:38.986: INFO: stdout: "true"
May 31 00:30:38.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-kitten-hxsgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:39.099: INFO: stderr: ""
May 31 00:30:39.100: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 31 00:30:39.100: INFO: validating pod update-demo-kitten-hxsgr
May 31 00:30:39.104: INFO: got data: {
  "image": "kitten.jpg"
}

May 31 00:30:39.104: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 31 00:30:39.104: INFO: update-demo-kitten-hxsgr is verified up and running
May 31 00:30:39.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-kitten-vwgm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:39.216: INFO: stderr: ""
May 31 00:30:39.216: INFO: stdout: "true"
May 31 00:30:39.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-kitten-vwgm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7nh7'
May 31 00:30:39.328: INFO: stderr: ""
May 31 00:30:39.328: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 31 00:30:39.328: INFO: validating pod update-demo-kitten-vwgm5
May 31 00:30:39.332: INFO: got data: {
  "image": "kitten.jpg"
}

May 31 00:30:39.332: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 31 00:30:39.332: INFO: update-demo-kitten-vwgm5 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:30:39.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l7nh7" for this suite.
May 31 00:31:01.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:31:01.383: INFO: namespace: e2e-tests-kubectl-l7nh7, resource: bindings, ignored listing per whitelist
May 31 00:31:01.427: INFO: namespace e2e-tests-kubectl-l7nh7 deletion completed in 22.092228279s

• [SLOW TEST:61.877 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:31:01.428: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
May 31 00:31:01.519: INFO: Waiting up to 5m0s for pod "pod-5e03dbd9-833b-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-bqrqq" to be "success or failure"
May 31 00:31:01.522: INFO: Pod "pod-5e03dbd9-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477833ms
May 31 00:31:03.525: INFO: Pod "pod-5e03dbd9-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006527458s
May 31 00:31:05.529: INFO: Pod "pod-5e03dbd9-833b-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010330285s
STEP: Saw pod success
May 31 00:31:05.529: INFO: Pod "pod-5e03dbd9-833b-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:31:05.531: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-5e03dbd9-833b-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:31:05.556: INFO: Waiting for pod pod-5e03dbd9-833b-11e9-88d7-16679dc4b203 to disappear
May 31 00:31:05.558: INFO: Pod pod-5e03dbd9-833b-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:31:05.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bqrqq" for this suite.
May 31 00:31:11.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:31:11.630: INFO: namespace: e2e-tests-emptydir-bqrqq, resource: bindings, ignored listing per whitelist
May 31 00:31:11.657: INFO: namespace e2e-tests-emptydir-bqrqq deletion completed in 6.09578707s

• [SLOW TEST:10.229 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:31:11.658: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
May 31 00:31:11.741: INFO: Waiting up to 5m0s for pod "pod-641be9de-833b-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-m5ts4" to be "success or failure"
May 31 00:31:11.746: INFO: Pod "pod-641be9de-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.565244ms
May 31 00:31:13.750: INFO: Pod "pod-641be9de-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008805456s
May 31 00:31:15.753: INFO: Pod "pod-641be9de-833b-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012121354s
STEP: Saw pod success
May 31 00:31:15.754: INFO: Pod "pod-641be9de-833b-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:31:15.756: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-641be9de-833b-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:31:15.769: INFO: Waiting for pod pod-641be9de-833b-11e9-88d7-16679dc4b203 to disappear
May 31 00:31:15.772: INFO: Pod pod-641be9de-833b-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:31:15.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-m5ts4" for this suite.
May 31 00:31:21.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:31:21.835: INFO: namespace: e2e-tests-emptydir-m5ts4, resource: bindings, ignored listing per whitelist
May 31 00:31:21.870: INFO: namespace e2e-tests-emptydir-m5ts4 deletion completed in 6.095456693s

• [SLOW TEST:10.213 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:31:21.871: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:31:21.959: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a32ccfc-833b-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-mdxdm" to be "success or failure"
May 31 00:31:21.964: INFO: Pod "downwardapi-volume-6a32ccfc-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.15625ms
May 31 00:31:23.967: INFO: Pod "downwardapi-volume-6a32ccfc-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008101625s
May 31 00:31:25.970: INFO: Pod "downwardapi-volume-6a32ccfc-833b-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011081995s
STEP: Saw pod success
May 31 00:31:25.970: INFO: Pod "downwardapi-volume-6a32ccfc-833b-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:31:25.973: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-6a32ccfc-833b-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:31:25.989: INFO: Waiting for pod downwardapi-volume-6a32ccfc-833b-11e9-88d7-16679dc4b203 to disappear
May 31 00:31:25.991: INFO: Pod downwardapi-volume-6a32ccfc-833b-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:31:25.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mdxdm" for this suite.
May 31 00:31:32.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:31:32.084: INFO: namespace: e2e-tests-projected-mdxdm, resource: bindings, ignored listing per whitelist
May 31 00:31:32.091: INFO: namespace e2e-tests-projected-mdxdm deletion completed in 6.096850335s

• [SLOW TEST:10.220 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:31:32.092: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
May 31 00:31:32.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-jvcf4'
May 31 00:31:32.430: INFO: stderr: ""
May 31 00:31:32.431: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 31 00:31:33.434: INFO: Selector matched 1 pods for map[app:redis]
May 31 00:31:33.434: INFO: Found 0 / 1
May 31 00:31:34.434: INFO: Selector matched 1 pods for map[app:redis]
May 31 00:31:34.434: INFO: Found 0 / 1
May 31 00:31:35.434: INFO: Selector matched 1 pods for map[app:redis]
May 31 00:31:35.434: INFO: Found 1 / 1
May 31 00:31:35.434: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 31 00:31:35.437: INFO: Selector matched 1 pods for map[app:redis]
May 31 00:31:35.437: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 31 00:31:35.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 patch pod redis-master-s8w7m --namespace=e2e-tests-kubectl-jvcf4 -p {"metadata":{"annotations":{"x":"y"}}}'
May 31 00:31:35.569: INFO: stderr: ""
May 31 00:31:35.569: INFO: stdout: "pod/redis-master-s8w7m patched\n"
STEP: checking annotations
May 31 00:31:35.571: INFO: Selector matched 1 pods for map[app:redis]
May 31 00:31:35.571: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:31:35.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jvcf4" for this suite.
May 31 00:31:57.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:31:57.617: INFO: namespace: e2e-tests-kubectl-jvcf4, resource: bindings, ignored listing per whitelist
May 31 00:31:57.667: INFO: namespace e2e-tests-kubectl-jvcf4 deletion completed in 22.092837844s

• [SLOW TEST:25.576 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:31:57.667: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-7f89305f-833b-11e9-88d7-16679dc4b203
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-7f89305f-833b-11e9-88d7-16679dc4b203
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:32:03.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dlzq6" for this suite.
May 31 00:32:25.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:32:25.882: INFO: namespace: e2e-tests-projected-dlzq6, resource: bindings, ignored listing per whitelist
May 31 00:32:25.901: INFO: namespace e2e-tests-projected-dlzq6 deletion completed in 22.090001704s

• [SLOW TEST:28.234 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:32:25.902: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 31 00:32:25.985: INFO: PodSpec: initContainers in spec.initContainers
May 31 00:33:10.973: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-905d4627-833b-11e9-88d7-16679dc4b203", GenerateName:"", Namespace:"e2e-tests-init-container-4bc98", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-4bc98/pods/pod-init-905d4627-833b-11e9-88d7-16679dc4b203", UID:"905e2181-833b-11e9-b3f2-001dd80c0014", ResourceVersion:"5781", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63694859545, loc:(*time.Location)(0x6c43b60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"985185655"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-svj72", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc42219c740), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-svj72", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-svj72", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-svj72", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc42102d588), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-linuxpool-10610181-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc421f896e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc42102d600)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc42102d620)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc42102d628), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694859546, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694859546, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694859546, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694859545, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.5", PodIP:"10.244.3.67", StartTime:(*v1.Time)(0xc421fe8780), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc420b782a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc420b78310)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://d441852db45d9cba892232f27f44a5481e2cf8e840d6ae12d5196234f6c39044"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc421fe87c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc421fe87a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:33:10.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-4bc98" for this suite.
May 31 00:33:32.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:33:33.038: INFO: namespace: e2e-tests-init-container-4bc98, resource: bindings, ignored listing per whitelist
May 31 00:33:33.068: INFO: namespace e2e-tests-init-container-4bc98 deletion completed in 22.086705461s

• [SLOW TEST:67.166 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:33:33.070: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-b8658333-833b-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:33:33.157: INFO: Waiting up to 5m0s for pod "pod-secrets-b86603a8-833b-11e9-88d7-16679dc4b203" in namespace "e2e-tests-secrets-nld5d" to be "success or failure"
May 31 00:33:33.176: INFO: Pod "pod-secrets-b86603a8-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 18.703378ms
May 31 00:33:35.178: INFO: Pod "pod-secrets-b86603a8-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021171715s
May 31 00:33:37.182: INFO: Pod "pod-secrets-b86603a8-833b-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024582458s
STEP: Saw pod success
May 31 00:33:37.182: INFO: Pod "pod-secrets-b86603a8-833b-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:33:37.184: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-secrets-b86603a8-833b-11e9-88d7-16679dc4b203 container secret-env-test: <nil>
STEP: delete the pod
May 31 00:33:37.197: INFO: Waiting for pod pod-secrets-b86603a8-833b-11e9-88d7-16679dc4b203 to disappear
May 31 00:33:37.202: INFO: Pod pod-secrets-b86603a8-833b-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:33:37.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nld5d" for this suite.
May 31 00:33:43.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:33:43.266: INFO: namespace: e2e-tests-secrets-nld5d, resource: bindings, ignored listing per whitelist
May 31 00:33:43.295: INFO: namespace e2e-tests-secrets-nld5d deletion completed in 6.090203476s

• [SLOW TEST:10.226 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:33:43.297: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-5vb8q
May 31 00:33:47.407: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-5vb8q
STEP: checking the pod's current state and verifying that restartCount is present
May 31 00:33:47.410: INFO: Initial restart count of pod liveness-exec is 0
May 31 00:34:41.501: INFO: Restart count of pod e2e-tests-container-probe-5vb8q/liveness-exec is now 1 (54.090876168s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:34:41.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-5vb8q" for this suite.
May 31 00:34:47.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:34:47.678: INFO: namespace: e2e-tests-container-probe-5vb8q, resource: bindings, ignored listing per whitelist
May 31 00:34:47.708: INFO: namespace e2e-tests-container-probe-5vb8q deletion completed in 6.194200535s

• [SLOW TEST:64.411 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:34:47.709: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:34:47.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-8p7mt" for this suite.
May 31 00:34:53.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:34:53.848: INFO: namespace: e2e-tests-services-8p7mt, resource: bindings, ignored listing per whitelist
May 31 00:34:53.885: INFO: namespace e2e-tests-services-8p7mt deletion completed in 6.089044916s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

• [SLOW TEST:6.177 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:34:53.886: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:34:53.980: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e892a321-833b-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-5prvf" to be "success or failure"
May 31 00:34:53.992: INFO: Pod "downwardapi-volume-e892a321-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 11.390409ms
May 31 00:34:55.995: INFO: Pod "downwardapi-volume-e892a321-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014645559s
May 31 00:34:57.998: INFO: Pod "downwardapi-volume-e892a321-833b-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017384601s
STEP: Saw pod success
May 31 00:34:57.998: INFO: Pod "downwardapi-volume-e892a321-833b-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:34:58.000: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-e892a321-833b-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:34:58.019: INFO: Waiting for pod downwardapi-volume-e892a321-833b-11e9-88d7-16679dc4b203 to disappear
May 31 00:34:58.021: INFO: Pod downwardapi-volume-e892a321-833b-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:34:58.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5prvf" for this suite.
May 31 00:35:04.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:35:04.058: INFO: namespace: e2e-tests-downward-api-5prvf, resource: bindings, ignored listing per whitelist
May 31 00:35:04.116: INFO: namespace e2e-tests-downward-api-5prvf deletion completed in 6.091166705s

• [SLOW TEST:10.230 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:35:04.117: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
May 31 00:35:04.204: INFO: Waiting up to 5m0s for pod "client-containers-eeaab0f9-833b-11e9-88d7-16679dc4b203" in namespace "e2e-tests-containers-xj99h" to be "success or failure"
May 31 00:35:04.206: INFO: Pod "client-containers-eeaab0f9-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010519ms
May 31 00:35:06.210: INFO: Pod "client-containers-eeaab0f9-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00534536s
May 31 00:35:08.213: INFO: Pod "client-containers-eeaab0f9-833b-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008593897s
STEP: Saw pod success
May 31 00:35:08.213: INFO: Pod "client-containers-eeaab0f9-833b-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:35:08.215: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod client-containers-eeaab0f9-833b-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:35:08.234: INFO: Waiting for pod client-containers-eeaab0f9-833b-11e9-88d7-16679dc4b203 to disappear
May 31 00:35:08.236: INFO: Pod client-containers-eeaab0f9-833b-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:35:08.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-xj99h" for this suite.
May 31 00:35:14.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:35:14.297: INFO: namespace: e2e-tests-containers-xj99h, resource: bindings, ignored listing per whitelist
May 31 00:35:14.337: INFO: namespace e2e-tests-containers-xj99h deletion completed in 6.097920064s

• [SLOW TEST:10.220 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:35:14.337: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
May 31 00:35:14.421: INFO: Waiting up to 5m0s for pod "client-containers-f4c1dc38-833b-11e9-88d7-16679dc4b203" in namespace "e2e-tests-containers-9qgb4" to be "success or failure"
May 31 00:35:14.430: INFO: Pod "client-containers-f4c1dc38-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 8.644025ms
May 31 00:35:16.433: INFO: Pod "client-containers-f4c1dc38-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011554328s
May 31 00:35:18.437: INFO: Pod "client-containers-f4c1dc38-833b-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015738914s
STEP: Saw pod success
May 31 00:35:18.437: INFO: Pod "client-containers-f4c1dc38-833b-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:35:18.439: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod client-containers-f4c1dc38-833b-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:35:18.463: INFO: Waiting for pod client-containers-f4c1dc38-833b-11e9-88d7-16679dc4b203 to disappear
May 31 00:35:18.468: INFO: Pod client-containers-f4c1dc38-833b-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:35:18.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-9qgb4" for this suite.
May 31 00:35:24.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:35:24.512: INFO: namespace: e2e-tests-containers-9qgb4, resource: bindings, ignored listing per whitelist
May 31 00:35:24.562: INFO: namespace e2e-tests-containers-9qgb4 deletion completed in 6.08934795s

• [SLOW TEST:10.224 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:35:24.562: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 31 00:35:24.651: INFO: Waiting up to 5m0s for pod "pod-fada5923-833b-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-dlrd5" to be "success or failure"
May 31 00:35:24.656: INFO: Pod "pod-fada5923-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.445378ms
May 31 00:35:26.659: INFO: Pod "pod-fada5923-833b-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008333699s
May 31 00:35:28.663: INFO: Pod "pod-fada5923-833b-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012129698s
STEP: Saw pod success
May 31 00:35:28.663: INFO: Pod "pod-fada5923-833b-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:35:28.665: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-fada5923-833b-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:35:28.678: INFO: Waiting for pod pod-fada5923-833b-11e9-88d7-16679dc4b203 to disappear
May 31 00:35:28.680: INFO: Pod pod-fada5923-833b-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:35:28.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dlrd5" for this suite.
May 31 00:35:34.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:35:34.720: INFO: namespace: e2e-tests-emptydir-dlrd5, resource: bindings, ignored listing per whitelist
May 31 00:35:34.772: INFO: namespace e2e-tests-emptydir-dlrd5 deletion completed in 6.089794012s

• [SLOW TEST:10.210 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:35:34.776: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-00f1a19e-833c-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:35:34.869: INFO: Waiting up to 5m0s for pod "pod-secrets-00f204b5-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-secrets-8tzpk" to be "success or failure"
May 31 00:35:34.871: INFO: Pod "pod-secrets-00f204b5-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06793ms
May 31 00:35:36.874: INFO: Pod "pod-secrets-00f204b5-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00541048s
May 31 00:35:38.877: INFO: Pod "pod-secrets-00f204b5-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008567293s
STEP: Saw pod success
May 31 00:35:38.878: INFO: Pod "pod-secrets-00f204b5-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:35:38.880: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-secrets-00f204b5-833c-11e9-88d7-16679dc4b203 container secret-volume-test: <nil>
STEP: delete the pod
May 31 00:35:38.896: INFO: Waiting for pod pod-secrets-00f204b5-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:35:38.899: INFO: Pod pod-secrets-00f204b5-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:35:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8tzpk" for this suite.
May 31 00:35:44.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:35:44.955: INFO: namespace: e2e-tests-secrets-8tzpk, resource: bindings, ignored listing per whitelist
May 31 00:35:45.001: INFO: namespace e2e-tests-secrets-8tzpk deletion completed in 6.099247314s

• [SLOW TEST:10.225 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:35:45.001: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-wzjmw
I0531 00:35:45.086112      21 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-wzjmw, replica count: 1
I0531 00:35:46.139928      21 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 00:35:47.140165      21 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 00:35:48.140457      21 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 00:35:48.250: INFO: Created: latency-svc-9wflp
May 31 00:35:48.280: INFO: Got endpoints: latency-svc-9wflp [39.633264ms]
May 31 00:35:48.295: INFO: Created: latency-svc-5zlcp
May 31 00:35:48.304: INFO: Got endpoints: latency-svc-5zlcp [21.375204ms]
May 31 00:35:48.306: INFO: Created: latency-svc-chhtt
May 31 00:35:48.313: INFO: Created: latency-svc-gdqzj
May 31 00:35:48.315: INFO: Got endpoints: latency-svc-chhtt [32.029956ms]
May 31 00:35:48.319: INFO: Got endpoints: latency-svc-gdqzj [37.014727ms]
May 31 00:35:48.323: INFO: Created: latency-svc-j4lrl
May 31 00:35:48.333: INFO: Got endpoints: latency-svc-j4lrl [49.919011ms]
May 31 00:35:48.335: INFO: Created: latency-svc-n8v8s
May 31 00:35:48.341: INFO: Created: latency-svc-qjg6x
May 31 00:35:48.342: INFO: Got endpoints: latency-svc-n8v8s [58.499433ms]
May 31 00:35:48.354: INFO: Created: latency-svc-4cnb4
May 31 00:35:48.360: INFO: Got endpoints: latency-svc-qjg6x [40.565478ms]
May 31 00:35:48.363: INFO: Created: latency-svc-nvffl
May 31 00:35:48.365: INFO: Got endpoints: latency-svc-4cnb4 [82.106169ms]
May 31 00:35:48.374: INFO: Created: latency-svc-c4jfn
May 31 00:35:48.378: INFO: Got endpoints: latency-svc-c4jfn [95.256456ms]
May 31 00:35:48.378: INFO: Got endpoints: latency-svc-nvffl [95.174055ms]
May 31 00:35:48.386: INFO: Created: latency-svc-f7txz
May 31 00:35:48.395: INFO: Created: latency-svc-sxpmf
May 31 00:35:48.407: INFO: Got endpoints: latency-svc-sxpmf [123.269555ms]
May 31 00:35:48.414: INFO: Created: latency-svc-kfw4t
May 31 00:35:48.414: INFO: Got endpoints: latency-svc-f7txz [130.954365ms]
May 31 00:35:48.418: INFO: Got endpoints: latency-svc-kfw4t [134.532116ms]
May 31 00:35:48.423: INFO: Created: latency-svc-qhxwp
May 31 00:35:48.433: INFO: Got endpoints: latency-svc-qhxwp [149.208625ms]
May 31 00:35:48.437: INFO: Created: latency-svc-gxssq
May 31 00:35:48.451: INFO: Got endpoints: latency-svc-gxssq [167.966992ms]
May 31 00:35:48.452: INFO: Created: latency-svc-7pcvr
May 31 00:35:48.459: INFO: Got endpoints: latency-svc-7pcvr [175.5164ms]
May 31 00:35:48.464: INFO: Created: latency-svc-kk45q
May 31 00:35:48.465: INFO: Got endpoints: latency-svc-kk45q [182.650502ms]
May 31 00:35:48.470: INFO: Created: latency-svc-n45qs
May 31 00:35:48.474: INFO: Got endpoints: latency-svc-n45qs [170.494928ms]
May 31 00:35:48.478: INFO: Created: latency-svc-vjdcn
May 31 00:35:48.485: INFO: Created: latency-svc-qvpmj
May 31 00:35:48.488: INFO: Got endpoints: latency-svc-vjdcn [172.881163ms]
May 31 00:35:48.492: INFO: Created: latency-svc-w5tkm
May 31 00:35:48.500: INFO: Created: latency-svc-f5zh4
May 31 00:35:48.508: INFO: Created: latency-svc-t9f54
May 31 00:35:48.515: INFO: Got endpoints: latency-svc-w5tkm [172.574359ms]
May 31 00:35:48.515: INFO: Got endpoints: latency-svc-qvpmj [182.426398ms]
May 31 00:35:48.516: INFO: Got endpoints: latency-svc-f5zh4 [155.477214ms]
May 31 00:35:48.519: INFO: Created: latency-svc-z2gp5
May 31 00:35:48.522: INFO: Got endpoints: latency-svc-t9f54 [156.674231ms]
May 31 00:35:48.534: INFO: Got endpoints: latency-svc-z2gp5 [155.971922ms]
May 31 00:35:48.536: INFO: Created: latency-svc-tn4sz
May 31 00:35:48.545: INFO: Created: latency-svc-jm9ml
May 31 00:35:48.548: INFO: Got endpoints: latency-svc-tn4sz [168.949106ms]
May 31 00:35:48.565: INFO: Got endpoints: latency-svc-jm9ml [157.808848ms]
May 31 00:35:48.569: INFO: Created: latency-svc-ffltp
May 31 00:35:48.580: INFO: Got endpoints: latency-svc-ffltp [165.254754ms]
May 31 00:35:48.582: INFO: Created: latency-svc-gdhvm
May 31 00:35:48.587: INFO: Got endpoints: latency-svc-gdhvm [168.472799ms]
May 31 00:35:48.593: INFO: Created: latency-svc-f29rf
May 31 00:35:48.600: INFO: Got endpoints: latency-svc-f29rf [166.772375ms]
May 31 00:35:48.601: INFO: Created: latency-svc-s9bl8
May 31 00:35:48.601: INFO: Got endpoints: latency-svc-s9bl8 [149.383027ms]
May 31 00:35:48.608: INFO: Created: latency-svc-cvj7d
May 31 00:35:48.616: INFO: Got endpoints: latency-svc-cvj7d [156.943936ms]
May 31 00:35:48.622: INFO: Created: latency-svc-74g62
May 31 00:35:48.627: INFO: Created: latency-svc-gncsw
May 31 00:35:48.628: INFO: Got endpoints: latency-svc-74g62 [162.414213ms]
May 31 00:35:48.639: INFO: Got endpoints: latency-svc-gncsw [164.642145ms]
May 31 00:35:48.643: INFO: Created: latency-svc-hnqpq
May 31 00:35:48.651: INFO: Got endpoints: latency-svc-hnqpq [162.939321ms]
May 31 00:35:48.654: INFO: Created: latency-svc-5gl4d
May 31 00:35:48.659: INFO: Got endpoints: latency-svc-5gl4d [143.429343ms]
May 31 00:35:48.663: INFO: Created: latency-svc-n7phb
May 31 00:35:48.678: INFO: Created: latency-svc-8v7lp
May 31 00:35:48.683: INFO: Got endpoints: latency-svc-8v7lp [167.698689ms]
May 31 00:35:48.683: INFO: Got endpoints: latency-svc-n7phb [167.82109ms]
May 31 00:35:48.694: INFO: Created: latency-svc-fz2zw
May 31 00:35:48.701: INFO: Created: latency-svc-9kng6
May 31 00:35:48.705: INFO: Created: latency-svc-7zwn8
May 31 00:35:48.709: INFO: Got endpoints: latency-svc-9kng6 [174.82729ms]
May 31 00:35:48.710: INFO: Got endpoints: latency-svc-fz2zw [188.115479ms]
May 31 00:35:48.715: INFO: Created: latency-svc-xkhdg
May 31 00:35:48.723: INFO: Created: latency-svc-pbl85
May 31 00:35:48.731: INFO: Created: latency-svc-g9spd
May 31 00:35:48.742: INFO: Created: latency-svc-tmh7n
May 31 00:35:48.751: INFO: Created: latency-svc-4glcm
May 31 00:35:48.759: INFO: Got endpoints: latency-svc-7zwn8 [211.763416ms]
May 31 00:35:48.764: INFO: Created: latency-svc-76cpd
May 31 00:35:48.769: INFO: Created: latency-svc-4wbg6
May 31 00:35:48.777: INFO: Created: latency-svc-2w7sb
May 31 00:35:48.788: INFO: Created: latency-svc-kd9d9
May 31 00:35:48.800: INFO: Created: latency-svc-24gg7
May 31 00:35:48.803: INFO: Got endpoints: latency-svc-xkhdg [238.792202ms]
May 31 00:35:48.822: INFO: Created: latency-svc-vtrp5
May 31 00:35:48.831: INFO: Created: latency-svc-qk5lr
May 31 00:35:48.838: INFO: Created: latency-svc-4nv9q
May 31 00:35:48.863: INFO: Got endpoints: latency-svc-pbl85 [282.93093ms]
May 31 00:35:48.866: INFO: Created: latency-svc-6ncmp
May 31 00:35:48.866: INFO: Created: latency-svc-cgqq5
May 31 00:35:48.872: INFO: Created: latency-svc-wwt5k
May 31 00:35:48.882: INFO: Created: latency-svc-k7t8j
May 31 00:35:48.904: INFO: Got endpoints: latency-svc-g9spd [316.879313ms]
May 31 00:35:48.917: INFO: Created: latency-svc-wcvbw
May 31 00:35:48.955: INFO: Got endpoints: latency-svc-tmh7n [353.671538ms]
May 31 00:35:48.965: INFO: Created: latency-svc-mslh7
May 31 00:35:49.005: INFO: Got endpoints: latency-svc-4glcm [405.227072ms]
May 31 00:35:49.017: INFO: Created: latency-svc-b8xd4
May 31 00:35:49.053: INFO: Got endpoints: latency-svc-76cpd [436.886622ms]
May 31 00:35:49.064: INFO: Created: latency-svc-mn9kt
May 31 00:35:49.104: INFO: Got endpoints: latency-svc-4wbg6 [475.548272ms]
May 31 00:35:49.115: INFO: Created: latency-svc-f4fhv
May 31 00:35:49.154: INFO: Got endpoints: latency-svc-2w7sb [514.582128ms]
May 31 00:35:49.167: INFO: Created: latency-svc-pzkqw
May 31 00:35:49.203: INFO: Got endpoints: latency-svc-kd9d9 [552.375366ms]
May 31 00:35:49.215: INFO: Created: latency-svc-79qk5
May 31 00:35:49.254: INFO: Got endpoints: latency-svc-24gg7 [595.638982ms]
May 31 00:35:49.267: INFO: Created: latency-svc-gc4mf
May 31 00:35:49.306: INFO: Got endpoints: latency-svc-vtrp5 [621.797154ms]
May 31 00:35:49.321: INFO: Created: latency-svc-6dlfs
May 31 00:35:49.357: INFO: Got endpoints: latency-svc-qk5lr [673.319488ms]
May 31 00:35:49.386: INFO: Created: latency-svc-4cdgq
May 31 00:35:49.404: INFO: Got endpoints: latency-svc-4nv9q [694.236586ms]
May 31 00:35:49.416: INFO: Created: latency-svc-fj5r9
May 31 00:35:49.454: INFO: Got endpoints: latency-svc-6ncmp [743.193381ms]
May 31 00:35:49.468: INFO: Created: latency-svc-jkqd8
May 31 00:35:49.509: INFO: Got endpoints: latency-svc-cgqq5 [749.603173ms]
May 31 00:35:49.529: INFO: Created: latency-svc-4fb2l
May 31 00:35:49.554: INFO: Got endpoints: latency-svc-wwt5k [750.594286ms]
May 31 00:35:49.566: INFO: Created: latency-svc-5cjv4
May 31 00:35:49.660: INFO: Got endpoints: latency-svc-wcvbw [756.212965ms]
May 31 00:35:49.660: INFO: Got endpoints: latency-svc-k7t8j [797.27015ms]
May 31 00:35:49.676: INFO: Created: latency-svc-n7lt2
May 31 00:35:49.684: INFO: Created: latency-svc-bsxkr
May 31 00:35:49.703: INFO: Got endpoints: latency-svc-mslh7 [748.265152ms]
May 31 00:35:49.723: INFO: Created: latency-svc-4wtqt
May 31 00:35:49.753: INFO: Got endpoints: latency-svc-b8xd4 [745.927218ms]
May 31 00:35:49.765: INFO: Created: latency-svc-8kd2r
May 31 00:35:49.802: INFO: Got endpoints: latency-svc-mn9kt [748.323653ms]
May 31 00:35:49.813: INFO: Created: latency-svc-2gtfk
May 31 00:35:49.857: INFO: Got endpoints: latency-svc-f4fhv [752.675315ms]
May 31 00:35:49.868: INFO: Created: latency-svc-rmt7k
May 31 00:35:49.907: INFO: Got endpoints: latency-svc-pzkqw [751.862803ms]
May 31 00:35:49.919: INFO: Created: latency-svc-mkvfm
May 31 00:35:49.959: INFO: Got endpoints: latency-svc-79qk5 [755.714458ms]
May 31 00:35:49.993: INFO: Created: latency-svc-gtcwl
May 31 00:35:50.004: INFO: Got endpoints: latency-svc-gc4mf [749.654671ms]
May 31 00:35:50.017: INFO: Created: latency-svc-zhb6f
May 31 00:35:50.058: INFO: Got endpoints: latency-svc-6dlfs [752.502612ms]
May 31 00:35:50.069: INFO: Created: latency-svc-nc6vm
May 31 00:35:50.110: INFO: Got endpoints: latency-svc-4cdgq [752.806316ms]
May 31 00:35:50.134: INFO: Created: latency-svc-kfbpr
May 31 00:35:50.162: INFO: Got endpoints: latency-svc-fj5r9 [757.899388ms]
May 31 00:35:50.176: INFO: Created: latency-svc-bmdqk
May 31 00:35:50.214: INFO: Got endpoints: latency-svc-jkqd8 [759.815915ms]
May 31 00:35:50.222: INFO: Created: latency-svc-pjt42
May 31 00:35:50.253: INFO: Got endpoints: latency-svc-4fb2l [743.363979ms]
May 31 00:35:50.261: INFO: Created: latency-svc-6wc4v
May 31 00:35:50.302: INFO: Got endpoints: latency-svc-5cjv4 [747.235335ms]
May 31 00:35:50.313: INFO: Created: latency-svc-ngsfz
May 31 00:35:50.353: INFO: Got endpoints: latency-svc-n7lt2 [692.814359ms]
May 31 00:35:50.364: INFO: Created: latency-svc-hvfst
May 31 00:35:50.404: INFO: Got endpoints: latency-svc-bsxkr [743.323578ms]
May 31 00:35:50.414: INFO: Created: latency-svc-l9q5s
May 31 00:35:50.452: INFO: Got endpoints: latency-svc-4wtqt [745.785913ms]
May 31 00:35:50.463: INFO: Created: latency-svc-htd64
May 31 00:35:50.503: INFO: Got endpoints: latency-svc-8kd2r [750.126974ms]
May 31 00:35:50.531: INFO: Created: latency-svc-gqc98
May 31 00:35:50.555: INFO: Got endpoints: latency-svc-2gtfk [751.176289ms]
May 31 00:35:50.566: INFO: Created: latency-svc-tzmzz
May 31 00:35:50.603: INFO: Got endpoints: latency-svc-rmt7k [746.078416ms]
May 31 00:35:50.614: INFO: Created: latency-svc-w5nsr
May 31 00:35:50.653: INFO: Got endpoints: latency-svc-mkvfm [746.234418ms]
May 31 00:35:50.664: INFO: Created: latency-svc-2m2kh
May 31 00:35:50.704: INFO: Got endpoints: latency-svc-gtcwl [744.35599ms]
May 31 00:35:50.715: INFO: Created: latency-svc-7njg5
May 31 00:35:50.755: INFO: Got endpoints: latency-svc-zhb6f [750.246373ms]
May 31 00:35:50.766: INFO: Created: latency-svc-hwpjq
May 31 00:35:50.804: INFO: Got endpoints: latency-svc-nc6vm [746.047714ms]
May 31 00:35:50.821: INFO: Created: latency-svc-fgcqz
May 31 00:35:50.854: INFO: Got endpoints: latency-svc-kfbpr [743.692481ms]
May 31 00:35:50.869: INFO: Created: latency-svc-q69fh
May 31 00:35:50.904: INFO: Got endpoints: latency-svc-bmdqk [742.050857ms]
May 31 00:35:50.916: INFO: Created: latency-svc-kqvv7
May 31 00:35:50.954: INFO: Got endpoints: latency-svc-pjt42 [740.262832ms]
May 31 00:35:50.966: INFO: Created: latency-svc-v4h49
May 31 00:35:51.003: INFO: Got endpoints: latency-svc-6wc4v [750.281774ms]
May 31 00:35:51.015: INFO: Created: latency-svc-f8rql
May 31 00:35:51.056: INFO: Got endpoints: latency-svc-ngsfz [754.759638ms]
May 31 00:35:51.070: INFO: Created: latency-svc-rhdwm
May 31 00:35:51.103: INFO: Got endpoints: latency-svc-hvfst [749.709865ms]
May 31 00:35:51.118: INFO: Created: latency-svc-kdnw9
May 31 00:35:51.153: INFO: Got endpoints: latency-svc-l9q5s [749.37686ms]
May 31 00:35:51.164: INFO: Created: latency-svc-npddl
May 31 00:35:51.203: INFO: Got endpoints: latency-svc-htd64 [750.378974ms]
May 31 00:35:51.215: INFO: Created: latency-svc-blvdh
May 31 00:35:51.254: INFO: Got endpoints: latency-svc-gqc98 [751.013583ms]
May 31 00:35:51.264: INFO: Created: latency-svc-8xmts
May 31 00:35:51.302: INFO: Got endpoints: latency-svc-tzmzz [747.147227ms]
May 31 00:35:51.314: INFO: Created: latency-svc-qlrw8
May 31 00:35:51.353: INFO: Got endpoints: latency-svc-w5nsr [749.447659ms]
May 31 00:35:51.363: INFO: Created: latency-svc-s428v
May 31 00:35:51.409: INFO: Got endpoints: latency-svc-2m2kh [755.732048ms]
May 31 00:35:51.420: INFO: Created: latency-svc-t2lj4
May 31 00:35:51.454: INFO: Got endpoints: latency-svc-7njg5 [749.915965ms]
May 31 00:35:51.465: INFO: Created: latency-svc-wrn8n
May 31 00:35:51.503: INFO: Got endpoints: latency-svc-hwpjq [748.001037ms]
May 31 00:35:51.518: INFO: Created: latency-svc-2rkb2
May 31 00:35:51.554: INFO: Got endpoints: latency-svc-fgcqz [749.58546ms]
May 31 00:35:51.566: INFO: Created: latency-svc-hxb8g
May 31 00:35:51.605: INFO: Got endpoints: latency-svc-q69fh [750.398071ms]
May 31 00:35:51.615: INFO: Created: latency-svc-dqd74
May 31 00:35:51.653: INFO: Got endpoints: latency-svc-kqvv7 [748.92115ms]
May 31 00:35:51.665: INFO: Created: latency-svc-fgnpv
May 31 00:35:51.705: INFO: Got endpoints: latency-svc-v4h49 [750.848177ms]
May 31 00:35:51.731: INFO: Created: latency-svc-nl8xz
May 31 00:35:51.758: INFO: Got endpoints: latency-svc-f8rql [754.577229ms]
May 31 00:35:51.788: INFO: Created: latency-svc-bd7j8
May 31 00:35:51.809: INFO: Got endpoints: latency-svc-rhdwm [752.209895ms]
May 31 00:35:51.821: INFO: Created: latency-svc-h49b5
May 31 00:35:51.857: INFO: Got endpoints: latency-svc-kdnw9 [753.861719ms]
May 31 00:35:51.872: INFO: Created: latency-svc-rlt2c
May 31 00:35:51.904: INFO: Got endpoints: latency-svc-npddl [750.241567ms]
May 31 00:35:51.916: INFO: Created: latency-svc-xqml9
May 31 00:35:51.953: INFO: Got endpoints: latency-svc-blvdh [749.663159ms]
May 31 00:35:51.962: INFO: Created: latency-svc-6wrwj
May 31 00:35:52.002: INFO: Got endpoints: latency-svc-8xmts [747.632231ms]
May 31 00:35:52.014: INFO: Created: latency-svc-rcpnk
May 31 00:35:52.052: INFO: Got endpoints: latency-svc-qlrw8 [749.74386ms]
May 31 00:35:52.069: INFO: Created: latency-svc-77djv
May 31 00:35:52.104: INFO: Got endpoints: latency-svc-s428v [750.890976ms]
May 31 00:35:52.116: INFO: Created: latency-svc-djdwr
May 31 00:35:52.152: INFO: Got endpoints: latency-svc-t2lj4 [742.866761ms]
May 31 00:35:52.162: INFO: Created: latency-svc-86ck7
May 31 00:35:52.206: INFO: Got endpoints: latency-svc-wrn8n [752.102992ms]
May 31 00:35:52.217: INFO: Created: latency-svc-ph2ln
May 31 00:35:52.261: INFO: Got endpoints: latency-svc-2rkb2 [757.678271ms]
May 31 00:35:52.270: INFO: Created: latency-svc-wpb7k
May 31 00:35:52.309: INFO: Got endpoints: latency-svc-hxb8g [755.494639ms]
May 31 00:35:52.319: INFO: Created: latency-svc-5mzdj
May 31 00:35:52.353: INFO: Got endpoints: latency-svc-dqd74 [747.754029ms]
May 31 00:35:52.365: INFO: Created: latency-svc-l2kqz
May 31 00:35:52.402: INFO: Got endpoints: latency-svc-fgnpv [748.949445ms]
May 31 00:35:52.419: INFO: Created: latency-svc-6n4tq
May 31 00:35:52.452: INFO: Got endpoints: latency-svc-nl8xz [747.407824ms]
May 31 00:35:52.462: INFO: Created: latency-svc-2rwzm
May 31 00:35:52.502: INFO: Got endpoints: latency-svc-bd7j8 [744.105876ms]
May 31 00:35:52.531: INFO: Created: latency-svc-n7xcv
May 31 00:35:52.560: INFO: Got endpoints: latency-svc-h49b5 [750.987873ms]
May 31 00:35:52.571: INFO: Created: latency-svc-tzjpb
May 31 00:35:52.603: INFO: Got endpoints: latency-svc-rlt2c [746.413908ms]
May 31 00:35:52.617: INFO: Created: latency-svc-7cxpz
May 31 00:35:52.654: INFO: Got endpoints: latency-svc-xqml9 [749.639054ms]
May 31 00:35:52.669: INFO: Created: latency-svc-g8fgm
May 31 00:35:52.704: INFO: Got endpoints: latency-svc-6wrwj [750.788569ms]
May 31 00:35:52.713: INFO: Created: latency-svc-krs5d
May 31 00:35:52.753: INFO: Got endpoints: latency-svc-rcpnk [750.752068ms]
May 31 00:35:52.768: INFO: Created: latency-svc-zbxs2
May 31 00:35:52.802: INFO: Got endpoints: latency-svc-77djv [750.358163ms]
May 31 00:35:52.821: INFO: Created: latency-svc-k7pss
May 31 00:35:52.853: INFO: Got endpoints: latency-svc-djdwr [748.902742ms]
May 31 00:35:52.865: INFO: Created: latency-svc-7j8xq
May 31 00:35:52.905: INFO: Got endpoints: latency-svc-86ck7 [751.953685ms]
May 31 00:35:52.915: INFO: Created: latency-svc-p8r7l
May 31 00:35:53.011: INFO: Got endpoints: latency-svc-ph2ln [804.35913ms]
May 31 00:35:53.012: INFO: Got endpoints: latency-svc-wpb7k [750.767468ms]
May 31 00:35:53.023: INFO: Created: latency-svc-q4bgd
May 31 00:35:53.034: INFO: Created: latency-svc-hg4vf
May 31 00:35:53.053: INFO: Got endpoints: latency-svc-5mzdj [743.86767ms]
May 31 00:35:53.068: INFO: Created: latency-svc-gkgfp
May 31 00:35:53.102: INFO: Got endpoints: latency-svc-l2kqz [749.845454ms]
May 31 00:35:53.112: INFO: Created: latency-svc-rkjfq
May 31 00:35:53.159: INFO: Got endpoints: latency-svc-6n4tq [756.764353ms]
May 31 00:35:53.170: INFO: Created: latency-svc-5jmb9
May 31 00:35:53.206: INFO: Got endpoints: latency-svc-2rwzm [753.78381ms]
May 31 00:35:53.218: INFO: Created: latency-svc-g9zwt
May 31 00:35:53.253: INFO: Got endpoints: latency-svc-n7xcv [750.698565ms]
May 31 00:35:53.263: INFO: Created: latency-svc-h5ncg
May 31 00:35:53.302: INFO: Got endpoints: latency-svc-tzjpb [742.463648ms]
May 31 00:35:53.313: INFO: Created: latency-svc-6rhsj
May 31 00:35:53.354: INFO: Got endpoints: latency-svc-7cxpz [750.538863ms]
May 31 00:35:53.363: INFO: Created: latency-svc-854tg
May 31 00:35:53.414: INFO: Got endpoints: latency-svc-g8fgm [760.127198ms]
May 31 00:35:53.428: INFO: Created: latency-svc-f2ndr
May 31 00:35:53.454: INFO: Got endpoints: latency-svc-krs5d [749.783651ms]
May 31 00:35:53.464: INFO: Created: latency-svc-pbjhd
May 31 00:35:53.502: INFO: Got endpoints: latency-svc-zbxs2 [748.615533ms]
May 31 00:35:53.513: INFO: Created: latency-svc-lgdtv
May 31 00:35:53.555: INFO: Got endpoints: latency-svc-k7pss [752.465888ms]
May 31 00:35:53.565: INFO: Created: latency-svc-j7rt8
May 31 00:35:53.603: INFO: Got endpoints: latency-svc-7j8xq [749.685448ms]
May 31 00:35:53.615: INFO: Created: latency-svc-vcg8w
May 31 00:35:53.653: INFO: Got endpoints: latency-svc-p8r7l [748.535132ms]
May 31 00:35:53.670: INFO: Created: latency-svc-k5whq
May 31 00:35:53.702: INFO: Got endpoints: latency-svc-q4bgd [691.278717ms]
May 31 00:35:53.711: INFO: Created: latency-svc-vkgzt
May 31 00:35:53.753: INFO: Got endpoints: latency-svc-hg4vf [741.160326ms]
May 31 00:35:53.771: INFO: Created: latency-svc-plbmg
May 31 00:35:53.804: INFO: Got endpoints: latency-svc-gkgfp [750.752362ms]
May 31 00:35:53.813: INFO: Created: latency-svc-mmx4q
May 31 00:35:53.853: INFO: Got endpoints: latency-svc-rkjfq [750.381257ms]
May 31 00:35:53.862: INFO: Created: latency-svc-ckj7f
May 31 00:35:53.903: INFO: Got endpoints: latency-svc-5jmb9 [743.635361ms]
May 31 00:35:53.912: INFO: Created: latency-svc-r84rr
May 31 00:35:53.953: INFO: Got endpoints: latency-svc-g9zwt [746.4155ms]
May 31 00:35:53.966: INFO: Created: latency-svc-rpvv7
May 31 00:35:54.003: INFO: Got endpoints: latency-svc-h5ncg [749.540945ms]
May 31 00:35:54.012: INFO: Created: latency-svc-779xs
May 31 00:35:54.054: INFO: Got endpoints: latency-svc-6rhsj [751.044066ms]
May 31 00:35:54.071: INFO: Created: latency-svc-55287
May 31 00:35:54.103: INFO: Got endpoints: latency-svc-854tg [749.25874ms]
May 31 00:35:54.115: INFO: Created: latency-svc-9k7p5
May 31 00:35:54.158: INFO: Got endpoints: latency-svc-f2ndr [743.297555ms]
May 31 00:35:54.179: INFO: Created: latency-svc-qg829
May 31 00:35:54.208: INFO: Got endpoints: latency-svc-pbjhd [753.947005ms]
May 31 00:35:54.221: INFO: Created: latency-svc-vwddt
May 31 00:35:54.260: INFO: Got endpoints: latency-svc-lgdtv [758.319067ms]
May 31 00:35:54.281: INFO: Created: latency-svc-qp8rp
May 31 00:35:54.313: INFO: Got endpoints: latency-svc-j7rt8 [758.436769ms]
May 31 00:35:54.329: INFO: Created: latency-svc-lpmn8
May 31 00:35:54.367: INFO: Got endpoints: latency-svc-vcg8w [763.845946ms]
May 31 00:35:54.385: INFO: Created: latency-svc-qsdj4
May 31 00:35:54.408: INFO: Got endpoints: latency-svc-k5whq [754.839817ms]
May 31 00:35:54.423: INFO: Created: latency-svc-gfxtl
May 31 00:35:54.452: INFO: Got endpoints: latency-svc-vkgzt [748.906732ms]
May 31 00:35:54.471: INFO: Created: latency-svc-vjr5n
May 31 00:35:54.506: INFO: Got endpoints: latency-svc-plbmg [752.598785ms]
May 31 00:35:54.536: INFO: Created: latency-svc-vj4sc
May 31 00:35:54.553: INFO: Got endpoints: latency-svc-mmx4q [748.708228ms]
May 31 00:35:54.566: INFO: Created: latency-svc-2cvwz
May 31 00:35:54.605: INFO: Got endpoints: latency-svc-ckj7f [752.33778ms]
May 31 00:35:54.628: INFO: Created: latency-svc-kf7dw
May 31 00:35:54.655: INFO: Got endpoints: latency-svc-r84rr [752.443881ms]
May 31 00:35:54.671: INFO: Created: latency-svc-cgf7q
May 31 00:35:54.705: INFO: Got endpoints: latency-svc-rpvv7 [752.374879ms]
May 31 00:35:54.722: INFO: Created: latency-svc-g6g24
May 31 00:35:54.755: INFO: Got endpoints: latency-svc-779xs [752.211977ms]
May 31 00:35:54.773: INFO: Created: latency-svc-7hcdh
May 31 00:35:54.803: INFO: Got endpoints: latency-svc-55287 [749.192434ms]
May 31 00:35:54.824: INFO: Created: latency-svc-kkqhq
May 31 00:35:54.856: INFO: Got endpoints: latency-svc-9k7p5 [752.937487ms]
May 31 00:35:54.873: INFO: Created: latency-svc-5gl84
May 31 00:35:54.909: INFO: Got endpoints: latency-svc-qg829 [751.215563ms]
May 31 00:35:54.933: INFO: Created: latency-svc-9z5ds
May 31 00:35:54.956: INFO: Got endpoints: latency-svc-vwddt [748.781128ms]
May 31 00:35:54.967: INFO: Created: latency-svc-fvh9z
May 31 00:35:55.003: INFO: Got endpoints: latency-svc-qp8rp [742.204335ms]
May 31 00:35:55.015: INFO: Created: latency-svc-nlw8t
May 31 00:35:55.081: INFO: Got endpoints: latency-svc-lpmn8 [767.640695ms]
May 31 00:35:55.101: INFO: Created: latency-svc-t4cfn
May 31 00:35:55.124: INFO: Got endpoints: latency-svc-qsdj4 [756.919043ms]
May 31 00:35:55.134: INFO: Created: latency-svc-dc68n
May 31 00:35:55.155: INFO: Got endpoints: latency-svc-gfxtl [746.480994ms]
May 31 00:35:55.175: INFO: Created: latency-svc-nwxqh
May 31 00:35:55.202: INFO: Got endpoints: latency-svc-vjr5n [750.45105ms]
May 31 00:35:55.217: INFO: Created: latency-svc-92mk8
May 31 00:35:55.253: INFO: Got endpoints: latency-svc-vj4sc [747.278905ms]
May 31 00:35:55.264: INFO: Created: latency-svc-q6bkz
May 31 00:35:55.303: INFO: Got endpoints: latency-svc-2cvwz [749.707339ms]
May 31 00:35:55.323: INFO: Created: latency-svc-5wxll
May 31 00:35:55.352: INFO: Got endpoints: latency-svc-kf7dw [746.840297ms]
May 31 00:35:55.363: INFO: Created: latency-svc-cpl95
May 31 00:35:55.403: INFO: Got endpoints: latency-svc-cgf7q [747.69171ms]
May 31 00:35:55.416: INFO: Created: latency-svc-5kmqw
May 31 00:35:55.455: INFO: Got endpoints: latency-svc-g6g24 [749.481335ms]
May 31 00:35:55.465: INFO: Created: latency-svc-wgxj5
May 31 00:35:55.513: INFO: Got endpoints: latency-svc-7hcdh [757.490348ms]
May 31 00:35:55.525: INFO: Created: latency-svc-6qfcz
May 31 00:35:55.554: INFO: Got endpoints: latency-svc-kkqhq [750.570149ms]
May 31 00:35:55.568: INFO: Created: latency-svc-66dnk
May 31 00:35:55.606: INFO: Got endpoints: latency-svc-5gl84 [747.1848ms]
May 31 00:35:55.616: INFO: Created: latency-svc-2vgfp
May 31 00:35:55.655: INFO: Got endpoints: latency-svc-9z5ds [746.192986ms]
May 31 00:35:55.669: INFO: Created: latency-svc-46db4
May 31 00:35:55.704: INFO: Got endpoints: latency-svc-fvh9z [747.143599ms]
May 31 00:35:55.728: INFO: Created: latency-svc-w45vr
May 31 00:35:55.755: INFO: Got endpoints: latency-svc-nlw8t [751.798164ms]
May 31 00:35:55.765: INFO: Created: latency-svc-gnz6h
May 31 00:35:55.803: INFO: Got endpoints: latency-svc-t4cfn [721.906041ms]
May 31 00:35:55.832: INFO: Created: latency-svc-pjvts
May 31 00:35:55.852: INFO: Got endpoints: latency-svc-dc68n [728.010528ms]
May 31 00:35:55.865: INFO: Created: latency-svc-swjjq
May 31 00:35:55.905: INFO: Got endpoints: latency-svc-nwxqh [749.418131ms]
May 31 00:35:55.915: INFO: Created: latency-svc-hvbk2
May 31 00:35:55.953: INFO: Got endpoints: latency-svc-92mk8 [750.510846ms]
May 31 00:35:55.963: INFO: Created: latency-svc-z994c
May 31 00:35:56.003: INFO: Got endpoints: latency-svc-q6bkz [749.561934ms]
May 31 00:35:56.018: INFO: Created: latency-svc-krbwr
May 31 00:35:56.059: INFO: Got endpoints: latency-svc-5wxll [754.2609ms]
May 31 00:35:56.070: INFO: Created: latency-svc-zk8w4
May 31 00:35:56.108: INFO: Got endpoints: latency-svc-cpl95 [755.100911ms]
May 31 00:35:56.156: INFO: Got endpoints: latency-svc-5kmqw [752.23997ms]
May 31 00:35:56.208: INFO: Got endpoints: latency-svc-wgxj5 [753.142182ms]
May 31 00:35:56.255: INFO: Got endpoints: latency-svc-6qfcz [742.498031ms]
May 31 00:35:56.361: INFO: Got endpoints: latency-svc-2vgfp [755.398713ms]
May 31 00:35:56.373: INFO: Got endpoints: latency-svc-66dnk [819.38672ms]
May 31 00:35:56.408: INFO: Got endpoints: latency-svc-46db4 [752.466271ms]
May 31 00:35:56.454: INFO: Got endpoints: latency-svc-w45vr [749.897634ms]
May 31 00:35:56.512: INFO: Got endpoints: latency-svc-gnz6h [757.44714ms]
May 31 00:35:56.553: INFO: Got endpoints: latency-svc-pjvts [749.932833ms]
May 31 00:35:56.603: INFO: Got endpoints: latency-svc-swjjq [751.184651ms]
May 31 00:35:56.653: INFO: Got endpoints: latency-svc-hvbk2 [748.641715ms]
May 31 00:35:56.705: INFO: Got endpoints: latency-svc-z994c [752.094763ms]
May 31 00:35:56.755: INFO: Got endpoints: latency-svc-krbwr [752.539369ms]
May 31 00:35:56.823: INFO: Got endpoints: latency-svc-zk8w4 [764.023832ms]
May 31 00:35:56.823: INFO: Latencies: [21.375204ms 32.029956ms 37.014727ms 40.565478ms 49.919011ms 58.499433ms 82.106169ms 95.174055ms 95.256456ms 123.269555ms 130.954365ms 134.532116ms 143.429343ms 149.208625ms 149.383027ms 155.477214ms 155.971922ms 156.674231ms 156.943936ms 157.808848ms 162.414213ms 162.939321ms 164.642145ms 165.254754ms 166.772375ms 167.698689ms 167.82109ms 167.966992ms 168.472799ms 168.949106ms 170.494928ms 172.574359ms 172.881163ms 174.82729ms 175.5164ms 182.426398ms 182.650502ms 188.115479ms 211.763416ms 238.792202ms 282.93093ms 316.879313ms 353.671538ms 405.227072ms 436.886622ms 475.548272ms 514.582128ms 552.375366ms 595.638982ms 621.797154ms 673.319488ms 691.278717ms 692.814359ms 694.236586ms 721.906041ms 728.010528ms 740.262832ms 741.160326ms 742.050857ms 742.204335ms 742.463648ms 742.498031ms 742.866761ms 743.193381ms 743.297555ms 743.323578ms 743.363979ms 743.635361ms 743.692481ms 743.86767ms 744.105876ms 744.35599ms 745.785913ms 745.927218ms 746.047714ms 746.078416ms 746.192986ms 746.234418ms 746.413908ms 746.4155ms 746.480994ms 746.840297ms 747.143599ms 747.147227ms 747.1848ms 747.235335ms 747.278905ms 747.407824ms 747.632231ms 747.69171ms 747.754029ms 748.001037ms 748.265152ms 748.323653ms 748.535132ms 748.615533ms 748.641715ms 748.708228ms 748.781128ms 748.902742ms 748.906732ms 748.92115ms 748.949445ms 749.192434ms 749.25874ms 749.37686ms 749.418131ms 749.447659ms 749.481335ms 749.540945ms 749.561934ms 749.58546ms 749.603173ms 749.639054ms 749.654671ms 749.663159ms 749.685448ms 749.707339ms 749.709865ms 749.74386ms 749.783651ms 749.845454ms 749.897634ms 749.915965ms 749.932833ms 750.126974ms 750.241567ms 750.246373ms 750.281774ms 750.358163ms 750.378974ms 750.381257ms 750.398071ms 750.45105ms 750.510846ms 750.538863ms 750.570149ms 750.594286ms 750.698565ms 750.752068ms 750.752362ms 750.767468ms 750.788569ms 750.848177ms 750.890976ms 750.987873ms 751.013583ms 751.044066ms 751.176289ms 751.184651ms 751.215563ms 751.798164ms 751.862803ms 751.953685ms 752.094763ms 752.102992ms 752.209895ms 752.211977ms 752.23997ms 752.33778ms 752.374879ms 752.443881ms 752.465888ms 752.466271ms 752.502612ms 752.539369ms 752.598785ms 752.675315ms 752.806316ms 752.937487ms 753.142182ms 753.78381ms 753.861719ms 753.947005ms 754.2609ms 754.577229ms 754.759638ms 754.839817ms 755.100911ms 755.398713ms 755.494639ms 755.714458ms 755.732048ms 756.212965ms 756.764353ms 756.919043ms 757.44714ms 757.490348ms 757.678271ms 757.899388ms 758.319067ms 758.436769ms 759.815915ms 760.127198ms 763.845946ms 764.023832ms 767.640695ms 797.27015ms 804.35913ms 819.38672ms]
May 31 00:35:56.824: INFO: 50 %ile: 748.906732ms
May 31 00:35:56.824: INFO: 90 %ile: 755.494639ms
May 31 00:35:56.824: INFO: 99 %ile: 804.35913ms
May 31 00:35:56.824: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:35:56.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-wzjmw" for this suite.
May 31 00:36:20.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:36:20.932: INFO: namespace: e2e-tests-svc-latency-wzjmw, resource: bindings, ignored listing per whitelist
May 31 00:36:20.943: INFO: namespace e2e-tests-svc-latency-wzjmw deletion completed in 24.111319009s

• [SLOW TEST:35.942 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:36:20.946: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
May 31 00:36:21.049: INFO: Waiting up to 5m0s for pod "pod-1c78501f-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-rms2t" to be "success or failure"
May 31 00:36:21.061: INFO: Pod "pod-1c78501f-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 11.977568ms
May 31 00:36:23.065: INFO: Pod "pod-1c78501f-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015354558s
May 31 00:36:25.068: INFO: Pod "pod-1c78501f-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018569715s
STEP: Saw pod success
May 31 00:36:25.068: INFO: Pod "pod-1c78501f-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:36:25.070: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-1c78501f-833c-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:36:25.090: INFO: Waiting for pod pod-1c78501f-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:36:25.092: INFO: Pod pod-1c78501f-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:36:25.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rms2t" for this suite.
May 31 00:36:31.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:36:31.122: INFO: namespace: e2e-tests-emptydir-rms2t, resource: bindings, ignored listing per whitelist
May 31 00:36:31.185: INFO: namespace e2e-tests-emptydir-rms2t deletion completed in 6.082103588s

• [SLOW TEST:10.239 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:36:31.185: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-22913726-833c-11e9-88d7-16679dc4b203
STEP: Creating secret with name s-test-opt-upd-229137f0-833c-11e9-88d7-16679dc4b203
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-22913726-833c-11e9-88d7-16679dc4b203
STEP: Updating secret s-test-opt-upd-229137f0-833c-11e9-88d7-16679dc4b203
STEP: Creating secret with name s-test-opt-create-22913806-833c-11e9-88d7-16679dc4b203
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:36:39.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bfjf5" for this suite.
May 31 00:37:01.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:37:01.399: INFO: namespace: e2e-tests-secrets-bfjf5, resource: bindings, ignored listing per whitelist
May 31 00:37:01.470: INFO: namespace e2e-tests-secrets-bfjf5 deletion completed in 22.098269371s

• [SLOW TEST:30.284 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:37:01.472: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 31 00:37:01.559: INFO: Waiting up to 5m0s for pod "pod-349d7bff-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-2x9x7" to be "success or failure"
May 31 00:37:01.563: INFO: Pod "pod-349d7bff-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.39826ms
May 31 00:37:03.567: INFO: Pod "pod-349d7bff-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008130944s
May 31 00:37:05.571: INFO: Pod "pod-349d7bff-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011972099s
May 31 00:37:07.574: INFO: Pod "pod-349d7bff-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015250717s
May 31 00:37:09.577: INFO: Pod "pod-349d7bff-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018408805s
May 31 00:37:11.580: INFO: Pod "pod-349d7bff-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.021613365s
STEP: Saw pod success
May 31 00:37:11.580: INFO: Pod "pod-349d7bff-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:37:11.583: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-349d7bff-833c-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:37:11.599: INFO: Waiting for pod pod-349d7bff-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:37:11.601: INFO: Pod pod-349d7bff-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:37:11.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2x9x7" for this suite.
May 31 00:37:17.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:37:17.673: INFO: namespace: e2e-tests-emptydir-2x9x7, resource: bindings, ignored listing per whitelist
May 31 00:37:17.699: INFO: namespace e2e-tests-emptydir-2x9x7 deletion completed in 6.095475973s

• [SLOW TEST:16.227 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:37:17.700: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:37:17.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e4a8fef-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-ljfp7" to be "success or failure"
May 31 00:37:17.798: INFO: Pod "downwardapi-volume-3e4a8fef-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62579ms
May 31 00:37:19.801: INFO: Pod "downwardapi-volume-3e4a8fef-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009493629s
May 31 00:37:21.804: INFO: Pod "downwardapi-volume-3e4a8fef-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012564443s
STEP: Saw pod success
May 31 00:37:21.804: INFO: Pod "downwardapi-volume-3e4a8fef-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:37:21.808: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-3e4a8fef-833c-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:37:21.829: INFO: Waiting for pod downwardapi-volume-3e4a8fef-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:37:21.831: INFO: Pod downwardapi-volume-3e4a8fef-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:37:21.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ljfp7" for this suite.
May 31 00:37:27.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:37:27.892: INFO: namespace: e2e-tests-projected-ljfp7, resource: bindings, ignored listing per whitelist
May 31 00:37:27.924: INFO: namespace e2e-tests-projected-ljfp7 deletion completed in 6.088222338s

• [SLOW TEST:10.225 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:37:27.925: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
May 31 00:37:28.012: INFO: Waiting up to 5m0s for pod "var-expansion-44621917-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-var-expansion-hncdh" to be "success or failure"
May 31 00:37:28.018: INFO: Pod "var-expansion-44621917-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.490388ms
May 31 00:37:30.021: INFO: Pod "var-expansion-44621917-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009437186s
May 31 00:37:32.024: INFO: Pod "var-expansion-44621917-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01264726s
STEP: Saw pod success
May 31 00:37:32.024: INFO: Pod "var-expansion-44621917-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:37:32.027: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod var-expansion-44621917-833c-11e9-88d7-16679dc4b203 container dapi-container: <nil>
STEP: delete the pod
May 31 00:37:32.041: INFO: Waiting for pod var-expansion-44621917-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:37:32.043: INFO: Pod var-expansion-44621917-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:37:32.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-hncdh" for this suite.
May 31 00:37:38.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:37:38.130: INFO: namespace: e2e-tests-var-expansion-hncdh, resource: bindings, ignored listing per whitelist
May 31 00:37:38.145: INFO: namespace e2e-tests-var-expansion-hncdh deletion completed in 6.098659951s

• [SLOW TEST:10.220 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:37:38.147: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:37:38.247: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a7bc0bd-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-llvhb" to be "success or failure"
May 31 00:37:38.255: INFO: Pod "downwardapi-volume-4a7bc0bd-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 8.457813ms
May 31 00:37:40.258: INFO: Pod "downwardapi-volume-4a7bc0bd-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011177369s
May 31 00:37:42.261: INFO: Pod "downwardapi-volume-4a7bc0bd-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014322702s
STEP: Saw pod success
May 31 00:37:42.261: INFO: Pod "downwardapi-volume-4a7bc0bd-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:37:42.264: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod downwardapi-volume-4a7bc0bd-833c-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:37:42.281: INFO: Waiting for pod downwardapi-volume-4a7bc0bd-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:37:42.285: INFO: Pod downwardapi-volume-4a7bc0bd-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:37:42.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-llvhb" for this suite.
May 31 00:37:48.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:37:48.363: INFO: namespace: e2e-tests-projected-llvhb, resource: bindings, ignored listing per whitelist
May 31 00:37:48.385: INFO: namespace e2e-tests-projected-llvhb deletion completed in 6.096599103s

• [SLOW TEST:10.238 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:37:48.385: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-509303e9-833c-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:37:48.468: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-50937aeb-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-h64vn" to be "success or failure"
May 31 00:37:48.471: INFO: Pod "pod-projected-secrets-50937aeb-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827537ms
May 31 00:37:50.485: INFO: Pod "pod-projected-secrets-50937aeb-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016927507s
May 31 00:37:52.488: INFO: Pod "pod-projected-secrets-50937aeb-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019693199s
STEP: Saw pod success
May 31 00:37:52.488: INFO: Pod "pod-projected-secrets-50937aeb-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:37:52.490: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-projected-secrets-50937aeb-833c-11e9-88d7-16679dc4b203 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 00:37:52.502: INFO: Waiting for pod pod-projected-secrets-50937aeb-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:37:52.507: INFO: Pod pod-projected-secrets-50937aeb-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:37:52.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h64vn" for this suite.
May 31 00:37:58.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:37:58.592: INFO: namespace: e2e-tests-projected-h64vn, resource: bindings, ignored listing per whitelist
May 31 00:37:58.603: INFO: namespace e2e-tests-projected-h64vn deletion completed in 6.093276846s

• [SLOW TEST:10.218 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:37:58.604: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0531 00:38:08.702976      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 00:38:08.703: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:38:08.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-tl4lv" for this suite.
May 31 00:38:14.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:38:14.759: INFO: namespace: e2e-tests-gc-tl4lv, resource: bindings, ignored listing per whitelist
May 31 00:38:14.789: INFO: namespace e2e-tests-gc-tl4lv deletion completed in 6.083788287s

• [SLOW TEST:16.185 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:38:14.792: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:38:14.900: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"60547c6d-833c-11e9-b3f2-001dd80c0014", Controller:(*bool)(0xc420efe596), BlockOwnerDeletion:(*bool)(0xc420efe597)}}
May 31 00:38:14.907: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"60527709-833c-11e9-b3f2-001dd80c0014", Controller:(*bool)(0xc42105f3ae), BlockOwnerDeletion:(*bool)(0xc42105f3af)}}
May 31 00:38:14.913: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"60534a9d-833c-11e9-b3f2-001dd80c0014", Controller:(*bool)(0xc42105f586), BlockOwnerDeletion:(*bool)(0xc42105f587)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:38:19.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-wps4d" for this suite.
May 31 00:38:25.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:38:25.952: INFO: namespace: e2e-tests-gc-wps4d, resource: bindings, ignored listing per whitelist
May 31 00:38:26.019: INFO: namespace e2e-tests-gc-wps4d deletion completed in 6.092035468s

• [SLOW TEST:11.227 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:38:26.019: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0531 00:38:56.628613      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 00:38:56.628: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:38:56.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-885n4" for this suite.
May 31 00:39:02.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:39:02.711: INFO: namespace: e2e-tests-gc-885n4, resource: bindings, ignored listing per whitelist
May 31 00:39:02.734: INFO: namespace e2e-tests-gc-885n4 deletion completed in 6.103598279s

• [SLOW TEST:36.715 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:39:02.735: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 31 00:39:02.845: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:02.849: INFO: Number of nodes with available pods: 0
May 31 00:39:02.849: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:03.853: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:03.857: INFO: Number of nodes with available pods: 0
May 31 00:39:03.857: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:04.853: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:04.857: INFO: Number of nodes with available pods: 0
May 31 00:39:04.857: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:05.853: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:05.858: INFO: Number of nodes with available pods: 1
May 31 00:39:05.858: INFO: Node k8s-linuxpool-10610181-1 is running more than one daemon pod
May 31 00:39:06.853: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:06.855: INFO: Number of nodes with available pods: 2
May 31 00:39:06.855: INFO: Node k8s-linuxpool-10610181-1 is running more than one daemon pod
May 31 00:39:07.853: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:07.856: INFO: Number of nodes with available pods: 2
May 31 00:39:07.856: INFO: Node k8s-linuxpool-10610181-1 is running more than one daemon pod
May 31 00:39:08.853: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:08.855: INFO: Number of nodes with available pods: 2
May 31 00:39:08.855: INFO: Node k8s-linuxpool-10610181-1 is running more than one daemon pod
May 31 00:39:09.853: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:09.856: INFO: Number of nodes with available pods: 2
May 31 00:39:09.856: INFO: Node k8s-linuxpool-10610181-1 is running more than one daemon pod
May 31 00:39:10.853: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:10.857: INFO: Number of nodes with available pods: 2
May 31 00:39:10.857: INFO: Node k8s-linuxpool-10610181-1 is running more than one daemon pod
May 31 00:39:11.853: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:11.856: INFO: Number of nodes with available pods: 3
May 31 00:39:11.856: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 31 00:39:11.870: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:11.874: INFO: Number of nodes with available pods: 2
May 31 00:39:11.874: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:12.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:12.880: INFO: Number of nodes with available pods: 2
May 31 00:39:12.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:13.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:13.880: INFO: Number of nodes with available pods: 2
May 31 00:39:13.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:14.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:14.881: INFO: Number of nodes with available pods: 2
May 31 00:39:14.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:15.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:15.881: INFO: Number of nodes with available pods: 2
May 31 00:39:15.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:16.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:16.880: INFO: Number of nodes with available pods: 2
May 31 00:39:16.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:17.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:17.881: INFO: Number of nodes with available pods: 2
May 31 00:39:17.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:18.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:18.881: INFO: Number of nodes with available pods: 2
May 31 00:39:18.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:19.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:19.881: INFO: Number of nodes with available pods: 2
May 31 00:39:19.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:20.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:20.880: INFO: Number of nodes with available pods: 2
May 31 00:39:20.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:21.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:21.881: INFO: Number of nodes with available pods: 2
May 31 00:39:21.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:22.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:22.880: INFO: Number of nodes with available pods: 2
May 31 00:39:22.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:23.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:23.881: INFO: Number of nodes with available pods: 2
May 31 00:39:23.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:24.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:24.881: INFO: Number of nodes with available pods: 2
May 31 00:39:24.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:25.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:25.881: INFO: Number of nodes with available pods: 2
May 31 00:39:25.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:26.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:26.880: INFO: Number of nodes with available pods: 2
May 31 00:39:26.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:27.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:27.880: INFO: Number of nodes with available pods: 2
May 31 00:39:27.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:28.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:28.880: INFO: Number of nodes with available pods: 2
May 31 00:39:28.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:29.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:29.881: INFO: Number of nodes with available pods: 2
May 31 00:39:29.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:30.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:30.881: INFO: Number of nodes with available pods: 2
May 31 00:39:30.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:31.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:31.881: INFO: Number of nodes with available pods: 2
May 31 00:39:31.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:32.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:32.881: INFO: Number of nodes with available pods: 2
May 31 00:39:32.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:33.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:33.880: INFO: Number of nodes with available pods: 2
May 31 00:39:33.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:34.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:34.879: INFO: Number of nodes with available pods: 2
May 31 00:39:34.879: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:35.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:35.880: INFO: Number of nodes with available pods: 2
May 31 00:39:35.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:36.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:36.880: INFO: Number of nodes with available pods: 2
May 31 00:39:36.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:37.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:37.881: INFO: Number of nodes with available pods: 2
May 31 00:39:37.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:38.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:38.880: INFO: Number of nodes with available pods: 2
May 31 00:39:38.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:39.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:39.881: INFO: Number of nodes with available pods: 2
May 31 00:39:39.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:40.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:40.881: INFO: Number of nodes with available pods: 2
May 31 00:39:40.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:41.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:41.881: INFO: Number of nodes with available pods: 2
May 31 00:39:41.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:42.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:42.880: INFO: Number of nodes with available pods: 2
May 31 00:39:42.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:43.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:43.881: INFO: Number of nodes with available pods: 2
May 31 00:39:43.881: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:44.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:44.880: INFO: Number of nodes with available pods: 2
May 31 00:39:44.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:45.879: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:45.882: INFO: Number of nodes with available pods: 2
May 31 00:39:45.882: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:46.879: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:46.882: INFO: Number of nodes with available pods: 2
May 31 00:39:46.882: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:47.878: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:47.880: INFO: Number of nodes with available pods: 2
May 31 00:39:47.880: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:39:48.877: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 00:39:48.881: INFO: Number of nodes with available pods: 3
May 31 00:39:48.881: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-qh9bv, will wait for the garbage collector to delete the pods
May 31 00:39:48.942: INFO: Deleting {extensions DaemonSet} daemon-set took: 6.094077ms
May 31 00:39:49.042: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.162565ms
May 31 00:40:26.545: INFO: Number of nodes with available pods: 0
May 31 00:40:26.545: INFO: Number of running nodes: 0, number of available pods: 0
May 31 00:40:26.550: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-qh9bv/daemonsets","resourceVersion":"9220"},"items":null}

May 31 00:40:26.553: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-qh9bv/pods","resourceVersion":"9220"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:40:26.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-qh9bv" for this suite.
May 31 00:40:32.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:40:32.616: INFO: namespace: e2e-tests-daemonsets-qh9bv, resource: bindings, ignored listing per whitelist
May 31 00:40:32.658: INFO: namespace e2e-tests-daemonsets-qh9bv deletion completed in 6.091373726s

• [SLOW TEST:89.924 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:40:32.659: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-b27dd548-833c-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:40:32.746: INFO: Waiting up to 5m0s for pod "pod-secrets-b27e6966-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-secrets-kp495" to be "success or failure"
May 31 00:40:32.752: INFO: Pod "pod-secrets-b27e6966-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.516869ms
May 31 00:40:34.755: INFO: Pod "pod-secrets-b27e6966-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008338527s
May 31 00:40:36.757: INFO: Pod "pod-secrets-b27e6966-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011003163s
STEP: Saw pod success
May 31 00:40:36.757: INFO: Pod "pod-secrets-b27e6966-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:40:36.759: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-secrets-b27e6966-833c-11e9-88d7-16679dc4b203 container secret-volume-test: <nil>
STEP: delete the pod
May 31 00:40:36.779: INFO: Waiting for pod pod-secrets-b27e6966-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:40:36.783: INFO: Pod pod-secrets-b27e6966-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:40:36.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kp495" for this suite.
May 31 00:40:42.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:40:42.862: INFO: namespace: e2e-tests-secrets-kp495, resource: bindings, ignored listing per whitelist
May 31 00:40:42.889: INFO: namespace e2e-tests-secrets-kp495 deletion completed in 6.100962045s

• [SLOW TEST:10.230 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:40:42.889: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-b8964edb-833c-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:40:42.971: INFO: Waiting up to 5m0s for pod "pod-secrets-b896a1d9-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-secrets-hsxr8" to be "success or failure"
May 31 00:40:42.973: INFO: Pod "pod-secrets-b896a1d9-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210828ms
May 31 00:40:44.976: INFO: Pod "pod-secrets-b896a1d9-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005378893s
May 31 00:40:46.979: INFO: Pod "pod-secrets-b896a1d9-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008259936s
STEP: Saw pod success
May 31 00:40:46.979: INFO: Pod "pod-secrets-b896a1d9-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:40:46.981: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-secrets-b896a1d9-833c-11e9-88d7-16679dc4b203 container secret-volume-test: <nil>
STEP: delete the pod
May 31 00:40:46.994: INFO: Waiting for pod pod-secrets-b896a1d9-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:40:46.997: INFO: Pod pod-secrets-b896a1d9-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:40:46.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hsxr8" for this suite.
May 31 00:40:53.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:40:53.172: INFO: namespace: e2e-tests-secrets-hsxr8, resource: bindings, ignored listing per whitelist
May 31 00:40:53.172: INFO: namespace e2e-tests-secrets-hsxr8 deletion completed in 6.17049611s

• [SLOW TEST:10.283 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:40:53.175: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-beb85e72-833c-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 00:40:53.265: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-beb8ede3-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-wngq5" to be "success or failure"
May 31 00:40:53.272: INFO: Pod "pod-projected-configmaps-beb8ede3-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 7.015086ms
May 31 00:40:55.275: INFO: Pod "pod-projected-configmaps-beb8ede3-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010105155s
May 31 00:40:57.278: INFO: Pod "pod-projected-configmaps-beb8ede3-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013408207s
STEP: Saw pod success
May 31 00:40:57.278: INFO: Pod "pod-projected-configmaps-beb8ede3-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:40:57.281: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-projected-configmaps-beb8ede3-833c-11e9-88d7-16679dc4b203 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 00:40:57.305: INFO: Waiting for pod pod-projected-configmaps-beb8ede3-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:40:57.308: INFO: Pod pod-projected-configmaps-beb8ede3-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:40:57.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wngq5" for this suite.
May 31 00:41:03.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:41:03.352: INFO: namespace: e2e-tests-projected-wngq5, resource: bindings, ignored listing per whitelist
May 31 00:41:03.415: INFO: namespace e2e-tests-projected-wngq5 deletion completed in 6.103611197s

• [SLOW TEST:10.241 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:41:03.416: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 31 00:41:03.501: INFO: Waiting up to 5m0s for pod "downward-api-c4d30180-833c-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-jwzfq" to be "success or failure"
May 31 00:41:03.505: INFO: Pod "downward-api-c4d30180-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.987349ms
May 31 00:41:05.508: INFO: Pod "downward-api-c4d30180-833c-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007375027s
May 31 00:41:07.512: INFO: Pod "downward-api-c4d30180-833c-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010853789s
STEP: Saw pod success
May 31 00:41:07.512: INFO: Pod "downward-api-c4d30180-833c-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:41:07.515: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod downward-api-c4d30180-833c-11e9-88d7-16679dc4b203 container dapi-container: <nil>
STEP: delete the pod
May 31 00:41:07.528: INFO: Waiting for pod downward-api-c4d30180-833c-11e9-88d7-16679dc4b203 to disappear
May 31 00:41:07.531: INFO: Pod downward-api-c4d30180-833c-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:41:07.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jwzfq" for this suite.
May 31 00:41:13.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:41:13.580: INFO: namespace: e2e-tests-downward-api-jwzfq, resource: bindings, ignored listing per whitelist
May 31 00:41:13.634: INFO: namespace e2e-tests-downward-api-jwzfq deletion completed in 6.09998217s

• [SLOW TEST:10.218 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:41:13.634: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 00:41:13.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-vjfgn'
May 31 00:41:14.268: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
May 31 00:41:14.268: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 31 00:41:14.284: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-79gnz]
May 31 00:41:14.284: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-79gnz" in namespace "e2e-tests-kubectl-vjfgn" to be "running and ready"
May 31 00:41:14.286: INFO: Pod "e2e-test-nginx-rc-79gnz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019525ms
May 31 00:41:16.289: INFO: Pod "e2e-test-nginx-rc-79gnz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004789899s
May 31 00:41:18.293: INFO: Pod "e2e-test-nginx-rc-79gnz": Phase="Running", Reason="", readiness=true. Elapsed: 4.009319177s
May 31 00:41:18.293: INFO: Pod "e2e-test-nginx-rc-79gnz" satisfied condition "running and ready"
May 31 00:41:18.293: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-79gnz]
May 31 00:41:18.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-vjfgn'
May 31 00:41:18.432: INFO: stderr: ""
May 31 00:41:18.432: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
May 31 00:41:18.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-vjfgn'
May 31 00:41:18.548: INFO: stderr: ""
May 31 00:41:18.548: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:41:18.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vjfgn" for this suite.
May 31 00:41:24.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:41:24.588: INFO: namespace: e2e-tests-kubectl-vjfgn, resource: bindings, ignored listing per whitelist
May 31 00:41:24.641: INFO: namespace e2e-tests-kubectl-vjfgn deletion completed in 6.089701349s

• [SLOW TEST:11.007 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:41:24.642: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 00:41:24.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-755bj'
May 31 00:41:24.921: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
May 31 00:41:24.921: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
May 31 00:41:26.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-755bj'
May 31 00:41:27.052: INFO: stderr: ""
May 31 00:41:27.052: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:41:27.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-755bj" for this suite.
May 31 00:41:49.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:41:49.086: INFO: namespace: e2e-tests-kubectl-755bj, resource: bindings, ignored listing per whitelist
May 31 00:41:49.145: INFO: namespace e2e-tests-kubectl-755bj deletion completed in 22.088461751s

• [SLOW TEST:24.503 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:41:49.149: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:41:49.237: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 31 00:41:54.240: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 31 00:41:54.240: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 31 00:41:56.243: INFO: Creating deployment "test-rollover-deployment"
May 31 00:41:56.250: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 31 00:41:58.256: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 31 00:41:58.260: INFO: Ensure that both replica sets have 1 created replica
May 31 00:41:58.264: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 31 00:41:58.270: INFO: Updating deployment test-rollover-deployment
May 31 00:41:58.270: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 31 00:42:00.287: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 31 00:42:00.292: INFO: Make sure deployment "test-rollover-deployment" is complete
May 31 00:42:00.296: INFO: all replica sets need to contain the pod-template-hash label
May 31 00:42:00.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860118, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:42:02.302: INFO: all replica sets need to contain the pod-template-hash label
May 31 00:42:02.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860120, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:42:04.302: INFO: all replica sets need to contain the pod-template-hash label
May 31 00:42:04.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860120, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:42:06.302: INFO: all replica sets need to contain the pod-template-hash label
May 31 00:42:06.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860120, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:42:08.302: INFO: all replica sets need to contain the pod-template-hash label
May 31 00:42:08.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860120, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:42:10.302: INFO: all replica sets need to contain the pod-template-hash label
May 31 00:42:10.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860120, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694860116, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 00:42:12.302: INFO: 
May 31 00:42:12.302: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 00:42:12.308: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-f98wb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-f98wb/deployments/test-rollover-deployment,UID:e4446f53-833c-11e9-b3f2-001dd80c0014,ResourceVersion:9663,Generation:2,CreationTimestamp:2019-05-31 00:41:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-31 00:41:56 +0000 UTC 2019-05-31 00:41:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-31 00:42:10 +0000 UTC 2019-05-31 00:41:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 31 00:42:12.310: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-f98wb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-f98wb/replicasets/test-rollover-deployment-5b76ff8c4,UID:e57971df-833c-11e9-b3f2-001dd80c0014,ResourceVersion:9654,Generation:2,CreationTimestamp:2019-05-31 00:41:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e4446f53-833c-11e9-b3f2-001dd80c0014 0xc4220f9c57 0xc4220f9c58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 31 00:42:12.311: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 31 00:42:12.311: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-f98wb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-f98wb/replicasets/test-rollover-controller,UID:e015ebea-833c-11e9-b3f2-001dd80c0014,ResourceVersion:9662,Generation:2,CreationTimestamp:2019-05-31 00:41:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e4446f53-833c-11e9-b3f2-001dd80c0014 0xc4220f9ba7 0xc4220f9ba8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 00:42:12.311: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-f98wb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-f98wb/replicasets/test-rollover-deployment-6975f4fb87,UID:e4468f7d-833c-11e9-b3f2-001dd80c0014,ResourceVersion:9626,Generation:2,CreationTimestamp:2019-05-31 00:41:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e4446f53-833c-11e9-b3f2-001dd80c0014 0xc4220f9d77 0xc4220f9d78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 00:42:12.314: INFO: Pod "test-rollover-deployment-5b76ff8c4-9p9l6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-9p9l6,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-f98wb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-f98wb/pods/test-rollover-deployment-5b76ff8c4-9p9l6,UID:e57d0efe-833c-11e9-b3f2-001dd80c0014,ResourceVersion:9636,Generation:0,CreationTimestamp:2019-05-31 00:41:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 e57971df-833c-11e9-b3f2-001dd80c0014 0xc42102d350 0xc42102d351}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-mnlrw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mnlrw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-mnlrw true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42102d3b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42102d3d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:41:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:42:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:42:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 00:41:58 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.3.87,StartTime:2019-05-31 00:41:58 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-31 00:42:00 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://488cdba95e93b616661e4540c8552f1a5c0f9d3482bc3b0718297e6ee3a36d3e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:42:12.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-f98wb" for this suite.
May 31 00:42:18.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:42:18.342: INFO: namespace: e2e-tests-deployment-f98wb, resource: bindings, ignored listing per whitelist
May 31 00:42:18.408: INFO: namespace e2e-tests-deployment-f98wb deletion completed in 6.091568414s

• [SLOW TEST:29.259 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:42:18.409: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 00:42:18.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-vdhgl'
May 31 00:42:18.689: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
May 31 00:42:18.689: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
May 31 00:42:22.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-vdhgl'
May 31 00:42:22.839: INFO: stderr: ""
May 31 00:42:22.839: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:42:22.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vdhgl" for this suite.
May 31 00:42:44.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:42:44.936: INFO: namespace: e2e-tests-kubectl-vdhgl, resource: bindings, ignored listing per whitelist
May 31 00:42:44.947: INFO: namespace e2e-tests-kubectl-vdhgl deletion completed in 22.102665177s

• [SLOW TEST:26.538 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:42:44.947: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
May 31 00:42:45.027: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 00:42:45.033: INFO: Waiting for terminating namespaces to be deleted...
May 31 00:42:45.035: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-10610181-0 before test
May 31 00:42:45.042: INFO: sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-nnw4d from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 00:42:45.042: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
May 31 00:42:45.042: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 00:42:45.042: INFO: azure-ip-masq-agent-pxdpd from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.042: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 00:42:45.042: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-31 00:10:47 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.042: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 00:42:45.042: INFO: kube-flannel-ds-lq7s2 from kube-system started at 2019-05-31 00:09:20 +0000 UTC (2 container statuses recorded)
May 31 00:42:45.042: INFO: 	Container install-cni ready: true, restart count 0
May 31 00:42:45.042: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 00:42:45.042: INFO: kube-proxy-xdfzd from kube-system started at 2019-05-31 00:09:27 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.042: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 00:42:45.042: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-10610181-1 before test
May 31 00:42:45.082: INFO: azure-ip-masq-agent-djqq5 from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.082: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 00:42:45.082: INFO: tiller-deploy-7bfcdc49d6-qklrt from kube-system started at 2019-05-31 00:09:43 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.082: INFO: 	Container tiller ready: true, restart count 0
May 31 00:42:45.082: INFO: sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-8lx44 from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 00:42:45.082: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
May 31 00:42:45.082: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 00:42:45.082: INFO: kube-flannel-ds-kt9wr from kube-system started at 2019-05-31 00:09:19 +0000 UTC (2 container statuses recorded)
May 31 00:42:45.082: INFO: 	Container install-cni ready: true, restart count 0
May 31 00:42:45.082: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 00:42:45.082: INFO: kube-proxy-656fb from kube-system started at 2019-05-31 00:09:27 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.082: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 00:42:45.082: INFO: metrics-server-67b4964794-lk2mw from kube-system started at 2019-05-31 00:09:43 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.082: INFO: 	Container metrics-server ready: true, restart count 0
May 31 00:42:45.082: INFO: kubernetes-dashboard-9bf969764-rpmqs from kube-system started at 2019-05-31 00:09:43 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.082: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 31 00:42:45.082: INFO: heapster-6f6cbcfcf6-gjcbw from kube-system started at 2019-05-31 00:09:43 +0000 UTC (2 container statuses recorded)
May 31 00:42:45.082: INFO: 	Container heapster ready: true, restart count 0
May 31 00:42:45.082: INFO: 	Container heapster-nanny ready: true, restart count 0
May 31 00:42:45.082: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-10610181-2 before test
May 31 00:42:45.088: INFO: coredns-69c4fccc6c-hxhd7 from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.088: INFO: 	Container coredns ready: true, restart count 0
May 31 00:42:45.088: INFO: sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-2mglt from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 00:42:45.088: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
May 31 00:42:45.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 00:42:45.088: INFO: kube-flannel-ds-9rzz8 from kube-system started at 2019-05-31 00:09:19 +0000 UTC (2 container statuses recorded)
May 31 00:42:45.088: INFO: 	Container install-cni ready: true, restart count 0
May 31 00:42:45.088: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 00:42:45.088: INFO: sonobuoy-e2e-job-32df4bb72ae04a14 from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 00:42:45.088: INFO: 	Container e2e ready: true, restart count 0
May 31 00:42:45.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 00:42:45.088: INFO: azure-ip-masq-agent-zg4rl from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.088: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 00:42:45.088: INFO: kube-proxy-6fd9h from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 00:42:45.088: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node k8s-linuxpool-10610181-0
STEP: verifying the node has the label node k8s-linuxpool-10610181-1
STEP: verifying the node has the label node k8s-linuxpool-10610181-2
May 31 00:42:45.137: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-linuxpool-10610181-0
May 31 00:42:45.137: INFO: Pod sonobuoy-e2e-job-32df4bb72ae04a14 requesting resource cpu=0m on Node k8s-linuxpool-10610181-2
May 31 00:42:45.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-2mglt requesting resource cpu=0m on Node k8s-linuxpool-10610181-2
May 31 00:42:45.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-8lx44 requesting resource cpu=0m on Node k8s-linuxpool-10610181-1
May 31 00:42:45.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-nnw4d requesting resource cpu=0m on Node k8s-linuxpool-10610181-0
May 31 00:42:45.138: INFO: Pod azure-ip-masq-agent-djqq5 requesting resource cpu=50m on Node k8s-linuxpool-10610181-1
May 31 00:42:45.138: INFO: Pod azure-ip-masq-agent-pxdpd requesting resource cpu=50m on Node k8s-linuxpool-10610181-0
May 31 00:42:45.138: INFO: Pod azure-ip-masq-agent-zg4rl requesting resource cpu=50m on Node k8s-linuxpool-10610181-2
May 31 00:42:45.138: INFO: Pod coredns-69c4fccc6c-hxhd7 requesting resource cpu=100m on Node k8s-linuxpool-10610181-2
May 31 00:42:45.138: INFO: Pod heapster-6f6cbcfcf6-gjcbw requesting resource cpu=176m on Node k8s-linuxpool-10610181-1
May 31 00:42:45.138: INFO: Pod kube-flannel-ds-9rzz8 requesting resource cpu=0m on Node k8s-linuxpool-10610181-2
May 31 00:42:45.138: INFO: Pod kube-flannel-ds-kt9wr requesting resource cpu=0m on Node k8s-linuxpool-10610181-1
May 31 00:42:45.139: INFO: Pod kube-flannel-ds-lq7s2 requesting resource cpu=0m on Node k8s-linuxpool-10610181-0
May 31 00:42:45.139: INFO: Pod kube-proxy-656fb requesting resource cpu=100m on Node k8s-linuxpool-10610181-1
May 31 00:42:45.139: INFO: Pod kube-proxy-6fd9h requesting resource cpu=100m on Node k8s-linuxpool-10610181-2
May 31 00:42:45.139: INFO: Pod kube-proxy-xdfzd requesting resource cpu=100m on Node k8s-linuxpool-10610181-0
May 31 00:42:45.139: INFO: Pod kubernetes-dashboard-9bf969764-rpmqs requesting resource cpu=300m on Node k8s-linuxpool-10610181-1
May 31 00:42:45.139: INFO: Pod metrics-server-67b4964794-lk2mw requesting resource cpu=0m on Node k8s-linuxpool-10610181-1
May 31 00:42:45.139: INFO: Pod tiller-deploy-7bfcdc49d6-qklrt requesting resource cpu=50m on Node k8s-linuxpool-10610181-1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0168c40f-833d-11e9-88d7-16679dc4b203.15a39dc4ef3bf85a], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-s4tbp/filler-pod-0168c40f-833d-11e9-88d7-16679dc4b203 to k8s-linuxpool-10610181-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0168c40f-833d-11e9-88d7-16679dc4b203.15a39dc53da4dd0a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0168c40f-833d-11e9-88d7-16679dc4b203.15a39dc553b1168c], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0168c40f-833d-11e9-88d7-16679dc4b203.15a39dc55fd069df], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-016a3ab3-833d-11e9-88d7-16679dc4b203.15a39dc4f02ea5b5], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-s4tbp/filler-pod-016a3ab3-833d-11e9-88d7-16679dc4b203 to k8s-linuxpool-10610181-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-016a3ab3-833d-11e9-88d7-16679dc4b203.15a39dc54daf0f46], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-016a3ab3-833d-11e9-88d7-16679dc4b203.15a39dc56a5ceb8c], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-016a3ab3-833d-11e9-88d7-16679dc4b203.15a39dc582ff93c6], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-016a3ab3-833d-11e9-88d7-16679dc4b203.15a39dc58e38ec40], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-016b71ec-833d-11e9-88d7-16679dc4b203.15a39dc4f04070c8], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-s4tbp/filler-pod-016b71ec-833d-11e9-88d7-16679dc4b203 to k8s-linuxpool-10610181-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-016b71ec-833d-11e9-88d7-16679dc4b203.15a39dc541d79dcb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-016b71ec-833d-11e9-88d7-16679dc4b203.15a39dc5565c84af], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-016b71ec-833d-11e9-88d7-16679dc4b203.15a39dc564540c87], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15a39dc5dfc0dd72], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node k8s-linuxpool-10610181-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-linuxpool-10610181-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-linuxpool-10610181-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:42:50.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-s4tbp" for this suite.
May 31 00:42:56.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:42:56.287: INFO: namespace: e2e-tests-sched-pred-s4tbp, resource: bindings, ignored listing per whitelist
May 31 00:42:56.327: INFO: namespace e2e-tests-sched-pred-s4tbp deletion completed in 6.087079384s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:11.380 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:42:56.328: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-8c52w
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-8c52w
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-8c52w
May 31 00:42:56.419: INFO: Found 0 stateful pods, waiting for 1
May 31 00:43:06.423: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 31 00:43:06.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-8c52w ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 00:43:06.685: INFO: stderr: ""
May 31 00:43:06.685: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 00:43:06.685: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 00:43:06.688: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 31 00:43:16.691: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 00:43:16.691: INFO: Waiting for statefulset status.replicas updated to 0
May 31 00:43:16.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
May 31 00:43:17.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994829899s
May 31 00:43:18.716: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991963028s
May 31 00:43:19.721: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981881577s
May 31 00:43:20.724: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977477295s
May 31 00:43:21.727: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974466934s
May 31 00:43:22.730: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971198773s
May 31 00:43:23.733: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968237218s
May 31 00:43:24.736: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.964987064s
May 31 00:43:25.740: INFO: Verifying statefulset ss doesn't scale past 1 for another 961.899815ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-8c52w
May 31 00:43:26.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-8c52w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 00:43:27.011: INFO: stderr: ""
May 31 00:43:27.012: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 00:43:27.012: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 00:43:27.016: INFO: Found 1 stateful pods, waiting for 3
May 31 00:43:37.020: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 00:43:37.020: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 00:43:37.020: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 31 00:43:37.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-8c52w ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 00:43:37.275: INFO: stderr: ""
May 31 00:43:37.275: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 00:43:37.275: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 00:43:37.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-8c52w ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 00:43:37.541: INFO: stderr: ""
May 31 00:43:37.541: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 00:43:37.541: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 00:43:37.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-8c52w ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 00:43:37.797: INFO: stderr: ""
May 31 00:43:37.797: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 00:43:37.797: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 00:43:37.797: INFO: Waiting for statefulset status.replicas updated to 0
May 31 00:43:37.800: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 31 00:43:47.806: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 00:43:47.806: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 31 00:43:47.806: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 31 00:43:47.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
May 31 00:43:48.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995681414s
May 31 00:43:49.824: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990867126s
May 31 00:43:50.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98762426s
May 31 00:43:51.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984080493s
May 31 00:43:52.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98047873s
May 31 00:43:53.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977129272s
May 31 00:43:54.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973356313s
May 31 00:43:55.883: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96984916s
May 31 00:43:56.887: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.460971ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-8c52w
May 31 00:43:57.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-8c52w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 00:43:58.142: INFO: stderr: ""
May 31 00:43:58.142: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 00:43:58.142: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 00:43:58.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-8c52w ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 00:43:58.450: INFO: stderr: ""
May 31 00:43:58.450: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 00:43:58.450: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 00:43:58.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-8c52w ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 00:43:58.766: INFO: stderr: ""
May 31 00:43:58.766: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 00:43:58.766: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 00:43:58.766: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 00:44:28.783: INFO: Deleting all statefulset in ns e2e-tests-statefulset-8c52w
May 31 00:44:28.786: INFO: Scaling statefulset ss to 0
May 31 00:44:28.793: INFO: Waiting for statefulset status.replicas updated to 0
May 31 00:44:28.794: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:44:28.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-8c52w" for this suite.
May 31 00:44:34.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:44:34.839: INFO: namespace: e2e-tests-statefulset-8c52w, resource: bindings, ignored listing per whitelist
May 31 00:44:34.904: INFO: namespace e2e-tests-statefulset-8c52w deletion completed in 6.09388487s

• [SLOW TEST:98.577 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:44:34.905: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 31 00:44:49.023: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:44:49.025: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:44:51.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:44:51.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:44:53.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:44:53.028: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:44:55.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:44:55.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:44:57.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:44:57.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:44:59.025: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:44:59.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:45:01.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:45:01.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:45:03.025: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:45:03.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:45:05.025: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:45:05.028: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:45:07.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:45:07.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:45:09.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:45:09.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:45:11.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:45:11.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:45:13.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:45:13.028: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:45:15.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:45:15.029: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 00:45:17.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 00:45:17.029: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:45:17.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-thrjz" for this suite.
May 31 00:45:39.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:45:39.096: INFO: namespace: e2e-tests-container-lifecycle-hook-thrjz, resource: bindings, ignored listing per whitelist
May 31 00:45:39.128: INFO: namespace e2e-tests-container-lifecycle-hook-thrjz deletion completed in 22.095955924s

• [SLOW TEST:64.224 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:45:39.130: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 31 00:45:43.750: INFO: Successfully updated pod "labelsupdate692b0fec-833d-11e9-88d7-16679dc4b203"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:45:45.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-f9fnb" for this suite.
May 31 00:46:07.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:46:07.856: INFO: namespace: e2e-tests-downward-api-f9fnb, resource: bindings, ignored listing per whitelist
May 31 00:46:07.893: INFO: namespace e2e-tests-downward-api-f9fnb deletion completed in 22.109833912s

• [SLOW TEST:28.763 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:46:07.893: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:46:07.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a5128c6-833d-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-cbgn2" to be "success or failure"
May 31 00:46:08.000: INFO: Pod "downwardapi-volume-7a5128c6-833d-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335971ms
May 31 00:46:10.003: INFO: Pod "downwardapi-volume-7a5128c6-833d-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008874063s
May 31 00:46:12.006: INFO: Pod "downwardapi-volume-7a5128c6-833d-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011921051s
STEP: Saw pod success
May 31 00:46:12.006: INFO: Pod "downwardapi-volume-7a5128c6-833d-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:46:12.009: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-7a5128c6-833d-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:46:12.022: INFO: Waiting for pod downwardapi-volume-7a5128c6-833d-11e9-88d7-16679dc4b203 to disappear
May 31 00:46:12.027: INFO: Pod downwardapi-volume-7a5128c6-833d-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:46:12.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cbgn2" for this suite.
May 31 00:46:18.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:46:18.072: INFO: namespace: e2e-tests-downward-api-cbgn2, resource: bindings, ignored listing per whitelist
May 31 00:46:18.133: INFO: namespace e2e-tests-downward-api-cbgn2 deletion completed in 6.103094257s

• [SLOW TEST:10.240 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:46:18.135: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-8070a020-833d-11e9-88d7-16679dc4b203
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:46:22.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-pjp44" for this suite.
May 31 00:46:44.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:46:44.395: INFO: namespace: e2e-tests-configmap-pjp44, resource: bindings, ignored listing per whitelist
May 31 00:46:44.401: INFO: namespace e2e-tests-configmap-pjp44 deletion completed in 22.104914268s

• [SLOW TEST:26.266 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:46:44.401: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:46:44.497: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9012df03-833d-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-szcrg" to be "success or failure"
May 31 00:46:44.503: INFO: Pod "downwardapi-volume-9012df03-833d-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.991066ms
May 31 00:46:46.505: INFO: Pod "downwardapi-volume-9012df03-833d-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008742185s
May 31 00:46:48.508: INFO: Pod "downwardapi-volume-9012df03-833d-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011687497s
STEP: Saw pod success
May 31 00:46:48.508: INFO: Pod "downwardapi-volume-9012df03-833d-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:46:48.511: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-9012df03-833d-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:46:48.542: INFO: Waiting for pod downwardapi-volume-9012df03-833d-11e9-88d7-16679dc4b203 to disappear
May 31 00:46:48.544: INFO: Pod downwardapi-volume-9012df03-833d-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:46:48.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-szcrg" for this suite.
May 31 00:46:54.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:46:54.576: INFO: namespace: e2e-tests-projected-szcrg, resource: bindings, ignored listing per whitelist
May 31 00:46:54.633: INFO: namespace e2e-tests-projected-szcrg deletion completed in 6.086032637s

• [SLOW TEST:10.232 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:46:54.634: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-962a5ef1-833d-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 00:46:54.722: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-962b4db2-833d-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-n6sxl" to be "success or failure"
May 31 00:46:54.728: INFO: Pod "pod-projected-configmaps-962b4db2-833d-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.261469ms
May 31 00:46:56.732: INFO: Pod "pod-projected-configmaps-962b4db2-833d-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009534747s
May 31 00:46:58.735: INFO: Pod "pod-projected-configmaps-962b4db2-833d-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012600413s
STEP: Saw pod success
May 31 00:46:58.735: INFO: Pod "pod-projected-configmaps-962b4db2-833d-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:46:58.737: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-projected-configmaps-962b4db2-833d-11e9-88d7-16679dc4b203 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 00:46:58.753: INFO: Waiting for pod pod-projected-configmaps-962b4db2-833d-11e9-88d7-16679dc4b203 to disappear
May 31 00:46:58.755: INFO: Pod pod-projected-configmaps-962b4db2-833d-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:46:58.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n6sxl" for this suite.
May 31 00:47:04.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:47:04.808: INFO: namespace: e2e-tests-projected-n6sxl, resource: bindings, ignored listing per whitelist
May 31 00:47:04.860: INFO: namespace e2e-tests-projected-n6sxl deletion completed in 6.102119275s

• [SLOW TEST:10.226 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:47:04.860: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
May 31 00:47:04.956: INFO: Waiting up to 5m0s for pod "pod-9c4414b6-833d-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-b4qgm" to be "success or failure"
May 31 00:47:04.966: INFO: Pod "pod-9c4414b6-833d-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 9.637807ms
May 31 00:47:07.001: INFO: Pod "pod-9c4414b6-833d-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044816993s
May 31 00:47:09.004: INFO: Pod "pod-9c4414b6-833d-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047748711s
STEP: Saw pod success
May 31 00:47:09.004: INFO: Pod "pod-9c4414b6-833d-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:47:09.007: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-9c4414b6-833d-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:47:09.030: INFO: Waiting for pod pod-9c4414b6-833d-11e9-88d7-16679dc4b203 to disappear
May 31 00:47:09.032: INFO: Pod pod-9c4414b6-833d-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:47:09.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b4qgm" for this suite.
May 31 00:47:15.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:47:15.084: INFO: namespace: e2e-tests-emptydir-b4qgm, resource: bindings, ignored listing per whitelist
May 31 00:47:15.132: INFO: namespace e2e-tests-emptydir-b4qgm deletion completed in 6.09708798s

• [SLOW TEST:10.272 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:47:15.133: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 00:47:15.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-l7djq'
May 31 00:47:15.337: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
May 31 00:47:15.337: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May 31 00:47:15.342: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
May 31 00:47:15.347: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 31 00:47:15.362: INFO: scanned /root for discovery docs: <nil>
May 31 00:47:15.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-l7djq'
May 31 00:47:31.154: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 31 00:47:31.154: INFO: stdout: "Created e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8\nScaling up e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 31 00:47:31.154: INFO: stdout: "Created e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8\nScaling up e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 31 00:47:31.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-l7djq'
May 31 00:47:31.282: INFO: stderr: ""
May 31 00:47:31.282: INFO: stdout: "e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8-gtfzj "
May 31 00:47:31.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8-gtfzj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7djq'
May 31 00:47:31.387: INFO: stderr: ""
May 31 00:47:31.387: INFO: stdout: "true"
May 31 00:47:31.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8-gtfzj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-l7djq'
May 31 00:47:31.498: INFO: stderr: ""
May 31 00:47:31.498: INFO: stdout: "nginx:1.14-alpine"
May 31 00:47:31.498: INFO: e2e-test-nginx-rc-f7071e738f335a2a4b4e2dd44d5b24e8-gtfzj is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
May 31 00:47:31.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-l7djq'
May 31 00:47:31.617: INFO: stderr: ""
May 31 00:47:31.617: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:47:31.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l7djq" for this suite.
May 31 00:47:37.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:47:37.695: INFO: namespace: e2e-tests-kubectl-l7djq, resource: bindings, ignored listing per whitelist
May 31 00:47:37.710: INFO: namespace e2e-tests-kubectl-l7djq deletion completed in 6.08593986s

• [SLOW TEST:22.577 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:47:37.711: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
May 31 00:47:37.795: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 31 00:47:37.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:47:38.053: INFO: stderr: ""
May 31 00:47:38.053: INFO: stdout: "service/redis-slave created\n"
May 31 00:47:38.053: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 31 00:47:38.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:47:38.321: INFO: stderr: ""
May 31 00:47:38.321: INFO: stdout: "service/redis-master created\n"
May 31 00:47:38.321: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 31 00:47:38.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:47:38.617: INFO: stderr: ""
May 31 00:47:38.617: INFO: stdout: "service/frontend created\n"
May 31 00:47:38.617: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 31 00:47:38.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:47:38.883: INFO: stderr: ""
May 31 00:47:38.883: INFO: stdout: "deployment.extensions/frontend created\n"
May 31 00:47:38.883: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 31 00:47:38.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:47:39.159: INFO: stderr: ""
May 31 00:47:39.159: INFO: stdout: "deployment.extensions/redis-master created\n"
May 31 00:47:39.159: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 31 00:47:39.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:47:39.926: INFO: stderr: ""
May 31 00:47:39.926: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
May 31 00:47:39.926: INFO: Waiting for all frontend pods to be Running.
May 31 00:48:14.978: INFO: Waiting for frontend to serve content.
May 31 00:48:14.991: INFO: Trying to add a new entry to the guestbook.
May 31 00:48:15.003: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 31 00:48:15.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:48:15.147: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 00:48:15.147: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 31 00:48:15.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:48:15.287: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 00:48:15.287: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 31 00:48:15.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:48:15.422: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 00:48:15.422: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 31 00:48:15.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:48:15.558: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 00:48:15.558: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 31 00:48:15.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:48:15.701: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 00:48:15.701: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 31 00:48:15.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xh5d7'
May 31 00:48:15.851: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 00:48:15.852: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:48:15.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xh5d7" for this suite.
May 31 00:48:57.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:48:57.923: INFO: namespace: e2e-tests-kubectl-xh5d7, resource: bindings, ignored listing per whitelist
May 31 00:48:57.964: INFO: namespace e2e-tests-kubectl-xh5d7 deletion completed in 42.106666069s

• [SLOW TEST:80.254 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:48:57.965: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0531 00:49:38.073271      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 00:49:38.073: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:49:38.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-k27w6" for this suite.
May 31 00:49:44.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:49:44.094: INFO: namespace: e2e-tests-gc-k27w6, resource: bindings, ignored listing per whitelist
May 31 00:49:44.172: INFO: namespace e2e-tests-gc-k27w6 deletion completed in 6.096044832s

• [SLOW TEST:46.207 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:49:44.172: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 31 00:49:44.249: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:49:49.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-fxbbn" for this suite.
May 31 00:50:11.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:50:11.389: INFO: namespace: e2e-tests-init-container-fxbbn, resource: bindings, ignored listing per whitelist
May 31 00:50:11.417: INFO: namespace e2e-tests-init-container-fxbbn deletion completed in 22.093128302s

• [SLOW TEST:27.245 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:50:11.419: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
May 31 00:50:11.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 --namespace=e2e-tests-kubectl-rz9c8 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 31 00:50:14.181: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 31 00:50:14.181: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:50:16.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rz9c8" for this suite.
May 31 00:50:28.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:50:28.271: INFO: namespace: e2e-tests-kubectl-rz9c8, resource: bindings, ignored listing per whitelist
May 31 00:50:28.286: INFO: namespace e2e-tests-kubectl-rz9c8 deletion completed in 12.097402956s

• [SLOW TEST:16.868 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:50:28.287: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
May 31 00:50:28.374: INFO: Waiting up to 5m0s for pod "pod-1583da40-833e-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-pklx4" to be "success or failure"
May 31 00:50:28.379: INFO: Pod "pod-1583da40-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.200756ms
May 31 00:50:30.383: INFO: Pod "pod-1583da40-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530939s
May 31 00:50:32.386: INFO: Pod "pod-1583da40-833e-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012446521s
STEP: Saw pod success
May 31 00:50:32.387: INFO: Pod "pod-1583da40-833e-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:50:32.389: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-1583da40-833e-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 00:50:32.407: INFO: Waiting for pod pod-1583da40-833e-11e9-88d7-16679dc4b203 to disappear
May 31 00:50:32.410: INFO: Pod pod-1583da40-833e-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:50:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pklx4" for this suite.
May 31 00:50:38.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:50:38.462: INFO: namespace: e2e-tests-emptydir-pklx4, resource: bindings, ignored listing per whitelist
May 31 00:50:38.577: INFO: namespace e2e-tests-emptydir-pklx4 deletion completed in 6.163855341s

• [SLOW TEST:10.290 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:50:38.577: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-m75tc/secret-test-1ba51319-833e-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:50:38.660: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ba57410-833e-11e9-88d7-16679dc4b203" in namespace "e2e-tests-secrets-m75tc" to be "success or failure"
May 31 00:50:38.666: INFO: Pod "pod-configmaps-1ba57410-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.353769ms
May 31 00:50:40.669: INFO: Pod "pod-configmaps-1ba57410-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009453818s
May 31 00:50:42.674: INFO: Pod "pod-configmaps-1ba57410-833e-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013802575s
STEP: Saw pod success
May 31 00:50:42.674: INFO: Pod "pod-configmaps-1ba57410-833e-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:50:42.678: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-configmaps-1ba57410-833e-11e9-88d7-16679dc4b203 container env-test: <nil>
STEP: delete the pod
May 31 00:50:42.696: INFO: Waiting for pod pod-configmaps-1ba57410-833e-11e9-88d7-16679dc4b203 to disappear
May 31 00:50:42.699: INFO: Pod pod-configmaps-1ba57410-833e-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:50:42.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-m75tc" for this suite.
May 31 00:50:48.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:50:48.804: INFO: namespace: e2e-tests-secrets-m75tc, resource: bindings, ignored listing per whitelist
May 31 00:50:48.808: INFO: namespace e2e-tests-secrets-m75tc deletion completed in 6.103634405s

• [SLOW TEST:10.231 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:50:48.811: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-lnt9x
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-lnt9x
STEP: Deleting pre-stop pod
May 31 00:51:03.932: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:51:03.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-lnt9x" for this suite.
May 31 00:51:41.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:51:41.995: INFO: namespace: e2e-tests-prestop-lnt9x, resource: bindings, ignored listing per whitelist
May 31 00:51:42.030: INFO: namespace e2e-tests-prestop-lnt9x deletion completed in 38.088595043s

• [SLOW TEST:53.219 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:51:42.030: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 00:51:42.130: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41788db8-833e-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-m8kh9" to be "success or failure"
May 31 00:51:42.136: INFO: Pod "downwardapi-volume-41788db8-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.771362ms
May 31 00:51:44.139: INFO: Pod "downwardapi-volume-41788db8-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008997337s
May 31 00:51:46.142: INFO: Pod "downwardapi-volume-41788db8-833e-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012593311s
STEP: Saw pod success
May 31 00:51:46.143: INFO: Pod "downwardapi-volume-41788db8-833e-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:51:46.145: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-41788db8-833e-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 00:51:46.166: INFO: Waiting for pod downwardapi-volume-41788db8-833e-11e9-88d7-16679dc4b203 to disappear
May 31 00:51:46.168: INFO: Pod downwardapi-volume-41788db8-833e-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:51:46.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m8kh9" for this suite.
May 31 00:51:52.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:51:52.239: INFO: namespace: e2e-tests-projected-m8kh9, resource: bindings, ignored listing per whitelist
May 31 00:51:52.280: INFO: namespace e2e-tests-projected-m8kh9 deletion completed in 6.105498496s

• [SLOW TEST:10.249 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:51:52.281: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 00:51:52.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-x2kgn'
May 31 00:51:52.825: INFO: stderr: ""
May 31 00:51:52.825: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 31 00:51:57.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-x2kgn -o json'
May 31 00:51:58.002: INFO: stderr: ""
May 31 00:51:58.002: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-05-31T00:51:52Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-x2kgn\",\n        \"resourceVersion\": \"11814\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-x2kgn/pods/e2e-test-nginx-pod\",\n        \"uid\": \"47d94415-833e-11e9-b3f2-001dd80c0014\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-dc4kv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"nodeName\": \"k8s-linuxpool-10610181-0\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-dc4kv\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-dc4kv\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-31T00:51:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-31T00:51:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-31T00:51:55Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-31T00:51:52Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://3e8a0f37cf98db67b4a342269f24443a4413bc30c5e14f0e3fbe76bb489c4740\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-31T00:51:54Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.112\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-31T00:51:52Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 31 00:51:58.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 replace -f - --namespace=e2e-tests-kubectl-x2kgn'
May 31 00:51:58.258: INFO: stderr: ""
May 31 00:51:58.258: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
May 31 00:51:58.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-x2kgn'
May 31 00:52:06.451: INFO: stderr: ""
May 31 00:52:06.451: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:52:06.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x2kgn" for this suite.
May 31 00:52:12.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:52:12.494: INFO: namespace: e2e-tests-kubectl-x2kgn, resource: bindings, ignored listing per whitelist
May 31 00:52:12.537: INFO: namespace e2e-tests-kubectl-x2kgn deletion completed in 6.083091704s

• [SLOW TEST:20.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:52:12.539: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-53a76b2e-833e-11e9-88d7-16679dc4b203
STEP: Creating configMap with name cm-test-opt-upd-53a76b60-833e-11e9-88d7-16679dc4b203
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-53a76b2e-833e-11e9-88d7-16679dc4b203
STEP: Updating configmap cm-test-opt-upd-53a76b60-833e-11e9-88d7-16679dc4b203
STEP: Creating configMap with name cm-test-opt-create-53a76b77-833e-11e9-88d7-16679dc4b203
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:53:39.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-f9lzh" for this suite.
May 31 00:54:01.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:54:01.186: INFO: namespace: e2e-tests-configmap-f9lzh, resource: bindings, ignored listing per whitelist
May 31 00:54:01.238: INFO: namespace e2e-tests-configmap-f9lzh deletion completed in 22.083865723s

• [SLOW TEST:108.699 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:54:01.240: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:54:01.378: INFO: Creating ReplicaSet my-hostname-basic-947aacf1-833e-11e9-88d7-16679dc4b203
May 31 00:54:01.386: INFO: Pod name my-hostname-basic-947aacf1-833e-11e9-88d7-16679dc4b203: Found 0 pods out of 1
May 31 00:54:06.389: INFO: Pod name my-hostname-basic-947aacf1-833e-11e9-88d7-16679dc4b203: Found 1 pods out of 1
May 31 00:54:06.389: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-947aacf1-833e-11e9-88d7-16679dc4b203" is running
May 31 00:54:06.391: INFO: Pod "my-hostname-basic-947aacf1-833e-11e9-88d7-16679dc4b203-r2kwl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 00:54:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 00:54:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 00:54:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 00:54:01 +0000 UTC Reason: Message:}])
May 31 00:54:06.392: INFO: Trying to dial the pod
May 31 00:54:11.402: INFO: Controller my-hostname-basic-947aacf1-833e-11e9-88d7-16679dc4b203: Got expected result from replica 1 [my-hostname-basic-947aacf1-833e-11e9-88d7-16679dc4b203-r2kwl]: "my-hostname-basic-947aacf1-833e-11e9-88d7-16679dc4b203-r2kwl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:54:11.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-nh8nm" for this suite.
May 31 00:54:17.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:54:17.502: INFO: namespace: e2e-tests-replicaset-nh8nm, resource: bindings, ignored listing per whitelist
May 31 00:54:17.502: INFO: namespace e2e-tests-replicaset-nh8nm deletion completed in 6.096426812s

• [SLOW TEST:16.262 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:54:17.503: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 31 00:54:22.115: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9e2318f4-833e-11e9-88d7-16679dc4b203"
May 31 00:54:22.115: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9e2318f4-833e-11e9-88d7-16679dc4b203" in namespace "e2e-tests-pods-fs2dl" to be "terminated due to deadline exceeded"
May 31 00:54:22.119: INFO: Pod "pod-update-activedeadlineseconds-9e2318f4-833e-11e9-88d7-16679dc4b203": Phase="Running", Reason="", readiness=true. Elapsed: 4.171744ms
May 31 00:54:24.122: INFO: Pod "pod-update-activedeadlineseconds-9e2318f4-833e-11e9-88d7-16679dc4b203": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.006824957s
May 31 00:54:24.122: INFO: Pod "pod-update-activedeadlineseconds-9e2318f4-833e-11e9-88d7-16679dc4b203" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:54:24.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fs2dl" for this suite.
May 31 00:54:30.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:54:30.157: INFO: namespace: e2e-tests-pods-fs2dl, resource: bindings, ignored listing per whitelist
May 31 00:54:30.216: INFO: namespace e2e-tests-pods-fs2dl deletion completed in 6.091577888s

• [SLOW TEST:12.714 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:54:30.219: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-n67sj/configmap-test-a5b8c486-833e-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 00:54:30.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5b92c82-833e-11e9-88d7-16679dc4b203" in namespace "e2e-tests-configmap-n67sj" to be "success or failure"
May 31 00:54:30.319: INFO: Pod "pod-configmaps-a5b92c82-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.311955ms
May 31 00:54:32.322: INFO: Pod "pod-configmaps-a5b92c82-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008529558s
May 31 00:54:34.325: INFO: Pod "pod-configmaps-a5b92c82-833e-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011476255s
STEP: Saw pod success
May 31 00:54:34.325: INFO: Pod "pod-configmaps-a5b92c82-833e-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:54:34.327: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-configmaps-a5b92c82-833e-11e9-88d7-16679dc4b203 container env-test: <nil>
STEP: delete the pod
May 31 00:54:34.344: INFO: Waiting for pod pod-configmaps-a5b92c82-833e-11e9-88d7-16679dc4b203 to disappear
May 31 00:54:34.346: INFO: Pod pod-configmaps-a5b92c82-833e-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:54:34.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-n67sj" for this suite.
May 31 00:54:40.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:54:40.411: INFO: namespace: e2e-tests-configmap-n67sj, resource: bindings, ignored listing per whitelist
May 31 00:54:40.443: INFO: namespace e2e-tests-configmap-n67sj deletion completed in 6.094228057s

• [SLOW TEST:10.224 seconds]
[sig-api-machinery] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:54:40.443: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-abcfd10d-833e-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:54:40.575: INFO: Waiting up to 5m0s for pod "pod-secrets-abd6b1a6-833e-11e9-88d7-16679dc4b203" in namespace "e2e-tests-secrets-7jrdm" to be "success or failure"
May 31 00:54:40.577: INFO: Pod "pod-secrets-abd6b1a6-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.337425ms
May 31 00:54:42.580: INFO: Pod "pod-secrets-abd6b1a6-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005415308s
May 31 00:54:44.583: INFO: Pod "pod-secrets-abd6b1a6-833e-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00782258s
STEP: Saw pod success
May 31 00:54:44.583: INFO: Pod "pod-secrets-abd6b1a6-833e-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:54:44.585: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-secrets-abd6b1a6-833e-11e9-88d7-16679dc4b203 container secret-volume-test: <nil>
STEP: delete the pod
May 31 00:54:44.600: INFO: Waiting for pod pod-secrets-abd6b1a6-833e-11e9-88d7-16679dc4b203 to disappear
May 31 00:54:44.602: INFO: Pod pod-secrets-abd6b1a6-833e-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:54:44.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7jrdm" for this suite.
May 31 00:54:50.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:54:50.670: INFO: namespace: e2e-tests-secrets-7jrdm, resource: bindings, ignored listing per whitelist
May 31 00:54:50.704: INFO: namespace e2e-tests-secrets-7jrdm deletion completed in 6.098834248s
STEP: Destroying namespace "e2e-tests-secret-namespace-wct6n" for this suite.
May 31 00:54:56.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:54:56.767: INFO: namespace: e2e-tests-secret-namespace-wct6n, resource: bindings, ignored listing per whitelist
May 31 00:54:56.793: INFO: namespace e2e-tests-secret-namespace-wct6n deletion completed in 6.088893711s

• [SLOW TEST:16.349 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:54:56.796: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-b58e9778-833e-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:54:56.882: INFO: Waiting up to 5m0s for pod "pod-secrets-b58f0ff4-833e-11e9-88d7-16679dc4b203" in namespace "e2e-tests-secrets-4j8pl" to be "success or failure"
May 31 00:54:56.885: INFO: Pod "pod-secrets-b58f0ff4-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289323ms
May 31 00:54:58.888: INFO: Pod "pod-secrets-b58f0ff4-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005210275s
May 31 00:55:00.891: INFO: Pod "pod-secrets-b58f0ff4-833e-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008526727s
STEP: Saw pod success
May 31 00:55:00.891: INFO: Pod "pod-secrets-b58f0ff4-833e-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:55:00.893: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-secrets-b58f0ff4-833e-11e9-88d7-16679dc4b203 container secret-volume-test: <nil>
STEP: delete the pod
May 31 00:55:00.911: INFO: Waiting for pod pod-secrets-b58f0ff4-833e-11e9-88d7-16679dc4b203 to disappear
May 31 00:55:00.914: INFO: Pod pod-secrets-b58f0ff4-833e-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:55:00.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4j8pl" for this suite.
May 31 00:55:06.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:55:06.975: INFO: namespace: e2e-tests-secrets-4j8pl, resource: bindings, ignored listing per whitelist
May 31 00:55:07.000: INFO: namespace e2e-tests-secrets-4j8pl deletion completed in 6.082517691s

• [SLOW TEST:10.204 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:55:07.000: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:55:25.098: INFO: Container started at 2019-05-31 00:55:09 +0000 UTC, pod became ready at 2019-05-31 00:55:24 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:55:25.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-szc2s" for this suite.
May 31 00:55:47.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:55:47.128: INFO: namespace: e2e-tests-container-probe-szc2s, resource: bindings, ignored listing per whitelist
May 31 00:55:47.196: INFO: namespace e2e-tests-container-probe-szc2s deletion completed in 22.095362406s

• [SLOW TEST:40.197 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:55:47.197: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-d399833a-833e-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 00:55:47.286: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d39a0332-833e-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-w9t4h" to be "success or failure"
May 31 00:55:47.291: INFO: Pod "pod-projected-secrets-d39a0332-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.84055ms
May 31 00:55:49.294: INFO: Pod "pod-projected-secrets-d39a0332-833e-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008193821s
May 31 00:55:51.297: INFO: Pod "pod-projected-secrets-d39a0332-833e-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011416387s
STEP: Saw pod success
May 31 00:55:51.297: INFO: Pod "pod-projected-secrets-d39a0332-833e-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:55:51.300: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-projected-secrets-d39a0332-833e-11e9-88d7-16679dc4b203 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 00:55:51.314: INFO: Waiting for pod pod-projected-secrets-d39a0332-833e-11e9-88d7-16679dc4b203 to disappear
May 31 00:55:51.317: INFO: Pod pod-projected-secrets-d39a0332-833e-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:55:51.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w9t4h" for this suite.
May 31 00:55:57.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:55:57.371: INFO: namespace: e2e-tests-projected-w9t4h, resource: bindings, ignored listing per whitelist
May 31 00:55:57.410: INFO: namespace e2e-tests-projected-w9t4h deletion completed in 6.088744099s

• [SLOW TEST:10.213 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:55:57.412: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 00:55:57.504: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 31 00:55:57.509: INFO: Number of nodes with available pods: 0
May 31 00:55:57.509: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 31 00:55:57.533: INFO: Number of nodes with available pods: 0
May 31 00:55:57.533: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:55:58.537: INFO: Number of nodes with available pods: 0
May 31 00:55:58.537: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:55:59.537: INFO: Number of nodes with available pods: 0
May 31 00:55:59.537: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:00.536: INFO: Number of nodes with available pods: 1
May 31 00:56:00.536: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 31 00:56:00.553: INFO: Number of nodes with available pods: 1
May 31 00:56:00.553: INFO: Number of running nodes: 0, number of available pods: 1
May 31 00:56:01.556: INFO: Number of nodes with available pods: 0
May 31 00:56:01.556: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 31 00:56:01.568: INFO: Number of nodes with available pods: 0
May 31 00:56:01.568: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:02.571: INFO: Number of nodes with available pods: 0
May 31 00:56:02.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:03.570: INFO: Number of nodes with available pods: 0
May 31 00:56:03.570: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:04.571: INFO: Number of nodes with available pods: 0
May 31 00:56:04.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:05.571: INFO: Number of nodes with available pods: 0
May 31 00:56:05.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:06.572: INFO: Number of nodes with available pods: 0
May 31 00:56:06.572: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:07.571: INFO: Number of nodes with available pods: 0
May 31 00:56:07.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:08.571: INFO: Number of nodes with available pods: 0
May 31 00:56:08.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:09.571: INFO: Number of nodes with available pods: 0
May 31 00:56:09.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:10.571: INFO: Number of nodes with available pods: 0
May 31 00:56:10.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:11.571: INFO: Number of nodes with available pods: 0
May 31 00:56:11.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:12.571: INFO: Number of nodes with available pods: 0
May 31 00:56:12.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:13.571: INFO: Number of nodes with available pods: 0
May 31 00:56:13.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:14.571: INFO: Number of nodes with available pods: 0
May 31 00:56:14.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:15.571: INFO: Number of nodes with available pods: 0
May 31 00:56:15.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:16.571: INFO: Number of nodes with available pods: 0
May 31 00:56:16.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:17.571: INFO: Number of nodes with available pods: 0
May 31 00:56:17.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:18.571: INFO: Number of nodes with available pods: 0
May 31 00:56:18.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:19.573: INFO: Number of nodes with available pods: 0
May 31 00:56:19.573: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:20.571: INFO: Number of nodes with available pods: 0
May 31 00:56:20.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:21.571: INFO: Number of nodes with available pods: 0
May 31 00:56:21.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:22.571: INFO: Number of nodes with available pods: 0
May 31 00:56:22.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:23.571: INFO: Number of nodes with available pods: 0
May 31 00:56:23.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:24.570: INFO: Number of nodes with available pods: 0
May 31 00:56:24.570: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:25.571: INFO: Number of nodes with available pods: 0
May 31 00:56:25.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:26.571: INFO: Number of nodes with available pods: 0
May 31 00:56:26.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:27.571: INFO: Number of nodes with available pods: 0
May 31 00:56:27.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:28.576: INFO: Number of nodes with available pods: 0
May 31 00:56:28.576: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:29.571: INFO: Number of nodes with available pods: 0
May 31 00:56:29.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:30.571: INFO: Number of nodes with available pods: 0
May 31 00:56:30.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:31.571: INFO: Number of nodes with available pods: 0
May 31 00:56:31.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:32.571: INFO: Number of nodes with available pods: 0
May 31 00:56:32.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:33.570: INFO: Number of nodes with available pods: 0
May 31 00:56:33.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:34.571: INFO: Number of nodes with available pods: 0
May 31 00:56:34.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:35.571: INFO: Number of nodes with available pods: 0
May 31 00:56:35.572: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:36.571: INFO: Number of nodes with available pods: 0
May 31 00:56:36.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:37.571: INFO: Number of nodes with available pods: 0
May 31 00:56:37.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:38.571: INFO: Number of nodes with available pods: 0
May 31 00:56:38.571: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 00:56:39.571: INFO: Number of nodes with available pods: 1
May 31 00:56:39.571: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-k7p29, will wait for the garbage collector to delete the pods
May 31 00:56:39.632: INFO: Deleting {extensions DaemonSet} daemon-set took: 4.79735ms
May 31 00:56:39.732: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.184335ms
May 31 00:57:16.535: INFO: Number of nodes with available pods: 0
May 31 00:57:16.536: INFO: Number of running nodes: 0, number of available pods: 0
May 31 00:57:16.538: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-k7p29/daemonsets","resourceVersion":"12643"},"items":null}

May 31 00:57:16.540: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-k7p29/pods","resourceVersion":"12643"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:57:16.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-k7p29" for this suite.
May 31 00:57:22.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:57:22.592: INFO: namespace: e2e-tests-daemonsets-k7p29, resource: bindings, ignored listing per whitelist
May 31 00:57:22.650: INFO: namespace e2e-tests-daemonsets-k7p29 deletion completed in 6.089403227s

• [SLOW TEST:85.238 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:57:22.650: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-0c7ef767-833f-11e9-88d7-16679dc4b203
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0c7ef767-833f-11e9-88d7-16679dc4b203
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:57:28.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-fv6js" for this suite.
May 31 00:57:50.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:57:50.860: INFO: namespace: e2e-tests-configmap-fv6js, resource: bindings, ignored listing per whitelist
May 31 00:57:50.904: INFO: namespace e2e-tests-configmap-fv6js deletion completed in 22.123736802s

• [SLOW TEST:28.253 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:57:50.904: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 31 00:57:55.592: INFO: Successfully updated pod "annotationupdate1d57d491-833f-11e9-88d7-16679dc4b203"
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:57:57.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j94jq" for this suite.
May 31 00:58:19.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:58:19.718: INFO: namespace: e2e-tests-projected-j94jq, resource: bindings, ignored listing per whitelist
May 31 00:58:19.754: INFO: namespace e2e-tests-projected-j94jq deletion completed in 22.139782268s

• [SLOW TEST:28.850 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:58:19.754: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-886fw
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-886fw
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-886fw
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-886fw
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-886fw
May 31 00:58:25.989: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-886fw, name: ss-0, uid: 32299e68-833f-11e9-b3f2-001dd80c0014, status phase: Pending. Waiting for statefulset controller to delete.
May 31 00:58:26.514: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-886fw, name: ss-0, uid: 32299e68-833f-11e9-b3f2-001dd80c0014, status phase: Failed. Waiting for statefulset controller to delete.
May 31 00:58:26.519: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-886fw, name: ss-0, uid: 32299e68-833f-11e9-b3f2-001dd80c0014, status phase: Failed. Waiting for statefulset controller to delete.
May 31 00:58:26.520: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-886fw
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-886fw
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-886fw and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 00:58:30.545: INFO: Deleting all statefulset in ns e2e-tests-statefulset-886fw
May 31 00:58:30.547: INFO: Scaling statefulset ss to 0
May 31 00:58:40.559: INFO: Waiting for statefulset status.replicas updated to 0
May 31 00:58:40.562: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:58:40.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-886fw" for this suite.
May 31 00:58:46.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:58:46.635: INFO: namespace: e2e-tests-statefulset-886fw, resource: bindings, ignored listing per whitelist
May 31 00:58:46.677: INFO: namespace e2e-tests-statefulset-886fw deletion completed in 6.103327752s

• [SLOW TEST:26.923 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:58:46.678: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 31 00:58:46.762: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-fw9kd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw9kd/configmaps/e2e-watch-test-watch-closed,UID:3e9402ca-833f-11e9-b3f2-001dd80c0014,ResourceVersion:13024,Generation:0,CreationTimestamp:2019-05-31 00:58:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 00:58:46.763: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-fw9kd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw9kd/configmaps/e2e-watch-test-watch-closed,UID:3e9402ca-833f-11e9-b3f2-001dd80c0014,ResourceVersion:13025,Generation:0,CreationTimestamp:2019-05-31 00:58:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 31 00:58:46.774: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-fw9kd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw9kd/configmaps/e2e-watch-test-watch-closed,UID:3e9402ca-833f-11e9-b3f2-001dd80c0014,ResourceVersion:13026,Generation:0,CreationTimestamp:2019-05-31 00:58:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 00:58:46.774: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-fw9kd,SelfLink:/api/v1/namespaces/e2e-tests-watch-fw9kd/configmaps/e2e-watch-test-watch-closed,UID:3e9402ca-833f-11e9-b3f2-001dd80c0014,ResourceVersion:13027,Generation:0,CreationTimestamp:2019-05-31 00:58:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:58:46.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-fw9kd" for this suite.
May 31 00:58:52.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:58:52.807: INFO: namespace: e2e-tests-watch-fw9kd, resource: bindings, ignored listing per whitelist
May 31 00:58:52.869: INFO: namespace e2e-tests-watch-fw9kd deletion completed in 6.092222918s

• [SLOW TEST:6.191 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:58:52.870: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-424398de-833f-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 00:58:52.949: INFO: Waiting up to 5m0s for pod "pod-configmaps-4244095a-833f-11e9-88d7-16679dc4b203" in namespace "e2e-tests-configmap-9srlc" to be "success or failure"
May 31 00:58:52.953: INFO: Pod "pod-configmaps-4244095a-833f-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.347034ms
May 31 00:58:54.955: INFO: Pod "pod-configmaps-4244095a-833f-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005911347s
May 31 00:58:56.958: INFO: Pod "pod-configmaps-4244095a-833f-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008509958s
STEP: Saw pod success
May 31 00:58:56.958: INFO: Pod "pod-configmaps-4244095a-833f-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 00:58:56.960: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-configmaps-4244095a-833f-11e9-88d7-16679dc4b203 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 00:58:56.978: INFO: Waiting for pod pod-configmaps-4244095a-833f-11e9-88d7-16679dc4b203 to disappear
May 31 00:58:56.981: INFO: Pod pod-configmaps-4244095a-833f-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:58:56.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9srlc" for this suite.
May 31 00:59:02.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:59:03.019: INFO: namespace: e2e-tests-configmap-9srlc, resource: bindings, ignored listing per whitelist
May 31 00:59:03.087: INFO: namespace e2e-tests-configmap-9srlc deletion completed in 6.103174294s

• [SLOW TEST:10.218 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:59:03.090: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
May 31 00:59:03.170: INFO: namespace e2e-tests-kubectl-hqxxp
May 31 00:59:03.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-hqxxp'
May 31 00:59:03.419: INFO: stderr: ""
May 31 00:59:03.419: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 31 00:59:04.423: INFO: Selector matched 1 pods for map[app:redis]
May 31 00:59:04.423: INFO: Found 0 / 1
May 31 00:59:05.422: INFO: Selector matched 1 pods for map[app:redis]
May 31 00:59:05.422: INFO: Found 0 / 1
May 31 00:59:06.422: INFO: Selector matched 1 pods for map[app:redis]
May 31 00:59:06.422: INFO: Found 1 / 1
May 31 00:59:06.422: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 31 00:59:06.424: INFO: Selector matched 1 pods for map[app:redis]
May 31 00:59:06.424: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 31 00:59:06.424: INFO: wait on redis-master startup in e2e-tests-kubectl-hqxxp 
May 31 00:59:06.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 logs redis-master-xs4sr redis-master --namespace=e2e-tests-kubectl-hqxxp'
May 31 00:59:06.546: INFO: stderr: ""
May 31 00:59:06.546: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 31 May 00:59:05.286 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 May 00:59:05.287 # Server started, Redis version 3.2.12\n1:M 31 May 00:59:05.287 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 May 00:59:05.287 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 31 00:59:06.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-hqxxp'
May 31 00:59:06.690: INFO: stderr: ""
May 31 00:59:06.690: INFO: stdout: "service/rm2 exposed\n"
May 31 00:59:06.696: INFO: Service rm2 in namespace e2e-tests-kubectl-hqxxp found.
STEP: exposing service
May 31 00:59:08.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-hqxxp'
May 31 00:59:08.850: INFO: stderr: ""
May 31 00:59:08.850: INFO: stdout: "service/rm3 exposed\n"
May 31 00:59:08.855: INFO: Service rm3 in namespace e2e-tests-kubectl-hqxxp found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 00:59:10.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hqxxp" for this suite.
May 31 00:59:24.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 00:59:24.916: INFO: namespace: e2e-tests-kubectl-hqxxp, resource: bindings, ignored listing per whitelist
May 31 00:59:24.952: INFO: namespace e2e-tests-kubectl-hqxxp deletion completed in 14.089449136s

• [SLOW TEST:21.862 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 00:59:24.953: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-vg9kh
May 31 00:59:29.041: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-vg9kh
STEP: checking the pod's current state and verifying that restartCount is present
May 31 00:59:29.043: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:03:29.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vg9kh" for this suite.
May 31 01:03:35.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:03:35.547: INFO: namespace: e2e-tests-container-probe-vg9kh, resource: bindings, ignored listing per whitelist
May 31 01:03:35.556: INFO: namespace e2e-tests-container-probe-vg9kh deletion completed in 6.090373855s

• [SLOW TEST:250.603 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:03:35.557: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2pl79 A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-2pl79;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2pl79 A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-2pl79;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2pl79.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-2pl79.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2pl79.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-2pl79.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2pl79.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-2pl79.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2pl79.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-2pl79.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-2pl79.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 111.147.0.10.in-addr.arpa. PTR)" && echo OK > /results/10.0.147.111_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 111.147.0.10.in-addr.arpa. PTR)" && echo OK > /results/10.0.147.111_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2pl79 A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-2pl79;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2pl79 A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-2pl79;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-2pl79.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-2pl79.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-2pl79.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-2pl79.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2pl79.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-2pl79.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-2pl79.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-2pl79.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-2pl79.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 111.147.0.10.in-addr.arpa. PTR)" && echo OK > /results/10.0.147.111_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 111.147.0.10.in-addr.arpa. PTR)" && echo OK > /results/10.0.147.111_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 01:04:05.700: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.703: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.706: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-2pl79 from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.709: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-2pl79 from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.713: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-2pl79.svc from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.718: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-2pl79.svc from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.722: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.725: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.744: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.747: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.750: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-2pl79 from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.752: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-2pl79 from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.755: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-2pl79.svc from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.760: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-2pl79.svc from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.763: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.766: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc from pod e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203: the server could not find the requested resource (get pods dns-test-eac775d6-833f-11e9-88d7-16679dc4b203)
May 31 01:04:05.784: INFO: Lookups using e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-2pl79 wheezy_tcp@dns-test-service.e2e-tests-dns-2pl79 wheezy_udp@dns-test-service.e2e-tests-dns-2pl79.svc wheezy_tcp@dns-test-service.e2e-tests-dns-2pl79.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-2pl79 jessie_tcp@dns-test-service.e2e-tests-dns-2pl79 jessie_udp@dns-test-service.e2e-tests-dns-2pl79.svc jessie_tcp@dns-test-service.e2e-tests-dns-2pl79.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-2pl79.svc]

May 31 01:04:15.789: INFO: DNS probes using e2e-tests-dns-2pl79/dns-test-eac775d6-833f-11e9-88d7-16679dc4b203 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:04:15.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-2pl79" for this suite.
May 31 01:04:21.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:04:21.892: INFO: namespace: e2e-tests-dns-2pl79, resource: bindings, ignored listing per whitelist
May 31 01:04:21.958: INFO: namespace e2e-tests-dns-2pl79 deletion completed in 6.123677804s

• [SLOW TEST:46.401 seconds]
[sig-network] DNS
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:04:21.958: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 01:04:22.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-066c77e8-8340-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-h45zf" to be "success or failure"
May 31 01:04:22.052: INFO: Pod "downwardapi-volume-066c77e8-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.861649ms
May 31 01:04:24.069: INFO: Pod "downwardapi-volume-066c77e8-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021450932s
May 31 01:04:26.072: INFO: Pod "downwardapi-volume-066c77e8-8340-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024953382s
STEP: Saw pod success
May 31 01:04:26.072: INFO: Pod "downwardapi-volume-066c77e8-8340-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:04:26.075: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-066c77e8-8340-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 01:04:26.096: INFO: Waiting for pod downwardapi-volume-066c77e8-8340-11e9-88d7-16679dc4b203 to disappear
May 31 01:04:26.098: INFO: Pod downwardapi-volume-066c77e8-8340-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:04:26.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h45zf" for this suite.
May 31 01:04:32.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:04:32.183: INFO: namespace: e2e-tests-projected-h45zf, resource: bindings, ignored listing per whitelist
May 31 01:04:32.192: INFO: namespace e2e-tests-projected-h45zf deletion completed in 6.089643441s

• [SLOW TEST:10.233 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:04:32.194: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-rzcd
STEP: Creating a pod to test atomic-volume-subpath
May 31 01:04:32.290: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rzcd" in namespace "e2e-tests-subpath-7k664" to be "success or failure"
May 31 01:04:32.299: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.168382ms
May 31 01:04:34.301: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011073021s
May 31 01:04:36.305: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 4.01418236s
May 31 01:04:38.308: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 6.017705103s
May 31 01:04:40.311: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 8.021063142s
May 31 01:04:42.315: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 10.02437348s
May 31 01:04:44.318: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 12.027596417s
May 31 01:04:46.321: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 14.030739151s
May 31 01:04:48.324: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 16.033703682s
May 31 01:04:50.327: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 18.037127917s
May 31 01:04:52.332: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 20.041225857s
May 31 01:04:54.335: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Running", Reason="", readiness=false. Elapsed: 22.044724691s
May 31 01:04:56.338: INFO: Pod "pod-subpath-test-downwardapi-rzcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.047640217s
STEP: Saw pod success
May 31 01:04:56.338: INFO: Pod "pod-subpath-test-downwardapi-rzcd" satisfied condition "success or failure"
May 31 01:04:56.340: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-subpath-test-downwardapi-rzcd container test-container-subpath-downwardapi-rzcd: <nil>
STEP: delete the pod
May 31 01:04:56.358: INFO: Waiting for pod pod-subpath-test-downwardapi-rzcd to disappear
May 31 01:04:56.361: INFO: Pod pod-subpath-test-downwardapi-rzcd no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rzcd
May 31 01:04:56.361: INFO: Deleting pod "pod-subpath-test-downwardapi-rzcd" in namespace "e2e-tests-subpath-7k664"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:04:56.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-7k664" for this suite.
May 31 01:05:02.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:05:02.431: INFO: namespace: e2e-tests-subpath-7k664, resource: bindings, ignored listing per whitelist
May 31 01:05:02.467: INFO: namespace e2e-tests-subpath-7k664 deletion completed in 6.097644771s

• [SLOW TEST:30.273 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:05:02.468: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-1e91ba78-8340-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 01:05:02.561: INFO: Waiting up to 5m0s for pod "pod-configmaps-1e923ede-8340-11e9-88d7-16679dc4b203" in namespace "e2e-tests-configmap-dxrd8" to be "success or failure"
May 31 01:05:02.564: INFO: Pod "pod-configmaps-1e923ede-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047821ms
May 31 01:05:04.566: INFO: Pod "pod-configmaps-1e923ede-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004740641s
May 31 01:05:06.569: INFO: Pod "pod-configmaps-1e923ede-8340-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00758036s
STEP: Saw pod success
May 31 01:05:06.569: INFO: Pod "pod-configmaps-1e923ede-8340-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:05:06.571: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-configmaps-1e923ede-8340-11e9-88d7-16679dc4b203 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 01:05:06.588: INFO: Waiting for pod pod-configmaps-1e923ede-8340-11e9-88d7-16679dc4b203 to disappear
May 31 01:05:06.593: INFO: Pod pod-configmaps-1e923ede-8340-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:05:06.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-dxrd8" for this suite.
May 31 01:05:12.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:05:12.647: INFO: namespace: e2e-tests-configmap-dxrd8, resource: bindings, ignored listing per whitelist
May 31 01:05:12.686: INFO: namespace e2e-tests-configmap-dxrd8 deletion completed in 6.087669152s

• [SLOW TEST:10.218 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:05:12.686: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-d6b76
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 01:05:12.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 31 01:05:38.837: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.244.3.127 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-d6b76 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:05:38.837: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:05:39.995: INFO: Found all expected endpoints: [netserver-0]
May 31 01:05:39.997: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.244.0.82 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-d6b76 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:05:39.997: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:05:41.147: INFO: Found all expected endpoints: [netserver-1]
May 31 01:05:41.150: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.244.2.24 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-d6b76 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:05:41.150: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:05:42.305: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:05:42.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-d6b76" for this suite.
May 31 01:06:04.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:06:04.338: INFO: namespace: e2e-tests-pod-network-test-d6b76, resource: bindings, ignored listing per whitelist
May 31 01:06:04.399: INFO: namespace e2e-tests-pod-network-test-d6b76 deletion completed in 22.090488345s

• [SLOW TEST:51.713 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:06:04.402: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-n6bmt
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 01:06:04.480: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 31 01:06:30.555: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.130:8080/dial?request=hostName&protocol=udp&host=10.244.0.83&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-n6bmt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:06:30.555: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:06:30.706: INFO: Waiting for endpoints: map[]
May 31 01:06:30.708: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.130:8080/dial?request=hostName&protocol=udp&host=10.244.2.25&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-n6bmt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:06:30.708: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:06:30.861: INFO: Waiting for endpoints: map[]
May 31 01:06:30.863: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.130:8080/dial?request=hostName&protocol=udp&host=10.244.3.129&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-n6bmt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:06:30.863: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:06:31.006: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:06:31.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-n6bmt" for this suite.
May 31 01:06:53.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:06:53.034: INFO: namespace: e2e-tests-pod-network-test-n6bmt, resource: bindings, ignored listing per whitelist
May 31 01:06:53.102: INFO: namespace e2e-tests-pod-network-test-n6bmt deletion completed in 22.091784107s

• [SLOW TEST:48.701 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:06:53.105: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 31 01:06:53.187: INFO: Waiting up to 5m0s for pod "downward-api-608274b1-8340-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-7p8pw" to be "success or failure"
May 31 01:06:53.194: INFO: Pod "downward-api-608274b1-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 7.058971ms
May 31 01:06:55.197: INFO: Pod "downward-api-608274b1-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010024539s
May 31 01:06:57.200: INFO: Pod "downward-api-608274b1-8340-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012994007s
STEP: Saw pod success
May 31 01:06:57.200: INFO: Pod "downward-api-608274b1-8340-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:06:57.203: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downward-api-608274b1-8340-11e9-88d7-16679dc4b203 container dapi-container: <nil>
STEP: delete the pod
May 31 01:06:57.217: INFO: Waiting for pod downward-api-608274b1-8340-11e9-88d7-16679dc4b203 to disappear
May 31 01:06:57.219: INFO: Pod downward-api-608274b1-8340-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:06:57.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7p8pw" for this suite.
May 31 01:07:03.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:07:03.309: INFO: namespace: e2e-tests-downward-api-7p8pw, resource: bindings, ignored listing per whitelist
May 31 01:07:03.312: INFO: namespace e2e-tests-downward-api-7p8pw deletion completed in 6.089922112s

• [SLOW TEST:10.208 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:07:03.313: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 31 01:07:11.426: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 01:07:11.429: INFO: Pod pod-with-prestop-http-hook still exists
May 31 01:07:13.429: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 01:07:13.432: INFO: Pod pod-with-prestop-http-hook still exists
May 31 01:07:15.429: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 01:07:15.432: INFO: Pod pod-with-prestop-http-hook still exists
May 31 01:07:17.429: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 01:07:17.432: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:07:17.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-d9h97" for this suite.
May 31 01:07:39.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:07:39.539: INFO: namespace: e2e-tests-container-lifecycle-hook-d9h97, resource: bindings, ignored listing per whitelist
May 31 01:07:39.553: INFO: namespace e2e-tests-container-lifecycle-hook-d9h97 deletion completed in 22.111185882s

• [SLOW TEST:36.241 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:07:39.560: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 01:07:39.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c334410-8340-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-shp5d" to be "success or failure"
May 31 01:07:39.648: INFO: Pod "downwardapi-volume-7c334410-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.388245ms
May 31 01:07:41.651: INFO: Pod "downwardapi-volume-7c334410-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007344793s
May 31 01:07:43.655: INFO: Pod "downwardapi-volume-7c334410-8340-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010749546s
STEP: Saw pod success
May 31 01:07:43.655: INFO: Pod "downwardapi-volume-7c334410-8340-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:07:43.658: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod downwardapi-volume-7c334410-8340-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 01:07:43.691: INFO: Waiting for pod downwardapi-volume-7c334410-8340-11e9-88d7-16679dc4b203 to disappear
May 31 01:07:43.694: INFO: Pod downwardapi-volume-7c334410-8340-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:07:43.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-shp5d" for this suite.
May 31 01:07:49.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:07:49.735: INFO: namespace: e2e-tests-projected-shp5d, resource: bindings, ignored listing per whitelist
May 31 01:07:49.781: INFO: namespace e2e-tests-projected-shp5d deletion completed in 6.084342299s

• [SLOW TEST:10.226 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:07:49.782: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-824b59a2-8340-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 01:07:49.871: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-824bc568-8340-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-ftl49" to be "success or failure"
May 31 01:07:49.878: INFO: Pod "pod-projected-secrets-824bc568-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.96997ms
May 31 01:07:51.881: INFO: Pod "pod-projected-secrets-824bc568-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009795013s
May 31 01:07:53.884: INFO: Pod "pod-projected-secrets-824bc568-8340-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012931059s
STEP: Saw pod success
May 31 01:07:53.884: INFO: Pod "pod-projected-secrets-824bc568-8340-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:07:53.886: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-projected-secrets-824bc568-8340-11e9-88d7-16679dc4b203 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 01:07:53.903: INFO: Waiting for pod pod-projected-secrets-824bc568-8340-11e9-88d7-16679dc4b203 to disappear
May 31 01:07:53.907: INFO: Pod pod-projected-secrets-824bc568-8340-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:07:53.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ftl49" for this suite.
May 31 01:07:59.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:07:59.966: INFO: namespace: e2e-tests-projected-ftl49, resource: bindings, ignored listing per whitelist
May 31 01:08:00.006: INFO: namespace e2e-tests-projected-ftl49 deletion completed in 6.095866602s

• [SLOW TEST:10.225 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:08:00.007: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 31 01:08:08.142: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:08.145: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:10.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:10.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:12.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:12.148: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:14.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:14.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:16.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:16.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:18.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:18.148: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:20.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:20.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:22.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:22.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:24.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:24.150: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:26.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:26.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:28.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:28.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:30.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:30.148: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:32.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:32.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:34.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:34.148: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:36.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:36.148: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 01:08:38.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 01:08:38.148: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:08:38.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-8pn6m" for this suite.
May 31 01:09:00.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:09:00.201: INFO: namespace: e2e-tests-container-lifecycle-hook-8pn6m, resource: bindings, ignored listing per whitelist
May 31 01:09:00.253: INFO: namespace e2e-tests-container-lifecycle-hook-8pn6m deletion completed in 22.09468598s

• [SLOW TEST:60.246 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:09:00.255: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 31 01:09:04.352: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-ac4ca99f-8340-11e9-88d7-16679dc4b203,GenerateName:,Namespace:e2e-tests-events-nrsd8,SelfLink:/api/v1/namespaces/e2e-tests-events-nrsd8/pods/send-events-ac4ca99f-8340-11e9-88d7-16679dc4b203,UID:ac4dbd05-8340-11e9-b3f2-001dd80c0014,ResourceVersion:14587,Generation:0,CreationTimestamp:2019-05-31 01:09:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 336173844,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cszhr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cszhr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-cszhr true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4212bd3a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4212bd3c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:09:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:09:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:09:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:09:00 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.0.88,StartTime:2019-05-31 01:09:00 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-31 01:09:02 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://b64c8ce4669b9e00a802dea5e80cc323de5ca5b9e1ef78bc92c7193b37f7b37d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 31 01:09:06.356: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 31 01:09:08.359: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:09:08.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-nrsd8" for this suite.
May 31 01:09:42.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:09:42.452: INFO: namespace: e2e-tests-events-nrsd8, resource: bindings, ignored listing per whitelist
May 31 01:09:42.519: INFO: namespace e2e-tests-events-nrsd8 deletion completed in 34.096827795s

• [SLOW TEST:42.265 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:09:42.520: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 31 01:09:50.628: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:50.628: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:50.772: INFO: Exec stderr: ""
May 31 01:09:50.772: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:50.772: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:50.925: INFO: Exec stderr: ""
May 31 01:09:50.925: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:50.925: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:51.061: INFO: Exec stderr: ""
May 31 01:09:51.061: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:51.061: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:51.205: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 31 01:09:51.205: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:51.205: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:51.335: INFO: Exec stderr: ""
May 31 01:09:51.335: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:51.335: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:51.471: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 31 01:09:51.471: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:51.471: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:51.609: INFO: Exec stderr: ""
May 31 01:09:51.610: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:51.610: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:51.748: INFO: Exec stderr: ""
May 31 01:09:51.748: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:51.748: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:51.886: INFO: Exec stderr: ""
May 31 01:09:51.886: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-x96gs PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:09:51.886: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:09:52.022: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:09:52.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-x96gs" for this suite.
May 31 01:10:42.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:10:42.072: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-x96gs, resource: bindings, ignored listing per whitelist
May 31 01:10:42.115: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-x96gs deletion completed in 50.089205804s

• [SLOW TEST:59.595 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:10:42.115: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 01:10:42.207: INFO: (0) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 5.239601ms)
May 31 01:10:42.214: INFO: (1) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 7.058236ms)
May 31 01:10:42.218: INFO: (2) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.191181ms)
May 31 01:10:42.222: INFO: (3) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.358964ms)
May 31 01:10:42.225: INFO: (4) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 2.905856ms)
May 31 01:10:42.228: INFO: (5) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 2.909857ms)
May 31 01:10:42.231: INFO: (6) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 2.949157ms)
May 31 01:10:42.234: INFO: (7) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.189362ms)
May 31 01:10:42.237: INFO: (8) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.183261ms)
May 31 01:10:42.241: INFO: (9) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.377565ms)
May 31 01:10:42.245: INFO: (10) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.557769ms)
May 31 01:10:42.248: INFO: (11) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.520668ms)
May 31 01:10:42.252: INFO: (12) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.483168ms)
May 31 01:10:42.255: INFO: (13) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.154661ms)
May 31 01:10:42.259: INFO: (14) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.14846ms)
May 31 01:10:42.263: INFO: (15) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.032578ms)
May 31 01:10:42.266: INFO: (16) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.031058ms)
May 31 01:10:42.269: INFO: (17) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.15816ms)
May 31 01:10:42.272: INFO: (18) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.281763ms)
May 31 01:10:42.276: INFO: (19) /api/v1/nodes/k8s-linuxpool-10610181-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.354564ms)
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:10:42.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-lgrr6" for this suite.
May 31 01:10:48.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:10:48.312: INFO: namespace: e2e-tests-proxy-lgrr6, resource: bindings, ignored listing per whitelist
May 31 01:10:48.362: INFO: namespace e2e-tests-proxy-lgrr6 deletion completed in 6.08268225s

• [SLOW TEST:6.247 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:10:48.362: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 01:10:48.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ecbbb023-8340-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-s54z7" to be "success or failure"
May 31 01:10:48.447: INFO: Pod "downwardapi-volume-ecbbb023-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.315583ms
May 31 01:10:50.450: INFO: Pod "downwardapi-volume-ecbbb023-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007071967s
May 31 01:10:52.453: INFO: Pod "downwardapi-volume-ecbbb023-8340-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009941994s
STEP: Saw pod success
May 31 01:10:52.453: INFO: Pod "downwardapi-volume-ecbbb023-8340-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:10:52.455: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-ecbbb023-8340-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 01:10:52.471: INFO: Waiting for pod downwardapi-volume-ecbbb023-8340-11e9-88d7-16679dc4b203 to disappear
May 31 01:10:52.473: INFO: Pod downwardapi-volume-ecbbb023-8340-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:10:52.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-s54z7" for this suite.
May 31 01:10:58.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:10:58.500: INFO: namespace: e2e-tests-downward-api-s54z7, resource: bindings, ignored listing per whitelist
May 31 01:10:58.563: INFO: namespace e2e-tests-downward-api-s54z7 deletion completed in 6.086406709s

• [SLOW TEST:10.201 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:10:58.563: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-f2d3ff75-8340-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 01:10:58.671: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2d479af-8340-11e9-88d7-16679dc4b203" in namespace "e2e-tests-configmap-4wnls" to be "success or failure"
May 31 01:10:58.675: INFO: Pod "pod-configmaps-f2d479af-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.864274ms
May 31 01:11:00.678: INFO: Pod "pod-configmaps-f2d479af-8340-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006734261s
May 31 01:11:02.681: INFO: Pod "pod-configmaps-f2d479af-8340-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009653092s
STEP: Saw pod success
May 31 01:11:02.681: INFO: Pod "pod-configmaps-f2d479af-8340-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:11:02.684: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-configmaps-f2d479af-8340-11e9-88d7-16679dc4b203 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 01:11:02.709: INFO: Waiting for pod pod-configmaps-f2d479af-8340-11e9-88d7-16679dc4b203 to disappear
May 31 01:11:02.712: INFO: Pod pod-configmaps-f2d479af-8340-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:11:02.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-4wnls" for this suite.
May 31 01:11:08.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:11:08.774: INFO: namespace: e2e-tests-configmap-4wnls, resource: bindings, ignored listing per whitelist
May 31 01:11:08.810: INFO: namespace e2e-tests-configmap-4wnls deletion completed in 6.094550266s

• [SLOW TEST:10.247 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:11:08.811: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 31 01:11:13.422: INFO: Successfully updated pod "labelsupdatef8ecb4bc-8340-11e9-88d7-16679dc4b203"
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:11:15.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sdhgc" for this suite.
May 31 01:11:37.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:11:37.517: INFO: namespace: e2e-tests-projected-sdhgc, resource: bindings, ignored listing per whitelist
May 31 01:11:37.534: INFO: namespace e2e-tests-projected-sdhgc deletion completed in 22.093894932s

• [SLOW TEST:28.723 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:11:37.536: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-0a0b5a5d-8341-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 01:11:37.628: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0a0bd3a6-8341-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-927gq" to be "success or failure"
May 31 01:11:37.633: INFO: Pod "pod-projected-configmaps-0a0bd3a6-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.665987ms
May 31 01:11:39.636: INFO: Pod "pod-projected-configmaps-0a0bd3a6-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007739994s
May 31 01:11:41.639: INFO: Pod "pod-projected-configmaps-0a0bd3a6-8341-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011044052s
STEP: Saw pod success
May 31 01:11:41.639: INFO: Pod "pod-projected-configmaps-0a0bd3a6-8341-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:11:41.642: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-projected-configmaps-0a0bd3a6-8341-11e9-88d7-16679dc4b203 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 01:11:41.655: INFO: Waiting for pod pod-projected-configmaps-0a0bd3a6-8341-11e9-88d7-16679dc4b203 to disappear
May 31 01:11:41.660: INFO: Pod pod-projected-configmaps-0a0bd3a6-8341-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:11:41.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-927gq" for this suite.
May 31 01:11:47.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:11:47.739: INFO: namespace: e2e-tests-projected-927gq, resource: bindings, ignored listing per whitelist
May 31 01:11:47.752: INFO: namespace e2e-tests-projected-927gq deletion completed in 6.088981803s

• [SLOW TEST:10.217 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:11:47.754: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 31 01:11:47.831: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:11:51.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-kxpsr" for this suite.
May 31 01:11:57.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:11:57.891: INFO: namespace: e2e-tests-init-container-kxpsr, resource: bindings, ignored listing per whitelist
May 31 01:11:57.902: INFO: namespace e2e-tests-init-container-kxpsr deletion completed in 6.105315094s

• [SLOW TEST:10.148 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:11:57.902: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-163080c3-8341-11e9-88d7-16679dc4b203
STEP: Creating secret with name s-test-opt-upd-1630819d-8341-11e9-88d7-16679dc4b203
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-163080c3-8341-11e9-88d7-16679dc4b203
STEP: Updating secret s-test-opt-upd-1630819d-8341-11e9-88d7-16679dc4b203
STEP: Creating secret with name s-test-opt-create-163081b1-8341-11e9-88d7-16679dc4b203
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:12:06.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wlgv8" for this suite.
May 31 01:12:28.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:12:28.264: INFO: namespace: e2e-tests-projected-wlgv8, resource: bindings, ignored listing per whitelist
May 31 01:12:28.273: INFO: namespace e2e-tests-projected-wlgv8 deletion completed in 22.114016877s

• [SLOW TEST:30.371 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:12:28.273: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
May 31 01:12:28.865: INFO: Waiting up to 5m0s for pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-hzlqh" in namespace "e2e-tests-svcaccounts-tflhw" to be "success or failure"
May 31 01:12:28.870: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-hzlqh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.020189ms
May 31 01:12:30.874: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-hzlqh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008873604s
May 31 01:12:32.899: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-hzlqh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034156951s
STEP: Saw pod success
May 31 01:12:32.900: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-hzlqh" satisfied condition "success or failure"
May 31 01:12:32.902: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-hzlqh container token-test: <nil>
STEP: delete the pod
May 31 01:12:32.923: INFO: Waiting for pod pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-hzlqh to disappear
May 31 01:12:32.925: INFO: Pod pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-hzlqh no longer exists
STEP: Creating a pod to test consume service account root CA
May 31 01:12:32.928: INFO: Waiting up to 5m0s for pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-47x7v" in namespace "e2e-tests-svcaccounts-tflhw" to be "success or failure"
May 31 01:12:32.930: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-47x7v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346342ms
May 31 01:12:34.933: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-47x7v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005130339s
May 31 01:12:36.936: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-47x7v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007653784s
STEP: Saw pod success
May 31 01:12:36.936: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-47x7v" satisfied condition "success or failure"
May 31 01:12:36.938: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-47x7v container root-ca-test: <nil>
STEP: delete the pod
May 31 01:12:36.956: INFO: Waiting for pod pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-47x7v to disappear
May 31 01:12:36.960: INFO: Pod pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-47x7v no longer exists
STEP: Creating a pod to test consume service account namespace
May 31 01:12:36.963: INFO: Waiting up to 5m0s for pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-fh5fn" in namespace "e2e-tests-svcaccounts-tflhw" to be "success or failure"
May 31 01:12:36.967: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-fh5fn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844169ms
May 31 01:12:38.970: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-fh5fn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006799973s
May 31 01:12:40.973: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-fh5fn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009988833s
STEP: Saw pod success
May 31 01:12:40.973: INFO: Pod "pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-fh5fn" satisfied condition "success or failure"
May 31 01:12:40.976: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-fh5fn container namespace-test: <nil>
STEP: delete the pod
May 31 01:12:41.000: INFO: Waiting for pod pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-fh5fn to disappear
May 31 01:12:41.003: INFO: Pod pod-service-account-2896d6eb-8341-11e9-88d7-16679dc4b203-fh5fn no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:12:41.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-tflhw" for this suite.
May 31 01:12:47.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:12:47.082: INFO: namespace: e2e-tests-svcaccounts-tflhw, resource: bindings, ignored listing per whitelist
May 31 01:12:47.096: INFO: namespace e2e-tests-svcaccounts-tflhw deletion completed in 6.090124712s

• [SLOW TEST:18.823 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:12:47.097: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
May 31 01:12:47.177: INFO: Waiting up to 5m0s for pod "var-expansion-33811873-8341-11e9-88d7-16679dc4b203" in namespace "e2e-tests-var-expansion-x9z48" to be "success or failure"
May 31 01:12:47.182: INFO: Pod "var-expansion-33811873-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.207292ms
May 31 01:12:49.185: INFO: Pod "var-expansion-33811873-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00845476s
May 31 01:12:51.188: INFO: Pod "var-expansion-33811873-8341-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011493978s
STEP: Saw pod success
May 31 01:12:51.188: INFO: Pod "var-expansion-33811873-8341-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:12:51.190: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod var-expansion-33811873-8341-11e9-88d7-16679dc4b203 container dapi-container: <nil>
STEP: delete the pod
May 31 01:12:51.217: INFO: Waiting for pod var-expansion-33811873-8341-11e9-88d7-16679dc4b203 to disappear
May 31 01:12:51.220: INFO: Pod var-expansion-33811873-8341-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:12:51.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-x9z48" for this suite.
May 31 01:12:57.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:12:57.301: INFO: namespace: e2e-tests-var-expansion-x9z48, resource: bindings, ignored listing per whitelist
May 31 01:12:57.329: INFO: namespace e2e-tests-var-expansion-x9z48 deletion completed in 6.104844047s

• [SLOW TEST:10.233 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:12:57.330: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-v9ltj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-v9ltj to expose endpoints map[]
May 31 01:12:57.418: INFO: Get endpoints failed (3.765066ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May 31 01:12:58.421: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-v9ltj exposes endpoints map[] (1.006552907s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-v9ltj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-v9ltj to expose endpoints map[pod1:[80]]
May 31 01:13:01.487: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-v9ltj exposes endpoints map[pod1:[80]] (3.059439946s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-v9ltj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-v9ltj to expose endpoints map[pod1:[80] pod2:[80]]
May 31 01:13:04.524: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-v9ltj exposes endpoints map[pod2:[80] pod1:[80]] (3.033583391s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-v9ltj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-v9ltj to expose endpoints map[pod2:[80]]
May 31 01:13:05.541: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-v9ltj exposes endpoints map[pod2:[80]] (1.013239242s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-v9ltj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-v9ltj to expose endpoints map[]
May 31 01:13:06.558: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-v9ltj exposes endpoints map[] (1.012678321s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:13:06.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-v9ltj" for this suite.
May 31 01:13:28.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:13:28.614: INFO: namespace: e2e-tests-services-v9ltj, resource: bindings, ignored listing per whitelist
May 31 01:13:28.672: INFO: namespace e2e-tests-services-v9ltj deletion completed in 22.094604913s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

• [SLOW TEST:31.343 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:13:28.674: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-4c4a7208-8341-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume secrets
May 31 01:13:28.764: INFO: Waiting up to 5m0s for pod "pod-secrets-4c4adf57-8341-11e9-88d7-16679dc4b203" in namespace "e2e-tests-secrets-n2zgj" to be "success or failure"
May 31 01:13:28.770: INFO: Pod "pod-secrets-4c4adf57-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.342192ms
May 31 01:13:30.773: INFO: Pod "pod-secrets-4c4adf57-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008340622s
May 31 01:13:32.776: INFO: Pod "pod-secrets-4c4adf57-8341-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011621714s
STEP: Saw pod success
May 31 01:13:32.776: INFO: Pod "pod-secrets-4c4adf57-8341-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:13:32.778: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-secrets-4c4adf57-8341-11e9-88d7-16679dc4b203 container secret-volume-test: <nil>
STEP: delete the pod
May 31 01:13:32.793: INFO: Waiting for pod pod-secrets-4c4adf57-8341-11e9-88d7-16679dc4b203 to disappear
May 31 01:13:32.796: INFO: Pod pod-secrets-4c4adf57-8341-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:13:32.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-n2zgj" for this suite.
May 31 01:13:38.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:13:38.857: INFO: namespace: e2e-tests-secrets-n2zgj, resource: bindings, ignored listing per whitelist
May 31 01:13:38.903: INFO: namespace e2e-tests-secrets-n2zgj deletion completed in 6.104549533s

• [SLOW TEST:10.229 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:13:38.904: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
May 31 01:13:43.522: INFO: Successfully updated pod "annotationupdate5263d90c-8341-11e9-88d7-16679dc4b203"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:13:45.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nphxh" for this suite.
May 31 01:14:07.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:14:07.589: INFO: namespace: e2e-tests-downward-api-nphxh, resource: bindings, ignored listing per whitelist
May 31 01:14:07.631: INFO: namespace e2e-tests-downward-api-nphxh deletion completed in 22.088181879s

• [SLOW TEST:28.728 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:14:07.632: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 01:14:07.720: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:14:08.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-xmgrl" for this suite.
May 31 01:14:14.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:14:14.841: INFO: namespace: e2e-tests-custom-resource-definition-xmgrl, resource: bindings, ignored listing per whitelist
May 31 01:14:14.907: INFO: namespace e2e-tests-custom-resource-definition-xmgrl deletion completed in 6.100369405s

• [SLOW TEST:7.275 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:14:14.908: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
May 31 01:14:14.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-mgc8g'
May 31 01:14:15.638: INFO: stderr: ""
May 31 01:14:15.638: INFO: stdout: "pod/pause created\n"
May 31 01:14:15.638: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 31 01:14:15.638: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-mgc8g" to be "running and ready"
May 31 01:14:15.644: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.792196ms
May 31 01:14:17.646: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008583158s
May 31 01:14:19.650: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.011772287s
May 31 01:14:19.650: INFO: Pod "pause" satisfied condition "running and ready"
May 31 01:14:19.650: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
May 31 01:14:19.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-mgc8g'
May 31 01:14:19.879: INFO: stderr: ""
May 31 01:14:19.879: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 31 01:14:19.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pod pause -L testing-label --namespace=e2e-tests-kubectl-mgc8g'
May 31 01:14:20.019: INFO: stderr: ""
May 31 01:14:20.019: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 31 01:14:20.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 label pods pause testing-label- --namespace=e2e-tests-kubectl-mgc8g'
May 31 01:14:20.165: INFO: stderr: ""
May 31 01:14:20.165: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 31 01:14:20.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pod pause -L testing-label --namespace=e2e-tests-kubectl-mgc8g'
May 31 01:14:20.281: INFO: stderr: ""
May 31 01:14:20.281: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
May 31 01:14:20.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-mgc8g'
May 31 01:14:20.395: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 01:14:20.395: INFO: stdout: "pod \"pause\" force deleted\n"
May 31 01:14:20.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-mgc8g'
May 31 01:14:20.516: INFO: stderr: "No resources found.\n"
May 31 01:14:20.516: INFO: stdout: ""
May 31 01:14:20.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -l name=pause --namespace=e2e-tests-kubectl-mgc8g -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 01:14:20.635: INFO: stderr: ""
May 31 01:14:20.635: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:14:20.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mgc8g" for this suite.
May 31 01:14:26.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:14:26.659: INFO: namespace: e2e-tests-kubectl-mgc8g, resource: bindings, ignored listing per whitelist
May 31 01:14:26.727: INFO: namespace e2e-tests-kubectl-mgc8g deletion completed in 6.088463701s

• [SLOW TEST:11.819 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:14:26.727: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-6ee5946e-8341-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 01:14:26.823: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ee5fbb0-8341-11e9-88d7-16679dc4b203" in namespace "e2e-tests-configmap-9w8jm" to be "success or failure"
May 31 01:14:26.832: INFO: Pod "pod-configmaps-6ee5fbb0-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 9.03615ms
May 31 01:14:28.835: INFO: Pod "pod-configmaps-6ee5fbb0-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011838093s
May 31 01:14:30.838: INFO: Pod "pod-configmaps-6ee5fbb0-8341-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0146767s
STEP: Saw pod success
May 31 01:14:30.838: INFO: Pod "pod-configmaps-6ee5fbb0-8341-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:14:30.841: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-configmaps-6ee5fbb0-8341-11e9-88d7-16679dc4b203 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 01:14:30.858: INFO: Waiting for pod pod-configmaps-6ee5fbb0-8341-11e9-88d7-16679dc4b203 to disappear
May 31 01:14:30.861: INFO: Pod pod-configmaps-6ee5fbb0-8341-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:14:30.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9w8jm" for this suite.
May 31 01:14:36.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:14:36.954: INFO: namespace: e2e-tests-configmap-9w8jm, resource: bindings, ignored listing per whitelist
May 31 01:14:36.960: INFO: namespace e2e-tests-configmap-9w8jm deletion completed in 6.095840226s

• [SLOW TEST:10.233 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:14:36.962: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-82lq
STEP: Creating a pod to test atomic-volume-subpath
May 31 01:14:37.063: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-82lq" in namespace "e2e-tests-subpath-5n8fh" to be "success or failure"
May 31 01:14:37.068: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.84988ms
May 31 01:14:39.071: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008011135s
May 31 01:14:41.074: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 4.01108005s
May 31 01:14:43.077: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 6.01425403s
May 31 01:14:45.080: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 8.017512074s
May 31 01:14:47.083: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 10.020795581s
May 31 01:14:49.087: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 12.023892348s
May 31 01:14:51.089: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 14.026646372s
May 31 01:14:53.093: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 16.030151671s
May 31 01:14:55.096: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 18.033571033s
May 31 01:14:57.099: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 20.036479649s
May 31 01:14:59.103: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Running", Reason="", readiness=false. Elapsed: 22.039860537s
May 31 01:15:01.106: INFO: Pod "pod-subpath-test-projected-82lq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042929784s
STEP: Saw pod success
May 31 01:15:01.106: INFO: Pod "pod-subpath-test-projected-82lq" satisfied condition "success or failure"
May 31 01:15:01.108: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-subpath-test-projected-82lq container test-container-subpath-projected-82lq: <nil>
STEP: delete the pod
May 31 01:15:01.127: INFO: Waiting for pod pod-subpath-test-projected-82lq to disappear
May 31 01:15:01.130: INFO: Pod pod-subpath-test-projected-82lq no longer exists
STEP: Deleting pod pod-subpath-test-projected-82lq
May 31 01:15:01.130: INFO: Deleting pod "pod-subpath-test-projected-82lq" in namespace "e2e-tests-subpath-5n8fh"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:15:01.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-5n8fh" for this suite.
May 31 01:15:07.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:15:07.202: INFO: namespace: e2e-tests-subpath-5n8fh, resource: bindings, ignored listing per whitelist
May 31 01:15:07.231: INFO: namespace e2e-tests-subpath-5n8fh deletion completed in 6.096473634s

• [SLOW TEST:30.269 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:15:07.231: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
May 31 01:15:07.306: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-190461910 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:15:07.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tpbmv" for this suite.
May 31 01:15:13.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:15:13.476: INFO: namespace: e2e-tests-kubectl-tpbmv, resource: bindings, ignored listing per whitelist
May 31 01:15:13.495: INFO: namespace e2e-tests-kubectl-tpbmv deletion completed in 6.095315176s

• [SLOW TEST:6.264 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:15:13.495: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 01:15:13.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-tndq5'
May 31 01:15:13.720: INFO: stderr: ""
May 31 01:15:13.720: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
May 31 01:15:13.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-tndq5'
May 31 01:15:25.667: INFO: stderr: ""
May 31 01:15:25.668: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:15:25.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tndq5" for this suite.
May 31 01:15:31.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:15:31.716: INFO: namespace: e2e-tests-kubectl-tndq5, resource: bindings, ignored listing per whitelist
May 31 01:15:31.772: INFO: namespace e2e-tests-kubectl-tndq5 deletion completed in 6.098194855s

• [SLOW TEST:18.276 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:15:31.772: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-95a9400f-8341-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 01:15:31.861: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-95a9c81b-8341-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-6clhd" to be "success or failure"
May 31 01:15:31.866: INFO: Pod "pod-projected-configmaps-95a9c81b-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.37017ms
May 31 01:15:33.869: INFO: Pod "pod-projected-configmaps-95a9c81b-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007339341s
May 31 01:15:35.872: INFO: Pod "pod-projected-configmaps-95a9c81b-8341-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010487982s
STEP: Saw pod success
May 31 01:15:35.872: INFO: Pod "pod-projected-configmaps-95a9c81b-8341-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:15:35.874: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-projected-configmaps-95a9c81b-8341-11e9-88d7-16679dc4b203 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 01:15:35.888: INFO: Waiting for pod pod-projected-configmaps-95a9c81b-8341-11e9-88d7-16679dc4b203 to disappear
May 31 01:15:35.891: INFO: Pod pod-projected-configmaps-95a9c81b-8341-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:15:35.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6clhd" for this suite.
May 31 01:15:41.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:15:41.988: INFO: namespace: e2e-tests-projected-6clhd, resource: bindings, ignored listing per whitelist
May 31 01:15:42.010: INFO: namespace e2e-tests-projected-6clhd deletion completed in 6.112832862s

• [SLOW TEST:10.238 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:15:42.012: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0531 01:15:43.140227      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 01:15:43.140: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:15:43.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-z2tl5" for this suite.
May 31 01:15:49.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:15:49.203: INFO: namespace: e2e-tests-gc-z2tl5, resource: bindings, ignored listing per whitelist
May 31 01:15:49.227: INFO: namespace e2e-tests-gc-z2tl5 deletion completed in 6.084387042s

• [SLOW TEST:7.216 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:15:49.227: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:15:55.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-4mrlx" for this suite.
May 31 01:16:01.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:16:01.467: INFO: namespace: e2e-tests-namespaces-4mrlx, resource: bindings, ignored listing per whitelist
May 31 01:16:01.525: INFO: namespace e2e-tests-namespaces-4mrlx deletion completed in 6.107996706s
STEP: Destroying namespace "e2e-tests-nsdeletetest-9mcm5" for this suite.
May 31 01:16:01.527: INFO: Namespace e2e-tests-nsdeletetest-9mcm5 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-4dvl5" for this suite.
May 31 01:16:07.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:16:07.601: INFO: namespace: e2e-tests-nsdeletetest-4dvl5, resource: bindings, ignored listing per whitelist
May 31 01:16:07.612: INFO: namespace e2e-tests-nsdeletetest-4dvl5 deletion completed in 6.084967446s

• [SLOW TEST:18.384 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:16:07.612: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-nhg75/configmap-test-ab067abf-8341-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 01:16:07.705: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab06f40b-8341-11e9-88d7-16679dc4b203" in namespace "e2e-tests-configmap-nhg75" to be "success or failure"
May 31 01:16:07.711: INFO: Pod "pod-configmaps-ab06f40b-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515486ms
May 31 01:16:09.714: INFO: Pod "pod-configmaps-ab06f40b-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008602173s
May 31 01:16:11.717: INFO: Pod "pod-configmaps-ab06f40b-8341-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011825531s
STEP: Saw pod success
May 31 01:16:11.717: INFO: Pod "pod-configmaps-ab06f40b-8341-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:16:11.720: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-configmaps-ab06f40b-8341-11e9-88d7-16679dc4b203 container env-test: <nil>
STEP: delete the pod
May 31 01:16:11.737: INFO: Waiting for pod pod-configmaps-ab06f40b-8341-11e9-88d7-16679dc4b203 to disappear
May 31 01:16:11.740: INFO: Pod pod-configmaps-ab06f40b-8341-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:16:11.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-nhg75" for this suite.
May 31 01:16:17.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:16:17.818: INFO: namespace: e2e-tests-configmap-nhg75, resource: bindings, ignored listing per whitelist
May 31 01:16:17.837: INFO: namespace e2e-tests-configmap-nhg75 deletion completed in 6.093680793s

• [SLOW TEST:10.225 seconds]
[sig-api-machinery] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:16:17.838: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-kr69j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-kr69j to expose endpoints map[]
May 31 01:16:17.972: INFO: Get endpoints failed (12.641697ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May 31 01:16:18.976: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-kr69j exposes endpoints map[] (1.015985542s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-kr69j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-kr69j to expose endpoints map[pod1:[100]]
May 31 01:16:22.008: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-kr69j exposes endpoints map[pod1:[100]] (3.025332127s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-kr69j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-kr69j to expose endpoints map[pod1:[100] pod2:[101]]
May 31 01:16:25.050: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-kr69j exposes endpoints map[pod1:[100] pod2:[101]] (3.031063845s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-kr69j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-kr69j to expose endpoints map[pod2:[101]]
May 31 01:16:25.073: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-kr69j exposes endpoints map[pod2:[101]] (17.814276ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-kr69j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-kr69j to expose endpoints map[]
May 31 01:16:26.084: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-kr69j exposes endpoints map[] (1.004572309s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:16:26.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-kr69j" for this suite.
May 31 01:16:48.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:16:48.136: INFO: namespace: e2e-tests-services-kr69j, resource: bindings, ignored listing per whitelist
May 31 01:16:48.199: INFO: namespace e2e-tests-services-kr69j deletion completed in 22.095105898s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

• [SLOW TEST:30.362 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:16:48.199: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 01:16:48.285: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3371aa5-8341-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-tnbqc" to be "success or failure"
May 31 01:16:48.287: INFO: Pod "downwardapi-volume-c3371aa5-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049832ms
May 31 01:16:50.290: INFO: Pod "downwardapi-volume-c3371aa5-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005167703s
May 31 01:16:52.293: INFO: Pod "downwardapi-volume-c3371aa5-8341-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008214544s
STEP: Saw pod success
May 31 01:16:52.293: INFO: Pod "downwardapi-volume-c3371aa5-8341-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:16:52.295: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-c3371aa5-8341-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 01:16:52.319: INFO: Waiting for pod downwardapi-volume-c3371aa5-8341-11e9-88d7-16679dc4b203 to disappear
May 31 01:16:52.321: INFO: Pod downwardapi-volume-c3371aa5-8341-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:16:52.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tnbqc" for this suite.
May 31 01:16:58.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:16:58.369: INFO: namespace: e2e-tests-projected-tnbqc, resource: bindings, ignored listing per whitelist
May 31 01:16:58.411: INFO: namespace e2e-tests-projected-tnbqc deletion completed in 6.086431729s

• [SLOW TEST:10.211 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:16:58.411: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 31 01:16:58.499: INFO: Waiting up to 5m0s for pod "pod-c94d87f2-8341-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-mvc87" to be "success or failure"
May 31 01:16:58.505: INFO: Pod "pod-c94d87f2-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.621186ms
May 31 01:17:00.521: INFO: Pod "pod-c94d87f2-8341-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021977112s
May 31 01:17:02.527: INFO: Pod "pod-c94d87f2-8341-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028326056s
STEP: Saw pod success
May 31 01:17:02.527: INFO: Pod "pod-c94d87f2-8341-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:17:02.537: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-c94d87f2-8341-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 01:17:02.554: INFO: Waiting for pod pod-c94d87f2-8341-11e9-88d7-16679dc4b203 to disappear
May 31 01:17:02.557: INFO: Pod pod-c94d87f2-8341-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:17:02.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mvc87" for this suite.
May 31 01:17:08.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:17:08.630: INFO: namespace: e2e-tests-emptydir-mvc87, resource: bindings, ignored listing per whitelist
May 31 01:17:08.648: INFO: namespace e2e-tests-emptydir-mvc87 deletion completed in 6.086736289s

• [SLOW TEST:10.237 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:17:08.650: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 31 01:17:08.744: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:17:08.747: INFO: Number of nodes with available pods: 0
May 31 01:17:08.748: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 01:17:09.752: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:17:09.755: INFO: Number of nodes with available pods: 0
May 31 01:17:09.755: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 01:17:10.751: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:17:10.757: INFO: Number of nodes with available pods: 0
May 31 01:17:10.757: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 01:17:11.752: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:17:11.755: INFO: Number of nodes with available pods: 3
May 31 01:17:11.755: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 31 01:17:11.768: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:17:11.772: INFO: Number of nodes with available pods: 3
May 31 01:17:11.772: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-n8nvd, will wait for the garbage collector to delete the pods
May 31 01:17:12.838: INFO: Deleting {extensions DaemonSet} daemon-set took: 4.385867ms
May 31 01:17:12.938: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.237023ms
May 31 01:17:55.740: INFO: Number of nodes with available pods: 0
May 31 01:17:55.740: INFO: Number of running nodes: 0, number of available pods: 0
May 31 01:17:55.742: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-n8nvd/daemonsets","resourceVersion":"16409"},"items":null}

May 31 01:17:55.744: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-n8nvd/pods","resourceVersion":"16409"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:17:55.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-n8nvd" for this suite.
May 31 01:18:01.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:18:01.838: INFO: namespace: e2e-tests-daemonsets-n8nvd, resource: bindings, ignored listing per whitelist
May 31 01:18:01.849: INFO: namespace e2e-tests-daemonsets-n8nvd deletion completed in 6.090265072s

• [SLOW TEST:53.199 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:18:01.850: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
May 31 01:18:05.949: INFO: Pod pod-hostip-ef1d483a-8341-11e9-88d7-16679dc4b203 has hostIP: 10.240.0.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:18:05.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-d558x" for this suite.
May 31 01:18:27.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:18:28.049: INFO: namespace: e2e-tests-pods-d558x, resource: bindings, ignored listing per whitelist
May 31 01:18:28.049: INFO: namespace e2e-tests-pods-d558x deletion completed in 22.096744967s

• [SLOW TEST:26.200 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:18:28.052: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-sq77k
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-sq77k
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-sq77k
May 31 01:18:28.149: INFO: Found 0 stateful pods, waiting for 1
May 31 01:18:38.153: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 31 01:18:38.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 01:18:38.420: INFO: stderr: ""
May 31 01:18:38.420: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 01:18:38.420: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 01:18:38.424: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 31 01:18:48.427: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 01:18:48.427: INFO: Waiting for statefulset status.replicas updated to 0
May 31 01:18:48.437: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:18:48.437: INFO: ss-0  k8s-linuxpool-10610181-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  }]
May 31 01:18:48.437: INFO: 
May 31 01:18:48.437: INFO: StatefulSet ss has not reached scale 3, at 1
May 31 01:18:49.441: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99750308s
May 31 01:18:50.446: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993463642s
May 31 01:18:51.450: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988958804s
May 31 01:18:52.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985206983s
May 31 01:18:53.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981478967s
May 31 01:18:54.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977832759s
May 31 01:18:55.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974205756s
May 31 01:18:56.468: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970997366s
May 31 01:18:57.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 967.196472ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-sq77k
May 31 01:18:58.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:18:58.755: INFO: stderr: ""
May 31 01:18:58.755: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 01:18:58.755: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 01:18:58.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:18:59.024: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
May 31 01:18:59.024: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 01:18:59.024: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 01:18:59.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:18:59.293: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
May 31 01:18:59.293: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 01:18:59.293: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 01:18:59.297: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 31 01:19:09.301: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 01:19:09.301: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 01:19:09.301: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 31 01:19:09.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 01:19:09.562: INFO: stderr: ""
May 31 01:19:09.562: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 01:19:09.562: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 01:19:09.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 01:19:09.851: INFO: stderr: ""
May 31 01:19:09.851: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 01:19:09.851: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 01:19:09.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 01:19:10.128: INFO: stderr: ""
May 31 01:19:10.129: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 01:19:10.129: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 01:19:10.129: INFO: Waiting for statefulset status.replicas updated to 0
May 31 01:19:10.131: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 31 01:19:20.137: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 01:19:20.137: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 31 01:19:20.137: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 31 01:19:20.147: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:20.147: INFO: ss-0  k8s-linuxpool-10610181-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  }]
May 31 01:19:20.147: INFO: ss-1  k8s-linuxpool-10610181-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:20.147: INFO: ss-2  k8s-linuxpool-10610181-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:20.147: INFO: 
May 31 01:19:20.147: INFO: StatefulSet ss has not reached scale 0, at 3
May 31 01:19:21.153: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:21.153: INFO: ss-0  k8s-linuxpool-10610181-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  }]
May 31 01:19:21.153: INFO: ss-1  k8s-linuxpool-10610181-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:21.153: INFO: ss-2  k8s-linuxpool-10610181-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:21.153: INFO: 
May 31 01:19:21.153: INFO: StatefulSet ss has not reached scale 0, at 3
May 31 01:19:22.157: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:22.157: INFO: ss-0  k8s-linuxpool-10610181-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  }]
May 31 01:19:22.157: INFO: ss-1  k8s-linuxpool-10610181-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:22.157: INFO: ss-2  k8s-linuxpool-10610181-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:22.157: INFO: 
May 31 01:19:22.157: INFO: StatefulSet ss has not reached scale 0, at 3
May 31 01:19:23.160: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:23.160: INFO: ss-0  k8s-linuxpool-10610181-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  }]
May 31 01:19:23.160: INFO: ss-1  k8s-linuxpool-10610181-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:23.160: INFO: ss-2  k8s-linuxpool-10610181-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:23.160: INFO: 
May 31 01:19:23.160: INFO: StatefulSet ss has not reached scale 0, at 3
May 31 01:19:24.165: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:24.165: INFO: ss-0  k8s-linuxpool-10610181-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  }]
May 31 01:19:24.165: INFO: ss-2  k8s-linuxpool-10610181-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:24.165: INFO: 
May 31 01:19:24.165: INFO: StatefulSet ss has not reached scale 0, at 2
May 31 01:19:25.168: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:25.168: INFO: ss-0  k8s-linuxpool-10610181-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:28 +0000 UTC  }]
May 31 01:19:25.168: INFO: ss-2  k8s-linuxpool-10610181-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:25.168: INFO: 
May 31 01:19:25.168: INFO: StatefulSet ss has not reached scale 0, at 2
May 31 01:19:26.171: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:26.171: INFO: ss-2  k8s-linuxpool-10610181-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:26.171: INFO: 
May 31 01:19:26.171: INFO: StatefulSet ss has not reached scale 0, at 1
May 31 01:19:27.175: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:27.175: INFO: ss-2  k8s-linuxpool-10610181-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:27.175: INFO: 
May 31 01:19:27.175: INFO: StatefulSet ss has not reached scale 0, at 1
May 31 01:19:28.178: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:28.178: INFO: ss-2  k8s-linuxpool-10610181-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:28.178: INFO: 
May 31 01:19:28.178: INFO: StatefulSet ss has not reached scale 0, at 1
May 31 01:19:29.181: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 01:19:29.181: INFO: ss-2  k8s-linuxpool-10610181-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:19:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:18:48 +0000 UTC  }]
May 31 01:19:29.181: INFO: 
May 31 01:19:29.181: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-sq77k
May 31 01:19:30.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:19:30.348: INFO: rc: 1
May 31 01:19:30.348: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc420e30360 exit status 1 <nil> <nil> true [0xc42143f5a8 0xc42143f600 0xc42143f638] [0xc42143f5a8 0xc42143f600 0xc42143f638] [0xc42143f5d0 0xc42143f628] [0x8fd520 0x8fd520] 0xc4216339e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 31 01:19:40.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:19:40.466: INFO: rc: 1
May 31 01:19:40.466: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42201c510 exit status 1 <nil> <nil> true [0xc420be60a0 0xc420be60e8 0xc420be6148] [0xc420be60a0 0xc420be60e8 0xc420be6148] [0xc420be60c8 0xc420be6120] [0x8fd520 0x8fd520] 0xc4220e01e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:19:50.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:19:50.574: INFO: rc: 1
May 31 01:19:50.574: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42201cab0 exit status 1 <nil> <nil> true [0xc420be6150 0xc420be6168 0xc420be6180] [0xc420be6150 0xc420be6168 0xc420be6180] [0xc420be6160 0xc420be6178] [0x8fd520 0x8fd520] 0xc4220e0360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:20:00.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:20:00.682: INFO: rc: 1
May 31 01:20:00.682: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420e30840 exit status 1 <nil> <nil> true [0xc42143f650 0xc42143f698 0xc42143f6c8] [0xc42143f650 0xc42143f698 0xc42143f6c8] [0xc42143f688 0xc42143f6b8] [0x8fd520 0x8fd520] 0xc421633b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:20:10.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:20:10.783: INFO: rc: 1
May 31 01:20:10.783: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42201cfc0 exit status 1 <nil> <nil> true [0xc420be6188 0xc420be61a0 0xc420be61b8] [0xc420be6188 0xc420be61a0 0xc420be61b8] [0xc420be6198 0xc420be61b0] [0x8fd520 0x8fd520] 0xc4220e0540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:20:20.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:20:20.910: INFO: rc: 1
May 31 01:20:20.910: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420e30db0 exit status 1 <nil> <nil> true [0xc42143f6f8 0xc42143f730 0xc42143f778] [0xc42143f6f8 0xc42143f730 0xc42143f778] [0xc42143f718 0xc42143f768] [0x8fd520 0x8fd520] 0xc421633da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:20:30.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:20:31.008: INFO: rc: 1
May 31 01:20:31.008: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421ab44e0 exit status 1 <nil> <nil> true [0xc420362028 0xc420362088 0xc420362190] [0xc420362028 0xc420362088 0xc420362190] [0xc420362070 0xc420362118] [0x8fd520 0x8fd520] 0xc420eee180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:20:41.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:20:41.119: INFO: rc: 1
May 31 01:20:41.119: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df4270 exit status 1 <nil> <nil> true [0xc42023e0b0 0xc42023e180 0xc42023e1f8] [0xc42023e0b0 0xc42023e180 0xc42023e1f8] [0xc42023e158 0xc42023e1a0] [0x8fd520 0x8fd520] 0xc4220400c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:20:51.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:20:51.231: INFO: rc: 1
May 31 01:20:51.231: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421ab4930 exit status 1 <nil> <nil> true [0xc420362238 0xc420362470 0xc4203624b0] [0xc420362238 0xc420362470 0xc4203624b0] [0xc420362430 0xc4203624a8] [0x8fd520 0x8fd520] 0xc420eee300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:21:01.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:21:01.335: INFO: rc: 1
May 31 01:21:01.335: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df4690 exit status 1 <nil> <nil> true [0xc42023e210 0xc42023e2f0 0xc42023fa98] [0xc42023e210 0xc42023e2f0 0xc42023fa98] [0xc42023e2e8 0xc42023fa68] [0x8fd520 0x8fd520] 0xc422040360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:21:11.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:21:11.445: INFO: rc: 1
May 31 01:21:11.445: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421ab4de0 exit status 1 <nil> <nil> true [0xc420362528 0xc420362558 0xc4203626d8] [0xc420362528 0xc420362558 0xc4203626d8] [0xc420362540 0xc420362680] [0x8fd520 0x8fd520] 0xc420eee4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:21:21.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:21:21.549: INFO: rc: 1
May 31 01:21:21.549: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df4d80 exit status 1 <nil> <nil> true [0xc42023fab0 0xc42023fb68 0xc42023fbf0] [0xc42023fab0 0xc42023fb68 0xc42023fbf0] [0xc42023fb38 0xc42023fbc8] [0x8fd520 0x8fd520] 0xc4220405a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:21:31.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:21:31.666: INFO: rc: 1
May 31 01:21:31.666: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df5320 exit status 1 <nil> <nil> true [0xc42023fbf8 0xc42023fc70 0xc42023fd48] [0xc42023fbf8 0xc42023fc70 0xc42023fd48] [0xc42023fc60 0xc42023fd10] [0x8fd520 0x8fd520] 0xc4220408a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:21:41.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:21:41.763: INFO: rc: 1
May 31 01:21:41.763: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421ab52c0 exit status 1 <nil> <nil> true [0xc4203626e8 0xc420362750 0xc4203627b0] [0xc4203626e8 0xc420362750 0xc4203627b0] [0xc420362720 0xc4203627a8] [0x8fd520 0x8fd520] 0xc420eee660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:21:51.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:21:51.871: INFO: rc: 1
May 31 01:21:51.871: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df5860 exit status 1 <nil> <nil> true [0xc42023fd80 0xc42023fe38 0xc42122c0e8] [0xc42023fd80 0xc42023fe38 0xc42122c0e8] [0xc42023fe00 0xc42122c0e0] [0x8fd520 0x8fd520] 0xc422040c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:22:01.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:22:01.978: INFO: rc: 1
May 31 01:22:01.978: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df5ce0 exit status 1 <nil> <nil> true [0xc42122c130 0xc42122c238 0xc42122c2c0] [0xc42122c130 0xc42122c238 0xc42122c2c0] [0xc42122c228 0xc42122c290] [0x8fd520 0x8fd520] 0xc422040fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:22:11.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:22:12.084: INFO: rc: 1
May 31 01:22:12.084: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421ab5800 exit status 1 <nil> <nil> true [0xc4203627d0 0xc420362808 0xc420362898] [0xc4203627d0 0xc420362808 0xc420362898] [0xc420362800 0xc420362858] [0x8fd520 0x8fd520] 0xc420eee7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:22:22.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:22:22.204: INFO: rc: 1
May 31 01:22:22.204: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421ab5ef0 exit status 1 <nil> <nil> true [0xc420362920 0xc4203629a0 0xc420362a88] [0xc420362920 0xc4203629a0 0xc420362a88] [0xc420362988 0xc420362a20] [0x8fd520 0x8fd520] 0xc420eee900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:22:32.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:22:32.319: INFO: rc: 1
May 31 01:22:32.319: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df42a0 exit status 1 <nil> <nil> true [0xc42023e0b0 0xc42023e180 0xc42023e1f8] [0xc42023e0b0 0xc42023e180 0xc42023e1f8] [0xc42023e158 0xc42023e1a0] [0x8fd520 0x8fd520] 0xc4220400c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:22:42.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:22:42.468: INFO: rc: 1
May 31 01:22:42.468: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df46f0 exit status 1 <nil> <nil> true [0xc42023e210 0xc42023e2f0 0xc42023fa98] [0xc42023e210 0xc42023e2f0 0xc42023fa98] [0xc42023e2e8 0xc42023fa68] [0x8fd520 0x8fd520] 0xc422040360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:22:52.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:22:52.588: INFO: rc: 1
May 31 01:22:52.588: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df4f30 exit status 1 <nil> <nil> true [0xc42023fab0 0xc42023fb68 0xc42023fbf0] [0xc42023fab0 0xc42023fb68 0xc42023fbf0] [0xc42023fb38 0xc42023fbc8] [0x8fd520 0x8fd520] 0xc4220405a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:23:02.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:23:02.708: INFO: rc: 1
May 31 01:23:02.708: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df53e0 exit status 1 <nil> <nil> true [0xc42023fbf8 0xc42023fc70 0xc42023fd48] [0xc42023fbf8 0xc42023fc70 0xc42023fd48] [0xc42023fc60 0xc42023fd10] [0x8fd520 0x8fd520] 0xc4220408a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:23:12.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:23:12.822: INFO: rc: 1
May 31 01:23:12.822: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421ab4510 exit status 1 <nil> <nil> true [0xc42122c090 0xc42122c130 0xc42122c238] [0xc42122c090 0xc42122c130 0xc42122c238] [0xc42122c0e8 0xc42122c228] [0x8fd520 0x8fd520] 0xc420eee180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:23:22.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:23:22.929: INFO: rc: 1
May 31 01:23:22.929: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df59e0 exit status 1 <nil> <nil> true [0xc42023fd80 0xc42023fe38 0xc420362070] [0xc42023fd80 0xc42023fe38 0xc420362070] [0xc42023fe00 0xc420362068] [0x8fd520 0x8fd520] 0xc422040c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:23:32.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:23:33.035: INFO: rc: 1
May 31 01:23:33.035: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421ab4990 exit status 1 <nil> <nil> true [0xc42122c270 0xc42122c370 0xc42122c5b0] [0xc42122c270 0xc42122c370 0xc42122c5b0] [0xc42122c2c0 0xc42122c5a0] [0x8fd520 0x8fd520] 0xc420eee300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:23:43.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:23:43.150: INFO: rc: 1
May 31 01:23:43.150: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc421ab4ea0 exit status 1 <nil> <nil> true [0xc42122c600 0xc42122c920 0xc42122c9a0] [0xc42122c600 0xc42122c920 0xc42122c9a0] [0xc42122c808 0xc42122c990] [0x8fd520 0x8fd520] 0xc420eee4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:23:53.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:23:53.266: INFO: rc: 1
May 31 01:23:53.266: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df5f80 exit status 1 <nil> <nil> true [0xc420362088 0xc420362190 0xc420362430] [0xc420362088 0xc420362190 0xc420362430] [0xc420362118 0xc420362418] [0x8fd520 0x8fd520] 0xc422040fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:24:03.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:24:03.379: INFO: rc: 1
May 31 01:24:03.379: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42217c540 exit status 1 <nil> <nil> true [0xc420362470 0xc4203624b0 0xc420362540] [0xc420362470 0xc4203624b0 0xc420362540] [0xc4203624a8 0xc420362538] [0x8fd520 0x8fd520] 0xc422041140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:24:13.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:24:13.485: INFO: rc: 1
May 31 01:24:13.485: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc42217ca80 exit status 1 <nil> <nil> true [0xc420362558 0xc4203626d8 0xc420362720] [0xc420362558 0xc4203626d8 0xc420362720] [0xc420362680 0xc420362708] [0x8fd520 0x8fd520] 0xc4220413e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:24:23.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:24:23.596: INFO: rc: 1
May 31 01:24:23.596: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc420df4270 exit status 1 <nil> <nil> true [0xc42023e110 0xc42023e190 0xc42023e210] [0xc42023e110 0xc42023e190 0xc42023e210] [0xc42023e180 0xc42023e1f8] [0x8fd520 0x8fd520] 0xc420eee180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 01:24:33.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 exec --namespace=e2e-tests-statefulset-sq77k ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 01:24:33.717: INFO: rc: 1
May 31 01:24:33.717: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
May 31 01:24:33.717: INFO: Scaling statefulset ss to 0
May 31 01:24:33.724: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 01:24:33.726: INFO: Deleting all statefulset in ns e2e-tests-statefulset-sq77k
May 31 01:24:33.728: INFO: Scaling statefulset ss to 0
May 31 01:24:33.734: INFO: Waiting for statefulset status.replicas updated to 0
May 31 01:24:33.736: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:24:33.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-sq77k" for this suite.
May 31 01:24:39.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:24:39.803: INFO: namespace: e2e-tests-statefulset-sq77k, resource: bindings, ignored listing per whitelist
May 31 01:24:39.845: INFO: namespace e2e-tests-statefulset-sq77k deletion completed in 6.097723856s

• [SLOW TEST:371.793 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:24:39.848: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-dc57e423-8342-11e9-88d7-16679dc4b203
May 31 01:24:39.940: INFO: Pod name my-hostname-basic-dc57e423-8342-11e9-88d7-16679dc4b203: Found 0 pods out of 1
May 31 01:24:44.943: INFO: Pod name my-hostname-basic-dc57e423-8342-11e9-88d7-16679dc4b203: Found 1 pods out of 1
May 31 01:24:44.943: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dc57e423-8342-11e9-88d7-16679dc4b203" are running
May 31 01:24:44.945: INFO: Pod "my-hostname-basic-dc57e423-8342-11e9-88d7-16679dc4b203-464m8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 01:24:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 01:24:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 01:24:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 01:24:39 +0000 UTC Reason: Message:}])
May 31 01:24:44.945: INFO: Trying to dial the pod
May 31 01:24:49.954: INFO: Controller my-hostname-basic-dc57e423-8342-11e9-88d7-16679dc4b203: Got expected result from replica 1 [my-hostname-basic-dc57e423-8342-11e9-88d7-16679dc4b203-464m8]: "my-hostname-basic-dc57e423-8342-11e9-88d7-16679dc4b203-464m8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:24:49.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-g9hzc" for this suite.
May 31 01:24:55.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:24:56.017: INFO: namespace: e2e-tests-replication-controller-g9hzc, resource: bindings, ignored listing per whitelist
May 31 01:24:56.060: INFO: namespace e2e-tests-replication-controller-g9hzc deletion completed in 6.103505947s

• [SLOW TEST:16.212 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:24:56.062: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-e6015c7f-8342-11e9-88d7-16679dc4b203
STEP: Creating secret with name secret-projected-all-test-volume-e6015c6e-8342-11e9-88d7-16679dc4b203
STEP: Creating a pod to test Check all projections for projected volume plugin
May 31 01:24:56.158: INFO: Waiting up to 5m0s for pod "projected-volume-e6015c42-8342-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-qrksf" to be "success or failure"
May 31 01:24:56.160: INFO: Pod "projected-volume-e6015c42-8342-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.584534ms
May 31 01:24:58.164: INFO: Pod "projected-volume-e6015c42-8342-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005849218s
May 31 01:25:00.169: INFO: Pod "projected-volume-e6015c42-8342-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011325519s
STEP: Saw pod success
May 31 01:25:00.169: INFO: Pod "projected-volume-e6015c42-8342-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:25:00.175: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod projected-volume-e6015c42-8342-11e9-88d7-16679dc4b203 container projected-all-volume-test: <nil>
STEP: delete the pod
May 31 01:25:00.191: INFO: Waiting for pod projected-volume-e6015c42-8342-11e9-88d7-16679dc4b203 to disappear
May 31 01:25:00.194: INFO: Pod projected-volume-e6015c42-8342-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:25:00.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qrksf" for this suite.
May 31 01:25:06.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:25:06.239: INFO: namespace: e2e-tests-projected-qrksf, resource: bindings, ignored listing per whitelist
May 31 01:25:06.297: INFO: namespace e2e-tests-projected-qrksf deletion completed in 6.099942426s

• [SLOW TEST:10.235 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:25:06.298: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
May 31 01:25:06.451: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-npjn9" to be "success or failure"
May 31 01:25:06.454: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127941ms
May 31 01:25:08.457: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006116663s
May 31 01:25:10.460: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008775071s
STEP: Saw pod success
May 31 01:25:10.460: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 31 01:25:10.463: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 31 01:25:10.480: INFO: Waiting for pod pod-host-path-test to disappear
May 31 01:25:10.482: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:25:10.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-npjn9" for this suite.
May 31 01:25:16.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:25:16.550: INFO: namespace: e2e-tests-hostpath-npjn9, resource: bindings, ignored listing per whitelist
May 31 01:25:16.568: INFO: namespace e2e-tests-hostpath-npjn9 deletion completed in 6.083132234s

• [SLOW TEST:10.271 seconds]
[sig-storage] HostPath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:25:16.569: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 01:25:16.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f23a1604-8342-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-tnnxp" to be "success or failure"
May 31 01:25:16.662: INFO: Pod "downwardapi-volume-f23a1604-8342-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 7.448997ms
May 31 01:25:18.665: INFO: Pod "downwardapi-volume-f23a1604-8342-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010546566s
May 31 01:25:20.668: INFO: Pod "downwardapi-volume-f23a1604-8342-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013882727s
STEP: Saw pod success
May 31 01:25:20.668: INFO: Pod "downwardapi-volume-f23a1604-8342-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:25:20.671: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod downwardapi-volume-f23a1604-8342-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 01:25:20.685: INFO: Waiting for pod downwardapi-volume-f23a1604-8342-11e9-88d7-16679dc4b203 to disappear
May 31 01:25:20.687: INFO: Pod downwardapi-volume-f23a1604-8342-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:25:20.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tnnxp" for this suite.
May 31 01:25:26.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:25:26.774: INFO: namespace: e2e-tests-projected-tnnxp, resource: bindings, ignored listing per whitelist
May 31 01:25:26.776: INFO: namespace e2e-tests-projected-tnnxp deletion completed in 6.0856671s

• [SLOW TEST:10.207 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:25:26.777: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 01:25:26.873: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8515e09-8342-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-jxjkn" to be "success or failure"
May 31 01:25:26.878: INFO: Pod "downwardapi-volume-f8515e09-8342-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042165ms
May 31 01:25:28.882: INFO: Pod "downwardapi-volume-f8515e09-8342-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008331282s
May 31 01:25:30.885: INFO: Pod "downwardapi-volume-f8515e09-8342-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011301483s
STEP: Saw pod success
May 31 01:25:30.885: INFO: Pod "downwardapi-volume-f8515e09-8342-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:25:30.887: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod downwardapi-volume-f8515e09-8342-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 01:25:30.906: INFO: Waiting for pod downwardapi-volume-f8515e09-8342-11e9-88d7-16679dc4b203 to disappear
May 31 01:25:30.912: INFO: Pod downwardapi-volume-f8515e09-8342-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:25:30.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jxjkn" for this suite.
May 31 01:25:36.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:25:36.988: INFO: namespace: e2e-tests-downward-api-jxjkn, resource: bindings, ignored listing per whitelist
May 31 01:25:37.007: INFO: namespace e2e-tests-downward-api-jxjkn deletion completed in 6.092267122s

• [SLOW TEST:10.230 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:25:37.007: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 01:25:37.092: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 31 01:25:37.102: INFO: Pod name sample-pod: Found 0 pods out of 1
May 31 01:25:42.105: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 31 01:25:42.105: INFO: Creating deployment "test-rolling-update-deployment"
May 31 01:25:42.109: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 31 01:25:42.119: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 31 01:25:44.125: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 31 01:25:44.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694862742, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694862742, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694862742, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694862742, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 01:25:46.130: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 01:25:46.137: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-vq7lz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vq7lz/deployments/test-rolling-update-deployment,UID:0166f4b2-8343-11e9-b3f2-001dd80c0014,ResourceVersion:17549,Generation:1,CreationTimestamp:2019-05-31 01:25:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-31 01:25:42 +0000 UTC 2019-05-31 01:25:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-31 01:25:44 +0000 UTC 2019-05-31 01:25:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 31 01:25:46.139: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-vq7lz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vq7lz/replicasets/test-rolling-update-deployment-65b7695dcf,UID:016991f2-8343-11e9-b3f2-001dd80c0014,ResourceVersion:17540,Generation:1,CreationTimestamp:2019-05-31 01:25:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0166f4b2-8343-11e9-b3f2-001dd80c0014 0xc4214c81a7 0xc4214c81a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 31 01:25:46.139: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 31 01:25:46.139: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-vq7lz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vq7lz/replicasets/test-rolling-update-controller,UID:fe6a0929-8342-11e9-b3f2-001dd80c0014,ResourceVersion:17548,Generation:2,CreationTimestamp:2019-05-31 01:25:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0166f4b2-8343-11e9-b3f2-001dd80c0014 0xc4214c806e 0xc4214c806f}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 01:25:46.142: INFO: Pod "test-rolling-update-deployment-65b7695dcf-mxxj5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-mxxj5,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-vq7lz,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vq7lz/pods/test-rolling-update-deployment-65b7695dcf-mxxj5,UID:016a4990-8343-11e9-b3f2-001dd80c0014,ResourceVersion:17539,Generation:0,CreationTimestamp:2019-05-31 01:25:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf 016991f2-8343-11e9-b3f2-001dd80c0014 0xc4214c8af7 0xc4214c8af8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8pv2c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8pv2c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8pv2c true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-10610181-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4214c8b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4214c8b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:25:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:25:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:25:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 01:25:42 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.3.158,StartTime:2019-05-31 01:25:42 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-31 01:25:44 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://13e364792019696e8fb23e95e4c709372ef9a46e3f267608cf675a75c7b6833e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:25:46.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-vq7lz" for this suite.
May 31 01:25:52.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:25:52.171: INFO: namespace: e2e-tests-deployment-vq7lz, resource: bindings, ignored listing per whitelist
May 31 01:25:52.244: INFO: namespace e2e-tests-deployment-vq7lz deletion completed in 6.098933069s

• [SLOW TEST:15.237 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:25:52.250: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 01:25:52.344: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
May 31 01:25:52.349: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-jhxlw/daemonsets","resourceVersion":"17592"},"items":null}

May 31 01:25:52.351: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-jhxlw/pods","resourceVersion":"17592"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:25:52.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-jhxlw" for this suite.
May 31 01:25:58.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:25:58.432: INFO: namespace: e2e-tests-daemonsets-jhxlw, resource: bindings, ignored listing per whitelist
May 31 01:25:58.466: INFO: namespace e2e-tests-daemonsets-jhxlw deletion completed in 6.10326443s

S [SKIPPING] [6.217 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  May 31 01:25:52.344: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:25:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 01:25:58.549: INFO: (0) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 5.714274ms)
May 31 01:25:58.553: INFO: (1) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.286143ms)
May 31 01:25:58.557: INFO: (2) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.304855ms)
May 31 01:25:58.563: INFO: (3) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 5.628972ms)
May 31 01:25:58.566: INFO: (4) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.13414ms)
May 31 01:25:58.569: INFO: (5) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.038739ms)
May 31 01:25:58.573: INFO: (6) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.331443ms)
May 31 01:25:58.579: INFO: (7) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 6.623286ms)
May 31 01:25:58.582: INFO: (8) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.024839ms)
May 31 01:25:58.588: INFO: (9) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 5.100865ms)
May 31 01:25:58.591: INFO: (10) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.513545ms)
May 31 01:25:58.595: INFO: (11) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.599746ms)
May 31 01:25:58.598: INFO: (12) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.165741ms)
May 31 01:25:58.602: INFO: (13) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.450045ms)
May 31 01:25:58.614: INFO: (14) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 12.018455ms)
May 31 01:25:58.618: INFO: (15) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.212641ms)
May 31 01:25:58.621: INFO: (16) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.580047ms)
May 31 01:25:58.626: INFO: (17) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.211755ms)
May 31 01:25:58.629: INFO: (18) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.300743ms)
May 31 01:25:58.633: INFO: (19) /api/v1/nodes/k8s-linuxpool-10610181-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.375744ms)
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:25:58.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-hbkpk" for this suite.
May 31 01:26:04.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:26:04.659: INFO: namespace: e2e-tests-proxy-hbkpk, resource: bindings, ignored listing per whitelist
May 31 01:26:04.725: INFO: namespace e2e-tests-proxy-hbkpk deletion completed in 6.086872923s

• [SLOW TEST:6.259 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:26:04.728: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-qb5l
STEP: Creating a pod to test atomic-volume-subpath
May 31 01:26:04.822: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qb5l" in namespace "e2e-tests-subpath-7644j" to be "success or failure"
May 31 01:26:04.830: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Pending", Reason="", readiness=false. Elapsed: 7.251594ms
May 31 01:26:06.833: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010943121s
May 31 01:26:08.836: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 4.014082732s
May 31 01:26:10.840: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 6.017229533s
May 31 01:26:12.843: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 8.020575327s
May 31 01:26:14.846: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 10.023681808s
May 31 01:26:16.849: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 12.026738579s
May 31 01:26:18.852: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 14.029235133s
May 31 01:26:20.855: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 16.032923693s
May 31 01:26:22.858: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 18.036145337s
May 31 01:26:24.862: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 20.039410572s
May 31 01:26:26.865: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Running", Reason="", readiness=false. Elapsed: 22.042698298s
May 31 01:26:28.870: INFO: Pod "pod-subpath-test-configmap-qb5l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.048029141s
STEP: Saw pod success
May 31 01:26:28.870: INFO: Pod "pod-subpath-test-configmap-qb5l" satisfied condition "success or failure"
May 31 01:26:28.873: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-subpath-test-configmap-qb5l container test-container-subpath-configmap-qb5l: <nil>
STEP: delete the pod
May 31 01:26:28.887: INFO: Waiting for pod pod-subpath-test-configmap-qb5l to disappear
May 31 01:26:28.889: INFO: Pod pod-subpath-test-configmap-qb5l no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qb5l
May 31 01:26:28.890: INFO: Deleting pod "pod-subpath-test-configmap-qb5l" in namespace "e2e-tests-subpath-7644j"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:26:28.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-7644j" for this suite.
May 31 01:26:34.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:26:34.915: INFO: namespace: e2e-tests-subpath-7644j, resource: bindings, ignored listing per whitelist
May 31 01:26:34.986: INFO: namespace e2e-tests-subpath-7644j deletion completed in 6.09157774s

• [SLOW TEST:30.259 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:26:34.986: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 01:26:35.076: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20f80b71-8343-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-7fsc4" to be "success or failure"
May 31 01:26:35.080: INFO: Pod "downwardapi-volume-20f80b71-8343-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247154ms
May 31 01:26:37.083: INFO: Pod "downwardapi-volume-20f80b71-8343-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007552432s
May 31 01:26:39.087: INFO: Pod "downwardapi-volume-20f80b71-8343-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011145905s
STEP: Saw pod success
May 31 01:26:39.087: INFO: Pod "downwardapi-volume-20f80b71-8343-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:26:39.089: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-20f80b71-8343-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 01:26:39.105: INFO: Waiting for pod downwardapi-volume-20f80b71-8343-11e9-88d7-16679dc4b203 to disappear
May 31 01:26:39.108: INFO: Pod downwardapi-volume-20f80b71-8343-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:26:39.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7fsc4" for this suite.
May 31 01:26:45.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:26:45.135: INFO: namespace: e2e-tests-downward-api-7fsc4, resource: bindings, ignored listing per whitelist
May 31 01:26:45.198: INFO: namespace e2e-tests-downward-api-7fsc4 deletion completed in 6.086435831s

• [SLOW TEST:10.211 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:26:45.198: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-cd6ck
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 01:26:45.346: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 31 01:27:09.418: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.244.3.161:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-cd6ck PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:27:09.418: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:27:09.566: INFO: Found all expected endpoints: [netserver-0]
May 31 01:27:09.568: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.244.2.29:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-cd6ck PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:27:09.568: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:27:09.720: INFO: Found all expected endpoints: [netserver-1]
May 31 01:27:09.723: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.244.0.109:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-cd6ck PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:27:09.723: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:27:09.872: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:27:09.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-cd6ck" for this suite.
May 31 01:27:31.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:27:31.915: INFO: namespace: e2e-tests-pod-network-test-cd6ck, resource: bindings, ignored listing per whitelist
May 31 01:27:31.987: INFO: namespace e2e-tests-pod-network-test-cd6ck deletion completed in 22.111367228s

• [SLOW TEST:46.789 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:27:31.989: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-42f32410-8343-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 01:27:32.091: INFO: Waiting up to 5m0s for pod "pod-configmaps-42f3a6f0-8343-11e9-88d7-16679dc4b203" in namespace "e2e-tests-configmap-qz4bj" to be "success or failure"
May 31 01:27:32.103: INFO: Pod "pod-configmaps-42f3a6f0-8343-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 11.967852ms
May 31 01:27:34.105: INFO: Pod "pod-configmaps-42f3a6f0-8343-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014616672s
May 31 01:27:36.108: INFO: Pod "pod-configmaps-42f3a6f0-8343-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01776159s
STEP: Saw pod success
May 31 01:27:36.108: INFO: Pod "pod-configmaps-42f3a6f0-8343-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:27:36.111: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-configmaps-42f3a6f0-8343-11e9-88d7-16679dc4b203 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 01:27:36.127: INFO: Waiting for pod pod-configmaps-42f3a6f0-8343-11e9-88d7-16679dc4b203 to disappear
May 31 01:27:36.129: INFO: Pod pod-configmaps-42f3a6f0-8343-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:27:36.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-qz4bj" for this suite.
May 31 01:27:42.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:27:42.175: INFO: namespace: e2e-tests-configmap-qz4bj, resource: bindings, ignored listing per whitelist
May 31 01:27:42.227: INFO: namespace e2e-tests-configmap-qz4bj deletion completed in 6.09507649s

• [SLOW TEST:10.238 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:27:42.230: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-490d8410-8343-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 01:27:42.327: INFO: Waiting up to 5m0s for pod "pod-configmaps-490e0725-8343-11e9-88d7-16679dc4b203" in namespace "e2e-tests-configmap-jjblk" to be "success or failure"
May 31 01:27:42.335: INFO: Pod "pod-configmaps-490e0725-8343-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 7.765498ms
May 31 01:27:44.338: INFO: Pod "pod-configmaps-490e0725-8343-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010319675s
May 31 01:27:46.341: INFO: Pod "pod-configmaps-490e0725-8343-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01336615s
STEP: Saw pod success
May 31 01:27:46.341: INFO: Pod "pod-configmaps-490e0725-8343-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:27:46.343: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-configmaps-490e0725-8343-11e9-88d7-16679dc4b203 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 01:27:46.360: INFO: Waiting for pod pod-configmaps-490e0725-8343-11e9-88d7-16679dc4b203 to disappear
May 31 01:27:46.362: INFO: Pod pod-configmaps-490e0725-8343-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:27:46.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jjblk" for this suite.
May 31 01:27:52.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:27:52.413: INFO: namespace: e2e-tests-configmap-jjblk, resource: bindings, ignored listing per whitelist
May 31 01:27:52.456: INFO: namespace e2e-tests-configmap-jjblk deletion completed in 6.090162301s

• [SLOW TEST:10.226 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:27:52.456: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:28:52.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-t9lrm" for this suite.
May 31 01:29:14.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:29:14.595: INFO: namespace: e2e-tests-container-probe-t9lrm, resource: bindings, ignored listing per whitelist
May 31 01:29:14.635: INFO: namespace e2e-tests-container-probe-t9lrm deletion completed in 22.090321805s

• [SLOW TEST:82.179 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:29:14.635: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
May 31 01:29:14.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-lvbw4'
May 31 01:29:15.380: INFO: stderr: ""
May 31 01:29:15.380: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
May 31 01:29:16.383: INFO: Selector matched 1 pods for map[app:redis]
May 31 01:29:16.383: INFO: Found 0 / 1
May 31 01:29:17.383: INFO: Selector matched 1 pods for map[app:redis]
May 31 01:29:17.383: INFO: Found 0 / 1
May 31 01:29:18.384: INFO: Selector matched 1 pods for map[app:redis]
May 31 01:29:18.384: INFO: Found 1 / 1
May 31 01:29:18.384: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 31 01:29:18.387: INFO: Selector matched 1 pods for map[app:redis]
May 31 01:29:18.387: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 31 01:29:18.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 logs redis-master-xdlpl redis-master --namespace=e2e-tests-kubectl-lvbw4'
May 31 01:29:18.507: INFO: stderr: ""
May 31 01:29:18.507: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 31 May 01:29:17.350 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 May 01:29:17.350 # Server started, Redis version 3.2.12\n1:M 31 May 01:29:17.350 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 May 01:29:17.350 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 31 01:29:18.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 log redis-master-xdlpl redis-master --namespace=e2e-tests-kubectl-lvbw4 --tail=1'
May 31 01:29:18.627: INFO: stderr: ""
May 31 01:29:18.627: INFO: stdout: "1:M 31 May 01:29:17.350 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 31 01:29:18.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 log redis-master-xdlpl redis-master --namespace=e2e-tests-kubectl-lvbw4 --limit-bytes=1'
May 31 01:29:18.757: INFO: stderr: ""
May 31 01:29:18.757: INFO: stdout: " "
STEP: exposing timestamps
May 31 01:29:18.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 log redis-master-xdlpl redis-master --namespace=e2e-tests-kubectl-lvbw4 --tail=1 --timestamps'
May 31 01:29:18.866: INFO: stderr: ""
May 31 01:29:18.866: INFO: stdout: "2019-05-31T01:29:17.350657284Z 1:M 31 May 01:29:17.350 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 31 01:29:21.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 log redis-master-xdlpl redis-master --namespace=e2e-tests-kubectl-lvbw4 --since=1s'
May 31 01:29:21.501: INFO: stderr: ""
May 31 01:29:21.501: INFO: stdout: ""
May 31 01:29:21.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 log redis-master-xdlpl redis-master --namespace=e2e-tests-kubectl-lvbw4 --since=24h'
May 31 01:29:21.626: INFO: stderr: ""
May 31 01:29:21.626: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 31 May 01:29:17.350 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 May 01:29:17.350 # Server started, Redis version 3.2.12\n1:M 31 May 01:29:17.350 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 May 01:29:17.350 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
May 31 01:29:21.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-lvbw4'
May 31 01:29:21.740: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 01:29:21.740: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 31 01:29:21.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-lvbw4'
May 31 01:29:21.857: INFO: stderr: "No resources found.\n"
May 31 01:29:21.857: INFO: stdout: ""
May 31 01:29:21.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -l name=nginx --namespace=e2e-tests-kubectl-lvbw4 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 01:29:21.972: INFO: stderr: ""
May 31 01:29:21.972: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:29:21.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lvbw4" for this suite.
May 31 01:29:27.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:29:28.059: INFO: namespace: e2e-tests-kubectl-lvbw4, resource: bindings, ignored listing per whitelist
May 31 01:29:28.068: INFO: namespace e2e-tests-kubectl-lvbw4 deletion completed in 6.092523866s

• [SLOW TEST:13.433 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:29:28.071: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
May 31 01:29:28.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 api-versions'
May 31 01:29:28.277: INFO: stderr: ""
May 31 01:29:28.277: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:29:28.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-44c8q" for this suite.
May 31 01:29:34.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:29:34.333: INFO: namespace: e2e-tests-kubectl-44c8q, resource: bindings, ignored listing per whitelist
May 31 01:29:34.427: INFO: namespace e2e-tests-kubectl-44c8q deletion completed in 6.146267673s

• [SLOW TEST:6.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:29:34.427: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 01:29:34.513: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 31 01:29:34.525: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:34.527: INFO: Number of nodes with available pods: 0
May 31 01:29:34.527: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 01:29:35.531: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:35.534: INFO: Number of nodes with available pods: 0
May 31 01:29:35.534: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 01:29:36.531: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:36.533: INFO: Number of nodes with available pods: 0
May 31 01:29:36.533: INFO: Node k8s-linuxpool-10610181-0 is running more than one daemon pod
May 31 01:29:37.530: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:37.533: INFO: Number of nodes with available pods: 3
May 31 01:29:37.533: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 31 01:29:37.552: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:37.552: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:37.552: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:37.558: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:38.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:38.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:38.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:38.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:39.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:39.561: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:39.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:39.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:40.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:40.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:40.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:40.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:41.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:41.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:41.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:41.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:42.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:42.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:42.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:42.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:43.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:43.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:43.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:43.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:44.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:44.561: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:44.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:44.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:45.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:45.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:45.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:45.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:46.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:46.561: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:46.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:46.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:47.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:47.561: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:47.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:47.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:48.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:48.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:48.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:48.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:49.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:49.561: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:49.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:49.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:50.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:50.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:50.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:50.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:51.563: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:51.563: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:51.563: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:51.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:52.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:52.561: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:52.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:52.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:53.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:53.561: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:53.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:53.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:54.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:54.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:54.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:54.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:55.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:55.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:55.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:55.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:56.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:56.561: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:56.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:56.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:57.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:57.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:57.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:57.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:58.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:58.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:58.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:58.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:29:59.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:59.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:59.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:29:59.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:00.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:00.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:00.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:00.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:01.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:01.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:01.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:01.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:02.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:02.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:02.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:02.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:03.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:03.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:03.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:03.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:04.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:04.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:04.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:04.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:05.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:05.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:05.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:05.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:06.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:06.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:06.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:06.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:07.563: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:07.563: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:07.563: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:07.567: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:08.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:08.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:08.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:08.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:09.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:09.561: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:09.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:09.572: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:10.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:10.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:10.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:10.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:11.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:11.562: INFO: Wrong image for pod: daemon-set-2sdkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:11.562: INFO: Pod daemon-set-2sdkn is not available
May 31 01:30:11.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:11.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:12.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:12.562: INFO: Pod daemon-set-bdj6f is not available
May 31 01:30:12.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:12.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:13.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:13.561: INFO: Pod daemon-set-bdj6f is not available
May 31 01:30:13.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:13.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:14.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:14.561: INFO: Pod daemon-set-bdj6f is not available
May 31 01:30:14.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:14.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:15.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:15.561: INFO: Pod daemon-set-bdj6f is not available
May 31 01:30:15.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:15.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:16.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:16.561: INFO: Pod daemon-set-bdj6f is not available
May 31 01:30:16.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:16.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:17.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:17.561: INFO: Pod daemon-set-bdj6f is not available
May 31 01:30:17.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:17.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:18.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:18.561: INFO: Pod daemon-set-bdj6f is not available
May 31 01:30:18.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:18.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:19.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:19.562: INFO: Pod daemon-set-bdj6f is not available
May 31 01:30:19.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:19.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:20.566: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:20.566: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:20.569: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:21.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:21.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:21.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:22.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:22.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:22.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:23.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:23.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:23.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:24.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:24.563: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:24.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:25.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:25.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:25.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:26.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:26.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:26.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:27.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:27.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:27.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:28.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:28.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:28.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:29.564: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:29.564: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:29.567: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:30.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:30.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:30.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:31.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:31.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:31.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:32.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:32.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:32.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:33.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:33.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:33.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:34.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:34.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:34.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:35.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:35.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:35.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:36.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:36.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:36.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:37.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:37.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:37.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:38.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:38.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:38.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:39.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:39.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:39.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:40.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:40.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:40.569: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:41.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:41.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:41.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:42.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:42.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:42.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:43.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:43.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:43.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:44.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:44.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:44.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:45.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:45.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:45.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:46.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:46.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:46.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:47.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:47.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:47.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:48.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:48.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:48.567: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:49.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:49.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:49.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:50.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:50.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:50.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:51.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:51.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:51.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:52.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:52.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:52.561: INFO: Pod daemon-set-szhhd is not available
May 31 01:30:52.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:53.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:53.561: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:53.561: INFO: Pod daemon-set-szhhd is not available
May 31 01:30:53.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:54.571: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:54.571: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:54.571: INFO: Pod daemon-set-szhhd is not available
May 31 01:30:54.576: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:55.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:55.562: INFO: Wrong image for pod: daemon-set-szhhd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:55.562: INFO: Pod daemon-set-szhhd is not available
May 31 01:30:55.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:56.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:56.562: INFO: Pod daemon-set-wpbbx is not available
May 31 01:30:56.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:57.564: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:57.564: INFO: Pod daemon-set-wpbbx is not available
May 31 01:30:57.567: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:58.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:58.562: INFO: Pod daemon-set-wpbbx is not available
May 31 01:30:58.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:30:59.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:30:59.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:00.594: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:00.598: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:01.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:01.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:02.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:02.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:03.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:03.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:04.622: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:04.627: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:05.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:05.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:06.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:06.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:07.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:07.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:08.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:08.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:09.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:09.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:10.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:10.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:11.563: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:11.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:12.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:12.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:13.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:13.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:14.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:14.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:15.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:15.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:16.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:16.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:17.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:17.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:18.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:18.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:19.563: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:19.567: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:20.568: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:20.578: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:21.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:21.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:22.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:22.568: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:23.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:23.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:24.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:24.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:25.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:25.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:26.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:26.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:27.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:27.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:28.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:28.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:29.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:29.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:30.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:30.561: INFO: Pod daemon-set-2msjb is not available
May 31 01:31:30.566: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:31.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:31.561: INFO: Pod daemon-set-2msjb is not available
May 31 01:31:31.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:32.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:32.562: INFO: Pod daemon-set-2msjb is not available
May 31 01:31:32.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:33.562: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:33.562: INFO: Pod daemon-set-2msjb is not available
May 31 01:31:33.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:34.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:34.561: INFO: Pod daemon-set-2msjb is not available
May 31 01:31:34.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:35.561: INFO: Wrong image for pod: daemon-set-2msjb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
May 31 01:31:35.562: INFO: Pod daemon-set-2msjb is not available
May 31 01:31:35.565: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:36.561: INFO: Pod daemon-set-lsbnn is not available
May 31 01:31:36.564: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 31 01:31:36.568: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:36.571: INFO: Number of nodes with available pods: 2
May 31 01:31:36.571: INFO: Node k8s-linuxpool-10610181-2 is running more than one daemon pod
May 31 01:31:37.574: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:37.576: INFO: Number of nodes with available pods: 2
May 31 01:31:37.576: INFO: Node k8s-linuxpool-10610181-2 is running more than one daemon pod
May 31 01:31:38.575: INFO: DaemonSet pods can't tolerate node k8s-master-10610181-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 01:31:38.577: INFO: Number of nodes with available pods: 3
May 31 01:31:38.577: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-fv7xn, will wait for the garbage collector to delete the pods
May 31 01:31:38.647: INFO: Deleting {extensions DaemonSet} daemon-set took: 6.085675ms
May 31 01:31:38.747: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.203331ms
May 31 01:31:51.050: INFO: Number of nodes with available pods: 0
May 31 01:31:51.050: INFO: Number of running nodes: 0, number of available pods: 0
May 31 01:31:51.053: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-fv7xn/daemonsets","resourceVersion":"18563"},"items":null}

May 31 01:31:51.055: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-fv7xn/pods","resourceVersion":"18563"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:31:51.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-fv7xn" for this suite.
May 31 01:31:57.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:31:57.139: INFO: namespace: e2e-tests-daemonsets-fv7xn, resource: bindings, ignored listing per whitelist
May 31 01:31:57.198: INFO: namespace e2e-tests-daemonsets-fv7xn deletion completed in 6.130071917s

• [SLOW TEST:142.771 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:31:57.201: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 31 01:32:05.462: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 31 01:32:05.464: INFO: Pod pod-with-poststart-http-hook still exists
May 31 01:32:07.465: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 31 01:32:07.468: INFO: Pod pod-with-poststart-http-hook still exists
May 31 01:32:09.465: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 31 01:32:09.468: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:32:09.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-9nk8w" for this suite.
May 31 01:32:31.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:32:31.513: INFO: namespace: e2e-tests-container-lifecycle-hook-9nk8w, resource: bindings, ignored listing per whitelist
May 31 01:32:31.568: INFO: namespace e2e-tests-container-lifecycle-hook-9nk8w deletion completed in 22.09565411s

• [SLOW TEST:34.368 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:32:31.570: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-mt796
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 01:32:31.709: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 31 01:32:57.785: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.169:8080/dial?request=hostName&protocol=http&host=10.244.0.115&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-mt796 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:32:57.785: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:32:57.946: INFO: Waiting for endpoints: map[]
May 31 01:32:57.948: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.169:8080/dial?request=hostName&protocol=http&host=10.244.2.32&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-mt796 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:32:57.948: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:32:58.100: INFO: Waiting for endpoints: map[]
May 31 01:32:58.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.169:8080/dial?request=hostName&protocol=http&host=10.244.3.168&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-mt796 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 01:32:58.103: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
May 31 01:32:58.270: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:32:58.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-mt796" for this suite.
May 31 01:33:20.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:33:20.339: INFO: namespace: e2e-tests-pod-network-test-mt796, resource: bindings, ignored listing per whitelist
May 31 01:33:20.391: INFO: namespace e2e-tests-pod-network-test-mt796 deletion completed in 22.117544463s

• [SLOW TEST:48.821 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:33:20.392: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 31 01:33:20.488: INFO: Waiting up to 5m0s for pod "downward-api-129cf65f-8344-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-v88fk" to be "success or failure"
May 31 01:33:20.491: INFO: Pod "downward-api-129cf65f-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.446742ms
May 31 01:33:22.496: INFO: Pod "downward-api-129cf65f-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008369442s
May 31 01:33:24.499: INFO: Pod "downward-api-129cf65f-8344-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011313815s
STEP: Saw pod success
May 31 01:33:24.499: INFO: Pod "downward-api-129cf65f-8344-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:33:24.501: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downward-api-129cf65f-8344-11e9-88d7-16679dc4b203 container dapi-container: <nil>
STEP: delete the pod
May 31 01:33:24.519: INFO: Waiting for pod downward-api-129cf65f-8344-11e9-88d7-16679dc4b203 to disappear
May 31 01:33:24.527: INFO: Pod downward-api-129cf65f-8344-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:33:24.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v88fk" for this suite.
May 31 01:33:30.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:33:30.574: INFO: namespace: e2e-tests-downward-api-v88fk, resource: bindings, ignored listing per whitelist
May 31 01:33:30.639: INFO: namespace e2e-tests-downward-api-v88fk deletion completed in 6.108302401s

• [SLOW TEST:10.247 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:33:30.639: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 31 01:33:30.736: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tmn4q,SelfLink:/api/v1/namespaces/e2e-tests-watch-tmn4q/configmaps/e2e-watch-test-label-changed,UID:18b81f86-8344-11e9-b3f2-001dd80c0014,ResourceVersion:18921,Generation:0,CreationTimestamp:2019-05-31 01:33:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 01:33:30.736: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tmn4q,SelfLink:/api/v1/namespaces/e2e-tests-watch-tmn4q/configmaps/e2e-watch-test-label-changed,UID:18b81f86-8344-11e9-b3f2-001dd80c0014,ResourceVersion:18922,Generation:0,CreationTimestamp:2019-05-31 01:33:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 31 01:33:30.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tmn4q,SelfLink:/api/v1/namespaces/e2e-tests-watch-tmn4q/configmaps/e2e-watch-test-label-changed,UID:18b81f86-8344-11e9-b3f2-001dd80c0014,ResourceVersion:18923,Generation:0,CreationTimestamp:2019-05-31 01:33:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 31 01:33:40.756: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tmn4q,SelfLink:/api/v1/namespaces/e2e-tests-watch-tmn4q/configmaps/e2e-watch-test-label-changed,UID:18b81f86-8344-11e9-b3f2-001dd80c0014,ResourceVersion:18940,Generation:0,CreationTimestamp:2019-05-31 01:33:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 01:33:40.756: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tmn4q,SelfLink:/api/v1/namespaces/e2e-tests-watch-tmn4q/configmaps/e2e-watch-test-label-changed,UID:18b81f86-8344-11e9-b3f2-001dd80c0014,ResourceVersion:18941,Generation:0,CreationTimestamp:2019-05-31 01:33:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 31 01:33:40.756: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-tmn4q,SelfLink:/api/v1/namespaces/e2e-tests-watch-tmn4q/configmaps/e2e-watch-test-label-changed,UID:18b81f86-8344-11e9-b3f2-001dd80c0014,ResourceVersion:18942,Generation:0,CreationTimestamp:2019-05-31 01:33:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:33:40.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-tmn4q" for this suite.
May 31 01:33:46.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:33:46.787: INFO: namespace: e2e-tests-watch-tmn4q, resource: bindings, ignored listing per whitelist
May 31 01:33:46.866: INFO: namespace e2e-tests-watch-tmn4q deletion completed in 6.10747999s

• [SLOW TEST:16.227 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:33:46.867: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
May 31 01:33:46.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 version --client'
May 31 01:33:47.011: INFO: stderr: ""
May 31 01:33:47.012: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.1\", GitCommit:\"4ed3216f3ec431b140b1d899130a69fc671678f4\", GitTreeState:\"clean\", BuildDate:\"2018-10-05T16:46:06Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 31 01:33:47.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-qdzg4'
May 31 01:33:47.266: INFO: stderr: ""
May 31 01:33:47.266: INFO: stdout: "replicationcontroller/redis-master created\n"
May 31 01:33:47.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-qdzg4'
May 31 01:33:47.522: INFO: stderr: ""
May 31 01:33:47.522: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 31 01:33:48.525: INFO: Selector matched 1 pods for map[app:redis]
May 31 01:33:48.525: INFO: Found 0 / 1
May 31 01:33:49.525: INFO: Selector matched 1 pods for map[app:redis]
May 31 01:33:49.525: INFO: Found 0 / 1
May 31 01:33:50.526: INFO: Selector matched 1 pods for map[app:redis]
May 31 01:33:50.526: INFO: Found 1 / 1
May 31 01:33:50.526: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 31 01:33:50.528: INFO: Selector matched 1 pods for map[app:redis]
May 31 01:33:50.528: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 31 01:33:50.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 describe pod redis-master-x5jtc --namespace=e2e-tests-kubectl-qdzg4'
May 31 01:33:50.669: INFO: stderr: ""
May 31 01:33:50.669: INFO: stdout: "Name:               redis-master-x5jtc\nNamespace:          e2e-tests-kubectl-qdzg4\nPriority:           0\nPriorityClassName:  <none>\nNode:               k8s-linuxpool-10610181-0/10.240.0.5\nStart Time:         Fri, 31 May 2019 01:33:47 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.244.3.172\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://27d8d66d981d5eb4396b3fda43255341a87f7419a28143d54477dc2d140128a3\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 31 May 2019 01:33:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rld6c (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rld6c:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rld6c\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                               Message\n  ----    ------     ----  ----                               -------\n  Normal  Scheduled  3s    default-scheduler                  Successfully assigned e2e-tests-kubectl-qdzg4/redis-master-x5jtc to k8s-linuxpool-10610181-0\n  Normal  Pulled     2s    kubelet, k8s-linuxpool-10610181-0  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, k8s-linuxpool-10610181-0  Created container\n  Normal  Started    1s    kubelet, k8s-linuxpool-10610181-0  Started container\n"
May 31 01:33:50.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 describe rc redis-master --namespace=e2e-tests-kubectl-qdzg4'
May 31 01:33:50.805: INFO: stderr: ""
May 31 01:33:50.805: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-qdzg4\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-x5jtc\n"
May 31 01:33:50.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 describe service redis-master --namespace=e2e-tests-kubectl-qdzg4'
May 31 01:33:50.921: INFO: stderr: ""
May 31 01:33:50.921: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-qdzg4\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.93.127\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.3.172:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 31 01:33:50.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 describe node k8s-linuxpool-10610181-0'
May 31 01:33:51.075: INFO: stderr: ""
May 31 01:33:51.076: INFO: stdout: "Name:               k8s-linuxpool-10610181-0\nRoles:              agent\nLabels:             agentpool=linuxpool\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=redmond\n                    failure-domain.beta.kubernetes.io/zone=1\n                    kubernetes.azure.com/cluster=rgk8s-68826\n                    kubernetes.io/hostname=k8s-linuxpool-10610181-0\n                    kubernetes.io/role=agent\n                    node-role.kubernetes.io/agent=\n                    storageprofile=managed\n                    storagetier=Standard_LRS\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"c6:d0:5a:ca:93:6b\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.240.0.5\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 31 May 2019 00:09:01 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  OutOfDisk        False   Fri, 31 May 2019 01:33:41 +0000   Fri, 31 May 2019 00:09:01 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure   False   Fri, 31 May 2019 01:33:41 +0000   Fri, 31 May 2019 00:09:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 31 May 2019 01:33:41 +0000   Fri, 31 May 2019 00:09:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 31 May 2019 01:33:41 +0000   Fri, 31 May 2019 00:09:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 31 May 2019 01:33:41 +0000   Fri, 31 May 2019 00:09:43 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.240.0.5\n  Hostname:    k8s-linuxpool-10610181-0\nCapacity:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              203234980Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7137156Ki\n pods:                           110\nAllocatable:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              187301357258\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         6369156Ki\n pods:                           110\nSystem Info:\n Machine ID:                 5585093538e84c848261917a11ad1fc4\n System UUID:                EE69E98D-C209-3A4A-8B0B-A0C0E3400870\n Boot ID:                    f1fcaab1-596d-40ce-92c8-80865cf3a601\n Kernel Version:             4.15.0-1022-azure\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://3.0.5\n Kubelet Version:            v1.12.8\n Kube-Proxy Version:         v1.12.8\nPodCIDR:                     10.244.3.0/24\nProviderID:                  azure:///subscriptions/751df60b-8fe7-44ff-b2e7-b3a118855ea4/resourceGroups/rgk8s-68826/providers/Microsoft.Compute/virtualMachines/k8s-linuxpool-10610181-0\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------\n  e2e-tests-kubectl-qdzg4    redis-master-x5jtc                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-nnw4d    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                azure-ip-masq-agent-pxdpd                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (4%)\n  kube-system                kube-flannel-ds-lq7s2                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                kube-proxy-xdfzd                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests   Limits\n  --------                       --------   ------\n  cpu                            150m (7%)  50m (2%)\n  memory                         50Mi (0%)  250Mi (4%)\n  attachable-volumes-azure-disk  0          0\nEvents:                          <none>\n"
May 31 01:33:51.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 describe namespace e2e-tests-kubectl-qdzg4'
May 31 01:33:51.216: INFO: stderr: ""
May 31 01:33:51.216: INFO: stdout: "Name:         e2e-tests-kubectl-qdzg4\nLabels:       e2e-framework=kubectl\n              e2e-run=b34b421a-8338-11e9-88d7-16679dc4b203\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:33:51.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qdzg4" for this suite.
May 31 01:34:13.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:34:13.284: INFO: namespace: e2e-tests-kubectl-qdzg4, resource: bindings, ignored listing per whitelist
May 31 01:34:13.305: INFO: namespace e2e-tests-kubectl-qdzg4 deletion completed in 22.086403192s

• [SLOW TEST:26.439 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:34:13.306: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-nxh4
STEP: Creating a pod to test atomic-volume-subpath
May 31 01:34:13.392: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nxh4" in namespace "e2e-tests-subpath-548v6" to be "success or failure"
May 31 01:34:13.398: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.700069ms
May 31 01:34:15.401: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008571239s
May 31 01:34:17.404: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 4.01186391s
May 31 01:34:19.415: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 6.023096674s
May 31 01:34:21.419: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 8.026613641s
May 31 01:34:23.422: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 10.030045603s
May 31 01:34:25.426: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 12.033206057s
May 31 01:34:27.429: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 14.036752213s
May 31 01:34:29.432: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 16.03982956s
May 31 01:34:31.436: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 18.043355508s
May 31 01:34:33.439: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 20.04671155s
May 31 01:34:35.442: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Running", Reason="", readiness=false. Elapsed: 22.049871387s
May 31 01:34:37.445: INFO: Pod "pod-subpath-test-configmap-nxh4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.052582714s
STEP: Saw pod success
May 31 01:34:37.445: INFO: Pod "pod-subpath-test-configmap-nxh4" satisfied condition "success or failure"
May 31 01:34:37.447: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-subpath-test-configmap-nxh4 container test-container-subpath-configmap-nxh4: <nil>
STEP: delete the pod
May 31 01:34:37.522: INFO: Waiting for pod pod-subpath-test-configmap-nxh4 to disappear
May 31 01:34:37.524: INFO: Pod pod-subpath-test-configmap-nxh4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nxh4
May 31 01:34:37.524: INFO: Deleting pod "pod-subpath-test-configmap-nxh4" in namespace "e2e-tests-subpath-548v6"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:34:37.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-548v6" for this suite.
May 31 01:34:43.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:34:43.623: INFO: namespace: e2e-tests-subpath-548v6, resource: bindings, ignored listing per whitelist
May 31 01:34:43.635: INFO: namespace e2e-tests-subpath-548v6 deletion completed in 6.101137885s

• [SLOW TEST:30.330 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:34:43.636: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 31 01:34:43.722: INFO: Waiting up to 5m0s for pod "downward-api-44397cf8-8344-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-jtdw4" to be "success or failure"
May 31 01:34:43.727: INFO: Pod "downward-api-44397cf8-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.223063ms
May 31 01:34:45.730: INFO: Pod "downward-api-44397cf8-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008632884s
May 31 01:34:47.734: INFO: Pod "downward-api-44397cf8-8344-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012154503s
STEP: Saw pod success
May 31 01:34:47.734: INFO: Pod "downward-api-44397cf8-8344-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:34:47.737: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downward-api-44397cf8-8344-11e9-88d7-16679dc4b203 container dapi-container: <nil>
STEP: delete the pod
May 31 01:34:47.757: INFO: Waiting for pod downward-api-44397cf8-8344-11e9-88d7-16679dc4b203 to disappear
May 31 01:34:47.759: INFO: Pod downward-api-44397cf8-8344-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:34:47.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jtdw4" for this suite.
May 31 01:34:53.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:34:53.855: INFO: namespace: e2e-tests-downward-api-jtdw4, resource: bindings, ignored listing per whitelist
May 31 01:34:53.859: INFO: namespace e2e-tests-downward-api-jtdw4 deletion completed in 6.095508961s

• [SLOW TEST:10.223 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:34:53.859: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
May 31 01:34:53.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 create -f - --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:34:54.204: INFO: stderr: ""
May 31 01:34:54.204: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 01:34:54.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:34:54.339: INFO: stderr: ""
May 31 01:34:54.339: INFO: stdout: "update-demo-nautilus-gknc8 update-demo-nautilus-x9fnk "
May 31 01:34:54.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-gknc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:34:54.469: INFO: stderr: ""
May 31 01:34:54.469: INFO: stdout: ""
May 31 01:34:54.469: INFO: update-demo-nautilus-gknc8 is created but not running
May 31 01:34:59.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:34:59.582: INFO: stderr: ""
May 31 01:34:59.582: INFO: stdout: "update-demo-nautilus-gknc8 update-demo-nautilus-x9fnk "
May 31 01:34:59.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-gknc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:34:59.703: INFO: stderr: ""
May 31 01:34:59.703: INFO: stdout: "true"
May 31 01:34:59.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-gknc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:34:59.818: INFO: stderr: ""
May 31 01:34:59.818: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 01:34:59.818: INFO: validating pod update-demo-nautilus-gknc8
May 31 01:34:59.833: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 01:34:59.833: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 01:34:59.833: INFO: update-demo-nautilus-gknc8 is verified up and running
May 31 01:34:59.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-x9fnk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:34:59.952: INFO: stderr: ""
May 31 01:34:59.952: INFO: stdout: "true"
May 31 01:34:59.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods update-demo-nautilus-x9fnk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:35:00.056: INFO: stderr: ""
May 31 01:35:00.056: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 01:35:00.056: INFO: validating pod update-demo-nautilus-x9fnk
May 31 01:35:00.060: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 01:35:00.060: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 01:35:00.060: INFO: update-demo-nautilus-x9fnk is verified up and running
STEP: using delete to clean up resources
May 31 01:35:00.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:35:00.176: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 01:35:00.176: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 31 01:35:00.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-ctl8g'
May 31 01:35:00.334: INFO: stderr: "No resources found.\n"
May 31 01:35:00.334: INFO: stdout: ""
May 31 01:35:00.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 get pods -l name=update-demo --namespace=e2e-tests-kubectl-ctl8g -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 01:35:00.466: INFO: stderr: ""
May 31 01:35:00.466: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:35:00.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ctl8g" for this suite.
May 31 01:35:22.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:35:22.558: INFO: namespace: e2e-tests-kubectl-ctl8g, resource: bindings, ignored listing per whitelist
May 31 01:35:22.560: INFO: namespace e2e-tests-kubectl-ctl8g deletion completed in 22.091026465s

• [SLOW TEST:28.701 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:35:22.560: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-5b6d48dc-8344-11e9-88d7-16679dc4b203
STEP: Creating a pod to test consume configMaps
May 31 01:35:22.653: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5b6dc9c6-8344-11e9-88d7-16679dc4b203" in namespace "e2e-tests-projected-jvrdp" to be "success or failure"
May 31 01:35:22.659: INFO: Pod "pod-projected-configmaps-5b6dc9c6-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903171ms
May 31 01:35:24.661: INFO: Pod "pod-projected-configmaps-5b6dc9c6-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008646317s
May 31 01:35:26.665: INFO: Pod "pod-projected-configmaps-5b6dc9c6-8344-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011953267s
STEP: Saw pod success
May 31 01:35:26.665: INFO: Pod "pod-projected-configmaps-5b6dc9c6-8344-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:35:26.667: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-projected-configmaps-5b6dc9c6-8344-11e9-88d7-16679dc4b203 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 01:35:26.685: INFO: Waiting for pod pod-projected-configmaps-5b6dc9c6-8344-11e9-88d7-16679dc4b203 to disappear
May 31 01:35:26.687: INFO: Pod pod-projected-configmaps-5b6dc9c6-8344-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:35:26.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jvrdp" for this suite.
May 31 01:35:32.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:35:32.790: INFO: namespace: e2e-tests-projected-jvrdp, resource: bindings, ignored listing per whitelist
May 31 01:35:32.798: INFO: namespace e2e-tests-projected-jvrdp deletion completed in 6.105859385s

• [SLOW TEST:10.237 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:35:32.798: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
May 31 01:35:32.949: INFO: Waiting up to 5m0s for pod "client-containers-61917ac1-8344-11e9-88d7-16679dc4b203" in namespace "e2e-tests-containers-qs9td" to be "success or failure"
May 31 01:35:32.951: INFO: Pod "client-containers-61917ac1-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 1.741621ms
May 31 01:35:34.954: INFO: Pod "client-containers-61917ac1-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004757953s
May 31 01:35:36.957: INFO: Pod "client-containers-61917ac1-8344-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007658481s
STEP: Saw pod success
May 31 01:35:36.957: INFO: Pod "client-containers-61917ac1-8344-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:35:36.960: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod client-containers-61917ac1-8344-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 01:35:36.979: INFO: Waiting for pod client-containers-61917ac1-8344-11e9-88d7-16679dc4b203 to disappear
May 31 01:35:36.984: INFO: Pod client-containers-61917ac1-8344-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:35:36.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-qs9td" for this suite.
May 31 01:35:42.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:35:43.037: INFO: namespace: e2e-tests-containers-qs9td, resource: bindings, ignored listing per whitelist
May 31 01:35:43.076: INFO: namespace e2e-tests-containers-qs9td deletion completed in 6.088310122s

• [SLOW TEST:10.278 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:35:43.076: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
May 31 01:35:43.162: INFO: Waiting up to 5m0s for pod "pod-67a7e350-8344-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-2ngft" to be "success or failure"
May 31 01:35:43.171: INFO: Pod "pod-67a7e350-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 8.461502ms
May 31 01:35:45.174: INFO: Pod "pod-67a7e350-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011816322s
May 31 01:35:47.177: INFO: Pod "pod-67a7e350-8344-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014581532s
STEP: Saw pod success
May 31 01:35:47.177: INFO: Pod "pod-67a7e350-8344-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:35:47.179: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-67a7e350-8344-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 01:35:47.198: INFO: Waiting for pod pod-67a7e350-8344-11e9-88d7-16679dc4b203 to disappear
May 31 01:35:47.201: INFO: Pod pod-67a7e350-8344-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:35:47.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2ngft" for this suite.
May 31 01:35:53.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:35:53.295: INFO: namespace: e2e-tests-emptydir-2ngft, resource: bindings, ignored listing per whitelist
May 31 01:35:53.295: INFO: namespace e2e-tests-emptydir-2ngft deletion completed in 6.090852504s

• [SLOW TEST:10.219 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:35:53.297: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
May 31 01:35:53.385: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 01:35:53.412: INFO: Waiting for terminating namespaces to be deleted...
May 31 01:35:53.414: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-10610181-0 before test
May 31 01:35:53.419: INFO: azure-ip-masq-agent-pxdpd from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.419: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 01:35:53.420: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-31 00:10:47 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.420: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 01:35:53.420: INFO: kube-flannel-ds-lq7s2 from kube-system started at 2019-05-31 00:09:20 +0000 UTC (2 container statuses recorded)
May 31 01:35:53.420: INFO: 	Container install-cni ready: true, restart count 0
May 31 01:35:53.420: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 01:35:53.420: INFO: kube-proxy-xdfzd from kube-system started at 2019-05-31 00:09:27 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.420: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 01:35:53.420: INFO: sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-nnw4d from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 01:35:53.420: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 01:35:53.420: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 31 01:35:53.421: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-10610181-1 before test
May 31 01:35:53.428: INFO: kubernetes-dashboard-9bf969764-rpmqs from kube-system started at 2019-05-31 00:09:43 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.428: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 31 01:35:53.428: INFO: heapster-6f6cbcfcf6-gjcbw from kube-system started at 2019-05-31 00:09:43 +0000 UTC (2 container statuses recorded)
May 31 01:35:53.428: INFO: 	Container heapster ready: true, restart count 0
May 31 01:35:53.428: INFO: 	Container heapster-nanny ready: true, restart count 0
May 31 01:35:53.428: INFO: azure-ip-masq-agent-djqq5 from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.428: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 01:35:53.428: INFO: tiller-deploy-7bfcdc49d6-qklrt from kube-system started at 2019-05-31 00:09:43 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.428: INFO: 	Container tiller ready: true, restart count 0
May 31 01:35:53.428: INFO: sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-8lx44 from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 01:35:53.428: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 01:35:53.428: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 31 01:35:53.428: INFO: kube-flannel-ds-kt9wr from kube-system started at 2019-05-31 00:09:19 +0000 UTC (2 container statuses recorded)
May 31 01:35:53.428: INFO: 	Container install-cni ready: true, restart count 0
May 31 01:35:53.428: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 01:35:53.428: INFO: kube-proxy-656fb from kube-system started at 2019-05-31 00:09:27 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.428: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 01:35:53.428: INFO: metrics-server-67b4964794-lk2mw from kube-system started at 2019-05-31 00:09:43 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.428: INFO: 	Container metrics-server ready: true, restart count 0
May 31 01:35:53.428: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-10610181-2 before test
May 31 01:35:53.433: INFO: azure-ip-masq-agent-zg4rl from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.433: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 01:35:53.434: INFO: kube-proxy-6fd9h from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.434: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 01:35:53.434: INFO: coredns-69c4fccc6c-hxhd7 from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:35:53.434: INFO: 	Container coredns ready: true, restart count 0
May 31 01:35:53.434: INFO: kube-flannel-ds-9rzz8 from kube-system started at 2019-05-31 00:09:19 +0000 UTC (2 container statuses recorded)
May 31 01:35:53.434: INFO: 	Container install-cni ready: true, restart count 0
May 31 01:35:53.434: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 01:35:53.434: INFO: sonobuoy-e2e-job-32df4bb72ae04a14 from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 01:35:53.434: INFO: 	Container e2e ready: true, restart count 0
May 31 01:35:53.435: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 01:35:53.435: INFO: sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-2mglt from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 01:35:53.435: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 01:35:53.435: INFO: 	Container sonobuoy-worker ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-702e231b-8344-11e9-88d7-16679dc4b203 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-702e231b-8344-11e9-88d7-16679dc4b203 off the node k8s-linuxpool-10610181-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-702e231b-8344-11e9-88d7-16679dc4b203
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:36:01.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-2z46z" for this suite.
May 31 01:36:19.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:36:19.661: INFO: namespace: e2e-tests-sched-pred-2z46z, resource: bindings, ignored listing per whitelist
May 31 01:36:19.672: INFO: namespace e2e-tests-sched-pred-2z46z deletion completed in 18.174427948s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:26.375 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:36:19.672: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-4jpmg
May 31 01:36:23.841: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-4jpmg
STEP: checking the pod's current state and verifying that restartCount is present
May 31 01:36:23.843: INFO: Initial restart count of pod liveness-http is 0
May 31 01:36:35.865: INFO: Restart count of pod e2e-tests-container-probe-4jpmg/liveness-http is now 1 (12.021771425s elapsed)
May 31 01:36:55.912: INFO: Restart count of pod e2e-tests-container-probe-4jpmg/liveness-http is now 2 (32.069059969s elapsed)
May 31 01:37:16.005: INFO: Restart count of pod e2e-tests-container-probe-4jpmg/liveness-http is now 3 (52.161530977s elapsed)
May 31 01:37:36.052: INFO: Restart count of pod e2e-tests-container-probe-4jpmg/liveness-http is now 4 (1m12.208401573s elapsed)
May 31 01:38:48.171: INFO: Restart count of pod e2e-tests-container-probe-4jpmg/liveness-http is now 5 (2m24.327965761s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:38:48.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-4jpmg" for this suite.
May 31 01:38:54.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:38:54.291: INFO: namespace: e2e-tests-container-probe-4jpmg, resource: bindings, ignored listing per whitelist
May 31 01:38:54.335: INFO: namespace e2e-tests-container-probe-4jpmg deletion completed in 6.149517276s

• [SLOW TEST:154.663 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:38:54.336: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:38:54.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-7qqg4" for this suite.
May 31 01:39:16.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:39:16.522: INFO: namespace: e2e-tests-pods-7qqg4, resource: bindings, ignored listing per whitelist
May 31 01:39:16.527: INFO: namespace e2e-tests-pods-7qqg4 deletion completed in 22.089628244s

• [SLOW TEST:22.191 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:39:16.530: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 01:39:16.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-77kxb'
May 31 01:39:17.280: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
May 31 01:39:17.280: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
May 31 01:39:17.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-190461910 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-77kxb'
May 31 01:39:17.403: INFO: stderr: ""
May 31 01:39:17.403: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:39:17.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-77kxb" for this suite.
May 31 01:39:39.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:39:39.494: INFO: namespace: e2e-tests-kubectl-77kxb, resource: bindings, ignored listing per whitelist
May 31 01:39:39.499: INFO: namespace e2e-tests-kubectl-77kxb deletion completed in 22.088636168s

• [SLOW TEST:22.969 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:39:39.499: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
May 31 01:39:39.580: INFO: Waiting up to 5m0s for pod "pod-f49242fd-8344-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-gxfw9" to be "success or failure"
May 31 01:39:39.589: INFO: Pod "pod-f49242fd-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 8.270999ms
May 31 01:39:41.592: INFO: Pod "pod-f49242fd-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011500317s
May 31 01:39:43.595: INFO: Pod "pod-f49242fd-8344-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014537131s
STEP: Saw pod success
May 31 01:39:43.595: INFO: Pod "pod-f49242fd-8344-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:39:43.597: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-f49242fd-8344-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 01:39:43.622: INFO: Waiting for pod pod-f49242fd-8344-11e9-88d7-16679dc4b203 to disappear
May 31 01:39:43.625: INFO: Pod pod-f49242fd-8344-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:39:43.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gxfw9" for this suite.
May 31 01:39:49.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:39:49.679: INFO: namespace: e2e-tests-emptydir-gxfw9, resource: bindings, ignored listing per whitelist
May 31 01:39:49.719: INFO: namespace e2e-tests-emptydir-gxfw9 deletion completed in 6.090309194s

• [SLOW TEST:10.220 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:39:49.719: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 31 01:39:49.801: INFO: Waiting up to 5m0s for pod "pod-faa9ce0f-8344-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-m7bbp" to be "success or failure"
May 31 01:39:49.808: INFO: Pod "pod-faa9ce0f-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 7.463688ms
May 31 01:39:51.811: INFO: Pod "pod-faa9ce0f-8344-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010280592s
May 31 01:39:53.814: INFO: Pod "pod-faa9ce0f-8344-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013006892s
STEP: Saw pod success
May 31 01:39:53.814: INFO: Pod "pod-faa9ce0f-8344-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:39:53.816: INFO: Trying to get logs from node k8s-linuxpool-10610181-2 pod pod-faa9ce0f-8344-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 01:39:53.834: INFO: Waiting for pod pod-faa9ce0f-8344-11e9-88d7-16679dc4b203 to disappear
May 31 01:39:53.836: INFO: Pod pod-faa9ce0f-8344-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:39:53.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-m7bbp" for this suite.
May 31 01:39:59.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:39:59.867: INFO: namespace: e2e-tests-emptydir-m7bbp, resource: bindings, ignored listing per whitelist
May 31 01:39:59.934: INFO: namespace e2e-tests-emptydir-m7bbp deletion completed in 6.093840106s

• [SLOW TEST:10.214 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:39:59.934: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-00c11cb2-8345-11e9-88d7-16679dc4b203
STEP: Creating configMap with name cm-test-opt-upd-00c11ce6-8345-11e9-88d7-16679dc4b203
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-00c11cb2-8345-11e9-88d7-16679dc4b203
STEP: Updating configmap cm-test-opt-upd-00c11ce6-8345-11e9-88d7-16679dc4b203
STEP: Creating configMap with name cm-test-opt-create-00c11cfc-8345-11e9-88d7-16679dc4b203
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:40:08.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-86f5m" for this suite.
May 31 01:40:30.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:40:30.162: INFO: namespace: e2e-tests-projected-86f5m, resource: bindings, ignored listing per whitelist
May 31 01:40:30.232: INFO: namespace e2e-tests-projected-86f5m deletion completed in 22.097311725s

• [SLOW TEST:30.298 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:40:30.233: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
May 31 01:40:34.350: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-12d1aea0-8345-11e9-88d7-16679dc4b203", GenerateName:"", Namespace:"e2e-tests-pods-q6czf", SelfLink:"/api/v1/namespaces/e2e-tests-pods-q6czf/pods/pod-submit-remove-12d1aea0-8345-11e9-88d7-16679dc4b203", UID:"12d30c9d-8345-11e9-b3f2-001dd80c0014", ResourceVersion:"20083", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63694863630, loc:(*time.Location)(0x6c43b60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"322452684"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-2b52v", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc422b4f900), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2b52v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc422fdfa88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-linuxpool-10610181-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc42276bc20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc422fdfad0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc422fdfb20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc422fdfb28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694863630, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694863632, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694863632, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694863630, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.6", PodIP:"10.244.0.122", StartTime:(*v1.Time)(0xc422edb380), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc422edb3a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7", ContainerID:"docker://92c3a199528802670d6ac413c25379bb3f995d3da7ad4d5772faa54c6ba6734e"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:40:45.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-q6czf" for this suite.
May 31 01:40:51.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:40:51.739: INFO: namespace: e2e-tests-pods-q6czf, resource: bindings, ignored listing per whitelist
May 31 01:40:51.797: INFO: namespace e2e-tests-pods-q6czf deletion completed in 6.091259631s

• [SLOW TEST:21.564 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:40:51.797: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
May 31 01:40:51.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1fab2f1f-8345-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-269bs" to be "success or failure"
May 31 01:40:51.888: INFO: Pod "downwardapi-volume-1fab2f1f-8345-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.283239ms
May 31 01:40:53.892: INFO: Pod "downwardapi-volume-1fab2f1f-8345-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006344988s
May 31 01:40:55.895: INFO: Pod "downwardapi-volume-1fab2f1f-8345-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009371134s
STEP: Saw pod success
May 31 01:40:55.895: INFO: Pod "downwardapi-volume-1fab2f1f-8345-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:40:55.897: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downwardapi-volume-1fab2f1f-8345-11e9-88d7-16679dc4b203 container client-container: <nil>
STEP: delete the pod
May 31 01:40:55.928: INFO: Waiting for pod downwardapi-volume-1fab2f1f-8345-11e9-88d7-16679dc4b203 to disappear
May 31 01:40:55.930: INFO: Pod downwardapi-volume-1fab2f1f-8345-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:40:55.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-269bs" for this suite.
May 31 01:41:01.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:41:02.001: INFO: namespace: e2e-tests-downward-api-269bs, resource: bindings, ignored listing per whitelist
May 31 01:41:02.031: INFO: namespace e2e-tests-downward-api-269bs deletion completed in 6.09687587s

• [SLOW TEST:10.234 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:41:02.033: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
May 31 01:41:02.123: INFO: Waiting up to 5m0s for pod "downward-api-25c52e47-8345-11e9-88d7-16679dc4b203" in namespace "e2e-tests-downward-api-kp8dn" to be "success or failure"
May 31 01:41:02.127: INFO: Pod "downward-api-25c52e47-8345-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.543842ms
May 31 01:41:04.130: INFO: Pod "downward-api-25c52e47-8345-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006323679s
May 31 01:41:06.133: INFO: Pod "downward-api-25c52e47-8345-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009557719s
STEP: Saw pod success
May 31 01:41:06.133: INFO: Pod "downward-api-25c52e47-8345-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:41:06.136: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod downward-api-25c52e47-8345-11e9-88d7-16679dc4b203 container dapi-container: <nil>
STEP: delete the pod
May 31 01:41:06.150: INFO: Waiting for pod downward-api-25c52e47-8345-11e9-88d7-16679dc4b203 to disappear
May 31 01:41:06.152: INFO: Pod downward-api-25c52e47-8345-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:41:06.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kp8dn" for this suite.
May 31 01:41:12.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:41:12.188: INFO: namespace: e2e-tests-downward-api-kp8dn, resource: bindings, ignored listing per whitelist
May 31 01:41:12.245: INFO: namespace e2e-tests-downward-api-kp8dn deletion completed in 6.08979476s

• [SLOW TEST:10.212 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:41:12.246: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
May 31 01:41:12.334: INFO: Waiting up to 5m0s for pod "pod-2bdb4db0-8345-11e9-88d7-16679dc4b203" in namespace "e2e-tests-emptydir-hb575" to be "success or failure"
May 31 01:41:12.337: INFO: Pod "pod-2bdb4db0-8345-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.993936ms
May 31 01:41:14.340: INFO: Pod "pod-2bdb4db0-8345-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005658362s
May 31 01:41:16.343: INFO: Pod "pod-2bdb4db0-8345-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008602191s
STEP: Saw pod success
May 31 01:41:16.343: INFO: Pod "pod-2bdb4db0-8345-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:41:16.345: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod pod-2bdb4db0-8345-11e9-88d7-16679dc4b203 container test-container: <nil>
STEP: delete the pod
May 31 01:41:16.359: INFO: Waiting for pod pod-2bdb4db0-8345-11e9-88d7-16679dc4b203 to disappear
May 31 01:41:16.361: INFO: Pod pod-2bdb4db0-8345-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:41:16.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hb575" for this suite.
May 31 01:41:22.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:41:22.438: INFO: namespace: e2e-tests-emptydir-hb575, resource: bindings, ignored listing per whitelist
May 31 01:41:22.455: INFO: namespace e2e-tests-emptydir-hb575 deletion completed in 6.091084949s

• [SLOW TEST:10.209 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:41:22.457: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-22flx.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-22flx.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-22flx.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-22flx.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-22flx.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-22flx.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 01:41:36.609: INFO: DNS probes using e2e-tests-dns-22flx/dns-test-31f11e4d-8345-11e9-88d7-16679dc4b203 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:41:36.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-22flx" for this suite.
May 31 01:41:42.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:41:42.665: INFO: namespace: e2e-tests-dns-22flx, resource: bindings, ignored listing per whitelist
May 31 01:41:42.723: INFO: namespace e2e-tests-dns-22flx deletion completed in 6.098012981s

• [SLOW TEST:20.267 seconds]
[sig-network] DNS
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:41:42.726: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
May 31 01:41:42.823: INFO: Waiting up to 5m0s for pod "var-expansion-3e06021c-8345-11e9-88d7-16679dc4b203" in namespace "e2e-tests-var-expansion-2dghn" to be "success or failure"
May 31 01:41:42.826: INFO: Pod "var-expansion-3e06021c-8345-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.475441ms
May 31 01:41:44.829: INFO: Pod "var-expansion-3e06021c-8345-11e9-88d7-16679dc4b203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006464247s
May 31 01:41:46.833: INFO: Pod "var-expansion-3e06021c-8345-11e9-88d7-16679dc4b203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009900457s
STEP: Saw pod success
May 31 01:41:46.833: INFO: Pod "var-expansion-3e06021c-8345-11e9-88d7-16679dc4b203" satisfied condition "success or failure"
May 31 01:41:46.835: INFO: Trying to get logs from node k8s-linuxpool-10610181-0 pod var-expansion-3e06021c-8345-11e9-88d7-16679dc4b203 container dapi-container: <nil>
STEP: delete the pod
May 31 01:41:46.856: INFO: Waiting for pod var-expansion-3e06021c-8345-11e9-88d7-16679dc4b203 to disappear
May 31 01:41:46.861: INFO: Pod var-expansion-3e06021c-8345-11e9-88d7-16679dc4b203 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:41:46.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-2dghn" for this suite.
May 31 01:41:52.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:41:52.951: INFO: namespace: e2e-tests-var-expansion-2dghn, resource: bindings, ignored listing per whitelist
May 31 01:41:52.965: INFO: namespace e2e-tests-var-expansion-2dghn deletion completed in 6.098032758s

• [SLOW TEST:10.239 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:41:52.965: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
May 31 01:41:53.047: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 01:41:53.054: INFO: Waiting for terminating namespaces to be deleted...
May 31 01:41:53.056: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-10610181-0 before test
May 31 01:41:53.061: INFO: azure-ip-masq-agent-pxdpd from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.061: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 01:41:53.061: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-31 00:10:47 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.061: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 01:41:53.061: INFO: kube-flannel-ds-lq7s2 from kube-system started at 2019-05-31 00:09:20 +0000 UTC (2 container statuses recorded)
May 31 01:41:53.061: INFO: 	Container install-cni ready: true, restart count 0
May 31 01:41:53.061: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 01:41:53.061: INFO: kube-proxy-xdfzd from kube-system started at 2019-05-31 00:09:27 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.061: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 01:41:53.061: INFO: sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-nnw4d from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 01:41:53.061: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 01:41:53.061: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 31 01:41:53.061: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-10610181-1 before test
May 31 01:41:53.068: INFO: heapster-6f6cbcfcf6-gjcbw from kube-system started at 2019-05-31 00:09:43 +0000 UTC (2 container statuses recorded)
May 31 01:41:53.068: INFO: 	Container heapster ready: true, restart count 0
May 31 01:41:53.068: INFO: 	Container heapster-nanny ready: true, restart count 0
May 31 01:41:53.068: INFO: azure-ip-masq-agent-djqq5 from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.068: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 01:41:53.068: INFO: tiller-deploy-7bfcdc49d6-qklrt from kube-system started at 2019-05-31 00:09:43 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.068: INFO: 	Container tiller ready: true, restart count 0
May 31 01:41:53.068: INFO: sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-8lx44 from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 01:41:53.068: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 01:41:53.068: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 31 01:41:53.068: INFO: kube-flannel-ds-kt9wr from kube-system started at 2019-05-31 00:09:19 +0000 UTC (2 container statuses recorded)
May 31 01:41:53.068: INFO: 	Container install-cni ready: true, restart count 0
May 31 01:41:53.068: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 01:41:53.068: INFO: kube-proxy-656fb from kube-system started at 2019-05-31 00:09:27 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.069: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 01:41:53.069: INFO: metrics-server-67b4964794-lk2mw from kube-system started at 2019-05-31 00:09:43 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.069: INFO: 	Container metrics-server ready: true, restart count 0
May 31 01:41:53.069: INFO: kubernetes-dashboard-9bf969764-rpmqs from kube-system started at 2019-05-31 00:09:43 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.069: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 31 01:41:53.069: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-10610181-2 before test
May 31 01:41:53.074: INFO: sonobuoy-systemd-logs-daemon-set-15f4938e227b471d-2mglt from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 01:41:53.074: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 01:41:53.074: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 31 01:41:53.074: INFO: kube-flannel-ds-9rzz8 from kube-system started at 2019-05-31 00:09:19 +0000 UTC (2 container statuses recorded)
May 31 01:41:53.074: INFO: 	Container install-cni ready: true, restart count 0
May 31 01:41:53.074: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 01:41:53.074: INFO: sonobuoy-e2e-job-32df4bb72ae04a14 from heptio-sonobuoy started at 2019-05-31 00:10:56 +0000 UTC (2 container statuses recorded)
May 31 01:41:53.074: INFO: 	Container e2e ready: true, restart count 0
May 31 01:41:53.074: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 01:41:53.074: INFO: azure-ip-masq-agent-zg4rl from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.074: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 01:41:53.074: INFO: kube-proxy-6fd9h from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.074: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 01:41:53.074: INFO: coredns-69c4fccc6c-hxhd7 from kube-system started at 2019-05-31 00:09:26 +0000 UTC (1 container statuses recorded)
May 31 01:41:53.074: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15a3a0ff00e511ac], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:41:54.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-72dxp" for this suite.
May 31 01:42:00.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:42:00.180: INFO: namespace: e2e-tests-sched-pred-72dxp, resource: bindings, ignored listing per whitelist
May 31 01:42:00.187: INFO: namespace e2e-tests-sched-pred-72dxp deletion completed in 6.08871883s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:7.222 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:42:00.187: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
May 31 01:42:00.283: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:42:04.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-xrrgk" for this suite.
May 31 01:42:10.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:42:10.851: INFO: namespace: e2e-tests-init-container-xrrgk, resource: bindings, ignored listing per whitelist
May 31 01:42:10.861: INFO: namespace e2e-tests-init-container-xrrgk deletion completed in 6.110552563s

• [SLOW TEST:10.673 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
May 31 01:42:10.861: INFO: >>> kubeConfig: /tmp/kubeconfig-190461910
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0531 01:42:16.969015      21 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 01:42:16.969: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
May 31 01:42:16.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9bvgp" for this suite.
May 31 01:42:22.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 01:42:23.000: INFO: namespace: e2e-tests-gc-9bvgp, resource: bindings, ignored listing per whitelist
May 31 01:42:23.061: INFO: namespace e2e-tests-gc-9bvgp deletion completed in 6.090409498s

• [SLOW TEST:12.201 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SMay 31 01:42:23.062: INFO: Running AfterSuite actions on all node
May 31 01:42:23.062: INFO: Running AfterSuite actions on node 1
May 31 01:42:23.062: INFO: Skipping dumping logs from cluster

Ran 187 of 1814 Specs in 5425.534 seconds
SUCCESS! -- 187 Passed | 0 Failed | 0 Pending | 1627 Skipped PASS

Ginkgo ran 1 suite in 1h30m27.129848311s
Test Suite Passed
