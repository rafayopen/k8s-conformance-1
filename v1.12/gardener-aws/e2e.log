Conformance test: not doing test setup.
Feb 28 07:23:19.754: INFO: Overriding default scale value of zero to 1
Feb 28 07:23:19.754: INFO: Overriding default milliseconds value of zero to 5000
I0228 07:23:20.381430   29661 e2e.go:304] Starting e2e run "b9164ddf-3b29-11e9-9760-9e5b40196308" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1551338599 - Will randomize all specs
Will run 188 of 2011 specs

Feb 28 07:23:20.655: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:23:20.657: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 28 07:23:20.885: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 28 07:23:21.172: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 28 07:23:21.172: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 28 07:23:21.172: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 28 07:23:21.233: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 28 07:23:21.233: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 28 07:23:21.233: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 28 07:23:21.233: INFO: e2e test version: v1.12.5
Feb 28 07:23:21.286: INFO: kube-apiserver version: v1.12.5
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:23:21.286: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
Feb 28 07:23:23.370: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 28 07:23:23.550: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-8b4tc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-bb9a61a3-3b29-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 07:23:23.976: INFO: Waiting up to 5m0s for pod "pod-secrets-bba2a25d-3b29-11e9-9760-9e5b40196308" in namespace "e2e-tests-secrets-8b4tc" to be "success or failure"
Feb 28 07:23:24.148: INFO: Pod "pod-secrets-bba2a25d-3b29-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 171.048874ms
Feb 28 07:23:26.202: INFO: Pod "pod-secrets-bba2a25d-3b29-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225099048s
Feb 28 07:23:28.256: INFO: Pod "pod-secrets-bba2a25d-3b29-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.279038557s
STEP: Saw pod success
Feb 28 07:23:28.256: INFO: Pod "pod-secrets-bba2a25d-3b29-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:23:28.309: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-secrets-bba2a25d-3b29-11e9-9760-9e5b40196308 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:23:28.560: INFO: Waiting for pod pod-secrets-bba2a25d-3b29-11e9-9760-9e5b40196308 to disappear
Feb 28 07:23:28.613: INFO: Pod pod-secrets-bba2a25d-3b29-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:23:28.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8b4tc" for this suite.
Feb 28 07:23:34.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:23:36.826: INFO: namespace: e2e-tests-secrets-8b4tc, resource: bindings, ignored listing per whitelist
Feb 28 07:23:36.933: INFO: namespace e2e-tests-secrets-8b4tc deletion completed in 8.266057819s

• [SLOW TEST:15.647 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:23:36.934: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-7bd98
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-c4b01e5c-3b29-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 07:23:39.217: INFO: Waiting up to 5m0s for pod "pod-configmaps-c4b84535-3b29-11e9-9760-9e5b40196308" in namespace "e2e-tests-configmap-7bd98" to be "success or failure"
Feb 28 07:23:39.271: INFO: Pod "pod-configmaps-c4b84535-3b29-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.628695ms
Feb 28 07:23:41.325: INFO: Pod "pod-configmaps-c4b84535-3b29-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107788382s
STEP: Saw pod success
Feb 28 07:23:41.325: INFO: Pod "pod-configmaps-c4b84535-3b29-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:23:41.379: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-c4b84535-3b29-11e9-9760-9e5b40196308 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:23:41.497: INFO: Waiting for pod pod-configmaps-c4b84535-3b29-11e9-9760-9e5b40196308 to disappear
Feb 28 07:23:41.552: INFO: Pod pod-configmaps-c4b84535-3b29-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:23:41.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7bd98" for this suite.
Feb 28 07:23:47.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:23:48.195: INFO: namespace: e2e-tests-configmap-7bd98, resource: bindings, ignored listing per whitelist
Feb 28 07:23:49.861: INFO: namespace e2e-tests-configmap-7bd98 deletion completed in 8.254134515s

• [SLOW TEST:12.928 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:23:49.862: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-t5w7p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 07:24:00.473: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:24:00.527: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 07:24:02.527: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:24:02.581: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 07:24:04.527: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:24:04.582: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 07:24:06.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:24:06.583: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 07:24:08.527: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:24:08.583: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 07:24:10.527: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:24:10.581: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 07:24:12.527: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:24:12.581: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 07:24:14.527: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 07:24:14.582: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:24:14.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-t5w7p" for this suite.
Feb 28 07:24:36.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:24:37.885: INFO: namespace: e2e-tests-container-lifecycle-hook-t5w7p, resource: bindings, ignored listing per whitelist
Feb 28 07:24:38.904: INFO: namespace e2e-tests-container-lifecycle-hook-t5w7p deletion completed in 24.205752527s

• [SLOW TEST:49.042 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:24:38.904: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-dbxs4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-dbxs4
Feb 28 07:24:45.244: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-dbxs4
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 07:24:45.297: INFO: Initial restart count of pod liveness-exec is 0
Feb 28 07:25:34.668: INFO: Restart count of pod e2e-tests-container-probe-dbxs4/liveness-exec is now 1 (49.371205339s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:25:34.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-dbxs4" for this suite.
Feb 28 07:25:40.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:25:42.781: INFO: namespace: e2e-tests-container-probe-dbxs4, resource: bindings, ignored listing per whitelist
Feb 28 07:25:42.996: INFO: namespace e2e-tests-container-probe-dbxs4 deletion completed in 8.214440931s

• [SLOW TEST:64.092 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:25:42.997: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-k8792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-k8792/configmap-test-0fd523e7-3b2a-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 07:25:45.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-0fdd52b4-3b2a-11e9-9760-9e5b40196308" in namespace "e2e-tests-configmap-k8792" to be "success or failure"
Feb 28 07:25:45.342: INFO: Pod "pod-configmaps-0fdd52b4-3b2a-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.499744ms
Feb 28 07:25:47.397: INFO: Pod "pod-configmaps-0fdd52b4-3b2a-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108631243s
STEP: Saw pod success
Feb 28 07:25:47.398: INFO: Pod "pod-configmaps-0fdd52b4-3b2a-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:25:47.452: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-0fdd52b4-3b2a-11e9-9760-9e5b40196308 container env-test: <nil>
STEP: delete the pod
Feb 28 07:25:47.568: INFO: Waiting for pod pod-configmaps-0fdd52b4-3b2a-11e9-9760-9e5b40196308 to disappear
Feb 28 07:25:47.622: INFO: Pod pod-configmaps-0fdd52b4-3b2a-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:25:47.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-k8792" for this suite.
Feb 28 07:25:53.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:25:55.248: INFO: namespace: e2e-tests-configmap-k8792, resource: bindings, ignored listing per whitelist
Feb 28 07:25:55.895: INFO: namespace e2e-tests-configmap-k8792 deletion completed in 8.219048854s

• [SLOW TEST:12.898 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:25:55.895: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-9c6jk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-wn9td
STEP: Creating secret with name secret-test-17854762-3b2a-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 07:25:58.592: INFO: Waiting up to 5m0s for pod "pod-secrets-17cb0da9-3b2a-11e9-9760-9e5b40196308" in namespace "e2e-tests-secrets-9c6jk" to be "success or failure"
Feb 28 07:25:58.646: INFO: Pod "pod-secrets-17cb0da9-3b2a-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 54.338286ms
Feb 28 07:26:00.702: INFO: Pod "pod-secrets-17cb0da9-3b2a-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.109528984s
STEP: Saw pod success
Feb 28 07:26:00.702: INFO: Pod "pod-secrets-17cb0da9-3b2a-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:26:00.756: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-secrets-17cb0da9-3b2a-11e9-9760-9e5b40196308 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:26:00.873: INFO: Waiting for pod pod-secrets-17cb0da9-3b2a-11e9-9760-9e5b40196308 to disappear
Feb 28 07:26:00.926: INFO: Pod pod-secrets-17cb0da9-3b2a-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:26:00.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-9c6jk" for this suite.
Feb 28 07:26:07.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:26:07.572: INFO: namespace: e2e-tests-secrets-9c6jk, resource: bindings, ignored listing per whitelist
Feb 28 07:26:09.217: INFO: namespace e2e-tests-secrets-9c6jk deletion completed in 8.236227739s
STEP: Destroying namespace "e2e-tests-secret-namespace-wn9td" for this suite.
Feb 28 07:26:15.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:26:16.874: INFO: namespace: e2e-tests-secret-namespace-wn9td, resource: bindings, ignored listing per whitelist
Feb 28 07:26:17.468: INFO: namespace e2e-tests-secret-namespace-wn9td deletion completed in 8.250412146s

• [SLOW TEST:21.573 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:26:17.468: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8892p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
Feb 28 07:26:19.593: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-8892p'
Feb 28 07:26:20.613: INFO: stderr: ""
Feb 28 07:26:20.613: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 28 07:26:21.668: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:26:21.668: INFO: Found 0 / 1
Feb 28 07:26:22.668: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:26:22.668: INFO: Found 0 / 1
Feb 28 07:26:23.667: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:26:23.667: INFO: Found 0 / 1
Feb 28 07:26:24.668: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:26:24.668: INFO: Found 1 / 1
Feb 28 07:26:24.668: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 07:26:24.722: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:26:24.722: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 28 07:26:24.722: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml logs redis-master-7nkb2 redis-master --namespace=e2e-tests-kubectl-8892p'
Feb 28 07:26:25.121: INFO: stderr: ""
Feb 28 07:26:25.121: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 07:26:23.608 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 07:26:23.609 # Server started, Redis version 3.2.12\n1:M 28 Feb 07:26:23.609 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 07:26:23.609 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 28 07:26:25.121: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-7nkb2 redis-master --namespace=e2e-tests-kubectl-8892p --tail=1'
Feb 28 07:26:25.517: INFO: stderr: ""
Feb 28 07:26:25.517: INFO: stdout: "1:M 28 Feb 07:26:23.609 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 28 07:26:25.517: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-7nkb2 redis-master --namespace=e2e-tests-kubectl-8892p --limit-bytes=1'
Feb 28 07:26:25.956: INFO: stderr: ""
Feb 28 07:26:25.956: INFO: stdout: " "
STEP: exposing timestamps
Feb 28 07:26:25.956: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-7nkb2 redis-master --namespace=e2e-tests-kubectl-8892p --tail=1 --timestamps'
Feb 28 07:26:26.330: INFO: stderr: ""
Feb 28 07:26:26.330: INFO: stdout: "2019-02-28T07:26:23.60929741Z 1:M 28 Feb 07:26:23.609 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 28 07:26:28.831: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-7nkb2 redis-master --namespace=e2e-tests-kubectl-8892p --since=1s'
Feb 28 07:26:29.202: INFO: stderr: ""
Feb 28 07:26:29.202: INFO: stdout: ""
Feb 28 07:26:29.203: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml log redis-master-7nkb2 redis-master --namespace=e2e-tests-kubectl-8892p --since=24h'
Feb 28 07:26:29.573: INFO: stderr: ""
Feb 28 07:26:29.573: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 07:26:23.608 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 07:26:23.609 # Server started, Redis version 3.2.12\n1:M 28 Feb 07:26:23.609 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 07:26:23.609 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
Feb 28 07:26:29.573: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8892p'
Feb 28 07:26:29.945: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:26:29.945: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 28 07:26:29.946: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-8892p'
Feb 28 07:26:30.333: INFO: stderr: "No resources found.\n"
Feb 28 07:26:30.333: INFO: stdout: ""
Feb 28 07:26:30.333: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-8892p -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 07:26:30.650: INFO: stderr: ""
Feb 28 07:26:30.650: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:26:30.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8892p" for this suite.
Feb 28 07:26:36.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:26:38.369: INFO: namespace: e2e-tests-kubectl-8892p, resource: bindings, ignored listing per whitelist
Feb 28 07:26:38.906: INFO: namespace e2e-tests-kubectl-8892p deletion completed in 8.202273632s

• [SLOW TEST:21.438 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:26:38.907: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-g9bln
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 28 07:26:41.136: INFO: Waiting up to 5m0s for pod "var-expansion-3126e1c4-3b2a-11e9-9760-9e5b40196308" in namespace "e2e-tests-var-expansion-g9bln" to be "success or failure"
Feb 28 07:26:41.190: INFO: Pod "var-expansion-3126e1c4-3b2a-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.609122ms
Feb 28 07:26:43.244: INFO: Pod "var-expansion-3126e1c4-3b2a-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107851689s
STEP: Saw pod success
Feb 28 07:26:43.244: INFO: Pod "var-expansion-3126e1c4-3b2a-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:26:43.298: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod var-expansion-3126e1c4-3b2a-11e9-9760-9e5b40196308 container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:26:43.413: INFO: Waiting for pod var-expansion-3126e1c4-3b2a-11e9-9760-9e5b40196308 to disappear
Feb 28 07:26:43.467: INFO: Pod var-expansion-3126e1c4-3b2a-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:26:43.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-g9bln" for this suite.
Feb 28 07:26:49.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:26:51.669: INFO: namespace: e2e-tests-var-expansion-g9bln, resource: bindings, ignored listing per whitelist
Feb 28 07:26:51.777: INFO: namespace e2e-tests-var-expansion-g9bln deletion completed in 8.255193365s

• [SLOW TEST:12.870 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:26:51.777: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-b9hrk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-38d70700-3b2a-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 07:26:54.089: INFO: Waiting up to 5m0s for pod "pod-secrets-38df4bb5-3b2a-11e9-9760-9e5b40196308" in namespace "e2e-tests-secrets-b9hrk" to be "success or failure"
Feb 28 07:26:54.142: INFO: Pod "pod-secrets-38df4bb5-3b2a-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.377193ms
Feb 28 07:26:56.196: INFO: Pod "pod-secrets-38df4bb5-3b2a-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107539012s
STEP: Saw pod success
Feb 28 07:26:56.196: INFO: Pod "pod-secrets-38df4bb5-3b2a-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:26:56.250: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-secrets-38df4bb5-3b2a-11e9-9760-9e5b40196308 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:26:56.366: INFO: Waiting for pod pod-secrets-38df4bb5-3b2a-11e9-9760-9e5b40196308 to disappear
Feb 28 07:26:56.421: INFO: Pod pod-secrets-38df4bb5-3b2a-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:26:56.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-b9hrk" for this suite.
Feb 28 07:27:02.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:27:02.853: INFO: namespace: e2e-tests-secrets-b9hrk, resource: bindings, ignored listing per whitelist
Feb 28 07:27:04.692: INFO: namespace e2e-tests-secrets-b9hrk deletion completed in 8.216710599s

• [SLOW TEST:12.915 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:27:04.692: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-jktv2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-jktv2
Feb 28 07:27:11.063: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-jktv2
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 07:27:11.117: INFO: Initial restart count of pod liveness-http is 0
Feb 28 07:27:29.673: INFO: Restart count of pod e2e-tests-container-probe-jktv2/liveness-http is now 1 (18.556457948s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:27:29.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-jktv2" for this suite.
Feb 28 07:27:35.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:27:36.374: INFO: namespace: e2e-tests-container-probe-jktv2, resource: bindings, ignored listing per whitelist
Feb 28 07:27:38.031: INFO: namespace e2e-tests-container-probe-jktv2 deletion completed in 8.248338295s

• [SLOW TEST:33.339 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:27:38.032: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zx7cq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-54612ced-3b2a-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 07:27:40.292: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-54697659-3b2a-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-zx7cq" to be "success or failure"
Feb 28 07:27:40.346: INFO: Pod "pod-projected-secrets-54697659-3b2a-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.93014ms
Feb 28 07:27:42.401: INFO: Pod "pod-projected-secrets-54697659-3b2a-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108896826s
STEP: Saw pod success
Feb 28 07:27:42.401: INFO: Pod "pod-projected-secrets-54697659-3b2a-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:27:42.455: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-secrets-54697659-3b2a-11e9-9760-9e5b40196308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:27:42.572: INFO: Waiting for pod pod-projected-secrets-54697659-3b2a-11e9-9760-9e5b40196308 to disappear
Feb 28 07:27:42.626: INFO: Pod pod-projected-secrets-54697659-3b2a-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:27:42.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zx7cq" for this suite.
Feb 28 07:27:48.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:27:49.262: INFO: namespace: e2e-tests-projected-zx7cq, resource: bindings, ignored listing per whitelist
Feb 28 07:27:50.940: INFO: namespace e2e-tests-projected-zx7cq deletion completed in 8.260154957s

• [SLOW TEST:12.909 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:27:50.940: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-mdtwm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:27:53.328: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c2e6868-3b2a-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-mdtwm" to be "success or failure"
Feb 28 07:27:53.382: INFO: Pod "downwardapi-volume-5c2e6868-3b2a-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.984248ms
Feb 28 07:27:55.437: INFO: Pod "downwardapi-volume-5c2e6868-3b2a-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108512366s
STEP: Saw pod success
Feb 28 07:27:55.437: INFO: Pod "downwardapi-volume-5c2e6868-3b2a-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:27:55.490: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-5c2e6868-3b2a-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 07:27:55.609: INFO: Waiting for pod downwardapi-volume-5c2e6868-3b2a-11e9-9760-9e5b40196308 to disappear
Feb 28 07:27:55.662: INFO: Pod downwardapi-volume-5c2e6868-3b2a-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:27:55.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mdtwm" for this suite.
Feb 28 07:28:01.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:28:02.647: INFO: namespace: e2e-tests-downward-api-mdtwm, resource: bindings, ignored listing per whitelist
Feb 28 07:28:04.218: INFO: namespace e2e-tests-downward-api-mdtwm deletion completed in 8.502096243s

• [SLOW TEST:13.278 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:28:04.219: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-b8l7l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 07:28:06.437: INFO: Waiting up to 5m0s for pod "pod-63feb9ac-3b2a-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-b8l7l" to be "success or failure"
Feb 28 07:28:06.491: INFO: Pod "pod-63feb9ac-3b2a-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.651946ms
Feb 28 07:28:08.545: INFO: Pod "pod-63feb9ac-3b2a-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107867202s
STEP: Saw pod success
Feb 28 07:28:08.545: INFO: Pod "pod-63feb9ac-3b2a-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:28:08.598: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-63feb9ac-3b2a-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 07:28:08.713: INFO: Waiting for pod pod-63feb9ac-3b2a-11e9-9760-9e5b40196308 to disappear
Feb 28 07:28:08.767: INFO: Pod pod-63feb9ac-3b2a-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:28:08.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b8l7l" for this suite.
Feb 28 07:28:14.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:28:15.572: INFO: namespace: e2e-tests-emptydir-b8l7l, resource: bindings, ignored listing per whitelist
Feb 28 07:28:17.038: INFO: namespace e2e-tests-emptydir-b8l7l deletion completed in 8.217235299s

• [SLOW TEST:12.819 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:28:17.038: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-fgb4l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:28:19.288: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 28 07:28:19.395: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 07:28:25.557: INFO: Creating deployment "test-rolling-update-deployment"
Feb 28 07:28:25.648: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 28 07:28:25.801: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 28 07:28:25.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686935705, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686935705, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686935705, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686935705, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 07:28:27.909: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 07:28:28.070: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-fgb4l,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fgb4l/deployments/test-rolling-update-deployment,UID:6f757574-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:2976,Generation:1,CreationTimestamp:2019-02-28 07:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 07:28:25 +0000 UTC 2019-02-28 07:28:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 07:28:26 +0000 UTC 2019-02-28 07:28:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 07:28:28.125: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-fgb4l,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fgb4l/replicasets/test-rolling-update-deployment-65b7695dcf,UID:6f788c45-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:2969,Generation:1,CreationTimestamp:2019-02-28 07:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 6f757574-3b2a-11e9-8db9-7e40ccd56e85 0xc001cae247 0xc001cae248}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 07:28:28.125: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 28 07:28:28.125: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-fgb4l,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fgb4l/replicasets/test-rolling-update-controller,UID:6bb3256d-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:2975,Generation:2,CreationTimestamp:2019-02-28 07:28:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 6f757574-3b2a-11e9-8db9-7e40ccd56e85 0xc001cae17f 0xc001cae190}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 07:28:28.179: INFO: Pod "test-rolling-update-deployment-65b7695dcf-q5zb7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-q5zb7,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-fgb4l,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fgb4l/pods/test-rolling-update-deployment-65b7695dcf-q5zb7,UID:6f78dcab-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:2968,Generation:0,CreationTimestamp:2019-02-28 07:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.18/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf 6f788c45-3b2a-11e9-8db9-7e40ccd56e85 0xc000de6d57 0xc000de6d58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-9mfrb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9mfrb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-9mfrb true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000de6dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000de6de0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:28:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:28:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:28:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:28:25 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.18,StartTime:2019-02-28 07:28:25 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 07:28:26 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://dc7bdb83102ef788926a0fedf54bb7f593f5d0aecc0be66ec101169995089afd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:28:28.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-fgb4l" for this suite.
Feb 28 07:28:34.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:28:35.703: INFO: namespace: e2e-tests-deployment-fgb4l, resource: bindings, ignored listing per whitelist
Feb 28 07:28:36.453: INFO: namespace e2e-tests-deployment-fgb4l deletion completed in 8.218999463s

• [SLOW TEST:19.414 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:28:36.453: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-pgm4f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:28:38.894: INFO: (0) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 151.675264ms)
Feb 28 07:28:39.021: INFO: (1) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 126.97443ms)
Feb 28 07:28:39.076: INFO: (2) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.429787ms)
Feb 28 07:28:39.132: INFO: (3) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.356604ms)
Feb 28 07:28:39.187: INFO: (4) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.232772ms)
Feb 28 07:28:39.242: INFO: (5) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.189904ms)
Feb 28 07:28:39.298: INFO: (6) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.247931ms)
Feb 28 07:28:39.353: INFO: (7) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 54.955681ms)
Feb 28 07:28:39.408: INFO: (8) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.067883ms)
Feb 28 07:28:39.463: INFO: (9) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 54.982049ms)
Feb 28 07:28:39.518: INFO: (10) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.315308ms)
Feb 28 07:28:39.573: INFO: (11) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.01058ms)
Feb 28 07:28:39.628: INFO: (12) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.195219ms)
Feb 28 07:28:39.684: INFO: (13) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.618378ms)
Feb 28 07:28:39.739: INFO: (14) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.005122ms)
Feb 28 07:28:39.794: INFO: (15) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.1033ms)
Feb 28 07:28:39.850: INFO: (16) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.162013ms)
Feb 28 07:28:39.905: INFO: (17) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.434356ms)
Feb 28 07:28:39.960: INFO: (18) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.077761ms)
Feb 28 07:28:40.015: INFO: (19) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 55.235151ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:28:40.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-pgm4f" for this suite.
Feb 28 07:28:46.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:28:47.916: INFO: namespace: e2e-tests-proxy-pgm4f, resource: bindings, ignored listing per whitelist
Feb 28 07:28:48.291: INFO: namespace e2e-tests-proxy-pgm4f deletion completed in 8.213455939s

• [SLOW TEST:11.838 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:28:48.292: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-zwd9k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:28:50.483: INFO: Creating deployment "test-recreate-deployment"
Feb 28 07:28:50.538: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 28 07:28:50.645: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 28 07:28:50.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686935730, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686935730, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686935730, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686935730, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 07:28:52.753: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 28 07:28:52.864: INFO: Updating deployment test-recreate-deployment
Feb 28 07:28:52.864: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 07:28:52.972: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-zwd9k,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zwd9k/deployments/test-recreate-deployment,UID:7e4b416b-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3081,Generation:2,CreationTimestamp:2019-02-28 07:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-28 07:28:52 +0000 UTC 2019-02-28 07:28:52 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-28 07:28:52 +0000 UTC 2019-02-28 07:28:50 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 28 07:28:53.027: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-zwd9k,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zwd9k/replicasets/test-recreate-deployment-7cf749666b,UID:7fb004a2-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3078,Generation:1,CreationTimestamp:2019-02-28 07:28:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 7e4b416b-3b2a-11e9-8db9-7e40ccd56e85 0xc001c91de7 0xc001c91de8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 07:28:53.027: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 28 07:28:53.027: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-zwd9k,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zwd9k/replicasets/test-recreate-deployment-79f694ff59,UID:7e4bf930-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3072,Generation:2,CreationTimestamp:2019-02-28 07:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 7e4b416b-3b2a-11e9-8db9-7e40ccd56e85 0xc001c91d37 0xc001c91d38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 07:28:53.081: INFO: Pod "test-recreate-deployment-7cf749666b-wvjjs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-wvjjs,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-zwd9k,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zwd9k/pods/test-recreate-deployment-7cf749666b-wvjjs,UID:7fb04c8c-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3080,Generation:0,CreationTimestamp:2019-02-28 07:28:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b 7fb004a2-3b2a-11e9-8db9-7e40ccd56e85 0xc001660617 0xc001660618}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-gzbx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gzbx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gzbx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001660680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016606a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:28:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:28:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:28:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:28:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:28:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:28:53.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-zwd9k" for this suite.
Feb 28 07:28:59.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:29:00.652: INFO: namespace: e2e-tests-deployment-zwd9k, resource: bindings, ignored listing per whitelist
Feb 28 07:29:01.353: INFO: namespace e2e-tests-deployment-zwd9k deletion completed in 8.217810076s

• [SLOW TEST:13.062 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:29:01.353: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rmr5z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-8608152b-3b2a-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 07:29:03.595: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-86105855-3b2a-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-rmr5z" to be "success or failure"
Feb 28 07:29:03.649: INFO: Pod "pod-projected-secrets-86105855-3b2a-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.798846ms
Feb 28 07:29:05.703: INFO: Pod "pod-projected-secrets-86105855-3b2a-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107961823s
STEP: Saw pod success
Feb 28 07:29:05.703: INFO: Pod "pod-projected-secrets-86105855-3b2a-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:29:05.759: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-secrets-86105855-3b2a-11e9-9760-9e5b40196308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:29:05.878: INFO: Waiting for pod pod-projected-secrets-86105855-3b2a-11e9-9760-9e5b40196308 to disappear
Feb 28 07:29:05.932: INFO: Pod pod-projected-secrets-86105855-3b2a-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:29:05.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rmr5z" for this suite.
Feb 28 07:29:12.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:29:12.463: INFO: namespace: e2e-tests-projected-rmr5z, resource: bindings, ignored listing per whitelist
Feb 28 07:29:14.303: INFO: namespace e2e-tests-projected-rmr5z deletion completed in 8.317030136s

• [SLOW TEST:12.950 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:29:14.303: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-v2jlv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-v2jlv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-v2jlv to expose endpoints map[]
Feb 28 07:29:16.695: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-v2jlv exposes endpoints map[] (53.554564ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-v2jlv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-v2jlv to expose endpoints map[pod1:[100]]
Feb 28 07:29:19.074: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-v2jlv exposes endpoints map[pod1:[100]] (2.324168187s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-v2jlv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-v2jlv to expose endpoints map[pod1:[100] pod2:[101]]
Feb 28 07:29:22.783: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-v2jlv exposes endpoints map[pod1:[100] pod2:[101]] (3.654068795s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-v2jlv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-v2jlv to expose endpoints map[pod2:[101]]
Feb 28 07:29:22.947: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-v2jlv exposes endpoints map[pod2:[101]] (107.097542ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-v2jlv
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-v2jlv to expose endpoints map[]
Feb 28 07:29:23.056: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-v2jlv exposes endpoints map[] (53.574184ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:29:23.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-v2jlv" for this suite.
Feb 28 07:29:45.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:29:46.570: INFO: namespace: e2e-tests-services-v2jlv, resource: bindings, ignored listing per whitelist
Feb 28 07:29:47.431: INFO: namespace e2e-tests-services-v2jlv deletion completed in 24.254301239s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:33.149 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:29:47.465: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-4knqj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-4knqj.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-4knqj.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-4knqj.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-4knqj.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-4knqj.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-4knqj.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 07:30:17.055: INFO: DNS probes using e2e-tests-dns-4knqj/dns-test-a182a1c8-3b2a-11e9-9760-9e5b40196308 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:30:17.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-4knqj" for this suite.
Feb 28 07:30:23.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:30:23.701: INFO: namespace: e2e-tests-dns-4knqj, resource: bindings, ignored listing per whitelist
Feb 28 07:30:25.420: INFO: namespace e2e-tests-dns-4knqj deletion completed in 8.251947675s

• [SLOW TEST:37.955 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:30:25.420: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cv49k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 07:30:27.596: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-cv49k'
Feb 28 07:30:28.276: INFO: stderr: ""
Feb 28 07:30:28.276: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 28 07:30:33.379: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-cv49k -o json'
Feb 28 07:30:33.848: INFO: stderr: ""
Feb 28 07:30:33.848: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.2.24/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-28T07:30:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-cv49k\",\n        \"resourceVersion\": \"3363\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-cv49k/pods/e2e-test-nginx-pod\",\n        \"uid\": \"b88afd43-3b2a-11e9-8db9-7e40ccd56e85\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8kn22\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"nodeName\": \"ip-10-250-9-188.eu-west-1.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8kn22\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8kn22\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T07:30:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T07:30:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T07:30:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T07:30:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://bad848a20b34e9d984505a2c365a89bac543d5c2ab5a404961acc7cbf5d4aaa6\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-28T07:30:29Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.9.188\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.2.24\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-28T07:30:28Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 28 07:30:33.848: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-cv49k'
Feb 28 07:30:34.960: INFO: stderr: ""
Feb 28 07:30:34.961: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Feb 28 07:30:35.015: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-cv49k'
Feb 28 07:30:36.931: INFO: stderr: ""
Feb 28 07:30:36.931: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:30:36.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cv49k" for this suite.
Feb 28 07:30:43.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:30:44.698: INFO: namespace: e2e-tests-kubectl-cv49k, resource: bindings, ignored listing per whitelist
Feb 28 07:30:45.277: INFO: namespace e2e-tests-kubectl-cv49k deletion completed in 8.291660025s

• [SLOW TEST:19.857 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:30:45.277: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-rhv77
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-hvfkx
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-nj4jq
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:30:54.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-rhv77" for this suite.
Feb 28 07:31:00.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:31:01.032: INFO: namespace: e2e-tests-namespaces-rhv77, resource: bindings, ignored listing per whitelist
Feb 28 07:31:02.765: INFO: namespace e2e-tests-namespaces-rhv77 deletion completed in 8.234684499s
STEP: Destroying namespace "e2e-tests-nsdeletetest-hvfkx" for this suite.
Feb 28 07:31:02.819: INFO: Namespace e2e-tests-nsdeletetest-hvfkx was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-nj4jq" for this suite.
Feb 28 07:31:08.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:31:10.052: INFO: namespace: e2e-tests-nsdeletetest-nj4jq, resource: bindings, ignored listing per whitelist
Feb 28 07:31:11.024: INFO: namespace e2e-tests-nsdeletetest-nj4jq deletion completed in 8.204686859s

• [SLOW TEST:25.747 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:31:11.024: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-k5dr9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:31:13.189: INFO: Creating deployment "nginx-deployment"
Feb 28 07:31:13.243: INFO: Waiting for observed generation 1
Feb 28 07:31:13.296: INFO: Waiting for all required pods to come up
Feb 28 07:31:13.352: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 28 07:31:23.460: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 28 07:31:23.567: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 28 07:31:23.676: INFO: Updating deployment nginx-deployment
Feb 28 07:31:23.676: INFO: Waiting for observed generation 2
Feb 28 07:31:23.747: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 28 07:31:23.801: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 28 07:31:23.854: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 07:31:24.015: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 28 07:31:24.015: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 28 07:31:24.068: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 07:31:24.176: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 28 07:31:24.176: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 28 07:31:24.283: INFO: Updating deployment nginx-deployment
Feb 28 07:31:24.283: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 28 07:31:24.448: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 28 07:31:26.601: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 07:31:26.708: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-k5dr9/deployments/nginx-deployment,UID:d35a76ec-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3707,Generation:3,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-28 07:31:24 +0000 UTC 2019-02-28 07:31:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-28 07:31:24 +0000 UTC 2019-02-28 07:31:13 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 28 07:31:26.762: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-k5dr9/replicasets/nginx-deployment-7dc8f79789,UID:d9930c53-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3705,Generation:3,CreationTimestamp:2019-02-28 07:31:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment d35a76ec-3b2a-11e9-8db9-7e40ccd56e85 0xc0015f4c67 0xc0015f4c68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 07:31:26.762: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 28 07:31:26.762: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-k5dr9/replicasets/nginx-deployment-7f9675fb8b,UID:d35b3420-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3701,Generation:3,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment d35a76ec-3b2a-11e9-8db9-7e40ccd56e85 0xc0015f4d27 0xc0015f4d28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 28 07:31:26.819: INFO: Pod "nginx-deployment-7dc8f79789-44qmr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-44qmr,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-44qmr,UID:d9f90814-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3691,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d39070 0xc000d39071}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d390e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.819: INFO: Pod "nginx-deployment-7dc8f79789-4j8tx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-4j8tx,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-4j8tx,UID:d9941d7c-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3697,Generation:0,CreationTimestamp:2019-02-28 07:31:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.31/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d391d0 0xc000d391d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d39240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.820: INFO: Pod "nginx-deployment-7dc8f79789-7gf2z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-7gf2z,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-7gf2z,UID:d9f90e66-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3704,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d39320 0xc000d39321}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d39390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d393b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.820: INFO: Pod "nginx-deployment-7dc8f79789-gnvs2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-gnvs2,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-gnvs2,UID:d9f9000d-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3688,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d39470 0xc000d39471}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d394e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.820: INFO: Pod "nginx-deployment-7dc8f79789-h52nw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-h52nw,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-h52nw,UID:d99483cf-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3698,Generation:0,CreationTimestamp:2019-02-28 07:31:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.32/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d395d0 0xc000d395d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d39640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.820: INFO: Pod "nginx-deployment-7dc8f79789-mnrnd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-mnrnd,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-mnrnd,UID:d998149a-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3716,Generation:0,CreationTimestamp:2019-02-28 07:31:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.17/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d39730 0xc000d39731}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d397a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d397c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.820: INFO: Pod "nginx-deployment-7dc8f79789-nnl88" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-nnl88,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-nnl88,UID:d9f90f09-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3706,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d39880 0xc000d39881}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d398f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.820: INFO: Pod "nginx-deployment-7dc8f79789-nxm5s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-nxm5s,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-nxm5s,UID:d9f072f7-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3685,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d399d0 0xc000d399d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d39a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.820: INFO: Pod "nginx-deployment-7dc8f79789-rh9m4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-rh9m4,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-rh9m4,UID:da26928e-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3710,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d39b20 0xc000d39b21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d39b90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39bb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.820: INFO: Pod "nginx-deployment-7dc8f79789-t9wb6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-t9wb6,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-t9wb6,UID:d9eff90b-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3674,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d39c70 0xc000d39c71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d39ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.821: INFO: Pod "nginx-deployment-7dc8f79789-tjcdq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-tjcdq,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-tjcdq,UID:d9f0a5d5-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3677,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d39dc0 0xc000d39dc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d39e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.821: INFO: Pod "nginx-deployment-7dc8f79789-w2nj8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-w2nj8,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-w2nj8,UID:d99478bf-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3713,Generation:0,CreationTimestamp:2019-02-28 07:31:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.15/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc000d39f20 0xc000d39f21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d39f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d39fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.821: INFO: Pod "nginx-deployment-7dc8f79789-wbjzk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-wbjzk,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7dc8f79789-wbjzk,UID:d998570a-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3715,Generation:0,CreationTimestamp:2019-02-28 07:31:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.16/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 d9930c53-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3e080 0xc001e3e081}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3e0f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3e110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.821: INFO: Pod "nginx-deployment-7f9675fb8b-242g7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-242g7,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-242g7,UID:d9f105e2-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3689,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3e1d0 0xc001e3e1d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3e230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3e250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.821: INFO: Pod "nginx-deployment-7f9675fb8b-4hclf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-4hclf,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-4hclf,UID:d9f9723d-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3708,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3e307 0xc001e3e308}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3e370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3e390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.821: INFO: Pod "nginx-deployment-7f9675fb8b-92ldh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-92ldh,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-92ldh,UID:d9f0fc85-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3699,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3e447 0xc001e3e448}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3e4b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3e4d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.821: INFO: Pod "nginx-deployment-7f9675fb8b-bfktg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-bfktg,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-bfktg,UID:d9f00fba-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3711,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.33/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3e597 0xc001e3e598}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3e600} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3e620}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.821: INFO: Pod "nginx-deployment-7f9675fb8b-fcjm2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-fcjm2,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-fcjm2,UID:d35cdc56-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3561,Generation:0,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.30/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3e6e7 0xc001e3e6e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3e750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3e770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.30,StartTime:2019-02-28 07:31:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 07:31:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://74255b18319eed2a6abcf19ff6d1f8f51c6da3123f6fe1ef42cba5816c1be9c3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.822: INFO: Pod "nginx-deployment-7f9675fb8b-h6kmm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-h6kmm,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-h6kmm,UID:d35d61ff-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3567,Generation:0,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.29/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3e840 0xc001e3e841}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3e8a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3e8c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.29,StartTime:2019-02-28 07:31:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 07:31:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://7b0ccea89621ae0807df81b884e29b1ed6815baa84b834465cbe0f6fdf7c42ad}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.822: INFO: Pod "nginx-deployment-7f9675fb8b-hpdfg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-hpdfg,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-hpdfg,UID:d35d4b3e-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3583,Generation:0,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.13/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3e990 0xc001e3e991}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3e9f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3ea10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:100.96.0.13,StartTime:2019-02-28 07:31:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 07:31:18 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://666be44ca00de5bead32506039be85c5c6aac89dbf5d16a7565ecce40cd0a484}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.822: INFO: Pod "nginx-deployment-7f9675fb8b-jmb2f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-jmb2f,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-jmb2f,UID:d35c64b4-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3558,Generation:0,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.25/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3eae0 0xc001e3eae1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3eb40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3eb60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.25,StartTime:2019-02-28 07:31:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 07:31:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://6dd9f8ce0bdabbdf85d57be8d7ba746b65029782dcd94144c5ab97c1de494fdd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.822: INFO: Pod "nginx-deployment-7f9675fb8b-kcqvh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-kcqvh,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-kcqvh,UID:d35ccf74-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3555,Generation:0,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.28/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3ec30 0xc001e3ec31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3ec90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3ecb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.28,StartTime:2019-02-28 07:31:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 07:31:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://de5162fc133dac6d78e07bc1f74c3a71ebaccfbf475f0c84fddda54257480ec5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.822: INFO: Pod "nginx-deployment-7f9675fb8b-kqmt6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-kqmt6,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-kqmt6,UID:d35cd72a-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3570,Generation:0,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.27/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3ed80 0xc001e3ed81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3ede0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3ee00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.27,StartTime:2019-02-28 07:31:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 07:31:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://71ad81ff1374839083ff5c82e46517a0a84df810401dae9b2587d7a536dd20ac}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.822: INFO: Pod "nginx-deployment-7f9675fb8b-l2vjc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-l2vjc,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-l2vjc,UID:da082943-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3717,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.35/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3eed0 0xc001e3eed1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3ef30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3ef50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.822: INFO: Pod "nginx-deployment-7f9675fb8b-lqng6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-lqng6,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-lqng6,UID:d9f009a2-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3712,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.34/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3f017 0xc001e3f018}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3f080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3f0a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.822: INFO: Pod "nginx-deployment-7f9675fb8b-mxg97" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-mxg97,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-mxg97,UID:da1769ab-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3709,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3f157 0xc001e3f158}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3f1c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3f1e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.823: INFO: Pod "nginx-deployment-7f9675fb8b-p7d52" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-p7d52,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-p7d52,UID:d9efaf4a-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3664,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3f297 0xc001e3f298}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3f300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3f320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.823: INFO: Pod "nginx-deployment-7f9675fb8b-p8qbr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-p8qbr,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-p8qbr,UID:d35c60e9-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3579,Generation:0,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.11/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3f3e7 0xc001e3f3e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3f450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3f470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:100.96.0.11,StartTime:2019-02-28 07:31:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 07:31:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://ceb9c22b29adef41781cc38699f9edb4edac51ba1868969bcf95608a2c9ed823}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.829: INFO: Pod "nginx-deployment-7f9675fb8b-q82km" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-q82km,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-q82km,UID:d9f1025d-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3687,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3f530 0xc001e3f531}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3f590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3f5b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.829: INFO: Pod "nginx-deployment-7f9675fb8b-s4xwr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-s4xwr,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-s4xwr,UID:d9f1038b-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3686,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3f667 0xc001e3f668}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-8-168.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3f6d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3f6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.8.168,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.829: INFO: Pod "nginx-deployment-7f9675fb8b-t7h6x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-t7h6x,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-t7h6x,UID:da082806-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3693,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3f7a7 0xc001e3f7a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3f810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3f830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.829: INFO: Pod "nginx-deployment-7f9675fb8b-wfzfr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-wfzfr,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-wfzfr,UID:da17905e-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3696,Generation:0,CreationTimestamp:2019-02-28 07:31:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3f8e7 0xc001e3f8e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3f950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3f970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:,StartTime:2019-02-28 07:31:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 07:31:26.830: INFO: Pod "nginx-deployment-7f9675fb8b-wn5p2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-wn5p2,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-k5dr9,SelfLink:/api/v1/namespaces/e2e-tests-deployment-k5dr9/pods/nginx-deployment-7f9675fb8b-wn5p2,UID:d35c14a9-3b2a-11e9-8db9-7e40ccd56e85,ResourceVersion:3564,Generation:0,CreationTimestamp:2019-02-28 07:31:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.26/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b d35b3420-3b2a-11e9-8db9-7e40ccd56e85 0xc001e3fa37 0xc001e3fa38}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-6hjk9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6hjk9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6hjk9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e3faa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e3fac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:31:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.26,StartTime:2019-02-28 07:31:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 07:31:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://5d8d7e2b0ff07797b94219c8b003ad6d6edc12d0997dfd7cf98a8c4b4df25067}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:31:26.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-k5dr9" for this suite.
Feb 28 07:31:33.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:31:34.553: INFO: namespace: e2e-tests-deployment-k5dr9, resource: bindings, ignored listing per whitelist
Feb 28 07:31:35.091: INFO: namespace e2e-tests-deployment-k5dr9 deletion completed in 8.207905297s

• [SLOW TEST:24.067 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:31:35.092: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-pfjc2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 28 07:31:37.272: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 07:31:37.380: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 07:31:37.433: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-8-168.eu-west-1.compute.internal before test
Feb 28 07:31:37.550: INFO: addons-kube-lego-648f8c9f5c-jhthm from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 07:31:37.551: INFO: node-exporter-pq7cp from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 07:31:37.551: INFO: metrics-server-54fc54bd68-wflk4 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 07:31:37.551: INFO: vpn-shoot-7bcff87c69-gp7q2 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 07:31:37.551: INFO: coredns-5f4748c5f-k5h6g from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container coredns ready: true, restart count 0
Feb 28 07:31:37.551: INFO: blackbox-exporter-58fd9b8556-xjpfq from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 07:31:37.551: INFO: kube-proxy-dkbnd from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 07:31:37.551: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-psl62 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 07:31:37.551: INFO: addons-nginx-ingress-controller-6b47c6c4cb-t86bf from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 07:31:37.551: INFO: addons-kubernetes-dashboard-5f64f76bd-ps8rt from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 07:31:37.551: INFO: calico-node-hjb94 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.551: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 07:31:37.551: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-9-188.eu-west-1.compute.internal before test
Feb 28 07:31:37.651: INFO: kube-proxy-4mpns from kube-system started at 2019-02-28 07:12:03 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.651: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 07:31:37.651: INFO: calico-node-kjkhj from kube-system started at 2019-02-28 07:12:03 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.651: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 07:31:37.651: INFO: node-exporter-8nnqh from kube-system started at 2019-02-28 07:12:03 +0000 UTC (1 container statuses recorded)
Feb 28 07:31:37.651: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node ip-10-250-8-168.eu-west-1.compute.internal
STEP: verifying the node has the label node ip-10-250-9-188.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod addons-kube-lego-648f8c9f5c-jhthm requesting resource cpu=20m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod addons-kubernetes-dashboard-5f64f76bd-ps8rt requesting resource cpu=50m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod addons-nginx-ingress-controller-6b47c6c4cb-t86bf requesting resource cpu=100m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-psl62 requesting resource cpu=0m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod blackbox-exporter-58fd9b8556-xjpfq requesting resource cpu=5m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod calico-node-hjb94 requesting resource cpu=100m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod calico-node-kjkhj requesting resource cpu=100m on Node ip-10-250-9-188.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod coredns-5f4748c5f-k5h6g requesting resource cpu=50m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod kube-proxy-4mpns requesting resource cpu=20m on Node ip-10-250-9-188.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod kube-proxy-dkbnd requesting resource cpu=20m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod metrics-server-54fc54bd68-wflk4 requesting resource cpu=20m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod node-exporter-8nnqh requesting resource cpu=5m on Node ip-10-250-9-188.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod node-exporter-pq7cp requesting resource cpu=5m on Node ip-10-250-8-168.eu-west-1.compute.internal
Feb 28 07:31:37.982: INFO: Pod vpn-shoot-7bcff87c69-gp7q2 requesting resource cpu=50m on Node ip-10-250-8-168.eu-west-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e21e877d-3b2a-11e9-9760-9e5b40196308.158776b0b6dd8a2f], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-pfjc2/filler-pod-e21e877d-3b2a-11e9-9760-9e5b40196308 to ip-10-250-9-188.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e21e877d-3b2a-11e9-9760-9e5b40196308.158776b0e475a515], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e21e877d-3b2a-11e9-9760-9e5b40196308.158776b0e7460b8e], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e21e877d-3b2a-11e9-9760-9e5b40196308.158776b0ef9150bd], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e226faec-3b2a-11e9-9760-9e5b40196308.158776b0ba197925], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-pfjc2/filler-pod-e226faec-3b2a-11e9-9760-9e5b40196308 to ip-10-250-8-168.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e226faec-3b2a-11e9-9760-9e5b40196308.158776b0ec2bf79e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e226faec-3b2a-11e9-9760-9e5b40196308.158776b0ef5c1403], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e226faec-3b2a-11e9-9760-9e5b40196308.158776b0f83f746c], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158776b1bec03bec], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node ip-10-250-8-168.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-250-9-188.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:31:43.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-pfjc2" for this suite.
Feb 28 07:31:50.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:31:50.422: INFO: namespace: e2e-tests-sched-pred-pfjc2, resource: bindings, ignored listing per whitelist
Feb 28 07:31:52.140: INFO: namespace e2e-tests-sched-pred-pfjc2 deletion completed in 8.240553732s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:17.049 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:31:52.141: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-m2xb7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-ebd60d48-3b2a-11e9-9760-9e5b40196308
STEP: Creating secret with name secret-projected-all-test-volume-ebd60d2d-3b2a-11e9-9760-9e5b40196308
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 28 07:31:54.449: INFO: Waiting up to 5m0s for pod "projected-volume-ebd60ce3-3b2a-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-m2xb7" to be "success or failure"
Feb 28 07:31:54.503: INFO: Pod "projected-volume-ebd60ce3-3b2a-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.839667ms
Feb 28 07:31:56.567: INFO: Pod "projected-volume-ebd60ce3-3b2a-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117203262s
STEP: Saw pod success
Feb 28 07:31:56.567: INFO: Pod "projected-volume-ebd60ce3-3b2a-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:31:56.620: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod projected-volume-ebd60ce3-3b2a-11e9-9760-9e5b40196308 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 28 07:31:56.736: INFO: Waiting for pod projected-volume-ebd60ce3-3b2a-11e9-9760-9e5b40196308 to disappear
Feb 28 07:31:56.789: INFO: Pod projected-volume-ebd60ce3-3b2a-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:31:56.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m2xb7" for this suite.
Feb 28 07:32:03.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:32:04.269: INFO: namespace: e2e-tests-projected-m2xb7, resource: bindings, ignored listing per whitelist
Feb 28 07:32:05.077: INFO: namespace e2e-tests-projected-m2xb7 deletion completed in 8.234055725s

• [SLOW TEST:12.937 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:32:05.078: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vbsxq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-f3cb4443-3b2a-11e9-9760-9e5b40196308
STEP: Creating configMap with name cm-test-opt-upd-f3cb448d-3b2a-11e9-9760-9e5b40196308
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f3cb4443-3b2a-11e9-9760-9e5b40196308
STEP: Updating configmap cm-test-opt-upd-f3cb448d-3b2a-11e9-9760-9e5b40196308
STEP: Creating configMap with name cm-test-opt-create-f3cb44b8-3b2a-11e9-9760-9e5b40196308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:33:34.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vbsxq" for this suite.
Feb 28 07:33:57.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:33:57.948: INFO: namespace: e2e-tests-projected-vbsxq, resource: bindings, ignored listing per whitelist
Feb 28 07:33:59.234: INFO: namespace e2e-tests-projected-vbsxq deletion completed in 24.204382958s

• [SLOW TEST:114.157 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:33:59.237: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-mzzx7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 07:34:07.947: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:08.000: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:10.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:10.066: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:12.001: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:12.055: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:14.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:14.056: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:16.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:16.055: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:18.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:18.070: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:20.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:20.058: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:22.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:22.055: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:24.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:24.054: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:26.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:26.055: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:28.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:28.055: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:30.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:30.054: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:32.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:32.055: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:34:34.001: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:34:34.054: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:34:34.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-mzzx7" for this suite.
Feb 28 07:34:56.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:34:56.955: INFO: namespace: e2e-tests-container-lifecycle-hook-mzzx7, resource: bindings, ignored listing per whitelist
Feb 28 07:34:58.361: INFO: namespace e2e-tests-container-lifecycle-hook-mzzx7 deletion completed in 24.251853423s

• [SLOW TEST:59.123 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:34:58.361: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-x6vsl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:35:00.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ae0b896-3b2b-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-x6vsl" to be "success or failure"
Feb 28 07:35:00.692: INFO: Pod "downwardapi-volume-5ae0b896-3b2b-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.552122ms
Feb 28 07:35:02.746: INFO: Pod "downwardapi-volume-5ae0b896-3b2b-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.10792014s
STEP: Saw pod success
Feb 28 07:35:02.746: INFO: Pod "downwardapi-volume-5ae0b896-3b2b-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:35:02.800: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-5ae0b896-3b2b-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 07:35:02.917: INFO: Waiting for pod downwardapi-volume-5ae0b896-3b2b-11e9-9760-9e5b40196308 to disappear
Feb 28 07:35:02.970: INFO: Pod downwardapi-volume-5ae0b896-3b2b-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:35:02.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x6vsl" for this suite.
Feb 28 07:35:09.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:35:10.859: INFO: namespace: e2e-tests-projected-x6vsl, resource: bindings, ignored listing per whitelist
Feb 28 07:35:11.290: INFO: namespace e2e-tests-projected-x6vsl deletion completed in 8.265797395s

• [SLOW TEST:12.930 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:35:11.295: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-hkrvb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 28 07:35:13.622: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-hkrvb,SelfLink:/api/v1/namespaces/e2e-tests-watch-hkrvb/configmaps/e2e-watch-test-watch-closed,UID:6290cbca-3b2b-11e9-8db9-7e40ccd56e85,ResourceVersion:4419,Generation:0,CreationTimestamp:2019-02-28 07:35:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 07:35:13.622: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-hkrvb,SelfLink:/api/v1/namespaces/e2e-tests-watch-hkrvb/configmaps/e2e-watch-test-watch-closed,UID:6290cbca-3b2b-11e9-8db9-7e40ccd56e85,ResourceVersion:4420,Generation:0,CreationTimestamp:2019-02-28 07:35:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 28 07:35:13.840: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-hkrvb,SelfLink:/api/v1/namespaces/e2e-tests-watch-hkrvb/configmaps/e2e-watch-test-watch-closed,UID:6290cbca-3b2b-11e9-8db9-7e40ccd56e85,ResourceVersion:4421,Generation:0,CreationTimestamp:2019-02-28 07:35:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 07:35:13.840: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-hkrvb,SelfLink:/api/v1/namespaces/e2e-tests-watch-hkrvb/configmaps/e2e-watch-test-watch-closed,UID:6290cbca-3b2b-11e9-8db9-7e40ccd56e85,ResourceVersion:4422,Generation:0,CreationTimestamp:2019-02-28 07:35:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:35:13.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-hkrvb" for this suite.
Feb 28 07:35:20.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:35:22.041: INFO: namespace: e2e-tests-watch-hkrvb, resource: bindings, ignored listing per whitelist
Feb 28 07:35:22.095: INFO: namespace e2e-tests-watch-hkrvb deletion completed in 8.200391442s

• [SLOW TEST:10.800 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:35:22.095: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xkhzn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 28 07:35:24.216: INFO: namespace e2e-tests-kubectl-xkhzn
Feb 28 07:35:24.216: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-xkhzn'
Feb 28 07:35:27.504: INFO: stderr: ""
Feb 28 07:35:27.504: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 07:35:28.559: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:35:28.559: INFO: Found 0 / 1
Feb 28 07:35:29.561: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:35:29.561: INFO: Found 1 / 1
Feb 28 07:35:29.561: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 07:35:29.617: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:35:29.617: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 07:35:29.617: INFO: wait on redis-master startup in e2e-tests-kubectl-xkhzn 
Feb 28 07:35:29.617: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml logs redis-master-sxwfm redis-master --namespace=e2e-tests-kubectl-xkhzn'
Feb 28 07:35:30.168: INFO: stderr: ""
Feb 28 07:35:30.168: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 07:35:28.350 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 07:35:28.350 # Server started, Redis version 3.2.12\n1:M 28 Feb 07:35:28.350 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 07:35:28.350 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 28 07:35:30.168: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-xkhzn'
Feb 28 07:35:30.663: INFO: stderr: ""
Feb 28 07:35:30.663: INFO: stdout: "service/rm2 exposed\n"
Feb 28 07:35:30.716: INFO: Service rm2 in namespace e2e-tests-kubectl-xkhzn found.
STEP: exposing service
Feb 28 07:35:32.826: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-xkhzn'
Feb 28 07:35:33.321: INFO: stderr: ""
Feb 28 07:35:33.321: INFO: stdout: "service/rm3 exposed\n"
Feb 28 07:35:33.374: INFO: Service rm3 in namespace e2e-tests-kubectl-xkhzn found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:35:35.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xkhzn" for this suite.
Feb 28 07:35:57.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:35:58.405: INFO: namespace: e2e-tests-kubectl-xkhzn, resource: bindings, ignored listing per whitelist
Feb 28 07:35:59.746: INFO: namespace e2e-tests-kubectl-xkhzn deletion completed in 24.200982012s

• [SLOW TEST:37.651 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:35:59.746: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-v4rkg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-v4rkg
Feb 28 07:36:04.242: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-v4rkg
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 07:36:04.295: INFO: Initial restart count of pod liveness-http is 0
Feb 28 07:36:20.790: INFO: Restart count of pod e2e-tests-container-probe-v4rkg/liveness-http is now 1 (16.495100231s elapsed)
Feb 28 07:36:41.361: INFO: Restart count of pod e2e-tests-container-probe-v4rkg/liveness-http is now 2 (37.066063777s elapsed)
Feb 28 07:36:59.874: INFO: Restart count of pod e2e-tests-container-probe-v4rkg/liveness-http is now 3 (55.578362723s elapsed)
Feb 28 07:37:20.421: INFO: Restart count of pod e2e-tests-container-probe-v4rkg/liveness-http is now 4 (1m16.125545238s elapsed)
Feb 28 07:38:22.094: INFO: Restart count of pod e2e-tests-container-probe-v4rkg/liveness-http is now 5 (2m17.798353897s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:38:22.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-v4rkg" for this suite.
Feb 28 07:38:28.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:38:30.341: INFO: namespace: e2e-tests-container-probe-v4rkg, resource: bindings, ignored listing per whitelist
Feb 28 07:38:30.449: INFO: namespace e2e-tests-container-probe-v4rkg deletion completed in 8.244728255s

• [SLOW TEST:150.703 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:38:30.450: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jc2l4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-d94c41a9-3b2b-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 07:38:32.789: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9547726-3b2b-11e9-9760-9e5b40196308" in namespace "e2e-tests-configmap-jc2l4" to be "success or failure"
Feb 28 07:38:32.843: INFO: Pod "pod-configmaps-d9547726-3b2b-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 54.059476ms
Feb 28 07:38:34.896: INFO: Pod "pod-configmaps-d9547726-3b2b-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107782841s
STEP: Saw pod success
Feb 28 07:38:34.897: INFO: Pod "pod-configmaps-d9547726-3b2b-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:38:34.950: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-d9547726-3b2b-11e9-9760-9e5b40196308 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:38:35.067: INFO: Waiting for pod pod-configmaps-d9547726-3b2b-11e9-9760-9e5b40196308 to disappear
Feb 28 07:38:35.120: INFO: Pod pod-configmaps-d9547726-3b2b-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:38:35.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jc2l4" for this suite.
Feb 28 07:38:41.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:38:41.757: INFO: namespace: e2e-tests-configmap-jc2l4, resource: bindings, ignored listing per whitelist
Feb 28 07:38:43.426: INFO: namespace e2e-tests-configmap-jc2l4 deletion completed in 8.251433537s

• [SLOW TEST:12.976 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:38:43.427: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zvldt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 28 07:38:45.595: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:46.460: INFO: stderr: ""
Feb 28 07:38:46.460: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 07:38:46.460: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:46.870: INFO: stderr: ""
Feb 28 07:38:46.870: INFO: stdout: "update-demo-nautilus-gs8cg update-demo-nautilus-pzts8 "
Feb 28 07:38:46.870: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gs8cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:47.290: INFO: stderr: ""
Feb 28 07:38:47.290: INFO: stdout: ""
Feb 28 07:38:47.290: INFO: update-demo-nautilus-gs8cg is created but not running
Feb 28 07:38:52.291: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:52.670: INFO: stderr: ""
Feb 28 07:38:52.670: INFO: stdout: "update-demo-nautilus-gs8cg update-demo-nautilus-pzts8 "
Feb 28 07:38:52.670: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gs8cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:53.038: INFO: stderr: ""
Feb 28 07:38:53.038: INFO: stdout: "true"
Feb 28 07:38:53.038: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gs8cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:53.436: INFO: stderr: ""
Feb 28 07:38:53.436: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:38:53.436: INFO: validating pod update-demo-nautilus-gs8cg
Feb 28 07:38:53.577: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:38:53.577: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:38:53.577: INFO: update-demo-nautilus-gs8cg is verified up and running
Feb 28 07:38:53.577: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-pzts8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:53.952: INFO: stderr: ""
Feb 28 07:38:53.952: INFO: stdout: "true"
Feb 28 07:38:53.952: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-pzts8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:54.314: INFO: stderr: ""
Feb 28 07:38:54.314: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:38:54.314: INFO: validating pod update-demo-nautilus-pzts8
Feb 28 07:38:54.457: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:38:54.457: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:38:54.457: INFO: update-demo-nautilus-pzts8 is verified up and running
STEP: scaling down the replication controller
Feb 28 07:38:54.468: INFO: scanned /root for discovery docs: <nil>
Feb 28 07:38:54.468: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:55.027: INFO: stderr: ""
Feb 28 07:38:55.027: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 07:38:55.027: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:38:55.422: INFO: stderr: ""
Feb 28 07:38:55.422: INFO: stdout: "update-demo-nautilus-gs8cg update-demo-nautilus-pzts8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 07:39:00.422: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:00.991: INFO: stderr: ""
Feb 28 07:39:00.991: INFO: stdout: "update-demo-nautilus-gs8cg update-demo-nautilus-pzts8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 07:39:06.000: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:06.528: INFO: stderr: ""
Feb 28 07:39:06.528: INFO: stdout: "update-demo-nautilus-gs8cg "
Feb 28 07:39:06.528: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gs8cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:07.078: INFO: stderr: ""
Feb 28 07:39:07.078: INFO: stdout: "true"
Feb 28 07:39:07.078: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gs8cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:07.589: INFO: stderr: ""
Feb 28 07:39:07.590: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:39:07.590: INFO: validating pod update-demo-nautilus-gs8cg
Feb 28 07:39:07.647: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:39:07.647: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:39:07.647: INFO: update-demo-nautilus-gs8cg is verified up and running
STEP: scaling up the replication controller
Feb 28 07:39:07.653: INFO: scanned /root for discovery docs: <nil>
Feb 28 07:39:07.653: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:08.328: INFO: stderr: ""
Feb 28 07:39:08.328: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 07:39:08.328: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:08.847: INFO: stderr: ""
Feb 28 07:39:08.847: INFO: stdout: "update-demo-nautilus-946fk update-demo-nautilus-gs8cg "
Feb 28 07:39:08.847: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-946fk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:09.300: INFO: stderr: ""
Feb 28 07:39:09.300: INFO: stdout: ""
Feb 28 07:39:09.300: INFO: update-demo-nautilus-946fk is created but not running
Feb 28 07:39:14.300: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:14.803: INFO: stderr: ""
Feb 28 07:39:14.803: INFO: stdout: "update-demo-nautilus-946fk update-demo-nautilus-gs8cg "
Feb 28 07:39:14.803: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-946fk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:15.229: INFO: stderr: ""
Feb 28 07:39:15.229: INFO: stdout: "true"
Feb 28 07:39:15.229: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-946fk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:15.635: INFO: stderr: ""
Feb 28 07:39:15.635: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:39:15.635: INFO: validating pod update-demo-nautilus-946fk
Feb 28 07:39:15.777: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:39:15.777: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:39:15.777: INFO: update-demo-nautilus-946fk is verified up and running
Feb 28 07:39:15.777: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gs8cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:16.376: INFO: stderr: ""
Feb 28 07:39:16.376: INFO: stdout: "true"
Feb 28 07:39:16.376: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-gs8cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:16.891: INFO: stderr: ""
Feb 28 07:39:16.891: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:39:16.891: INFO: validating pod update-demo-nautilus-gs8cg
Feb 28 07:39:16.953: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:39:16.953: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:39:16.953: INFO: update-demo-nautilus-gs8cg is verified up and running
STEP: using delete to clean up resources
Feb 28 07:39:16.953: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:17.518: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:39:17.518: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 07:39:17.518: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-zvldt'
Feb 28 07:39:18.241: INFO: stderr: "No resources found.\n"
Feb 28 07:39:18.241: INFO: stdout: ""
Feb 28 07:39:18.241: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-zvldt -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 07:39:18.656: INFO: stderr: ""
Feb 28 07:39:18.656: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:39:18.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zvldt" for this suite.
Feb 28 07:39:24.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:39:25.212: INFO: namespace: e2e-tests-kubectl-zvldt, resource: bindings, ignored listing per whitelist
Feb 28 07:39:27.007: INFO: namespace e2e-tests-kubectl-zvldt deletion completed in 8.296857386s

• [SLOW TEST:43.581 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:39:27.007: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-xmdjr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:39:29.300: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 07:39:31.409: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 07:39:33.839: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-xmdjr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xmdjr/deployments/test-cleanup-deployment,UID:fc60fdeb-3b2b-11e9-8db9-7e40ccd56e85,ResourceVersion:5078,Generation:1,CreationTimestamp:2019-02-28 07:39:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 07:39:31 +0000 UTC 2019-02-28 07:39:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 07:39:32 +0000 UTC 2019-02-28 07:39:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-755f6b95cc" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 07:39:33.894: INFO: New ReplicaSet "test-cleanup-deployment-755f6b95cc" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc,GenerateName:,Namespace:e2e-tests-deployment-xmdjr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xmdjr/replicasets/test-cleanup-deployment-755f6b95cc,UID:fc622954-3b2b-11e9-8db9-7e40ccd56e85,ResourceVersion:5071,Generation:1,CreationTimestamp:2019-02-28 07:39:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment fc60fdeb-3b2b-11e9-8db9-7e40ccd56e85 0xc002266d67 0xc002266d68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 07:39:33.948: INFO: Pod "test-cleanup-deployment-755f6b95cc-8nzts" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc-8nzts,GenerateName:test-cleanup-deployment-755f6b95cc-,Namespace:e2e-tests-deployment-xmdjr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xmdjr/pods/test-cleanup-deployment-755f6b95cc-8nzts,UID:fc627ce3-3b2b-11e9-8db9-7e40ccd56e85,ResourceVersion:5070,Generation:0,CreationTimestamp:2019-02-28 07:39:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.55/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-755f6b95cc fc622954-3b2b-11e9-8db9-7e40ccd56e85 0xc000fbe557 0xc000fbe558}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-s9jgr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s9jgr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-s9jgr true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000fbe5c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000fbe5e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:39:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:39:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:39:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:39:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.55,StartTime:2019-02-28 07:39:31 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 07:39:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://05847a8ed2861620d2ed82b52e8082a8618a198f7d800fe210e2affb1b78f165}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:39:33.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xmdjr" for this suite.
Feb 28 07:39:40.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:39:41.731: INFO: namespace: e2e-tests-deployment-xmdjr, resource: bindings, ignored listing per whitelist
Feb 28 07:39:42.220: INFO: namespace e2e-tests-deployment-xmdjr deletion completed in 8.217443528s

• [SLOW TEST:15.213 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:39:42.220: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-h4vmb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-h4vmb
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 07:39:44.379: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 07:40:09.315: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.57:8080/dial?request=hostName&protocol=http&host=100.96.0.30&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-h4vmb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:40:09.315: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:40:09.992: INFO: Waiting for endpoints: map[]
Feb 28 07:40:10.047: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.57:8080/dial?request=hostName&protocol=http&host=100.96.2.56&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-h4vmb PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:40:10.047: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 07:40:10.718: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:40:10.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-h4vmb" for this suite.
Feb 28 07:40:32.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:40:34.800: INFO: namespace: e2e-tests-pod-network-test-h4vmb, resource: bindings, ignored listing per whitelist
Feb 28 07:40:35.081: INFO: namespace e2e-tests-pod-network-test-h4vmb deletion completed in 24.308228001s

• [SLOW TEST:52.861 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:40:35.081: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-n8mh2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 28 07:40:37.327: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-n8mh2" to be "success or failure"
Feb 28 07:40:37.381: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 54.05326ms
Feb 28 07:40:39.435: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107975368s
STEP: Saw pod success
Feb 28 07:40:39.435: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 28 07:40:39.489: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 28 07:40:39.608: INFO: Waiting for pod pod-host-path-test to disappear
Feb 28 07:40:39.664: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:40:39.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-n8mh2" for this suite.
Feb 28 07:40:45.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:40:46.413: INFO: namespace: e2e-tests-hostpath-n8mh2, resource: bindings, ignored listing per whitelist
Feb 28 07:40:47.969: INFO: namespace e2e-tests-hostpath-n8mh2 deletion completed in 8.25061173s

• [SLOW TEST:12.888 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:40:47.969: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-cv7wk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 07:40:50.228: INFO: Waiting up to 5m0s for pod "pod-2b400e57-3b2c-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-cv7wk" to be "success or failure"
Feb 28 07:40:50.282: INFO: Pod "pod-2b400e57-3b2c-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.282907ms
Feb 28 07:40:52.337: INFO: Pod "pod-2b400e57-3b2c-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108283914s
Feb 28 07:40:54.391: INFO: Pod "pod-2b400e57-3b2c-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.16290064s
STEP: Saw pod success
Feb 28 07:40:54.391: INFO: Pod "pod-2b400e57-3b2c-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:40:54.445: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-2b400e57-3b2c-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 07:40:54.564: INFO: Waiting for pod pod-2b400e57-3b2c-11e9-9760-9e5b40196308 to disappear
Feb 28 07:40:54.618: INFO: Pod pod-2b400e57-3b2c-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:40:54.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cv7wk" for this suite.
Feb 28 07:41:00.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:02.295: INFO: namespace: e2e-tests-emptydir-cv7wk, resource: bindings, ignored listing per whitelist
Feb 28 07:41:02.943: INFO: namespace e2e-tests-emptydir-cv7wk deletion completed in 8.270982903s

• [SLOW TEST:14.974 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:41:02.944: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-5qmb8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:41:05.192: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 07:41:07.300: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 28 07:41:09.354: INFO: Creating deployment "test-rollover-deployment"
Feb 28 07:41:09.461: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 28 07:41:09.515: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 28 07:41:09.627: INFO: Ensure that both replica sets have 1 created replica
Feb 28 07:41:09.735: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 28 07:41:09.845: INFO: Updating deployment test-rollover-deployment
Feb 28 07:41:09.845: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 28 07:41:09.899: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 28 07:41:10.007: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 28 07:41:10.115: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 07:41:10.115: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 07:41:12.226: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 07:41:12.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936471, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 07:41:14.224: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 07:41:14.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936471, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 07:41:16.223: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 07:41:16.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936471, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 07:41:18.232: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 07:41:18.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936471, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 07:41:20.223: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 07:41:20.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936471, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936469, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 07:41:22.223: INFO: 
Feb 28 07:41:22.223: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 07:41:22.384: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-5qmb8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5qmb8/deployments/test-rollover-deployment,UID:36b2120f-3b2c-11e9-8db9-7e40ccd56e85,ResourceVersion:5422,Generation:2,CreationTimestamp:2019-02-28 07:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 07:41:09 +0000 UTC 2019-02-28 07:41:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 07:41:21 +0000 UTC 2019-02-28 07:41:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 07:41:22.439: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-5qmb8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5qmb8/replicasets/test-rollover-deployment-5b76ff8c4,UID:36f52287-3b2c-11e9-8db9-7e40ccd56e85,ResourceVersion:5415,Generation:2,CreationTimestamp:2019-02-28 07:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 36b2120f-3b2c-11e9-8db9-7e40ccd56e85 0xc000de7650 0xc000de7651}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 07:41:22.439: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 28 07:41:22.439: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-5qmb8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5qmb8/replicasets/test-rollover-controller,UID:34267dea-3b2c-11e9-8db9-7e40ccd56e85,ResourceVersion:5421,Generation:2,CreationTimestamp:2019-02-28 07:41:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 36b2120f-3b2c-11e9-8db9-7e40ccd56e85 0xc000de750f 0xc000de7520}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 07:41:22.439: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-5qmb8,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-5qmb8/replicasets/test-rollover-deployment-6975f4fb87,UID:36b348e5-3b2c-11e9-8db9-7e40ccd56e85,ResourceVersion:5386,Generation:2,CreationTimestamp:2019-02-28 07:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 36b2120f-3b2c-11e9-8db9-7e40ccd56e85 0xc000de7707 0xc000de7708}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 07:41:22.493: INFO: Pod "test-rollover-deployment-5b76ff8c4-dgr89" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-dgr89,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-5qmb8,SelfLink:/api/v1/namespaces/e2e-tests-deployment-5qmb8/pods/test-rollover-deployment-5b76ff8c4-dgr89,UID:36f6ad93-3b2c-11e9-8db9-7e40ccd56e85,ResourceVersion:5393,Generation:0,CreationTimestamp:2019-02-28 07:41:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.61/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 36f52287-3b2c-11e9-8db9-7e40ccd56e85 0xc0025a2db0 0xc0025a2db1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wn29s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wn29s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-wn29s true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025a2e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025a2e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:41:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:41:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:41:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:41:09 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.61,StartTime:2019-02-28 07:41:09 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 07:41:10 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://d65123c2e13eccdbba97b3ebc886dec720edc42c826464d2068f3622678112c1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:41:22.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-5qmb8" for this suite.
Feb 28 07:41:28.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:29.533: INFO: namespace: e2e-tests-deployment-5qmb8, resource: bindings, ignored listing per whitelist
Feb 28 07:41:30.824: INFO: namespace e2e-tests-deployment-5qmb8 deletion completed in 8.276813623s

• [SLOW TEST:27.880 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:41:30.824: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-h5f2w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-h5f2w
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-h5f2w
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-h5f2w
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-h5f2w
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-h5f2w
Feb 28 07:41:35.363: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-h5f2w, name: ss-0, uid: 46294b7d-3b2c-11e9-8db9-7e40ccd56e85, status phase: Pending. Waiting for statefulset controller to delete.
Feb 28 07:41:35.364: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-h5f2w, name: ss-0, uid: 46294b7d-3b2c-11e9-8db9-7e40ccd56e85, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 07:41:35.406: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-h5f2w, name: ss-0, uid: 46294b7d-3b2c-11e9-8db9-7e40ccd56e85, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 07:41:35.408: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-h5f2w
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-h5f2w
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-h5f2w and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 07:41:37.571: INFO: Deleting all statefulset in ns e2e-tests-statefulset-h5f2w
Feb 28 07:41:37.624: INFO: Scaling statefulset ss to 0
Feb 28 07:41:57.841: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:41:57.895: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:41:58.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-h5f2w" for this suite.
Feb 28 07:42:04.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:42:06.054: INFO: namespace: e2e-tests-statefulset-h5f2w, resource: bindings, ignored listing per whitelist
Feb 28 07:42:06.322: INFO: namespace e2e-tests-statefulset-h5f2w deletion completed in 8.20992548s

• [SLOW TEST:35.498 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:42:06.323: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-dp9tm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 07:42:12.969: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:13.023: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:15.023: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:15.077: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:17.023: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:17.078: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:19.023: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:19.077: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:21.023: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:21.078: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:23.023: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:23.078: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:25.024: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:25.079: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:27.024: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:27.078: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:29.024: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:29.079: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:31.023: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:31.078: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:42:33.024: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:42:33.078: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:42:33.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-dp9tm" for this suite.
Feb 28 07:42:55.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:42:55.518: INFO: namespace: e2e-tests-container-lifecycle-hook-dp9tm, resource: bindings, ignored listing per whitelist
Feb 28 07:42:57.404: INFO: namespace e2e-tests-container-lifecycle-hook-dp9tm deletion completed in 24.211358443s

• [SLOW TEST:51.082 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:42:57.405: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bk8kq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-7861c943-3b2c-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 07:42:59.688: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-786a008c-3b2c-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-bk8kq" to be "success or failure"
Feb 28 07:42:59.742: INFO: Pod "pod-projected-configmaps-786a008c-3b2c-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.915945ms
Feb 28 07:43:01.797: INFO: Pod "pod-projected-configmaps-786a008c-3b2c-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108874044s
STEP: Saw pod success
Feb 28 07:43:01.797: INFO: Pod "pod-projected-configmaps-786a008c-3b2c-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:43:01.852: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-configmaps-786a008c-3b2c-11e9-9760-9e5b40196308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:43:01.979: INFO: Waiting for pod pod-projected-configmaps-786a008c-3b2c-11e9-9760-9e5b40196308 to disappear
Feb 28 07:43:02.038: INFO: Pod pod-projected-configmaps-786a008c-3b2c-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:43:02.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bk8kq" for this suite.
Feb 28 07:43:08.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:43:10.339: INFO: namespace: e2e-tests-projected-bk8kq, resource: bindings, ignored listing per whitelist
Feb 28 07:43:10.339: INFO: namespace e2e-tests-projected-bk8kq deletion completed in 8.246686598s

• [SLOW TEST:12.934 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:43:10.339: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-lkxqn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-92j4
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 07:43:12.671: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-92j4" in namespace "e2e-tests-subpath-lkxqn" to be "success or failure"
Feb 28 07:43:12.725: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Pending", Reason="", readiness=false. Elapsed: 53.929654ms
Feb 28 07:43:14.780: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109687656s
Feb 28 07:43:16.836: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Running", Reason="", readiness=false. Elapsed: 4.164820846s
Feb 28 07:43:18.891: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Running", Reason="", readiness=false. Elapsed: 6.220003428s
Feb 28 07:43:20.947: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Running", Reason="", readiness=false. Elapsed: 8.276381566s
Feb 28 07:43:23.002: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Running", Reason="", readiness=false. Elapsed: 10.331574871s
Feb 28 07:43:25.057: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Running", Reason="", readiness=false. Elapsed: 12.386423551s
Feb 28 07:43:27.113: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Running", Reason="", readiness=false. Elapsed: 14.442346294s
Feb 28 07:43:29.167: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Running", Reason="", readiness=false. Elapsed: 16.496212206s
Feb 28 07:43:31.222: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Running", Reason="", readiness=false. Elapsed: 18.551289634s
Feb 28 07:43:33.277: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Running", Reason="", readiness=false. Elapsed: 20.606138188s
Feb 28 07:43:35.332: INFO: Pod "pod-subpath-test-downwardapi-92j4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.661277467s
STEP: Saw pod success
Feb 28 07:43:35.332: INFO: Pod "pod-subpath-test-downwardapi-92j4" satisfied condition "success or failure"
Feb 28 07:43:35.386: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-subpath-test-downwardapi-92j4 container test-container-subpath-downwardapi-92j4: <nil>
STEP: delete the pod
Feb 28 07:43:35.504: INFO: Waiting for pod pod-subpath-test-downwardapi-92j4 to disappear
Feb 28 07:43:35.558: INFO: Pod pod-subpath-test-downwardapi-92j4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-92j4
Feb 28 07:43:35.558: INFO: Deleting pod "pod-subpath-test-downwardapi-92j4" in namespace "e2e-tests-subpath-lkxqn"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:43:35.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-lkxqn" for this suite.
Feb 28 07:43:41.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:43:43.158: INFO: namespace: e2e-tests-subpath-lkxqn, resource: bindings, ignored listing per whitelist
Feb 28 07:43:43.915: INFO: namespace e2e-tests-subpath-lkxqn deletion completed in 8.24828184s

• [SLOW TEST:33.576 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:43:43.916: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ppjt9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 28 07:43:46.082: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:46.846: INFO: stderr: ""
Feb 28 07:43:46.846: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 07:43:46.846: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:47.155: INFO: stderr: ""
Feb 28 07:43:47.155: INFO: stdout: "update-demo-nautilus-c4bmm update-demo-nautilus-vxjqb "
Feb 28 07:43:47.155: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-c4bmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:47.457: INFO: stderr: ""
Feb 28 07:43:47.457: INFO: stdout: ""
Feb 28 07:43:47.457: INFO: update-demo-nautilus-c4bmm is created but not running
Feb 28 07:43:52.458: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:52.911: INFO: stderr: ""
Feb 28 07:43:52.911: INFO: stdout: "update-demo-nautilus-c4bmm update-demo-nautilus-vxjqb "
Feb 28 07:43:52.911: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-c4bmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:53.221: INFO: stderr: ""
Feb 28 07:43:53.221: INFO: stdout: "true"
Feb 28 07:43:53.221: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-c4bmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:53.530: INFO: stderr: ""
Feb 28 07:43:53.530: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:43:53.530: INFO: validating pod update-demo-nautilus-c4bmm
Feb 28 07:43:53.671: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:43:53.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:43:53.671: INFO: update-demo-nautilus-c4bmm is verified up and running
Feb 28 07:43:53.671: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-vxjqb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:53.987: INFO: stderr: ""
Feb 28 07:43:53.987: INFO: stdout: "true"
Feb 28 07:43:53.987: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-vxjqb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:54.290: INFO: stderr: ""
Feb 28 07:43:54.290: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:43:54.290: INFO: validating pod update-demo-nautilus-vxjqb
Feb 28 07:43:54.431: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:43:54.431: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:43:54.432: INFO: update-demo-nautilus-vxjqb is verified up and running
STEP: using delete to clean up resources
Feb 28 07:43:54.432: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:54.811: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:43:54.811: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 07:43:54.811: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-ppjt9'
Feb 28 07:43:55.167: INFO: stderr: "No resources found.\n"
Feb 28 07:43:55.167: INFO: stdout: ""
Feb 28 07:43:55.167: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-ppjt9 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 07:43:55.472: INFO: stderr: ""
Feb 28 07:43:55.472: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:43:55.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ppjt9" for this suite.
Feb 28 07:44:17.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:44:18.281: INFO: namespace: e2e-tests-kubectl-ppjt9, resource: bindings, ignored listing per whitelist
Feb 28 07:44:19.731: INFO: namespace e2e-tests-kubectl-ppjt9 deletion completed in 24.204398664s

• [SLOW TEST:35.816 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:44:19.732: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-t5fsl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 07:44:21.881: INFO: PodSpec: initContainers in spec.initContainers
Feb 28 07:45:03.880: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a9703bd7-3b2c-11e9-9760-9e5b40196308", GenerateName:"", Namespace:"e2e-tests-init-container-t5fsl", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-t5fsl/pods/pod-init-a9703bd7-3b2c-11e9-9760-9e5b40196308", UID:"a973cc1d-3b2c-11e9-8db9-7e40ccd56e85", ResourceVersion:"6114", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686936661, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"881549679"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.2.67/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-jgbj2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001c4b280), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-jgbj2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-jgbj2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-jgbj2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002094498), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-9-188.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0017d92c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002094510)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002094530)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002094538), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936661, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936661, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936661, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686936661, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.9.188", PodIP:"100.96.2.67", StartTime:(*v1.Time)(0xc0017e4aa0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001f54cb0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001f54d20)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://78e109307ea06f2cdaa771fcd58bb7289b8c9b285c46c1af0085c79a2ff73fc5"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017e4ae0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017e4ac0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:45:03.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-t5fsl" for this suite.
Feb 28 07:45:26.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:45:27.993: INFO: namespace: e2e-tests-init-container-t5fsl, resource: bindings, ignored listing per whitelist
Feb 28 07:45:28.210: INFO: namespace e2e-tests-init-container-t5fsl deletion completed in 24.273983104s

• [SLOW TEST:68.479 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:45:28.211: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2zjbl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 07:45:30.439: INFO: Waiting up to 5m0s for pod "downward-api-d2446ff7-3b2c-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-2zjbl" to be "success or failure"
Feb 28 07:45:30.493: INFO: Pod "downward-api-d2446ff7-3b2c-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 54.23846ms
Feb 28 07:45:32.548: INFO: Pod "downward-api-d2446ff7-3b2c-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.109433718s
STEP: Saw pod success
Feb 28 07:45:32.548: INFO: Pod "downward-api-d2446ff7-3b2c-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:45:32.602: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downward-api-d2446ff7-3b2c-11e9-9760-9e5b40196308 container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:45:32.720: INFO: Waiting for pod downward-api-d2446ff7-3b2c-11e9-9760-9e5b40196308 to disappear
Feb 28 07:45:32.774: INFO: Pod downward-api-d2446ff7-3b2c-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:45:32.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2zjbl" for this suite.
Feb 28 07:45:38.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:45:39.580: INFO: namespace: e2e-tests-downward-api-2zjbl, resource: bindings, ignored listing per whitelist
Feb 28 07:45:41.065: INFO: namespace e2e-tests-downward-api-2zjbl deletion completed in 8.236352942s

• [SLOW TEST:12.854 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:45:41.065: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-bmbgp
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-d9ede394-3b2c-11e9-9760-9e5b40196308
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:45:45.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bmbgp" for this suite.
Feb 28 07:46:07.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:46:09.414: INFO: namespace: e2e-tests-configmap-bmbgp, resource: bindings, ignored listing per whitelist
Feb 28 07:46:09.899: INFO: namespace e2e-tests-configmap-bmbgp deletion completed in 24.21429504s

• [SLOW TEST:28.834 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:46:09.899: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-9q5nl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0228 07:46:52.643772   29661 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:46:52.643: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:46:52.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9q5nl" for this suite.
Feb 28 07:46:58.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:47:00.087: INFO: namespace: e2e-tests-gc-9q5nl, resource: bindings, ignored listing per whitelist
Feb 28 07:47:00.954: INFO: namespace e2e-tests-gc-9q5nl deletion completed in 8.256555708s

• [SLOW TEST:51.055 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:47:00.954: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8rkjz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-099c7285-3b2d-11e9-9760-9e5b40196308
STEP: Creating secret with name s-test-opt-upd-099c72f1-3b2d-11e9-9760-9e5b40196308
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-099c7285-3b2d-11e9-9760-9e5b40196308
STEP: Updating secret s-test-opt-upd-099c72f1-3b2d-11e9-9760-9e5b40196308
STEP: Creating secret with name s-test-opt-create-099c731b-3b2d-11e9-9760-9e5b40196308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:48:38.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8rkjz" for this suite.
Feb 28 07:49:01.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:02.713: INFO: namespace: e2e-tests-projected-8rkjz, resource: bindings, ignored listing per whitelist
Feb 28 07:49:03.090: INFO: namespace e2e-tests-projected-8rkjz deletion completed in 24.229070235s

• [SLOW TEST:122.135 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:49:03.090: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-n8nlw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 28 07:49:05.279: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml api-versions'
Feb 28 07:49:05.998: INFO: stderr: ""
Feb 28 07:49:05.998: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:49:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n8nlw" for this suite.
Feb 28 07:49:12.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:13.754: INFO: namespace: e2e-tests-kubectl-n8nlw, resource: bindings, ignored listing per whitelist
Feb 28 07:49:14.361: INFO: namespace e2e-tests-kubectl-n8nlw deletion completed in 8.307382231s

• [SLOW TEST:11.271 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:49:14.361: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-fptf4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-fptf4
I0228 07:49:16.562495   29661 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-fptf4, replica count: 1
I0228 07:49:17.663042   29661 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 07:49:18.663260   29661 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 07:49:18.821: INFO: Created: latency-svc-nfsrb
Feb 28 07:49:18.825: INFO: Got endpoints: latency-svc-nfsrb [61.685772ms]
Feb 28 07:49:18.883: INFO: Created: latency-svc-44x7z
Feb 28 07:49:18.887: INFO: Created: latency-svc-qh9kg
Feb 28 07:49:18.888: INFO: Got endpoints: latency-svc-44x7z [62.520778ms]
Feb 28 07:49:18.890: INFO: Got endpoints: latency-svc-qh9kg [64.501033ms]
Feb 28 07:49:18.891: INFO: Created: latency-svc-8vg6f
Feb 28 07:49:18.892: INFO: Got endpoints: latency-svc-8vg6f [66.962813ms]
Feb 28 07:49:18.895: INFO: Created: latency-svc-85r6c
Feb 28 07:49:18.896: INFO: Got endpoints: latency-svc-85r6c [70.930317ms]
Feb 28 07:49:18.899: INFO: Created: latency-svc-58hx5
Feb 28 07:49:18.901: INFO: Got endpoints: latency-svc-58hx5 [75.014123ms]
Feb 28 07:49:18.905: INFO: Created: latency-svc-wkw94
Feb 28 07:49:18.906: INFO: Got endpoints: latency-svc-wkw94 [79.999624ms]
Feb 28 07:49:18.909: INFO: Created: latency-svc-5rmxk
Feb 28 07:49:18.910: INFO: Got endpoints: latency-svc-5rmxk [83.754422ms]
Feb 28 07:49:18.913: INFO: Created: latency-svc-4t52k
Feb 28 07:49:18.914: INFO: Got endpoints: latency-svc-4t52k [88.304683ms]
Feb 28 07:49:18.917: INFO: Created: latency-svc-7b9zm
Feb 28 07:49:18.918: INFO: Got endpoints: latency-svc-7b9zm [91.775267ms]
Feb 28 07:49:18.921: INFO: Created: latency-svc-rj2dm
Feb 28 07:49:18.922: INFO: Got endpoints: latency-svc-rj2dm [95.55798ms]
Feb 28 07:49:18.926: INFO: Created: latency-svc-lkcdw
Feb 28 07:49:18.927: INFO: Got endpoints: latency-svc-lkcdw [100.820514ms]
Feb 28 07:49:18.930: INFO: Created: latency-svc-gsmbh
Feb 28 07:49:18.932: INFO: Got endpoints: latency-svc-gsmbh [104.964365ms]
Feb 28 07:49:18.934: INFO: Created: latency-svc-2m9vs
Feb 28 07:49:18.935: INFO: Got endpoints: latency-svc-2m9vs [108.779717ms]
Feb 28 07:49:18.938: INFO: Created: latency-svc-cvs6n
Feb 28 07:49:18.939: INFO: Got endpoints: latency-svc-cvs6n [113.162551ms]
Feb 28 07:49:18.943: INFO: Created: latency-svc-b68q4
Feb 28 07:49:18.945: INFO: Got endpoints: latency-svc-b68q4 [118.1959ms]
Feb 28 07:49:18.947: INFO: Created: latency-svc-stxjt
Feb 28 07:49:18.949: INFO: Got endpoints: latency-svc-stxjt [61.247887ms]
Feb 28 07:49:18.952: INFO: Created: latency-svc-nw5q8
Feb 28 07:49:18.953: INFO: Got endpoints: latency-svc-nw5q8 [62.311875ms]
Feb 28 07:49:18.956: INFO: Created: latency-svc-pd52v
Feb 28 07:49:18.957: INFO: Got endpoints: latency-svc-pd52v [65.109676ms]
Feb 28 07:49:18.967: INFO: Created: latency-svc-mkwbf
Feb 28 07:49:18.968: INFO: Got endpoints: latency-svc-mkwbf [71.629891ms]
Feb 28 07:49:18.970: INFO: Created: latency-svc-rxh2l
Feb 28 07:49:18.972: INFO: Got endpoints: latency-svc-rxh2l [71.768517ms]
Feb 28 07:49:18.976: INFO: Created: latency-svc-x7crp
Feb 28 07:49:18.977: INFO: Got endpoints: latency-svc-x7crp [71.158598ms]
Feb 28 07:49:18.980: INFO: Created: latency-svc-bq25g
Feb 28 07:49:18.981: INFO: Got endpoints: latency-svc-bq25g [71.386538ms]
Feb 28 07:49:18.984: INFO: Created: latency-svc-j9xgg
Feb 28 07:49:18.986: INFO: Got endpoints: latency-svc-j9xgg [71.632869ms]
Feb 28 07:49:18.989: INFO: Created: latency-svc-wrbhz
Feb 28 07:49:18.990: INFO: Got endpoints: latency-svc-wrbhz [71.995206ms]
Feb 28 07:49:18.998: INFO: Created: latency-svc-gkgv6
Feb 28 07:49:19.001: INFO: Got endpoints: latency-svc-gkgv6 [78.523696ms]
Feb 28 07:49:19.005: INFO: Created: latency-svc-xzwqk
Feb 28 07:49:19.006: INFO: Got endpoints: latency-svc-xzwqk [78.962401ms]
Feb 28 07:49:19.010: INFO: Created: latency-svc-cnxz6
Feb 28 07:49:19.014: INFO: Created: latency-svc-5sg7g
Feb 28 07:49:19.014: INFO: Got endpoints: latency-svc-cnxz6 [82.90062ms]
Feb 28 07:49:19.016: INFO: Got endpoints: latency-svc-5sg7g [80.085557ms]
Feb 28 07:49:19.020: INFO: Created: latency-svc-pwql5
Feb 28 07:49:19.025: INFO: Got endpoints: latency-svc-pwql5 [85.424462ms]
Feb 28 07:49:19.026: INFO: Created: latency-svc-92gxg
Feb 28 07:49:19.028: INFO: Got endpoints: latency-svc-92gxg [82.995595ms]
Feb 28 07:49:19.030: INFO: Created: latency-svc-6qbkt
Feb 28 07:49:19.034: INFO: Got endpoints: latency-svc-6qbkt [61.457931ms]
Feb 28 07:49:19.034: INFO: Created: latency-svc-mlfx9
Feb 28 07:49:19.034: INFO: Got endpoints: latency-svc-mlfx9 [85.48479ms]
Feb 28 07:49:19.037: INFO: Created: latency-svc-srsgr
Feb 28 07:49:19.038: INFO: Got endpoints: latency-svc-srsgr [85.635056ms]
Feb 28 07:49:19.043: INFO: Created: latency-svc-8jtvw
Feb 28 07:49:19.046: INFO: Created: latency-svc-lpxs2
Feb 28 07:49:19.051: INFO: Created: latency-svc-h8bq5
Feb 28 07:49:19.054: INFO: Created: latency-svc-vnnrs
Feb 28 07:49:19.059: INFO: Created: latency-svc-j92gq
Feb 28 07:49:19.064: INFO: Created: latency-svc-c9qkq
Feb 28 07:49:19.067: INFO: Created: latency-svc-n88qc
Feb 28 07:49:19.071: INFO: Created: latency-svc-z7lh7
Feb 28 07:49:19.075: INFO: Created: latency-svc-rsgll
Feb 28 07:49:19.079: INFO: Created: latency-svc-m8rbn
Feb 28 07:49:19.083: INFO: Created: latency-svc-zmwmj
Feb 28 07:49:19.086: INFO: Created: latency-svc-wk8bz
Feb 28 07:49:19.086: INFO: Got endpoints: latency-svc-8jtvw [128.632012ms]
Feb 28 07:49:19.091: INFO: Created: latency-svc-6plxn
Feb 28 07:49:19.095: INFO: Created: latency-svc-db8m2
Feb 28 07:49:19.098: INFO: Created: latency-svc-xr9m8
Feb 28 07:49:19.134: INFO: Got endpoints: latency-svc-lpxs2 [166.215159ms]
Feb 28 07:49:19.144: INFO: Created: latency-svc-zfnbk
Feb 28 07:49:19.185: INFO: Got endpoints: latency-svc-h8bq5 [207.633723ms]
Feb 28 07:49:19.192: INFO: Created: latency-svc-vwrtk
Feb 28 07:49:19.235: INFO: Got endpoints: latency-svc-vnnrs [253.318129ms]
Feb 28 07:49:19.243: INFO: Created: latency-svc-7gpxj
Feb 28 07:49:19.286: INFO: Got endpoints: latency-svc-j92gq [299.886578ms]
Feb 28 07:49:19.292: INFO: Created: latency-svc-4trgx
Feb 28 07:49:19.335: INFO: Got endpoints: latency-svc-c9qkq [345.304522ms]
Feb 28 07:49:19.344: INFO: Created: latency-svc-5bx86
Feb 28 07:49:19.385: INFO: Got endpoints: latency-svc-n88qc [384.201062ms]
Feb 28 07:49:19.393: INFO: Created: latency-svc-7zmlj
Feb 28 07:49:19.435: INFO: Got endpoints: latency-svc-z7lh7 [428.29157ms]
Feb 28 07:49:19.442: INFO: Created: latency-svc-z5dlt
Feb 28 07:49:19.485: INFO: Got endpoints: latency-svc-rsgll [468.944614ms]
Feb 28 07:49:19.493: INFO: Created: latency-svc-9jdm5
Feb 28 07:49:19.535: INFO: Got endpoints: latency-svc-m8rbn [520.282719ms]
Feb 28 07:49:19.543: INFO: Created: latency-svc-tqsts
Feb 28 07:49:19.585: INFO: Got endpoints: latency-svc-zmwmj [560.006178ms]
Feb 28 07:49:19.593: INFO: Created: latency-svc-tdblg
Feb 28 07:49:19.635: INFO: Got endpoints: latency-svc-wk8bz [607.238678ms]
Feb 28 07:49:19.643: INFO: Created: latency-svc-mk26h
Feb 28 07:49:19.693: INFO: Created: latency-svc-87knb
Feb 28 07:49:19.708: INFO: Got endpoints: latency-svc-6plxn [673.764472ms]
Feb 28 07:49:19.735: INFO: Got endpoints: latency-svc-db8m2 [700.342767ms]
Feb 28 07:49:19.766: INFO: Created: latency-svc-ngx6l
Feb 28 07:49:19.785: INFO: Got endpoints: latency-svc-xr9m8 [746.494629ms]
Feb 28 07:49:19.793: INFO: Created: latency-svc-pq74c
Feb 28 07:49:19.836: INFO: Got endpoints: latency-svc-zfnbk [749.065269ms]
Feb 28 07:49:19.843: INFO: Created: latency-svc-cjd49
Feb 28 07:49:19.885: INFO: Got endpoints: latency-svc-vwrtk [750.314982ms]
Feb 28 07:49:19.894: INFO: Created: latency-svc-5b9cq
Feb 28 07:49:19.935: INFO: Got endpoints: latency-svc-7gpxj [749.687079ms]
Feb 28 07:49:19.942: INFO: Created: latency-svc-5p9b6
Feb 28 07:49:19.985: INFO: Got endpoints: latency-svc-4trgx [750.109958ms]
Feb 28 07:49:19.993: INFO: Created: latency-svc-nw7bk
Feb 28 07:49:20.035: INFO: Got endpoints: latency-svc-5bx86 [749.35368ms]
Feb 28 07:49:20.048: INFO: Created: latency-svc-2l5lt
Feb 28 07:49:20.085: INFO: Got endpoints: latency-svc-7zmlj [749.584326ms]
Feb 28 07:49:20.093: INFO: Created: latency-svc-ttxsc
Feb 28 07:49:20.135: INFO: Got endpoints: latency-svc-z5dlt [749.822819ms]
Feb 28 07:49:20.147: INFO: Created: latency-svc-xr2nx
Feb 28 07:49:20.185: INFO: Got endpoints: latency-svc-9jdm5 [749.79941ms]
Feb 28 07:49:20.192: INFO: Created: latency-svc-pn84t
Feb 28 07:49:20.235: INFO: Got endpoints: latency-svc-tqsts [750.134115ms]
Feb 28 07:49:20.243: INFO: Created: latency-svc-jdwc5
Feb 28 07:49:20.285: INFO: Got endpoints: latency-svc-tdblg [750.19086ms]
Feb 28 07:49:20.295: INFO: Created: latency-svc-5rwn5
Feb 28 07:49:20.335: INFO: Got endpoints: latency-svc-mk26h [750.136389ms]
Feb 28 07:49:20.343: INFO: Created: latency-svc-zmjkf
Feb 28 07:49:20.385: INFO: Got endpoints: latency-svc-87knb [750.039375ms]
Feb 28 07:49:20.393: INFO: Created: latency-svc-rspcp
Feb 28 07:49:20.435: INFO: Got endpoints: latency-svc-ngx6l [726.987123ms]
Feb 28 07:49:20.444: INFO: Created: latency-svc-ps2qg
Feb 28 07:49:20.485: INFO: Got endpoints: latency-svc-pq74c [750.017325ms]
Feb 28 07:49:20.494: INFO: Created: latency-svc-tdmn4
Feb 28 07:49:20.535: INFO: Got endpoints: latency-svc-cjd49 [749.891007ms]
Feb 28 07:49:20.545: INFO: Created: latency-svc-hpqzt
Feb 28 07:49:20.585: INFO: Got endpoints: latency-svc-5b9cq [749.2226ms]
Feb 28 07:49:20.593: INFO: Created: latency-svc-282m9
Feb 28 07:49:20.637: INFO: Got endpoints: latency-svc-5p9b6 [751.906001ms]
Feb 28 07:49:20.648: INFO: Created: latency-svc-f7tnh
Feb 28 07:49:20.685: INFO: Got endpoints: latency-svc-nw7bk [750.411094ms]
Feb 28 07:49:20.695: INFO: Created: latency-svc-4krsx
Feb 28 07:49:20.735: INFO: Got endpoints: latency-svc-2l5lt [749.79082ms]
Feb 28 07:49:20.743: INFO: Created: latency-svc-z8k7t
Feb 28 07:49:20.785: INFO: Got endpoints: latency-svc-ttxsc [749.544018ms]
Feb 28 07:49:20.793: INFO: Created: latency-svc-h5zxh
Feb 28 07:49:20.837: INFO: Got endpoints: latency-svc-xr2nx [751.577542ms]
Feb 28 07:49:20.845: INFO: Created: latency-svc-cz4ct
Feb 28 07:49:20.887: INFO: Got endpoints: latency-svc-pn84t [752.441968ms]
Feb 28 07:49:20.896: INFO: Created: latency-svc-d8s6b
Feb 28 07:49:20.937: INFO: Got endpoints: latency-svc-jdwc5 [751.830464ms]
Feb 28 07:49:20.946: INFO: Created: latency-svc-hmm8n
Feb 28 07:49:20.985: INFO: Got endpoints: latency-svc-5rwn5 [750.210025ms]
Feb 28 07:49:20.995: INFO: Created: latency-svc-4f4f4
Feb 28 07:49:21.035: INFO: Got endpoints: latency-svc-zmjkf [749.670287ms]
Feb 28 07:49:21.043: INFO: Created: latency-svc-qktx6
Feb 28 07:49:21.085: INFO: Got endpoints: latency-svc-rspcp [749.636285ms]
Feb 28 07:49:21.095: INFO: Created: latency-svc-lrv97
Feb 28 07:49:21.136: INFO: Got endpoints: latency-svc-ps2qg [750.867902ms]
Feb 28 07:49:21.144: INFO: Created: latency-svc-dz75r
Feb 28 07:49:21.189: INFO: Got endpoints: latency-svc-tdmn4 [753.637564ms]
Feb 28 07:49:21.196: INFO: Created: latency-svc-nkctm
Feb 28 07:49:21.235: INFO: Got endpoints: latency-svc-hpqzt [749.906243ms]
Feb 28 07:49:21.247: INFO: Created: latency-svc-7kb4h
Feb 28 07:49:21.286: INFO: Got endpoints: latency-svc-282m9 [750.784517ms]
Feb 28 07:49:21.295: INFO: Created: latency-svc-2n8c7
Feb 28 07:49:21.335: INFO: Got endpoints: latency-svc-f7tnh [749.964243ms]
Feb 28 07:49:21.343: INFO: Created: latency-svc-jmhzm
Feb 28 07:49:21.385: INFO: Got endpoints: latency-svc-4krsx [747.908689ms]
Feb 28 07:49:21.394: INFO: Created: latency-svc-8vjs4
Feb 28 07:49:21.435: INFO: Got endpoints: latency-svc-z8k7t [749.631091ms]
Feb 28 07:49:21.443: INFO: Created: latency-svc-rxhd4
Feb 28 07:49:21.485: INFO: Got endpoints: latency-svc-h5zxh [750.235296ms]
Feb 28 07:49:21.493: INFO: Created: latency-svc-2bxrf
Feb 28 07:49:21.535: INFO: Got endpoints: latency-svc-cz4ct [749.929705ms]
Feb 28 07:49:21.543: INFO: Created: latency-svc-trh7k
Feb 28 07:49:21.590: INFO: Got endpoints: latency-svc-d8s6b [752.889863ms]
Feb 28 07:49:21.600: INFO: Created: latency-svc-v2swf
Feb 28 07:49:21.636: INFO: Got endpoints: latency-svc-hmm8n [748.412058ms]
Feb 28 07:49:21.650: INFO: Created: latency-svc-nwb9k
Feb 28 07:49:21.686: INFO: Got endpoints: latency-svc-4f4f4 [749.061554ms]
Feb 28 07:49:21.695: INFO: Created: latency-svc-qlmk6
Feb 28 07:49:21.735: INFO: Got endpoints: latency-svc-qktx6 [749.505904ms]
Feb 28 07:49:21.744: INFO: Created: latency-svc-wmql9
Feb 28 07:49:21.785: INFO: Got endpoints: latency-svc-lrv97 [749.966757ms]
Feb 28 07:49:21.793: INFO: Created: latency-svc-tg9mg
Feb 28 07:49:21.835: INFO: Got endpoints: latency-svc-dz75r [749.72983ms]
Feb 28 07:49:21.843: INFO: Created: latency-svc-svf8b
Feb 28 07:49:21.885: INFO: Got endpoints: latency-svc-nkctm [748.465701ms]
Feb 28 07:49:21.892: INFO: Created: latency-svc-rkvm5
Feb 28 07:49:21.935: INFO: Got endpoints: latency-svc-7kb4h [746.183923ms]
Feb 28 07:49:21.943: INFO: Created: latency-svc-c8876
Feb 28 07:49:21.985: INFO: Got endpoints: latency-svc-2n8c7 [749.705896ms]
Feb 28 07:49:21.996: INFO: Created: latency-svc-wlgml
Feb 28 07:49:22.036: INFO: Got endpoints: latency-svc-jmhzm [749.929207ms]
Feb 28 07:49:22.043: INFO: Created: latency-svc-8zwgn
Feb 28 07:49:22.085: INFO: Got endpoints: latency-svc-8vjs4 [749.992417ms]
Feb 28 07:49:22.094: INFO: Created: latency-svc-zsn94
Feb 28 07:49:22.135: INFO: Got endpoints: latency-svc-rxhd4 [750.041932ms]
Feb 28 07:49:22.143: INFO: Created: latency-svc-6mxl8
Feb 28 07:49:22.185: INFO: Got endpoints: latency-svc-2bxrf [749.922931ms]
Feb 28 07:49:22.193: INFO: Created: latency-svc-zl4g9
Feb 28 07:49:22.235: INFO: Got endpoints: latency-svc-trh7k [749.702974ms]
Feb 28 07:49:22.243: INFO: Created: latency-svc-48pgq
Feb 28 07:49:22.286: INFO: Got endpoints: latency-svc-v2swf [750.70278ms]
Feb 28 07:49:22.293: INFO: Created: latency-svc-kdvwz
Feb 28 07:49:22.335: INFO: Got endpoints: latency-svc-nwb9k [745.65339ms]
Feb 28 07:49:22.353: INFO: Created: latency-svc-gwdgt
Feb 28 07:49:22.385: INFO: Got endpoints: latency-svc-qlmk6 [748.943977ms]
Feb 28 07:49:22.395: INFO: Created: latency-svc-p6czh
Feb 28 07:49:22.436: INFO: Got endpoints: latency-svc-wmql9 [749.813161ms]
Feb 28 07:49:22.443: INFO: Created: latency-svc-q9jdr
Feb 28 07:49:22.485: INFO: Got endpoints: latency-svc-tg9mg [750.021261ms]
Feb 28 07:49:22.494: INFO: Created: latency-svc-4jj9j
Feb 28 07:49:22.535: INFO: Got endpoints: latency-svc-svf8b [749.698507ms]
Feb 28 07:49:22.543: INFO: Created: latency-svc-9lhn8
Feb 28 07:49:22.585: INFO: Got endpoints: latency-svc-rkvm5 [749.994472ms]
Feb 28 07:49:22.594: INFO: Created: latency-svc-m5bdw
Feb 28 07:49:22.635: INFO: Got endpoints: latency-svc-c8876 [749.846283ms]
Feb 28 07:49:22.644: INFO: Created: latency-svc-zl7kf
Feb 28 07:49:22.685: INFO: Got endpoints: latency-svc-wlgml [749.789883ms]
Feb 28 07:49:22.693: INFO: Created: latency-svc-hd8rf
Feb 28 07:49:22.735: INFO: Got endpoints: latency-svc-8zwgn [750.298639ms]
Feb 28 07:49:22.743: INFO: Created: latency-svc-p6d6d
Feb 28 07:49:22.785: INFO: Got endpoints: latency-svc-zsn94 [749.454787ms]
Feb 28 07:49:22.793: INFO: Created: latency-svc-7mlqz
Feb 28 07:49:22.835: INFO: Got endpoints: latency-svc-6mxl8 [749.653928ms]
Feb 28 07:49:22.843: INFO: Created: latency-svc-xrdrd
Feb 28 07:49:22.885: INFO: Got endpoints: latency-svc-zl4g9 [749.993026ms]
Feb 28 07:49:22.892: INFO: Created: latency-svc-rfv2d
Feb 28 07:49:22.935: INFO: Got endpoints: latency-svc-48pgq [749.986525ms]
Feb 28 07:49:22.945: INFO: Created: latency-svc-28gjp
Feb 28 07:49:22.986: INFO: Got endpoints: latency-svc-kdvwz [751.396751ms]
Feb 28 07:49:22.993: INFO: Created: latency-svc-4mqwr
Feb 28 07:49:23.035: INFO: Got endpoints: latency-svc-gwdgt [748.929818ms]
Feb 28 07:49:23.045: INFO: Created: latency-svc-22brw
Feb 28 07:49:23.087: INFO: Got endpoints: latency-svc-p6czh [751.668587ms]
Feb 28 07:49:23.095: INFO: Created: latency-svc-prb9f
Feb 28 07:49:23.139: INFO: Got endpoints: latency-svc-q9jdr [754.214295ms]
Feb 28 07:49:23.148: INFO: Created: latency-svc-4cbzf
Feb 28 07:49:23.185: INFO: Got endpoints: latency-svc-4jj9j [749.460128ms]
Feb 28 07:49:23.198: INFO: Created: latency-svc-6csvc
Feb 28 07:49:23.235: INFO: Got endpoints: latency-svc-9lhn8 [750.071465ms]
Feb 28 07:49:23.243: INFO: Created: latency-svc-hm24c
Feb 28 07:49:23.285: INFO: Got endpoints: latency-svc-m5bdw [750.660842ms]
Feb 28 07:49:23.293: INFO: Created: latency-svc-wsbzk
Feb 28 07:49:23.335: INFO: Got endpoints: latency-svc-zl7kf [750.47857ms]
Feb 28 07:49:23.344: INFO: Created: latency-svc-6n8lf
Feb 28 07:49:23.385: INFO: Got endpoints: latency-svc-hd8rf [750.12965ms]
Feb 28 07:49:23.393: INFO: Created: latency-svc-24jh4
Feb 28 07:49:23.436: INFO: Got endpoints: latency-svc-p6d6d [751.470649ms]
Feb 28 07:49:23.443: INFO: Created: latency-svc-6dwxx
Feb 28 07:49:23.485: INFO: Got endpoints: latency-svc-7mlqz [749.700331ms]
Feb 28 07:49:23.495: INFO: Created: latency-svc-5vrfj
Feb 28 07:49:23.535: INFO: Got endpoints: latency-svc-xrdrd [749.849206ms]
Feb 28 07:49:23.548: INFO: Created: latency-svc-8grxx
Feb 28 07:49:23.652: INFO: Got endpoints: latency-svc-rfv2d [817.009411ms]
Feb 28 07:49:23.652: INFO: Got endpoints: latency-svc-28gjp [767.53208ms]
Feb 28 07:49:23.658: INFO: Created: latency-svc-zwwlw
Feb 28 07:49:23.685: INFO: Got endpoints: latency-svc-4mqwr [750.284157ms]
Feb 28 07:49:23.712: INFO: Created: latency-svc-jk5b5
Feb 28 07:49:23.715: INFO: Created: latency-svc-tkjnz
Feb 28 07:49:23.735: INFO: Got endpoints: latency-svc-22brw [748.586286ms]
Feb 28 07:49:23.745: INFO: Created: latency-svc-w7988
Feb 28 07:49:23.787: INFO: Got endpoints: latency-svc-prb9f [752.213294ms]
Feb 28 07:49:23.793: INFO: Created: latency-svc-f29p2
Feb 28 07:49:23.835: INFO: Got endpoints: latency-svc-4cbzf [748.219526ms]
Feb 28 07:49:23.849: INFO: Created: latency-svc-46jnw
Feb 28 07:49:23.885: INFO: Got endpoints: latency-svc-6csvc [746.353149ms]
Feb 28 07:49:23.894: INFO: Created: latency-svc-5dxwn
Feb 28 07:49:23.935: INFO: Got endpoints: latency-svc-hm24c [749.534915ms]
Feb 28 07:49:23.943: INFO: Created: latency-svc-7zzm5
Feb 28 07:49:23.985: INFO: Got endpoints: latency-svc-wsbzk [749.760427ms]
Feb 28 07:49:23.995: INFO: Created: latency-svc-f2s5n
Feb 28 07:49:24.035: INFO: Got endpoints: latency-svc-6n8lf [749.445935ms]
Feb 28 07:49:24.043: INFO: Created: latency-svc-6gw6t
Feb 28 07:49:24.085: INFO: Got endpoints: latency-svc-24jh4 [749.262029ms]
Feb 28 07:49:24.093: INFO: Created: latency-svc-ff22f
Feb 28 07:49:24.135: INFO: Got endpoints: latency-svc-6dwxx [749.81802ms]
Feb 28 07:49:24.143: INFO: Created: latency-svc-jswph
Feb 28 07:49:24.185: INFO: Got endpoints: latency-svc-5vrfj [748.62928ms]
Feb 28 07:49:24.193: INFO: Created: latency-svc-kfzph
Feb 28 07:49:24.235: INFO: Got endpoints: latency-svc-8grxx [750.186591ms]
Feb 28 07:49:24.243: INFO: Created: latency-svc-vdgxg
Feb 28 07:49:24.285: INFO: Got endpoints: latency-svc-zwwlw [749.772214ms]
Feb 28 07:49:24.293: INFO: Created: latency-svc-gp687
Feb 28 07:49:24.335: INFO: Got endpoints: latency-svc-jk5b5 [682.738898ms]
Feb 28 07:49:24.344: INFO: Created: latency-svc-h92qc
Feb 28 07:49:24.386: INFO: Got endpoints: latency-svc-tkjnz [733.073982ms]
Feb 28 07:49:24.393: INFO: Created: latency-svc-g9dgc
Feb 28 07:49:24.436: INFO: Got endpoints: latency-svc-w7988 [750.243102ms]
Feb 28 07:49:24.445: INFO: Created: latency-svc-7flsg
Feb 28 07:49:24.485: INFO: Got endpoints: latency-svc-f29p2 [749.872729ms]
Feb 28 07:49:24.494: INFO: Created: latency-svc-4vflg
Feb 28 07:49:24.535: INFO: Got endpoints: latency-svc-46jnw [747.72676ms]
Feb 28 07:49:24.543: INFO: Created: latency-svc-c9cnr
Feb 28 07:49:24.585: INFO: Got endpoints: latency-svc-5dxwn [749.375969ms]
Feb 28 07:49:24.593: INFO: Created: latency-svc-f4bpd
Feb 28 07:49:24.634: INFO: Got endpoints: latency-svc-7zzm5 [748.872862ms]
Feb 28 07:49:24.643: INFO: Created: latency-svc-49zbs
Feb 28 07:49:24.685: INFO: Got endpoints: latency-svc-f2s5n [749.912983ms]
Feb 28 07:49:24.692: INFO: Created: latency-svc-cpsdr
Feb 28 07:49:24.735: INFO: Got endpoints: latency-svc-6gw6t [749.997264ms]
Feb 28 07:49:24.744: INFO: Created: latency-svc-qnhxn
Feb 28 07:49:24.785: INFO: Got endpoints: latency-svc-ff22f [749.955762ms]
Feb 28 07:49:24.793: INFO: Created: latency-svc-2s9pg
Feb 28 07:49:24.835: INFO: Got endpoints: latency-svc-jswph [750.043386ms]
Feb 28 07:49:24.843: INFO: Created: latency-svc-9wkmz
Feb 28 07:49:24.885: INFO: Got endpoints: latency-svc-kfzph [750.411373ms]
Feb 28 07:49:24.894: INFO: Created: latency-svc-m669s
Feb 28 07:49:24.935: INFO: Got endpoints: latency-svc-vdgxg [750.335375ms]
Feb 28 07:49:24.947: INFO: Created: latency-svc-42x2v
Feb 28 07:49:24.985: INFO: Got endpoints: latency-svc-gp687 [749.731314ms]
Feb 28 07:49:24.994: INFO: Created: latency-svc-4z548
Feb 28 07:49:25.035: INFO: Got endpoints: latency-svc-h92qc [749.658852ms]
Feb 28 07:49:25.043: INFO: Created: latency-svc-gvhrc
Feb 28 07:49:25.085: INFO: Got endpoints: latency-svc-g9dgc [749.988172ms]
Feb 28 07:49:25.093: INFO: Created: latency-svc-bqkm9
Feb 28 07:49:25.138: INFO: Got endpoints: latency-svc-7flsg [751.906445ms]
Feb 28 07:49:25.143: INFO: Created: latency-svc-9hm4m
Feb 28 07:49:25.185: INFO: Got endpoints: latency-svc-4vflg [749.113484ms]
Feb 28 07:49:25.196: INFO: Created: latency-svc-plnqw
Feb 28 07:49:25.235: INFO: Got endpoints: latency-svc-c9cnr [749.672039ms]
Feb 28 07:49:25.246: INFO: Created: latency-svc-r5f7v
Feb 28 07:49:25.285: INFO: Got endpoints: latency-svc-f4bpd [749.946776ms]
Feb 28 07:49:25.295: INFO: Created: latency-svc-7xzc5
Feb 28 07:49:25.361: INFO: Got endpoints: latency-svc-49zbs [776.135458ms]
Feb 28 07:49:25.361: INFO: Created: latency-svc-chst7
Feb 28 07:49:25.385: INFO: Got endpoints: latency-svc-cpsdr [750.45215ms]
Feb 28 07:49:25.419: INFO: Created: latency-svc-vsl8b
Feb 28 07:49:25.435: INFO: Got endpoints: latency-svc-qnhxn [749.847587ms]
Feb 28 07:49:25.443: INFO: Created: latency-svc-hd6dw
Feb 28 07:49:25.485: INFO: Got endpoints: latency-svc-2s9pg [750.034168ms]
Feb 28 07:49:25.493: INFO: Created: latency-svc-4kcjr
Feb 28 07:49:25.535: INFO: Got endpoints: latency-svc-9wkmz [749.947954ms]
Feb 28 07:49:25.546: INFO: Created: latency-svc-2nd8w
Feb 28 07:49:25.585: INFO: Got endpoints: latency-svc-m669s [750.052339ms]
Feb 28 07:49:25.593: INFO: Created: latency-svc-6275b
Feb 28 07:49:25.635: INFO: Got endpoints: latency-svc-42x2v [749.686868ms]
Feb 28 07:49:25.643: INFO: Created: latency-svc-bm227
Feb 28 07:49:25.685: INFO: Got endpoints: latency-svc-4z548 [749.61607ms]
Feb 28 07:49:25.693: INFO: Created: latency-svc-674w2
Feb 28 07:49:25.735: INFO: Got endpoints: latency-svc-gvhrc [749.75152ms]
Feb 28 07:49:25.743: INFO: Created: latency-svc-5qw79
Feb 28 07:49:25.785: INFO: Got endpoints: latency-svc-bqkm9 [750.001677ms]
Feb 28 07:49:25.792: INFO: Created: latency-svc-dbcdx
Feb 28 07:49:25.835: INFO: Got endpoints: latency-svc-9hm4m [749.864729ms]
Feb 28 07:49:25.843: INFO: Created: latency-svc-fs4vp
Feb 28 07:49:25.886: INFO: Got endpoints: latency-svc-plnqw [748.080019ms]
Feb 28 07:49:25.893: INFO: Created: latency-svc-lx8p4
Feb 28 07:49:25.935: INFO: Got endpoints: latency-svc-r5f7v [750.011097ms]
Feb 28 07:49:25.945: INFO: Created: latency-svc-qzq2q
Feb 28 07:49:25.985: INFO: Got endpoints: latency-svc-7xzc5 [750.179454ms]
Feb 28 07:49:25.993: INFO: Created: latency-svc-gs4z2
Feb 28 07:49:26.035: INFO: Got endpoints: latency-svc-chst7 [750.089679ms]
Feb 28 07:49:26.044: INFO: Created: latency-svc-fkwhf
Feb 28 07:49:26.085: INFO: Got endpoints: latency-svc-vsl8b [723.607753ms]
Feb 28 07:49:26.093: INFO: Created: latency-svc-n7vmd
Feb 28 07:49:26.136: INFO: Got endpoints: latency-svc-hd6dw [750.642841ms]
Feb 28 07:49:26.143: INFO: Created: latency-svc-6bzjp
Feb 28 07:49:26.186: INFO: Got endpoints: latency-svc-4kcjr [751.383493ms]
Feb 28 07:49:26.197: INFO: Created: latency-svc-bqdk5
Feb 28 07:49:26.236: INFO: Got endpoints: latency-svc-2nd8w [750.571704ms]
Feb 28 07:49:26.245: INFO: Created: latency-svc-fzlkv
Feb 28 07:49:26.286: INFO: Got endpoints: latency-svc-6275b [750.799867ms]
Feb 28 07:49:26.295: INFO: Created: latency-svc-ttcff
Feb 28 07:49:26.336: INFO: Got endpoints: latency-svc-bm227 [750.958125ms]
Feb 28 07:49:26.346: INFO: Created: latency-svc-js5m7
Feb 28 07:49:26.385: INFO: Got endpoints: latency-svc-674w2 [750.114569ms]
Feb 28 07:49:26.393: INFO: Created: latency-svc-k2w2l
Feb 28 07:49:26.434: INFO: Got endpoints: latency-svc-5qw79 [749.356184ms]
Feb 28 07:49:26.442: INFO: Created: latency-svc-82477
Feb 28 07:49:26.485: INFO: Got endpoints: latency-svc-dbcdx [750.301412ms]
Feb 28 07:49:26.494: INFO: Created: latency-svc-5hbnr
Feb 28 07:49:26.535: INFO: Got endpoints: latency-svc-fs4vp [749.821395ms]
Feb 28 07:49:26.543: INFO: Created: latency-svc-4j9pr
Feb 28 07:49:26.586: INFO: Got endpoints: latency-svc-lx8p4 [751.411287ms]
Feb 28 07:49:26.596: INFO: Created: latency-svc-jhnqw
Feb 28 07:49:26.635: INFO: Got endpoints: latency-svc-qzq2q [749.537899ms]
Feb 28 07:49:26.645: INFO: Created: latency-svc-vlbrp
Feb 28 07:49:26.685: INFO: Got endpoints: latency-svc-gs4z2 [749.643322ms]
Feb 28 07:49:26.694: INFO: Created: latency-svc-8wvbp
Feb 28 07:49:26.735: INFO: Got endpoints: latency-svc-fkwhf [750.132666ms]
Feb 28 07:49:26.786: INFO: Got endpoints: latency-svc-n7vmd [750.675997ms]
Feb 28 07:49:26.835: INFO: Got endpoints: latency-svc-6bzjp [750.561971ms]
Feb 28 07:49:26.886: INFO: Got endpoints: latency-svc-bqdk5 [750.506708ms]
Feb 28 07:49:26.935: INFO: Got endpoints: latency-svc-fzlkv [748.814764ms]
Feb 28 07:49:26.988: INFO: Got endpoints: latency-svc-ttcff [752.551462ms]
Feb 28 07:49:27.036: INFO: Got endpoints: latency-svc-js5m7 [749.921919ms]
Feb 28 07:49:27.085: INFO: Got endpoints: latency-svc-k2w2l [749.010007ms]
Feb 28 07:49:27.135: INFO: Got endpoints: latency-svc-82477 [749.77842ms]
Feb 28 07:49:27.185: INFO: Got endpoints: latency-svc-5hbnr [750.085496ms]
Feb 28 07:49:27.235: INFO: Got endpoints: latency-svc-4j9pr [750.230425ms]
Feb 28 07:49:27.285: INFO: Got endpoints: latency-svc-jhnqw [750.413901ms]
Feb 28 07:49:27.336: INFO: Got endpoints: latency-svc-vlbrp [749.797534ms]
Feb 28 07:49:27.385: INFO: Got endpoints: latency-svc-8wvbp [749.179603ms]
Feb 28 07:49:27.385: INFO: Latencies: [61.247887ms 61.457931ms 62.311875ms 62.520778ms 64.501033ms 65.109676ms 66.962813ms 70.930317ms 71.158598ms 71.386538ms 71.629891ms 71.632869ms 71.768517ms 71.995206ms 75.014123ms 78.523696ms 78.962401ms 79.999624ms 80.085557ms 82.90062ms 82.995595ms 83.754422ms 85.424462ms 85.48479ms 85.635056ms 88.304683ms 91.775267ms 95.55798ms 100.820514ms 104.964365ms 108.779717ms 113.162551ms 118.1959ms 128.632012ms 166.215159ms 207.633723ms 253.318129ms 299.886578ms 345.304522ms 384.201062ms 428.29157ms 468.944614ms 520.282719ms 560.006178ms 607.238678ms 673.764472ms 682.738898ms 700.342767ms 723.607753ms 726.987123ms 733.073982ms 745.65339ms 746.183923ms 746.353149ms 746.494629ms 747.72676ms 747.908689ms 748.080019ms 748.219526ms 748.412058ms 748.465701ms 748.586286ms 748.62928ms 748.814764ms 748.872862ms 748.929818ms 748.943977ms 749.010007ms 749.061554ms 749.065269ms 749.113484ms 749.179603ms 749.2226ms 749.262029ms 749.35368ms 749.356184ms 749.375969ms 749.445935ms 749.454787ms 749.460128ms 749.505904ms 749.534915ms 749.537899ms 749.544018ms 749.584326ms 749.61607ms 749.631091ms 749.636285ms 749.643322ms 749.653928ms 749.658852ms 749.670287ms 749.672039ms 749.686868ms 749.687079ms 749.698507ms 749.700331ms 749.702974ms 749.705896ms 749.72983ms 749.731314ms 749.75152ms 749.760427ms 749.772214ms 749.77842ms 749.789883ms 749.79082ms 749.797534ms 749.79941ms 749.813161ms 749.81802ms 749.821395ms 749.822819ms 749.846283ms 749.847587ms 749.849206ms 749.864729ms 749.872729ms 749.891007ms 749.906243ms 749.912983ms 749.921919ms 749.922931ms 749.929207ms 749.929705ms 749.946776ms 749.947954ms 749.955762ms 749.964243ms 749.966757ms 749.986525ms 749.988172ms 749.992417ms 749.993026ms 749.994472ms 749.997264ms 750.001677ms 750.011097ms 750.017325ms 750.021261ms 750.034168ms 750.039375ms 750.041932ms 750.043386ms 750.052339ms 750.071465ms 750.085496ms 750.089679ms 750.109958ms 750.114569ms 750.12965ms 750.132666ms 750.134115ms 750.136389ms 750.179454ms 750.186591ms 750.19086ms 750.210025ms 750.230425ms 750.235296ms 750.243102ms 750.284157ms 750.298639ms 750.301412ms 750.314982ms 750.335375ms 750.411094ms 750.411373ms 750.413901ms 750.45215ms 750.47857ms 750.506708ms 750.561971ms 750.571704ms 750.642841ms 750.660842ms 750.675997ms 750.70278ms 750.784517ms 750.799867ms 750.867902ms 750.958125ms 751.383493ms 751.396751ms 751.411287ms 751.470649ms 751.577542ms 751.668587ms 751.830464ms 751.906001ms 751.906445ms 752.213294ms 752.441968ms 752.551462ms 752.889863ms 753.637564ms 754.214295ms 767.53208ms 776.135458ms 817.009411ms]
Feb 28 07:49:27.386: INFO: 50 %ile: 749.731314ms
Feb 28 07:49:27.386: INFO: 90 %ile: 750.867902ms
Feb 28 07:49:27.386: INFO: 99 %ile: 776.135458ms
Feb 28 07:49:27.386: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:49:27.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-fptf4" for this suite.
Feb 28 07:49:47.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:49.269: INFO: namespace: e2e-tests-svc-latency-fptf4, resource: bindings, ignored listing per whitelist
Feb 28 07:49:49.706: INFO: namespace e2e-tests-svc-latency-fptf4 deletion completed in 22.265647159s

• [SLOW TEST:35.345 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:49:49.706: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-jmp76
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 28 07:49:51.940: INFO: Waiting up to 5m0s for pod "client-containers-6e22ae09-3b2d-11e9-9760-9e5b40196308" in namespace "e2e-tests-containers-jmp76" to be "success or failure"
Feb 28 07:49:51.994: INFO: Pod "client-containers-6e22ae09-3b2d-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.609214ms
Feb 28 07:49:54.048: INFO: Pod "client-containers-6e22ae09-3b2d-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108099087s
Feb 28 07:49:56.103: INFO: Pod "client-containers-6e22ae09-3b2d-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.163005425s
STEP: Saw pod success
Feb 28 07:49:56.103: INFO: Pod "client-containers-6e22ae09-3b2d-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:49:56.157: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod client-containers-6e22ae09-3b2d-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 07:49:56.276: INFO: Waiting for pod client-containers-6e22ae09-3b2d-11e9-9760-9e5b40196308 to disappear
Feb 28 07:49:56.330: INFO: Pod client-containers-6e22ae09-3b2d-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:49:56.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-jmp76" for this suite.
Feb 28 07:50:02.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:04.169: INFO: namespace: e2e-tests-containers-jmp76, resource: bindings, ignored listing per whitelist
Feb 28 07:50:04.600: INFO: namespace e2e-tests-containers-jmp76 deletion completed in 8.215026943s

• [SLOW TEST:14.894 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:50:04.600: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-9jcp9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0228 07:50:07.166362   29661 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:50:07.166: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:50:07.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9jcp9" for this suite.
Feb 28 07:50:13.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:14.911: INFO: namespace: e2e-tests-gc-9jcp9, resource: bindings, ignored listing per whitelist
Feb 28 07:50:15.450: INFO: namespace e2e-tests-gc-9jcp9 deletion completed in 8.230015569s

• [SLOW TEST:10.850 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:50:15.450: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2nl9j
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 07:50:17.660: INFO: Waiting up to 5m0s for pod "pod-7d773e84-3b2d-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-2nl9j" to be "success or failure"
Feb 28 07:50:17.714: INFO: Pod "pod-7d773e84-3b2d-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.683475ms
Feb 28 07:50:19.771: INFO: Pod "pod-7d773e84-3b2d-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.110750063s
STEP: Saw pod success
Feb 28 07:50:19.771: INFO: Pod "pod-7d773e84-3b2d-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:50:19.828: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-7d773e84-3b2d-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 07:50:20.041: INFO: Waiting for pod pod-7d773e84-3b2d-11e9-9760-9e5b40196308 to disappear
Feb 28 07:50:20.096: INFO: Pod pod-7d773e84-3b2d-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:50:20.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2nl9j" for this suite.
Feb 28 07:50:26.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:27.352: INFO: namespace: e2e-tests-emptydir-2nl9j, resource: bindings, ignored listing per whitelist
Feb 28 07:50:28.419: INFO: namespace e2e-tests-emptydir-2nl9j deletion completed in 8.267754018s

• [SLOW TEST:12.969 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:50:28.419: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zx2sc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 28 07:50:30.674: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-zx2sc'
Feb 28 07:50:33.808: INFO: stderr: ""
Feb 28 07:50:33.808: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 07:50:34.863: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:50:34.864: INFO: Found 0 / 1
Feb 28 07:50:35.863: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:50:35.863: INFO: Found 1 / 1
Feb 28 07:50:35.863: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 28 07:50:35.917: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:50:35.917: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 07:50:35.917: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml patch pod redis-master-lvqxd --namespace=e2e-tests-kubectl-zx2sc -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 28 07:50:36.275: INFO: stderr: ""
Feb 28 07:50:36.275: INFO: stdout: "pod/redis-master-lvqxd patched\n"
STEP: checking annotations
Feb 28 07:50:36.329: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:50:36.329: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:50:36.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zx2sc" for this suite.
Feb 28 07:50:58.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:58.969: INFO: namespace: e2e-tests-kubectl-zx2sc, resource: bindings, ignored listing per whitelist
Feb 28 07:51:00.645: INFO: namespace e2e-tests-kubectl-zx2sc deletion completed in 24.261642407s

• [SLOW TEST:32.226 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:51:00.646: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qzqlj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 07:51:02.877: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-qzqlj'
Feb 28 07:51:03.211: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 07:51:03.211: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Feb 28 07:51:03.323: INFO: scanned /root for discovery docs: <nil>
Feb 28 07:51:03.323: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-qzqlj'
Feb 28 07:51:15.663: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 07:51:15.663: INFO: stdout: "Created e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518\nScaling up e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 28 07:51:15.663: INFO: stdout: "Created e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518\nScaling up e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 28 07:51:15.664: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qzqlj'
Feb 28 07:51:16.096: INFO: stderr: ""
Feb 28 07:51:16.096: INFO: stdout: "e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518-4j2vj "
Feb 28 07:51:16.096: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518-4j2vj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qzqlj'
Feb 28 07:51:16.407: INFO: stderr: ""
Feb 28 07:51:16.407: INFO: stdout: "true"
Feb 28 07:51:16.407: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518-4j2vj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qzqlj'
Feb 28 07:51:16.724: INFO: stderr: ""
Feb 28 07:51:16.724: INFO: stdout: "nginx:1.14-alpine"
Feb 28 07:51:16.724: INFO: e2e-test-nginx-rc-6cb96e2cd9cfc792062aee71e262c518-4j2vj is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
Feb 28 07:51:16.724: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-qzqlj'
Feb 28 07:51:17.136: INFO: stderr: ""
Feb 28 07:51:17.136: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:51:17.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qzqlj" for this suite.
Feb 28 07:51:23.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:51:25.358: INFO: namespace: e2e-tests-kubectl-qzqlj, resource: bindings, ignored listing per whitelist
Feb 28 07:51:25.413: INFO: namespace e2e-tests-kubectl-qzqlj deletion completed in 8.22201696s

• [SLOW TEST:24.767 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:51:25.413: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-cbbkl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 28 07:51:31.859: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-a72ced41-3b2d-11e9-9760-9e5b40196308,GenerateName:,Namespace:e2e-tests-events-cbbkl,SelfLink:/api/v1/namespaces/e2e-tests-events-cbbkl/pods/send-events-a72ced41-3b2d-11e9-9760-9e5b40196308,UID:a7309d58-3b2d-11e9-8db9-7e40ccd56e85,ResourceVersion:8480,Generation:0,CreationTimestamp:2019-02-28 07:51:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 581732825,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.83/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-dv6pf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dv6pf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-dv6pf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-188.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004ba720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004ba740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:51:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:51:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:51:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:51:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.188,PodIP:100.96.2.83,StartTime:2019-02-28 07:51:27 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-28 07:51:29 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://13ff978492c393907bae94f0acc21fb852a921e3b45edac7ad74f0f2a6151851}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 28 07:51:33.917: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 28 07:51:35.972: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:51:36.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-cbbkl" for this suite.
Feb 28 07:52:14.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:52:14.574: INFO: namespace: e2e-tests-events-cbbkl, resource: bindings, ignored listing per whitelist
Feb 28 07:52:16.329: INFO: namespace e2e-tests-events-cbbkl deletion completed in 40.246656193s

• [SLOW TEST:50.916 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:52:16.329: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-xgj9k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 28 07:52:20.863: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-c583a52d-3b2d-11e9-9760-9e5b40196308", GenerateName:"", Namespace:"e2e-tests-pods-xgj9k", SelfLink:"/api/v1/namespaces/e2e-tests-pods-xgj9k/pods/pod-submit-remove-c583a52d-3b2d-11e9-9760-9e5b40196308", UID:"c597c774-3b2d-11e9-8db9-7e40ccd56e85", ResourceVersion:"8594", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686937138, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"481698934"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.2.84/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ccnbx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001448480), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ccnbx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001c3a448), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-9-188.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001925c80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001c3a480)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001c3a4a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001c3a4a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937138, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937140, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937140, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937138, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.9.188", PodIP:"100.96.2.84", StartTime:(*v1.Time)(0xc0025bb9a0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0025bba00), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://33f90a2abbf51f11b38563006d327d384c4342230f5aef7010ff94ba9a693d30"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:52:32.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xgj9k" for this suite.
Feb 28 07:52:39.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:52:41.024: INFO: namespace: e2e-tests-pods-xgj9k, resource: bindings, ignored listing per whitelist
Feb 28 07:52:41.186: INFO: namespace e2e-tests-pods-xgj9k deletion completed in 8.213252655s

• [SLOW TEST:24.857 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:52:41.186: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-4w655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 07:52:43.377: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:52:45.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-4w655" for this suite.
Feb 28 07:52:51.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:52:53.140: INFO: namespace: e2e-tests-init-container-4w655, resource: bindings, ignored listing per whitelist
Feb 28 07:52:53.785: INFO: namespace e2e-tests-init-container-4w655 deletion completed in 8.21632504s

• [SLOW TEST:12.599 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:52:53.785: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-9kmhc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-dbe0cc06-3b2d-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 07:52:56.111: INFO: Waiting up to 5m0s for pod "pod-configmaps-dbe91572-3b2d-11e9-9760-9e5b40196308" in namespace "e2e-tests-configmap-9kmhc" to be "success or failure"
Feb 28 07:52:56.165: INFO: Pod "pod-configmaps-dbe91572-3b2d-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.872232ms
Feb 28 07:52:58.219: INFO: Pod "pod-configmaps-dbe91572-3b2d-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108129002s
STEP: Saw pod success
Feb 28 07:52:58.219: INFO: Pod "pod-configmaps-dbe91572-3b2d-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:52:58.273: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-dbe91572-3b2d-11e9-9760-9e5b40196308 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:52:58.390: INFO: Waiting for pod pod-configmaps-dbe91572-3b2d-11e9-9760-9e5b40196308 to disappear
Feb 28 07:52:58.444: INFO: Pod pod-configmaps-dbe91572-3b2d-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:52:58.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9kmhc" for this suite.
Feb 28 07:53:04.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:06.123: INFO: namespace: e2e-tests-configmap-9kmhc, resource: bindings, ignored listing per whitelist
Feb 28 07:53:06.716: INFO: namespace e2e-tests-configmap-9kmhc deletion completed in 8.218055895s

• [SLOW TEST:12.931 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:53:06.717: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ttvj4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 28 07:53:08.880: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml cluster-info'
Feb 28 07:53:09.290: INFO: stderr: ""
Feb 28 07:53:09.290: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:53:09.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ttvj4" for this suite.
Feb 28 07:53:15.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:16.704: INFO: namespace: e2e-tests-kubectl-ttvj4, resource: bindings, ignored listing per whitelist
Feb 28 07:53:17.567: INFO: namespace e2e-tests-kubectl-ttvj4 deletion completed in 8.221562563s

• [SLOW TEST:10.850 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:53:17.567: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-flmkv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 28 07:53:20.377: INFO: Waiting up to 5m0s for pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-77srw" in namespace "e2e-tests-svcaccounts-flmkv" to be "success or failure"
Feb 28 07:53:20.431: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-77srw": Phase="Pending", Reason="", readiness=false. Elapsed: 53.903271ms
Feb 28 07:53:22.486: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-77srw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108352533s
STEP: Saw pod success
Feb 28 07:53:22.486: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-77srw" satisfied condition "success or failure"
Feb 28 07:53:22.544: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-77srw container token-test: <nil>
STEP: delete the pod
Feb 28 07:53:22.668: INFO: Waiting for pod pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-77srw to disappear
Feb 28 07:53:22.721: INFO: Pod pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-77srw no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 28 07:53:22.777: INFO: Waiting up to 5m0s for pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-lqfpt" in namespace "e2e-tests-svcaccounts-flmkv" to be "success or failure"
Feb 28 07:53:22.831: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-lqfpt": Phase="Pending", Reason="", readiness=false. Elapsed: 53.768754ms
Feb 28 07:53:24.885: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-lqfpt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107836722s
Feb 28 07:53:26.940: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-lqfpt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.16289315s
STEP: Saw pod success
Feb 28 07:53:26.940: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-lqfpt" satisfied condition "success or failure"
Feb 28 07:53:26.994: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-lqfpt container root-ca-test: <nil>
STEP: delete the pod
Feb 28 07:53:27.113: INFO: Waiting for pod pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-lqfpt to disappear
Feb 28 07:53:27.167: INFO: Pod pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-lqfpt no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 28 07:53:27.222: INFO: Waiting up to 5m0s for pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-dcrww" in namespace "e2e-tests-svcaccounts-flmkv" to be "success or failure"
Feb 28 07:53:27.276: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-dcrww": Phase="Pending", Reason="", readiness=false. Elapsed: 53.896863ms
Feb 28 07:53:29.331: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-dcrww": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.10887928s
STEP: Saw pod success
Feb 28 07:53:29.331: INFO: Pod "pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-dcrww" satisfied condition "success or failure"
Feb 28 07:53:29.385: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-dcrww container namespace-test: <nil>
STEP: delete the pod
Feb 28 07:53:29.517: INFO: Waiting for pod pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-dcrww to disappear
Feb 28 07:53:29.572: INFO: Pod pod-service-account-ea5f98ff-3b2d-11e9-9760-9e5b40196308-dcrww no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:53:29.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-flmkv" for this suite.
Feb 28 07:53:35.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:36.224: INFO: namespace: e2e-tests-svcaccounts-flmkv, resource: bindings, ignored listing per whitelist
Feb 28 07:53:37.851: INFO: namespace e2e-tests-svcaccounts-flmkv deletion completed in 8.223190131s

• [SLOW TEST:20.284 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:53:37.851: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7nbp6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:53:40.079: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml version --client'
Feb 28 07:53:40.150: INFO: stderr: ""
Feb 28 07:53:40.150: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-28T07:21:02Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 28 07:53:40.204: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-7nbp6'
Feb 28 07:53:40.775: INFO: stderr: ""
Feb 28 07:53:40.775: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 28 07:53:40.775: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-7nbp6'
Feb 28 07:53:41.282: INFO: stderr: ""
Feb 28 07:53:41.282: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 07:53:42.342: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:53:42.342: INFO: Found 0 / 1
Feb 28 07:53:43.338: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:53:43.338: INFO: Found 1 / 1
Feb 28 07:53:43.338: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 07:53:43.392: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:53:43.392: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 07:53:43.392: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe pod redis-master-sph68 --namespace=e2e-tests-kubectl-7nbp6'
Feb 28 07:53:43.815: INFO: stderr: ""
Feb 28 07:53:43.815: INFO: stdout: "Name:               redis-master-sph68\nNamespace:          e2e-tests-kubectl-7nbp6\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-10-250-9-188.eu-west-1.compute.internal/10.250.9.188\nStart Time:         Thu, 28 Feb 2019 07:53:40 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.2.91/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.2.91\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://e863cf766e430bb07d9e49b449c7c83d59eb95a1dab63ac41cbfe24cfcc9d210\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 28 Feb 2019 07:53:41 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vxj62 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-vxj62:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-vxj62\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                 Message\n  ----    ------     ----  ----                                                 -------\n  Normal  Scheduled  3s    default-scheduler                                    Successfully assigned e2e-tests-kubectl-7nbp6/redis-master-sph68 to ip-10-250-9-188.eu-west-1.compute.internal\n  Normal  Pulled     2s    kubelet, ip-10-250-9-188.eu-west-1.compute.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, ip-10-250-9-188.eu-west-1.compute.internal  Created container\n  Normal  Started    2s    kubelet, ip-10-250-9-188.eu-west-1.compute.internal  Started container\n"
Feb 28 07:53:43.815: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-7nbp6'
Feb 28 07:53:44.308: INFO: stderr: ""
Feb 28 07:53:44.308: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-7nbp6\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-sph68\n"
Feb 28 07:53:44.308: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-7nbp6'
Feb 28 07:53:44.801: INFO: stderr: ""
Feb 28 07:53:44.801: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-7nbp6\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.65.199.83\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.2.91:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 28 07:53:44.856: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe node ip-10-250-8-168.eu-west-1.compute.internal'
Feb 28 07:53:45.395: INFO: stderr: ""
Feb 28 07:53:45.395: INFO: stdout: "Name:               ip-10-250-8-168.eu-west-1.compute.internal\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m4.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-1\n                    failure-domain.beta.kubernetes.io/zone=eu-west-1a\n                    kubernetes.io/hostname=ip-10-250-8-168.eu-west-1.compute.internal\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.8.168/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 28 Feb 2019 07:12:00 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  OutOfDisk        False   Thu, 28 Feb 2019 07:53:44 +0000   Thu, 28 Feb 2019 07:12:00 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure   False   Thu, 28 Feb 2019 07:53:44 +0000   Thu, 28 Feb 2019 07:12:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 28 Feb 2019 07:53:44 +0000   Thu, 28 Feb 2019 07:12:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 28 Feb 2019 07:53:44 +0000   Thu, 28 Feb 2019 07:12:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 28 Feb 2019 07:53:44 +0000   Thu, 28 Feb 2019 07:12:20 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.8.168\n  InternalDNS:  ip-10-250-8-168.eu-west-1.compute.internal\n  Hostname:     ip-10-250-8-168.eu-west-1.compute.internal\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         2\n ephemeral-storage:           17897500Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      8169008Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         1920m\n ephemeral-storage:           17410687987\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      6873069153\n pods:                        110\nSystem Info:\n Machine ID:                 901b0b4b53e84d409b0155da817059cf\n System UUID:                EC208008-A405-A534-B2B4-AD77039AD5D1\n Boot ID:                    1dac00d9-8923-43b0-92e1-02ee479a56f1\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.12.5\n Kube-Proxy Version:         v1.12.5\nPodCIDR:                     100.96.0.0/24\nProviderID:                  aws:///eu-west-1a/i-05a0750079c4ccaa8\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------\n  kube-system                addons-kube-lego-648f8c9f5c-jhthm                                  20m (1%)      50m (2%)    8Mi (0%)         32Mi (0%)\n  kube-system                addons-kubernetes-dashboard-5f64f76bd-ps8rt                        50m (2%)      100m (5%)   50Mi (0%)        256Mi (3%)\n  kube-system                addons-nginx-ingress-controller-6b47c6c4cb-t86bf                   100m (5%)     2 (104%)    100Mi (1%)       800Mi (12%)\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-psl62    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                blackbox-exporter-58fd9b8556-xjpfq                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)\n  kube-system                calico-node-hjb94                                                  100m (5%)     500m (26%)  100Mi (1%)       700Mi (10%)\n  kube-system                coredns-5f4748c5f-k5h6g                                            50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)\n  kube-system                kube-proxy-dkbnd                                                   20m (1%)      900m (46%)  64Mi (0%)        200Mi (3%)\n  kube-system                metrics-server-54fc54bd68-wflk4                                    20m (1%)      80m (4%)    100Mi (1%)       400Mi (6%)\n  kube-system                node-exporter-pq7cp                                                5m (0%)       15m (0%)    10Mi (0%)        50Mi (0%)\n  kube-system                vpn-shoot-7bcff87c69-gp7q2                                         50m (2%)      100m (5%)   50Mi (0%)        100Mi (1%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         420m (21%)  3855m (200%)\n  memory                      502Mi (7%)  2673Mi (40%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type    Reason                   Age   From                                                    Message\n  ----    ------                   ----  ----                                                    -------\n  Normal  Starting                 41m   kubelet, ip-10-250-8-168.eu-west-1.compute.internal     Starting kubelet.\n  Normal  NodeHasSufficientDisk    41m   kubelet, ip-10-250-8-168.eu-west-1.compute.internal     Node ip-10-250-8-168.eu-west-1.compute.internal status is now: NodeHasSufficientDisk\n  Normal  NodeHasSufficientMemory  41m   kubelet, ip-10-250-8-168.eu-west-1.compute.internal     Node ip-10-250-8-168.eu-west-1.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    41m   kubelet, ip-10-250-8-168.eu-west-1.compute.internal     Node ip-10-250-8-168.eu-west-1.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     41m   kubelet, ip-10-250-8-168.eu-west-1.compute.internal     Node ip-10-250-8-168.eu-west-1.compute.internal status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  41m   kubelet, ip-10-250-8-168.eu-west-1.compute.internal     Updated Node Allocatable limit across pods\n  Normal  Starting                 41m   kube-proxy, ip-10-250-8-168.eu-west-1.compute.internal  Starting kube-proxy.\n  Normal  NodeReady                41m   kubelet, ip-10-250-8-168.eu-west-1.compute.internal     Node ip-10-250-8-168.eu-west-1.compute.internal status is now: NodeReady\n"
Feb 28 07:53:45.395: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml describe namespace e2e-tests-kubectl-7nbp6'
Feb 28 07:53:45.881: INFO: stderr: ""
Feb 28 07:53:45.881: INFO: stdout: "Name:         e2e-tests-kubectl-7nbp6\nLabels:       e2e-framework=kubectl\n              e2e-run=b9164ddf-3b29-11e9-9760-9e5b40196308\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:53:45.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7nbp6" for this suite.
Feb 28 07:54:08.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:09.065: INFO: namespace: e2e-tests-kubectl-7nbp6, resource: bindings, ignored listing per whitelist
Feb 28 07:54:10.143: INFO: namespace e2e-tests-kubectl-7nbp6 deletion completed in 24.207693135s

• [SLOW TEST:32.292 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:54:10.144: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-nj24p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 07:54:12.346: INFO: Waiting up to 5m0s for pod "pod-09596726-3b2e-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-nj24p" to be "success or failure"
Feb 28 07:54:12.400: INFO: Pod "pod-09596726-3b2e-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014709ms
Feb 28 07:54:14.456: INFO: Pod "pod-09596726-3b2e-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.109776157s
STEP: Saw pod success
Feb 28 07:54:14.456: INFO: Pod "pod-09596726-3b2e-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:54:14.510: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-09596726-3b2e-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 07:54:14.626: INFO: Waiting for pod pod-09596726-3b2e-11e9-9760-9e5b40196308 to disappear
Feb 28 07:54:14.679: INFO: Pod pod-09596726-3b2e-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:54:14.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nj24p" for this suite.
Feb 28 07:54:20.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:21.811: INFO: namespace: e2e-tests-emptydir-nj24p, resource: bindings, ignored listing per whitelist
Feb 28 07:54:23.060: INFO: namespace e2e-tests-emptydir-nj24p deletion completed in 8.326537873s

• [SLOW TEST:12.917 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:54:23.061: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2pv22
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:54:25.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11084144-3b2e-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-2pv22" to be "success or failure"
Feb 28 07:54:25.289: INFO: Pod "downwardapi-volume-11084144-3b2e-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.516179ms
Feb 28 07:54:27.343: INFO: Pod "downwardapi-volume-11084144-3b2e-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.1083191s
STEP: Saw pod success
Feb 28 07:54:27.344: INFO: Pod "downwardapi-volume-11084144-3b2e-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:54:27.397: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-11084144-3b2e-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 07:54:27.514: INFO: Waiting for pod downwardapi-volume-11084144-3b2e-11e9-9760-9e5b40196308 to disappear
Feb 28 07:54:27.567: INFO: Pod downwardapi-volume-11084144-3b2e-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:54:27.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2pv22" for this suite.
Feb 28 07:54:33.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:35.091: INFO: namespace: e2e-tests-downward-api-2pv22, resource: bindings, ignored listing per whitelist
Feb 28 07:54:35.848: INFO: namespace e2e-tests-downward-api-2pv22 deletion completed in 8.225907412s

• [SLOW TEST:12.787 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:54:35.848: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-qwqqp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 07:54:38.126: INFO: Waiting up to 5m0s for pod "pod-18b72e03-3b2e-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-qwqqp" to be "success or failure"
Feb 28 07:54:38.180: INFO: Pod "pod-18b72e03-3b2e-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 54.634602ms
Feb 28 07:54:40.236: INFO: Pod "pod-18b72e03-3b2e-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.110309403s
STEP: Saw pod success
Feb 28 07:54:40.236: INFO: Pod "pod-18b72e03-3b2e-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:54:40.290: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-18b72e03-3b2e-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 07:54:40.409: INFO: Waiting for pod pod-18b72e03-3b2e-11e9-9760-9e5b40196308 to disappear
Feb 28 07:54:40.462: INFO: Pod pod-18b72e03-3b2e-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:54:40.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qwqqp" for this suite.
Feb 28 07:54:46.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:48.019: INFO: namespace: e2e-tests-emptydir-qwqqp, resource: bindings, ignored listing per whitelist
Feb 28 07:54:48.718: INFO: namespace e2e-tests-emptydir-qwqqp deletion completed in 8.201337432s

• [SLOW TEST:12.870 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:54:48.718: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-lfph5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:54:50.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-205a2db5-3b2e-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-lfph5" to be "success or failure"
Feb 28 07:54:50.993: INFO: Pod "downwardapi-volume-205a2db5-3b2e-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009771ms
Feb 28 07:54:53.048: INFO: Pod "downwardapi-volume-205a2db5-3b2e-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108730905s
STEP: Saw pod success
Feb 28 07:54:53.048: INFO: Pod "downwardapi-volume-205a2db5-3b2e-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 07:54:53.104: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-205a2db5-3b2e-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 07:54:53.220: INFO: Waiting for pod downwardapi-volume-205a2db5-3b2e-11e9-9760-9e5b40196308 to disappear
Feb 28 07:54:53.273: INFO: Pod downwardapi-volume-205a2db5-3b2e-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:54:53.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lfph5" for this suite.
Feb 28 07:54:59.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:00.249: INFO: namespace: e2e-tests-downward-api-lfph5, resource: bindings, ignored listing per whitelist
Feb 28 07:55:01.552: INFO: namespace e2e-tests-downward-api-lfph5 deletion completed in 8.224137188s

• [SLOW TEST:12.834 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:55:01.552: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-v76xj
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:55:03.832: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:55:04.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-v76xj" for this suite.
Feb 28 07:55:10.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:11.742: INFO: namespace: e2e-tests-custom-resource-definition-v76xj, resource: bindings, ignored listing per whitelist
Feb 28 07:55:12.547: INFO: namespace e2e-tests-custom-resource-definition-v76xj deletion completed in 8.233927106s

• [SLOW TEST:10.995 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:55:12.547: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-d7vp8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 28 07:55:15.078: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 28 07:55:15.078: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:15.622: INFO: stderr: ""
Feb 28 07:55:15.622: INFO: stdout: "service/redis-slave created\n"
Feb 28 07:55:15.623: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 28 07:55:15.623: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:16.136: INFO: stderr: ""
Feb 28 07:55:16.136: INFO: stdout: "service/redis-master created\n"
Feb 28 07:55:16.136: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 28 07:55:16.137: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:16.613: INFO: stderr: ""
Feb 28 07:55:16.613: INFO: stdout: "service/frontend created\n"
Feb 28 07:55:16.614: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 28 07:55:16.614: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:17.109: INFO: stderr: ""
Feb 28 07:55:17.109: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 28 07:55:17.109: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 28 07:55:17.109: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:17.596: INFO: stderr: ""
Feb 28 07:55:17.596: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 28 07:55:17.596: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 28 07:55:17.597: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:18.107: INFO: stderr: ""
Feb 28 07:55:18.107: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 28 07:55:18.107: INFO: Waiting for all frontend pods to be Running.
Feb 28 07:55:43.209: INFO: Waiting for frontend to serve content.
Feb 28 07:55:43.355: INFO: Trying to add a new entry to the guestbook.
Feb 28 07:55:43.498: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 28 07:55:43.642: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:44.186: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:55:44.186: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 07:55:44.190: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:44.546: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:55:44.546: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 07:55:44.547: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:44.912: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:55:44.913: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 07:55:44.913: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:45.276: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:55:45.276: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 07:55:45.276: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:45.638: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:55:45.638: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 07:55:45.638: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-d7vp8'
Feb 28 07:55:46.023: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:55:46.023: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:55:46.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d7vp8" for this suite.
Feb 28 07:56:24.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:24.837: INFO: namespace: e2e-tests-kubectl-d7vp8, resource: bindings, ignored listing per whitelist
Feb 28 07:56:26.290: INFO: namespace e2e-tests-kubectl-d7vp8 deletion completed in 40.212699449s

• [SLOW TEST:73.743 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:56:26.290: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-w2mrv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-w2mrv
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-w2mrv
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-w2mrv
Feb 28 07:56:28.699: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 28 07:56:38.755: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 28 07:56:38.811: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:56:39.884: INFO: stderr: ""
Feb 28 07:56:39.884: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:56:39.884: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:56:39.938: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 28 07:56:49.994: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:56:49.994: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:56:50.210: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999429s
Feb 28 07:56:51.266: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.94520511s
Feb 28 07:56:52.321: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.889786339s
Feb 28 07:56:53.378: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.834245846s
Feb 28 07:56:54.433: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.777399865s
Feb 28 07:56:55.488: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.722374827s
Feb 28 07:56:56.543: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.667832088s
Feb 28 07:56:57.598: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.612888839s
Feb 28 07:56:58.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.558076212s
Feb 28 07:56:59.707: INFO: Verifying statefulset ss doesn't scale past 1 for another 503.578298ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-w2mrv
Feb 28 07:57:00.762: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:57:01.801: INFO: stderr: ""
Feb 28 07:57:01.802: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:57:01.802: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:57:01.856: INFO: Found 1 stateful pods, waiting for 3
Feb 28 07:57:11.911: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:57:11.911: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:57:11.911: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 28 07:57:12.019: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:57:13.079: INFO: stderr: ""
Feb 28 07:57:13.079: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:57:13.079: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:57:13.079: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:57:14.070: INFO: stderr: ""
Feb 28 07:57:14.070: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:57:14.070: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:57:14.070: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:57:15.101: INFO: stderr: ""
Feb 28 07:57:15.101: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:57:15.101: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:57:15.101: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:57:15.155: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 28 07:57:25.266: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:57:25.266: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:57:25.266: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 07:57:25.431: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999514s
Feb 28 07:57:26.487: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.944924575s
Feb 28 07:57:27.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.889738109s
Feb 28 07:57:28.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.835535813s
Feb 28 07:57:29.651: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.780485734s
Feb 28 07:57:30.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.725752076s
Feb 28 07:57:31.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.670850121s
Feb 28 07:57:32.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.615869601s
Feb 28 07:57:33.871: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.56117746s
Feb 28 07:57:34.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 504.178715ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-w2mrv
Feb 28 07:57:35.982: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:57:36.956: INFO: stderr: ""
Feb 28 07:57:36.956: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:57:36.956: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:57:36.956: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:57:38.076: INFO: stderr: ""
Feb 28 07:57:38.076: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:57:38.076: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:57:38.076: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:57:38.719: INFO: rc: 1
Feb 28 07:57:38.729: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001985620 exit status 1 <nil> <nil> true [0xc0021443e8 0xc002144400 0xc002144418] [0xc0021443e8 0xc002144400 0xc002144418] [0xc0021443f8 0xc002144410] [0x932420 0x932420] 0xc002637920 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 28 07:57:48.736: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:57:49.315: INFO: rc: 1
Feb 28 07:57:49.315: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019858f0 exit status 1 <nil> <nil> true [0xc002144420 0xc002144438 0xc002144450] [0xc002144420 0xc002144438 0xc002144450] [0xc002144430 0xc002144448] [0x932420 0x932420] 0xc002637c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:57:59.315: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:57:59.640: INFO: rc: 1
Feb 28 07:57:59.640: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001985cb0 exit status 1 <nil> <nil> true [0xc002144458 0xc002144470 0xc002144488] [0xc002144458 0xc002144470 0xc002144488] [0xc002144468 0xc002144480] [0x932420 0x932420] 0xc002637f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:58:09.640: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:58:10.031: INFO: rc: 1
Feb 28 07:58:10.031: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a855f0 exit status 1 <nil> <nil> true [0xc0011ca970 0xc0011ca988 0xc0011ca9a0] [0xc0011ca970 0xc0011ca988 0xc0011ca9a0] [0xc0011ca980 0xc0011ca998] [0x932420 0x932420] 0xc001a046c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:58:20.031: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:58:20.378: INFO: rc: 1
Feb 28 07:58:20.378: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00169e2a0 exit status 1 <nil> <nil> true [0xc0000d4248 0xc0000d45d0 0xc0000d4670] [0xc0000d4248 0xc0000d45d0 0xc0000d4670] [0xc0000d4598 0xc0000d4638] [0x932420 0x932420] 0xc0018c1500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:58:30.379: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:58:30.685: INFO: rc: 1
Feb 28 07:58:30.686: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00067e270 exit status 1 <nil> <nil> true [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac2d8 0xc0001ac440] [0x932420 0x932420] 0xc0017d8480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:58:40.686: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:58:41.000: INFO: rc: 1
Feb 28 07:58:41.000: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00150e2a0 exit status 1 <nil> <nil> true [0xc0013e6000 0xc0013e6030 0xc0013e6078] [0xc0013e6000 0xc0013e6030 0xc0013e6078] [0xc0013e6028 0xc0013e6060] [0x932420 0x932420] 0xc001ca2300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:58:51.000: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:58:51.359: INFO: rc: 1
Feb 28 07:58:51.359: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f02d0 exit status 1 <nil> <nil> true [0xc00219c018 0xc00219c058 0xc00219c0c0] [0xc00219c018 0xc00219c058 0xc00219c0c0] [0xc00219c048 0xc00219c0b0] [0x932420 0x932420] 0xc000d3c240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:59:01.359: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:59:01.679: INFO: rc: 1
Feb 28 07:59:01.679: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00150e5a0 exit status 1 <nil> <nil> true [0xc0013e6080 0xc0013e60a0 0xc0013e60b8] [0xc0013e6080 0xc0013e60a0 0xc0013e60b8] [0xc0013e6098 0xc0013e60b0] [0x932420 0x932420] 0xc001ca2660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:59:11.680: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:59:12.045: INFO: rc: 1
Feb 28 07:59:12.046: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00067e4e0 exit status 1 <nil> <nil> true [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac520 0xc0001acb48] [0x932420 0x932420] 0xc0017d87e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:59:22.046: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:59:22.355: INFO: rc: 1
Feb 28 07:59:22.355: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00067e810 exit status 1 <nil> <nil> true [0xc0001acb80 0xc0001acc68 0xc0001acd68] [0xc0001acb80 0xc0001acc68 0xc0001acd68] [0xc0001acc50 0xc0001acce0] [0x932420 0x932420] 0xc0017d8ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:59:32.356: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:59:32.755: INFO: rc: 1
Feb 28 07:59:32.755: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f0570 exit status 1 <nil> <nil> true [0xc00219c0d8 0xc00219c110 0xc00219c150] [0xc00219c0d8 0xc00219c110 0xc00219c150] [0xc00219c0f8 0xc00219c130] [0x932420 0x932420] 0xc000d3c540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:59:42.756: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:59:43.088: INFO: rc: 1
Feb 28 07:59:43.089: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f0810 exit status 1 <nil> <nil> true [0xc00219c160 0xc00219c1a8 0xc00219c1e0] [0xc00219c160 0xc00219c1a8 0xc00219c1e0] [0xc00219c190 0xc00219c1c8] [0x932420 0x932420] 0xc000d3c840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 07:59:53.089: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:59:53.526: INFO: rc: 1
Feb 28 07:59:53.526: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f0ab0 exit status 1 <nil> <nil> true [0xc00219c1f8 0xc00219c248 0xc00219c290] [0xc00219c1f8 0xc00219c248 0xc00219c290] [0xc00219c238 0xc00219c270] [0x932420 0x932420] 0xc000d3d200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:00:03.527: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:00:03.870: INFO: rc: 1
Feb 28 08:00:03.870: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f0d80 exit status 1 <nil> <nil> true [0xc00219c2a0 0xc00219c2e8 0xc00219c328] [0xc00219c2a0 0xc00219c2e8 0xc00219c328] [0xc00219c2d8 0xc00219c318] [0x932420 0x932420] 0xc000d3d9e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:00:13.871: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:00:14.243: INFO: rc: 1
Feb 28 08:00:14.243: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00150e930 exit status 1 <nil> <nil> true [0xc0013e60c0 0xc0013e60d8 0xc0013e60f0] [0xc0013e60c0 0xc0013e60d8 0xc0013e60f0] [0xc0013e60d0 0xc0013e60e8] [0x932420 0x932420] 0xc001ca3200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:00:24.243: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:00:24.550: INFO: rc: 1
Feb 28 08:00:24.551: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f0300 exit status 1 <nil> <nil> true [0xc00219c030 0xc00219c068 0xc00219c0d8] [0xc00219c030 0xc00219c068 0xc00219c0d8] [0xc00219c058 0xc00219c0c0] [0x932420 0x932420] 0xc000d3c240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:00:34.552: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:00:34.861: INFO: rc: 1
Feb 28 08:00:34.861: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00150e2d0 exit status 1 <nil> <nil> true [0xc0013e6000 0xc0013e6030 0xc0013e6078] [0xc0013e6000 0xc0013e6030 0xc0013e6078] [0xc0013e6028 0xc0013e6060] [0x932420 0x932420] 0xc001ca2300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:00:44.861: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:00:45.174: INFO: rc: 1
Feb 28 08:00:45.174: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00150e600 exit status 1 <nil> <nil> true [0xc0013e6080 0xc0013e60a0 0xc0013e60b8] [0xc0013e6080 0xc0013e60a0 0xc0013e60b8] [0xc0013e6098 0xc0013e60b0] [0x932420 0x932420] 0xc001ca2660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:00:55.174: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:01:00.519: INFO: rc: 1
Feb 28 08:01:00.519: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f05d0 exit status 1 <nil> <nil> true [0xc00219c0e8 0xc00219c120 0xc00219c160] [0xc00219c0e8 0xc00219c120 0xc00219c160] [0xc00219c110 0xc00219c150] [0x932420 0x932420] 0xc000d3c540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:01:10.519: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:01:10.826: INFO: rc: 1
Feb 28 08:01:10.826: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00169e2d0 exit status 1 <nil> <nil> true [0xc0000d4040 0xc0000d4598 0xc0000d4638] [0xc0000d4040 0xc0000d4598 0xc0000d4638] [0xc0000d44b0 0xc0000d4630] [0x932420 0x932420] 0xc0018c1500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:01:20.827: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:01:21.254: INFO: rc: 1
Feb 28 08:01:21.254: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f08d0 exit status 1 <nil> <nil> true [0xc00219c180 0xc00219c1b8 0xc00219c1f8] [0xc00219c180 0xc00219c1b8 0xc00219c1f8] [0xc00219c1a8 0xc00219c1e0] [0x932420 0x932420] 0xc000d3c840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:01:31.254: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:01:31.562: INFO: rc: 1
Feb 28 08:01:31.562: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f0c00 exit status 1 <nil> <nil> true [0xc00219c208 0xc00219c260 0xc00219c2a0] [0xc00219c208 0xc00219c260 0xc00219c2a0] [0xc00219c248 0xc00219c290] [0x932420 0x932420] 0xc000d3d200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:01:41.562: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:01:41.864: INFO: rc: 1
Feb 28 08:01:41.864: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021f0f30 exit status 1 <nil> <nil> true [0xc00219c2c8 0xc00219c310 0xc00219c338] [0xc00219c2c8 0xc00219c310 0xc00219c338] [0xc00219c2e8 0xc00219c328] [0x932420 0x932420] 0xc000d3d9e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:01:51.865: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:01:52.196: INFO: rc: 1
Feb 28 08:01:52.196: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00067e2a0 exit status 1 <nil> <nil> true [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac2d8 0xc0001ac440] [0x932420 0x932420] 0xc0017d8480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:02:02.196: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:02:02.766: INFO: rc: 1
Feb 28 08:02:02.766: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00067e5a0 exit status 1 <nil> <nil> true [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac520 0xc0001acb48] [0x932420 0x932420] 0xc0017d87e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:02:12.767: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:02:13.088: INFO: rc: 1
Feb 28 08:02:13.088: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00150ec60 exit status 1 <nil> <nil> true [0xc0013e60c0 0xc0013e60d8 0xc0013e60f0] [0xc0013e60c0 0xc0013e60d8 0xc0013e60f0] [0xc0013e60d0 0xc0013e60e8] [0x932420 0x932420] 0xc001ca3200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:02:23.089: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:02:23.431: INFO: rc: 1
Feb 28 08:02:23.431: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00067e270 exit status 1 <nil> <nil> true [0xc0001ac008 0xc0001ac3f0 0xc0001ac480] [0xc0001ac008 0xc0001ac3f0 0xc0001ac480] [0xc0001ac3b0 0xc0001ac478] [0x932420 0x932420] 0xc0017d8480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:02:33.431: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:02:33.762: INFO: rc: 1
Feb 28 08:02:33.762: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00150e2a0 exit status 1 <nil> <nil> true [0xc0013e6000 0xc0013e6030 0xc0013e6078] [0xc0013e6000 0xc0013e6030 0xc0013e6078] [0xc0013e6028 0xc0013e6060] [0x932420 0x932420] 0xc001ca2300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:02:43.762: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-w2mrv ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:02:44.088: INFO: rc: 1
Feb 28 08:02:44.088: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 28 08:02:44.089: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:02:44.255: INFO: Deleting all statefulset in ns e2e-tests-statefulset-w2mrv
Feb 28 08:02:44.315: INFO: Scaling statefulset ss to 0
Feb 28 08:02:44.488: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:02:44.545: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:02:44.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-w2mrv" for this suite.
Feb 28 08:02:50.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:02:51.593: INFO: namespace: e2e-tests-statefulset-w2mrv, resource: bindings, ignored listing per whitelist
Feb 28 08:02:53.171: INFO: namespace e2e-tests-statefulset-w2mrv deletion completed in 8.39026287s

• [SLOW TEST:386.880 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:02:53.171: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-r4778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 08:02:55.647: INFO: Waiting up to 5m0s for pod "pod-4142273d-3b2f-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-r4778" to be "success or failure"
Feb 28 08:02:55.707: INFO: Pod "pod-4142273d-3b2f-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.830771ms
Feb 28 08:02:57.763: INFO: Pod "pod-4142273d-3b2f-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.115083219s
STEP: Saw pod success
Feb 28 08:02:57.763: INFO: Pod "pod-4142273d-3b2f-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:02:57.818: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-4142273d-3b2f-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 08:02:57.937: INFO: Waiting for pod pod-4142273d-3b2f-11e9-9760-9e5b40196308 to disappear
Feb 28 08:02:57.993: INFO: Pod pod-4142273d-3b2f-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:02:57.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-r4778" for this suite.
Feb 28 08:03:04.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:04.400: INFO: namespace: e2e-tests-emptydir-r4778, resource: bindings, ignored listing per whitelist
Feb 28 08:03:06.416: INFO: namespace e2e-tests-emptydir-r4778 deletion completed in 8.365358549s

• [SLOW TEST:13.245 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:03:06.416: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gwwfd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 28 08:03:08.805: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix246985460/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:03:08.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gwwfd" for this suite.
Feb 28 08:03:15.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:17.061: INFO: namespace: e2e-tests-kubectl-gwwfd, resource: bindings, ignored listing per whitelist
Feb 28 08:03:17.363: INFO: namespace e2e-tests-kubectl-gwwfd deletion completed in 8.417637679s

• [SLOW TEST:10.947 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:03:17.363: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-jkfld
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:03:19.760: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4fa10ffa-3b2f-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-jkfld" to be "success or failure"
Feb 28 08:03:19.822: INFO: Pod "downwardapi-volume-4fa10ffa-3b2f-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 61.689582ms
Feb 28 08:03:21.882: INFO: Pod "downwardapi-volume-4fa10ffa-3b2f-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121767597s
STEP: Saw pod success
Feb 28 08:03:21.882: INFO: Pod "downwardapi-volume-4fa10ffa-3b2f-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:03:21.943: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-4fa10ffa-3b2f-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:03:22.063: INFO: Waiting for pod downwardapi-volume-4fa10ffa-3b2f-11e9-9760-9e5b40196308 to disappear
Feb 28 08:03:22.120: INFO: Pod downwardapi-volume-4fa10ffa-3b2f-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:03:22.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jkfld" for this suite.
Feb 28 08:03:28.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:28.948: INFO: namespace: e2e-tests-downward-api-jkfld, resource: bindings, ignored listing per whitelist
Feb 28 08:03:30.603: INFO: namespace e2e-tests-downward-api-jkfld deletion completed in 8.422942861s

• [SLOW TEST:13.240 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:03:30.604: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-nwfn8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 08:03:33.293: INFO: Number of nodes with available pods: 0
Feb 28 08:03:33.294: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:03:34.408: INFO: Number of nodes with available pods: 0
Feb 28 08:03:34.408: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:03:35.405: INFO: Number of nodes with available pods: 1
Feb 28 08:03:35.405: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:03:36.403: INFO: Number of nodes with available pods: 2
Feb 28 08:03:36.403: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 28 08:03:36.677: INFO: Number of nodes with available pods: 1
Feb 28 08:03:36.677: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:03:37.788: INFO: Number of nodes with available pods: 1
Feb 28 08:03:37.788: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:03:38.788: INFO: Number of nodes with available pods: 2
Feb 28 08:03:38.788: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-nwfn8, will wait for the garbage collector to delete the pods
Feb 28 08:03:39.109: INFO: Deleting {extensions DaemonSet} daemon-set took: 54.851915ms
Feb 28 08:03:39.210: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.293043ms
Feb 28 08:04:13.164: INFO: Number of nodes with available pods: 0
Feb 28 08:04:13.164: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:04:13.221: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-nwfn8/daemonsets","resourceVersion":"10445"},"items":null}

Feb 28 08:04:13.276: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-nwfn8/pods","resourceVersion":"10445"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:04:13.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-nwfn8" for this suite.
Feb 28 08:04:19.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:04:20.448: INFO: namespace: e2e-tests-daemonsets-nwfn8, resource: bindings, ignored listing per whitelist
Feb 28 08:04:21.803: INFO: namespace e2e-tests-daemonsets-nwfn8 deletion completed in 8.286661559s

• [SLOW TEST:51.199 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:04:21.803: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-rrr9w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-rrr9w
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 28 08:04:24.165: INFO: Found 1 stateful pods, waiting for 3
Feb 28 08:04:34.222: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:04:34.222: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:04:34.222: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:04:34.386: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rrr9w ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:04:35.593: INFO: stderr: ""
Feb 28 08:04:35.593: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:04:35.593: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 08:04:45.942: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 28 08:04:46.109: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rrr9w ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:04:47.118: INFO: stderr: ""
Feb 28 08:04:47.118: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:04:47.118: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Feb 28 08:05:07.471: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rrr9w ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:05:08.541: INFO: stderr: ""
Feb 28 08:05:08.542: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:05:08.542: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:05:18.886: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 28 08:05:19.058: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rrr9w ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:05:20.086: INFO: stderr: ""
Feb 28 08:05:20.086: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:05:20.086: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:05:20.314: INFO: Waiting for StatefulSet e2e-tests-statefulset-rrr9w/ss2 to complete update
Feb 28 08:05:20.314: INFO: Waiting for Pod e2e-tests-statefulset-rrr9w/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 28 08:05:20.314: INFO: Waiting for Pod e2e-tests-statefulset-rrr9w/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 28 08:05:20.314: INFO: Waiting for Pod e2e-tests-statefulset-rrr9w/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 28 08:05:30.422: INFO: Waiting for StatefulSet e2e-tests-statefulset-rrr9w/ss2 to complete update
Feb 28 08:05:30.422: INFO: Waiting for Pod e2e-tests-statefulset-rrr9w/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 28 08:05:30.422: INFO: Waiting for Pod e2e-tests-statefulset-rrr9w/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 28 08:05:30.422: INFO: Waiting for Pod e2e-tests-statefulset-rrr9w/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 28 08:05:40.422: INFO: Waiting for StatefulSet e2e-tests-statefulset-rrr9w/ss2 to complete update
Feb 28 08:05:40.422: INFO: Waiting for Pod e2e-tests-statefulset-rrr9w/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:05:50.424: INFO: Deleting all statefulset in ns e2e-tests-statefulset-rrr9w
Feb 28 08:05:50.479: INFO: Scaling statefulset ss2 to 0
Feb 28 08:06:30.707: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:06:30.762: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:06:30.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-rrr9w" for this suite.
Feb 28 08:06:37.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:06:39.208: INFO: namespace: e2e-tests-statefulset-rrr9w, resource: bindings, ignored listing per whitelist
Feb 28 08:06:39.372: INFO: namespace e2e-tests-statefulset-rrr9w deletion completed in 8.389411048s

• [SLOW TEST:137.569 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:06:39.372: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-fwk9h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 28 08:06:41.741: INFO: Waiting up to 5m0s for pod "var-expansion-c805abbb-3b2f-11e9-9760-9e5b40196308" in namespace "e2e-tests-var-expansion-fwk9h" to be "success or failure"
Feb 28 08:06:41.796: INFO: Pod "var-expansion-c805abbb-3b2f-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 54.316822ms
Feb 28 08:06:43.850: INFO: Pod "var-expansion-c805abbb-3b2f-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.109044808s
STEP: Saw pod success
Feb 28 08:06:43.850: INFO: Pod "var-expansion-c805abbb-3b2f-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:06:43.904: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod var-expansion-c805abbb-3b2f-11e9-9760-9e5b40196308 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:06:44.023: INFO: Waiting for pod var-expansion-c805abbb-3b2f-11e9-9760-9e5b40196308 to disappear
Feb 28 08:06:44.079: INFO: Pod var-expansion-c805abbb-3b2f-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:06:44.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-fwk9h" for this suite.
Feb 28 08:06:50.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:06:51.688: INFO: namespace: e2e-tests-var-expansion-fwk9h, resource: bindings, ignored listing per whitelist
Feb 28 08:06:52.557: INFO: namespace e2e-tests-var-expansion-fwk9h deletion completed in 8.420364336s

• [SLOW TEST:13.185 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:06:52.558: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-ml892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0228 08:07:05.587674   29661 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:07:05.587: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:07:05.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-ml892" for this suite.
Feb 28 08:07:11.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:07:13.437: INFO: namespace: e2e-tests-gc-ml892, resource: bindings, ignored listing per whitelist
Feb 28 08:07:13.953: INFO: namespace e2e-tests-gc-ml892 deletion completed in 8.31103802s

• [SLOW TEST:21.396 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:07:13.953: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-66cgx
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-dc924f68-3b2f-11e9-9760-9e5b40196308
STEP: Creating secret with name s-test-opt-upd-dc924fce-3b2f-11e9-9760-9e5b40196308
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-dc924f68-3b2f-11e9-9760-9e5b40196308
STEP: Updating secret s-test-opt-upd-dc924fce-3b2f-11e9-9760-9e5b40196308
STEP: Creating secret with name s-test-opt-create-dc925081-3b2f-11e9-9760-9e5b40196308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:07:21.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-66cgx" for this suite.
Feb 28 08:07:43.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:07:44.566: INFO: namespace: e2e-tests-secrets-66cgx, resource: bindings, ignored listing per whitelist
Feb 28 08:07:45.417: INFO: namespace e2e-tests-secrets-66cgx deletion completed in 24.27643129s

• [SLOW TEST:31.464 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:07:45.418: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-997d7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-ef4ce890-3b2f-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 08:07:47.692: INFO: Waiting up to 5m0s for pod "pod-secrets-ef5535e3-3b2f-11e9-9760-9e5b40196308" in namespace "e2e-tests-secrets-997d7" to be "success or failure"
Feb 28 08:07:47.746: INFO: Pod "pod-secrets-ef5535e3-3b2f-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 53.780452ms
Feb 28 08:07:49.804: INFO: Pod "pod-secrets-ef5535e3-3b2f-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111483566s
STEP: Saw pod success
Feb 28 08:07:49.804: INFO: Pod "pod-secrets-ef5535e3-3b2f-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:07:49.858: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-secrets-ef5535e3-3b2f-11e9-9760-9e5b40196308 container secret-env-test: <nil>
STEP: delete the pod
Feb 28 08:07:49.974: INFO: Waiting for pod pod-secrets-ef5535e3-3b2f-11e9-9760-9e5b40196308 to disappear
Feb 28 08:07:50.031: INFO: Pod pod-secrets-ef5535e3-3b2f-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:07:50.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-997d7" for this suite.
Feb 28 08:07:56.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:07:56.995: INFO: namespace: e2e-tests-secrets-997d7, resource: bindings, ignored listing per whitelist
Feb 28 08:07:58.361: INFO: namespace e2e-tests-secrets-997d7 deletion completed in 8.274243499s

• [SLOW TEST:12.943 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:07:58.361: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-582md
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-f71a5396-3b2f-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 08:08:00.780: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f7227f6d-3b2f-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-582md" to be "success or failure"
Feb 28 08:08:00.834: INFO: Pod "pod-projected-secrets-f7227f6d-3b2f-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 54.640477ms
Feb 28 08:08:02.894: INFO: Pod "pod-projected-secrets-f7227f6d-3b2f-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.114043104s
STEP: Saw pod success
Feb 28 08:08:02.894: INFO: Pod "pod-projected-secrets-f7227f6d-3b2f-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:08:02.953: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-secrets-f7227f6d-3b2f-11e9-9760-9e5b40196308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:08:03.081: INFO: Waiting for pod pod-projected-secrets-f7227f6d-3b2f-11e9-9760-9e5b40196308 to disappear
Feb 28 08:08:03.137: INFO: Pod pod-projected-secrets-f7227f6d-3b2f-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:08:03.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-582md" for this suite.
Feb 28 08:08:09.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:11.378: INFO: namespace: e2e-tests-projected-582md, resource: bindings, ignored listing per whitelist
Feb 28 08:08:11.613: INFO: namespace e2e-tests-projected-582md deletion completed in 8.420999047s

• [SLOW TEST:13.253 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:08:11.614: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-zqmqx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-ff0bc4ac-3b2f-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 08:08:14.115: INFO: Waiting up to 5m0s for pod "pod-secrets-ff14ab65-3b2f-11e9-9760-9e5b40196308" in namespace "e2e-tests-secrets-zqmqx" to be "success or failure"
Feb 28 08:08:14.174: INFO: Pod "pod-secrets-ff14ab65-3b2f-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.317414ms
Feb 28 08:08:16.242: INFO: Pod "pod-secrets-ff14ab65-3b2f-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.126149044s
STEP: Saw pod success
Feb 28 08:08:16.242: INFO: Pod "pod-secrets-ff14ab65-3b2f-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:08:16.298: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-secrets-ff14ab65-3b2f-11e9-9760-9e5b40196308 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:08:16.421: INFO: Waiting for pod pod-secrets-ff14ab65-3b2f-11e9-9760-9e5b40196308 to disappear
Feb 28 08:08:16.479: INFO: Pod pod-secrets-ff14ab65-3b2f-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:08:16.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-zqmqx" for this suite.
Feb 28 08:08:22.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:25.022: INFO: namespace: e2e-tests-secrets-zqmqx, resource: bindings, ignored listing per whitelist
Feb 28 08:08:25.138: INFO: namespace e2e-tests-secrets-zqmqx deletion completed in 8.601553371s

• [SLOW TEST:13.524 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:08:25.138: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-j2ch4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:08:27.392: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml version'
Feb 28 08:08:27.907: INFO: stderr: ""
Feb 28 08:08:27.907: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-28T07:21:02Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"clean\", BuildDate:\"2019-01-16T18:14:49Z\", GoVersion:\"go1.10.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:08:27.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-j2ch4" for this suite.
Feb 28 08:08:34.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:35.148: INFO: namespace: e2e-tests-kubectl-j2ch4, resource: bindings, ignored listing per whitelist
Feb 28 08:08:36.368: INFO: namespace e2e-tests-kubectl-j2ch4 deletion completed in 8.403859708s

• [SLOW TEST:11.229 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:08:36.368: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-dqmbb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 08:08:41.688: INFO: Successfully updated pod "annotationupdate0dc532e8-3b30-11e9-9760-9e5b40196308"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:08:43.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dqmbb" for this suite.
Feb 28 08:09:08.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:09:08.584: INFO: namespace: e2e-tests-downward-api-dqmbb, resource: bindings, ignored listing per whitelist
Feb 28 08:09:10.334: INFO: namespace e2e-tests-downward-api-dqmbb deletion completed in 26.463643379s

• [SLOW TEST:33.967 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:09:10.335: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-76n9s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 28 08:09:12.970: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-76n9s,SelfLink:/api/v1/namespaces/e2e-tests-watch-76n9s/configmaps/e2e-watch-test-resource-version,UID:21fb0d9a-3b30-11e9-8db9-7e40ccd56e85,ResourceVersion:11468,Generation:0,CreationTimestamp:2019-02-28 08:09:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:09:12.970: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-76n9s,SelfLink:/api/v1/namespaces/e2e-tests-watch-76n9s/configmaps/e2e-watch-test-resource-version,UID:21fb0d9a-3b30-11e9-8db9-7e40ccd56e85,ResourceVersion:11469,Generation:0,CreationTimestamp:2019-02-28 08:09:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:09:12.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-76n9s" for this suite.
Feb 28 08:09:19.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:09:21.064: INFO: namespace: e2e-tests-watch-76n9s, resource: bindings, ignored listing per whitelist
Feb 28 08:09:21.335: INFO: namespace e2e-tests-watch-76n9s deletion completed in 8.310540242s

• [SLOW TEST:11.000 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:09:21.335: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-hzqcc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-hzqcc
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:09:23.576: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:09:42.521: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.2.124:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-hzqcc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:09:42.521: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 08:09:43.205: INFO: Found all expected endpoints: [netserver-0]
Feb 28 08:09:43.261: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.0.49:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-hzqcc PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:09:43.261: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 08:09:43.939: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:09:43.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-hzqcc" for this suite.
Feb 28 08:10:06.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:07.174: INFO: namespace: e2e-tests-pod-network-test-hzqcc, resource: bindings, ignored listing per whitelist
Feb 28 08:10:08.377: INFO: namespace e2e-tests-pod-network-test-hzqcc deletion completed in 24.377361999s

• [SLOW TEST:47.042 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:10:08.379: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-gsx4w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:10:10.891: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 08:10:11.067: INFO: Number of nodes with available pods: 0
Feb 28 08:10:11.067: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:10:12.180: INFO: Number of nodes with available pods: 1
Feb 28 08:10:12.181: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:10:13.182: INFO: Number of nodes with available pods: 2
Feb 28 08:10:13.182: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 28 08:10:13.552: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:13.552: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:14.672: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:14.673: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:15.669: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:15.670: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:16.671: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:16.671: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:17.669: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:17.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:18.674: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:18.674: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:19.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:19.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:20.669: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:20.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:21.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:21.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:22.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:22.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:23.668: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:23.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:24.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:24.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:25.673: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:25.674: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:26.667: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:26.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:27.667: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:27.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:28.668: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:28.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:29.669: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:29.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:30.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:30.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:31.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:31.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:32.670: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:32.670: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:33.673: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:33.673: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:34.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:34.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:35.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:35.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:36.667: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:36.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:37.668: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:37.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:38.667: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:38.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:39.667: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:39.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:40.667: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:40.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:41.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:41.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:42.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:42.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:43.672: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:43.672: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:44.667: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:44.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:45.674: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:45.674: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:46.666: INFO: Wrong image for pod: daemon-set-6lzcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:46.666: INFO: Pod daemon-set-6lzcd is not available
Feb 28 08:10:46.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:47.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:47.666: INFO: Pod daemon-set-kzdmw is not available
Feb 28 08:10:48.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:48.669: INFO: Pod daemon-set-kzdmw is not available
Feb 28 08:10:49.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:49.666: INFO: Pod daemon-set-kzdmw is not available
Feb 28 08:10:50.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:51.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:52.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:53.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:54.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:55.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:56.673: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:57.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:58.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:10:59.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:00.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:01.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:02.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:03.670: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:04.671: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:05.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:06.671: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:07.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:08.671: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:09.671: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:10.671: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:11.672: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:12.670: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:13.671: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:14.675: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:15.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:16.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:17.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:18.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:19.672: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:20.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:21.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:21.670: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:22.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:22.669: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:23.670: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:23.670: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:24.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:24.670: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:25.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:25.666: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:26.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:26.668: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:27.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:27.667: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:28.669: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:28.669: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:29.672: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:29.672: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:30.666: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:30.667: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:31.667: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:31.667: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:32.668: INFO: Wrong image for pod: daemon-set-bc5pv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:11:32.668: INFO: Pod daemon-set-bc5pv is not available
Feb 28 08:11:33.668: INFO: Pod daemon-set-z76sz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 28 08:11:33.834: INFO: Number of nodes with available pods: 1
Feb 28 08:11:33.834: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:11:34.943: INFO: Number of nodes with available pods: 2
Feb 28 08:11:34.943: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-gsx4w, will wait for the garbage collector to delete the pods
Feb 28 08:11:35.434: INFO: Deleting {extensions DaemonSet} daemon-set took: 55.663512ms
Feb 28 08:11:35.535: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.267992ms
Feb 28 08:11:42.989: INFO: Number of nodes with available pods: 0
Feb 28 08:11:42.989: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:11:43.043: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-gsx4w/daemonsets","resourceVersion":"11852"},"items":null}

Feb 28 08:11:43.097: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-gsx4w/pods","resourceVersion":"11852"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:11:43.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-gsx4w" for this suite.
Feb 28 08:11:49.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:11:51.331: INFO: namespace: e2e-tests-daemonsets-gsx4w, resource: bindings, ignored listing per whitelist
Feb 28 08:11:51.548: INFO: namespace e2e-tests-daemonsets-gsx4w deletion completed in 8.231533705s

• [SLOW TEST:103.169 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:11:51.548: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-w5kkp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:11:54.048: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 28 08:11:54.157: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-w5kkp/daemonsets","resourceVersion":"11883"},"items":null}

Feb 28 08:11:54.211: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-w5kkp/pods","resourceVersion":"11883"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:11:54.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-w5kkp" for this suite.
Feb 28 08:12:00.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:12:01.959: INFO: namespace: e2e-tests-daemonsets-w5kkp, resource: bindings, ignored listing per whitelist
Feb 28 08:12:02.715: INFO: namespace e2e-tests-daemonsets-w5kkp deletion completed in 8.273312456s

S [SKIPPING] [11.166 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 28 08:11:54.048: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:12:02.715: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rw9x5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-88b31c76-3b30-11e9-9760-9e5b40196308
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-88b31c76-3b30-11e9-9760-9e5b40196308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:12:09.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rw9x5" for this suite.
Feb 28 08:12:31.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:12:33.691: INFO: namespace: e2e-tests-projected-rw9x5, resource: bindings, ignored listing per whitelist
Feb 28 08:12:33.852: INFO: namespace e2e-tests-projected-rw9x5 deletion completed in 24.329632139s

• [SLOW TEST:31.137 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:12:33.853: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-cqm2d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-cqm2d
Feb 28 08:12:38.182: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-cqm2d
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:12:38.241: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:16:39.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cqm2d" for this suite.
Feb 28 08:16:45.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:16:46.122: INFO: namespace: e2e-tests-container-probe-cqm2d, resource: bindings, ignored listing per whitelist
Feb 28 08:16:47.868: INFO: namespace e2e-tests-container-probe-cqm2d deletion completed in 8.46526016s

• [SLOW TEST:254.016 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:16:47.869: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-hgz9t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0228 08:17:21.094386   29661 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:17:21.094: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:17:21.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-hgz9t" for this suite.
Feb 28 08:17:27.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:17:28.472: INFO: namespace: e2e-tests-gc-hgz9t, resource: bindings, ignored listing per whitelist
Feb 28 08:17:29.742: INFO: namespace e2e-tests-gc-hgz9t deletion completed in 8.590058757s

• [SLOW TEST:41.873 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:17:29.743: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-wjf2f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-kg4l
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:17:32.279: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kg4l" in namespace "e2e-tests-subpath-wjf2f" to be "success or failure"
Feb 28 08:17:32.335: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Pending", Reason="", readiness=false. Elapsed: 55.697947ms
Feb 28 08:17:34.398: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119265493s
Feb 28 08:17:36.461: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Running", Reason="", readiness=false. Elapsed: 4.181837031s
Feb 28 08:17:38.517: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Running", Reason="", readiness=false. Elapsed: 6.237973674s
Feb 28 08:17:40.576: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Running", Reason="", readiness=false. Elapsed: 8.297227547s
Feb 28 08:17:42.638: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Running", Reason="", readiness=false. Elapsed: 10.358683627s
Feb 28 08:17:44.703: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Running", Reason="", readiness=false. Elapsed: 12.424065445s
Feb 28 08:17:46.765: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Running", Reason="", readiness=false. Elapsed: 14.48640523s
Feb 28 08:17:48.830: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Running", Reason="", readiness=false. Elapsed: 16.551383418s
Feb 28 08:17:50.886: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Running", Reason="", readiness=false. Elapsed: 18.607209622s
Feb 28 08:17:52.946: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Running", Reason="", readiness=false. Elapsed: 20.666526557s
Feb 28 08:17:55.004: INFO: Pod "pod-subpath-test-configmap-kg4l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.725442498s
STEP: Saw pod success
Feb 28 08:17:55.004: INFO: Pod "pod-subpath-test-configmap-kg4l" satisfied condition "success or failure"
Feb 28 08:17:55.064: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-subpath-test-configmap-kg4l container test-container-subpath-configmap-kg4l: <nil>
STEP: delete the pod
Feb 28 08:17:55.197: INFO: Waiting for pod pod-subpath-test-configmap-kg4l to disappear
Feb 28 08:17:55.261: INFO: Pod pod-subpath-test-configmap-kg4l no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kg4l
Feb 28 08:17:55.261: INFO: Deleting pod "pod-subpath-test-configmap-kg4l" in namespace "e2e-tests-subpath-wjf2f"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:17:55.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-wjf2f" for this suite.
Feb 28 08:18:01.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:18:02.084: INFO: namespace: e2e-tests-subpath-wjf2f, resource: bindings, ignored listing per whitelist
Feb 28 08:18:03.864: INFO: namespace e2e-tests-subpath-wjf2f deletion completed in 8.478947072s

• [SLOW TEST:34.121 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:18:03.864: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xkqdt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:18:06.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6024acd5-3b31-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-xkqdt" to be "success or failure"
Feb 28 08:18:06.519: INFO: Pod "downwardapi-volume-6024acd5-3b31-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 60.789413ms
Feb 28 08:18:08.581: INFO: Pod "downwardapi-volume-6024acd5-3b31-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.122567774s
STEP: Saw pod success
Feb 28 08:18:08.582: INFO: Pod "downwardapi-volume-6024acd5-3b31-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:18:08.642: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-6024acd5-3b31-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:18:08.773: INFO: Waiting for pod downwardapi-volume-6024acd5-3b31-11e9-9760-9e5b40196308 to disappear
Feb 28 08:18:08.922: INFO: Pod downwardapi-volume-6024acd5-3b31-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:18:08.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xkqdt" for this suite.
Feb 28 08:18:15.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:18:17.697: INFO: namespace: e2e-tests-projected-xkqdt, resource: bindings, ignored listing per whitelist
Feb 28 08:18:17.697: INFO: namespace e2e-tests-projected-xkqdt deletion completed in 8.715304454s

• [SLOW TEST:13.833 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:18:17.699: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-x28ft
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0228 08:18:30.588045   29661 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:18:30.588: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:18:30.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-x28ft" for this suite.
Feb 28 08:18:36.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:18:37.956: INFO: namespace: e2e-tests-gc-x28ft, resource: bindings, ignored listing per whitelist
Feb 28 08:18:39.087: INFO: namespace e2e-tests-gc-x28ft deletion completed in 8.43745951s

• [SLOW TEST:21.387 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:18:39.087: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-txbd5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 08:18:41.564: INFO: Waiting up to 5m0s for pod "pod-75117f1a-3b31-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-txbd5" to be "success or failure"
Feb 28 08:18:41.623: INFO: Pod "pod-75117f1a-3b31-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.892043ms
Feb 28 08:18:43.682: INFO: Pod "pod-75117f1a-3b31-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117516972s
STEP: Saw pod success
Feb 28 08:18:43.682: INFO: Pod "pod-75117f1a-3b31-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:18:43.739: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-75117f1a-3b31-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 08:18:43.864: INFO: Waiting for pod pod-75117f1a-3b31-11e9-9760-9e5b40196308 to disappear
Feb 28 08:18:43.924: INFO: Pod pod-75117f1a-3b31-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:18:43.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-txbd5" for this suite.
Feb 28 08:18:50.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:18:51.119: INFO: namespace: e2e-tests-emptydir-txbd5, resource: bindings, ignored listing per whitelist
Feb 28 08:18:52.526: INFO: namespace e2e-tests-emptydir-txbd5 deletion completed in 8.539469285s

• [SLOW TEST:13.440 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:18:52.527: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-f2s2g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 28 08:18:54.895: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 08:18:55.014: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 08:18:55.072: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-8-168.eu-west-1.compute.internal before test
Feb 28 08:18:55.203: INFO: addons-kube-lego-648f8c9f5c-jhthm from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 08:18:55.203: INFO: node-exporter-pq7cp from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:18:55.203: INFO: metrics-server-54fc54bd68-wflk4 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 08:18:55.203: INFO: vpn-shoot-7bcff87c69-gp7q2 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 08:18:55.203: INFO: coredns-5f4748c5f-k5h6g from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container coredns ready: true, restart count 0
Feb 28 08:18:55.203: INFO: blackbox-exporter-58fd9b8556-xjpfq from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 08:18:55.203: INFO: kube-proxy-dkbnd from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:18:55.203: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-psl62 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 08:18:55.203: INFO: addons-nginx-ingress-controller-6b47c6c4cb-t86bf from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 08:18:55.203: INFO: addons-kubernetes-dashboard-5f64f76bd-ps8rt from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 08:18:55.203: INFO: calico-node-hjb94 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.203: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:18:55.203: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-9-188.eu-west-1.compute.internal before test
Feb 28 08:18:55.270: INFO: calico-node-kjkhj from kube-system started at 2019-02-28 07:12:03 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.270: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:18:55.270: INFO: node-exporter-8nnqh from kube-system started at 2019-02-28 07:12:03 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.270: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:18:55.270: INFO: kube-proxy-4mpns from kube-system started at 2019-02-28 07:12:03 +0000 UTC (1 container statuses recorded)
Feb 28 08:18:55.270: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7ea56fca-3b31-11e9-9760-9e5b40196308 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-7ea56fca-3b31-11e9-9760-9e5b40196308 off the node ip-10-250-9-188.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7ea56fca-3b31-11e9-9760-9e5b40196308
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:19:00.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-f2s2g" for this suite.
Feb 28 08:19:08.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:10.970: INFO: namespace: e2e-tests-sched-pred-f2s2g, resource: bindings, ignored listing per whitelist
Feb 28 08:19:11.457: INFO: namespace e2e-tests-sched-pred-f2s2g deletion completed in 10.810477935s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:18.931 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:19:11.458: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-fwx98
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-887eace7-3b31-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:19:14.221: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-88883072-3b31-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-fwx98" to be "success or failure"
Feb 28 08:19:14.281: INFO: Pod "pod-projected-configmaps-88883072-3b31-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.713797ms
Feb 28 08:19:16.338: INFO: Pod "pod-projected-configmaps-88883072-3b31-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11742384s
STEP: Saw pod success
Feb 28 08:19:16.338: INFO: Pod "pod-projected-configmaps-88883072-3b31-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:19:16.402: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-configmaps-88883072-3b31-11e9-9760-9e5b40196308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:19:16.533: INFO: Waiting for pod pod-projected-configmaps-88883072-3b31-11e9-9760-9e5b40196308 to disappear
Feb 28 08:19:16.594: INFO: Pod pod-projected-configmaps-88883072-3b31-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:19:16.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fwx98" for this suite.
Feb 28 08:19:22.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:25.165: INFO: namespace: e2e-tests-projected-fwx98, resource: bindings, ignored listing per whitelist
Feb 28 08:19:25.288: INFO: namespace e2e-tests-projected-fwx98 deletion completed in 8.574134552s

• [SLOW TEST:13.830 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:19:25.289: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-bdmjb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-vm5w
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:19:27.884: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-vm5w" in namespace "e2e-tests-subpath-bdmjb" to be "success or failure"
Feb 28 08:19:27.944: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Pending", Reason="", readiness=false. Elapsed: 60.204321ms
Feb 28 08:19:30.007: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.122616629s
Feb 28 08:19:32.069: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 4.184946209s
Feb 28 08:19:34.131: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 6.246578129s
Feb 28 08:19:36.190: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 8.306450791s
Feb 28 08:19:38.251: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 10.366753354s
Feb 28 08:19:40.312: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 12.427644818s
Feb 28 08:19:42.374: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 14.490320892s
Feb 28 08:19:44.435: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 16.550889501s
Feb 28 08:19:46.495: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 18.61115413s
Feb 28 08:19:48.555: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 20.670772708s
Feb 28 08:19:50.613: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Running", Reason="", readiness=false. Elapsed: 22.729409197s
Feb 28 08:19:52.674: INFO: Pod "pod-subpath-test-projected-vm5w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.790446327s
STEP: Saw pod success
Feb 28 08:19:52.675: INFO: Pod "pod-subpath-test-projected-vm5w" satisfied condition "success or failure"
Feb 28 08:19:52.736: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-subpath-test-projected-vm5w container test-container-subpath-projected-vm5w: <nil>
STEP: delete the pod
Feb 28 08:19:52.861: INFO: Waiting for pod pod-subpath-test-projected-vm5w to disappear
Feb 28 08:19:52.917: INFO: Pod pod-subpath-test-projected-vm5w no longer exists
STEP: Deleting pod pod-subpath-test-projected-vm5w
Feb 28 08:19:52.917: INFO: Deleting pod "pod-subpath-test-projected-vm5w" in namespace "e2e-tests-subpath-bdmjb"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:19:52.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-bdmjb" for this suite.
Feb 28 08:19:59.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:01.527: INFO: namespace: e2e-tests-subpath-bdmjb, resource: bindings, ignored listing per whitelist
Feb 28 08:20:01.724: INFO: namespace e2e-tests-subpath-bdmjb deletion completed in 8.632480932s

• [SLOW TEST:36.436 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:20:01.725: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-h427v
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-a66f20c6-3b31-11e9-9760-9e5b40196308
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-a66f20c6-3b31-11e9-9760-9e5b40196308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:20:08.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-h427v" for this suite.
Feb 28 08:20:31.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:32.383: INFO: namespace: e2e-tests-configmap-h427v, resource: bindings, ignored listing per whitelist
Feb 28 08:20:33.454: INFO: namespace e2e-tests-configmap-h427v deletion completed in 24.502256746s

• [SLOW TEST:31.729 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:20:33.454: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-7tz6p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-b941bfc7-3b31-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:20:36.020: INFO: Waiting up to 5m0s for pod "pod-configmaps-b94a8bd2-3b31-11e9-9760-9e5b40196308" in namespace "e2e-tests-configmap-7tz6p" to be "success or failure"
Feb 28 08:20:36.077: INFO: Pod "pod-configmaps-b94a8bd2-3b31-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 56.774669ms
Feb 28 08:20:38.136: INFO: Pod "pod-configmaps-b94a8bd2-3b31-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.115321196s
STEP: Saw pod success
Feb 28 08:20:38.136: INFO: Pod "pod-configmaps-b94a8bd2-3b31-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:20:38.195: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-b94a8bd2-3b31-11e9-9760-9e5b40196308 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:20:38.323: INFO: Waiting for pod pod-configmaps-b94a8bd2-3b31-11e9-9760-9e5b40196308 to disappear
Feb 28 08:20:38.382: INFO: Pod pod-configmaps-b94a8bd2-3b31-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:20:38.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7tz6p" for this suite.
Feb 28 08:20:44.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:45.146: INFO: namespace: e2e-tests-configmap-7tz6p, resource: bindings, ignored listing per whitelist
Feb 28 08:20:46.910: INFO: namespace e2e-tests-configmap-7tz6p deletion completed in 8.468098662s

• [SLOW TEST:13.456 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:20:46.911: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-j7d7g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:20:49.304: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c135705d-3b31-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-j7d7g" to be "success or failure"
Feb 28 08:20:49.362: INFO: Pod "downwardapi-volume-c135705d-3b31-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.385138ms
Feb 28 08:20:51.423: INFO: Pod "downwardapi-volume-c135705d-3b31-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11867163s
STEP: Saw pod success
Feb 28 08:20:51.423: INFO: Pod "downwardapi-volume-c135705d-3b31-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:20:51.484: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-c135705d-3b31-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:20:51.615: INFO: Waiting for pod downwardapi-volume-c135705d-3b31-11e9-9760-9e5b40196308 to disappear
Feb 28 08:20:51.677: INFO: Pod downwardapi-volume-c135705d-3b31-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:20:51.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j7d7g" for this suite.
Feb 28 08:20:57.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:58.974: INFO: namespace: e2e-tests-projected-j7d7g, resource: bindings, ignored listing per whitelist
Feb 28 08:21:00.252: INFO: namespace e2e-tests-projected-j7d7g deletion completed in 8.513767779s

• [SLOW TEST:13.342 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:21:00.253: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-m4hvc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-c93aa25a-3b31-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 08:21:02.819: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c9436e34-3b31-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-m4hvc" to be "success or failure"
Feb 28 08:21:02.878: INFO: Pod "pod-projected-secrets-c9436e34-3b31-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.114636ms
Feb 28 08:21:04.935: INFO: Pod "pod-projected-secrets-c9436e34-3b31-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.115788398s
STEP: Saw pod success
Feb 28 08:21:04.935: INFO: Pod "pod-projected-secrets-c9436e34-3b31-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:21:04.990: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-secrets-c9436e34-3b31-11e9-9760-9e5b40196308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:21:05.116: INFO: Waiting for pod pod-projected-secrets-c9436e34-3b31-11e9-9760-9e5b40196308 to disappear
Feb 28 08:21:05.175: INFO: Pod pod-projected-secrets-c9436e34-3b31-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:21:05.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m4hvc" for this suite.
Feb 28 08:21:11.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:21:12.753: INFO: namespace: e2e-tests-projected-m4hvc, resource: bindings, ignored listing per whitelist
Feb 28 08:21:13.703: INFO: namespace e2e-tests-projected-m4hvc deletion completed in 8.470574838s

• [SLOW TEST:13.450 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:21:13.703: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cvprs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:21:16.038: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-cvprs'
Feb 28 08:21:18.889: INFO: stderr: ""
Feb 28 08:21:18.889: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
Feb 28 08:21:18.950: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-cvprs'
Feb 28 08:21:21.531: INFO: stderr: ""
Feb 28 08:21:21.531: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:21:21.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cvprs" for this suite.
Feb 28 08:21:27.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:21:30.046: INFO: namespace: e2e-tests-kubectl-cvprs, resource: bindings, ignored listing per whitelist
Feb 28 08:21:30.103: INFO: namespace e2e-tests-kubectl-cvprs deletion completed in 8.509854311s

• [SLOW TEST:16.400 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:21:30.104: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6pkvk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-6pkvk/secret-test-dafc39aa-3b31-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 08:21:32.605: INFO: Waiting up to 5m0s for pod "pod-configmaps-db04a5d2-3b31-11e9-9760-9e5b40196308" in namespace "e2e-tests-secrets-6pkvk" to be "success or failure"
Feb 28 08:21:32.664: INFO: Pod "pod-configmaps-db04a5d2-3b31-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.362039ms
Feb 28 08:21:34.721: INFO: Pod "pod-configmaps-db04a5d2-3b31-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.115654439s
STEP: Saw pod success
Feb 28 08:21:34.721: INFO: Pod "pod-configmaps-db04a5d2-3b31-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:21:34.779: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-db04a5d2-3b31-11e9-9760-9e5b40196308 container env-test: <nil>
STEP: delete the pod
Feb 28 08:21:34.902: INFO: Waiting for pod pod-configmaps-db04a5d2-3b31-11e9-9760-9e5b40196308 to disappear
Feb 28 08:21:34.960: INFO: Pod pod-configmaps-db04a5d2-3b31-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:21:34.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6pkvk" for this suite.
Feb 28 08:21:41.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:21:42.785: INFO: namespace: e2e-tests-secrets-6pkvk, resource: bindings, ignored listing per whitelist
Feb 28 08:21:43.500: INFO: namespace e2e-tests-secrets-6pkvk deletion completed in 8.480063341s

• [SLOW TEST:13.397 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:21:43.501: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ldgpw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-e3092b08-3b31-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:21:46.118: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e31220d4-3b31-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-ldgpw" to be "success or failure"
Feb 28 08:21:46.180: INFO: Pod "pod-projected-configmaps-e31220d4-3b31-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 61.906817ms
Feb 28 08:21:48.240: INFO: Pod "pod-projected-configmaps-e31220d4-3b31-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121500204s
STEP: Saw pod success
Feb 28 08:21:48.240: INFO: Pod "pod-projected-configmaps-e31220d4-3b31-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:21:48.299: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-configmaps-e31220d4-3b31-11e9-9760-9e5b40196308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:21:48.432: INFO: Waiting for pod pod-projected-configmaps-e31220d4-3b31-11e9-9760-9e5b40196308 to disappear
Feb 28 08:21:48.492: INFO: Pod pod-projected-configmaps-e31220d4-3b31-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:21:48.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ldgpw" for this suite.
Feb 28 08:21:54.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:21:56.212: INFO: namespace: e2e-tests-projected-ldgpw, resource: bindings, ignored listing per whitelist
Feb 28 08:21:57.075: INFO: namespace e2e-tests-projected-ldgpw deletion completed in 8.523929842s

• [SLOW TEST:13.574 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:21:57.075: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-xcgtv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-cvbdn in namespace e2e-tests-proxy-xcgtv
I0228 08:21:59.506729   29661 runners.go:180] Created replication controller with name: proxy-service-cvbdn, namespace: e2e-tests-proxy-xcgtv, replica count: 1
I0228 08:22:00.607406   29661 runners.go:180] proxy-service-cvbdn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 08:22:01.607958   29661 runners.go:180] proxy-service-cvbdn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 08:22:02.608265   29661 runners.go:180] proxy-service-cvbdn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 08:22:03.608649   29661 runners.go:180] proxy-service-cvbdn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:22:04.609085   29661 runners.go:180] proxy-service-cvbdn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:22:05.609531   29661 runners.go:180] proxy-service-cvbdn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:22:06.609763   29661 runners.go:180] proxy-service-cvbdn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:22:07.610052   29661 runners.go:180] proxy-service-cvbdn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 08:22:07.683: INFO: setup took 8.297529477s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 28 08:22:07.755: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 71.657849ms)
Feb 28 08:22:07.755: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 71.093012ms)
Feb 28 08:22:07.755: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 71.113903ms)
Feb 28 08:22:07.755: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 71.69338ms)
Feb 28 08:22:07.761: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 76.91926ms)
Feb 28 08:22:07.761: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 76.893344ms)
Feb 28 08:22:07.761: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 77.511584ms)
Feb 28 08:22:07.800: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 115.865013ms)
Feb 28 08:22:07.800: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 115.954203ms)
Feb 28 08:22:07.800: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 116.019639ms)
Feb 28 08:22:07.800: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 115.934085ms)
Feb 28 08:22:07.801: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 117.071331ms)
Feb 28 08:22:07.801: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 117.455289ms)
Feb 28 08:22:07.801: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 118.230573ms)
Feb 28 08:22:07.803: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 119.270814ms)
Feb 28 08:22:07.803: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 119.154632ms)
Feb 28 08:22:07.865: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 61.206523ms)
Feb 28 08:22:07.865: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.444686ms)
Feb 28 08:22:07.865: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 59.178975ms)
Feb 28 08:22:07.865: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 61.625261ms)
Feb 28 08:22:07.865: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 61.578609ms)
Feb 28 08:22:07.865: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 62.347232ms)
Feb 28 08:22:07.865: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 59.643706ms)
Feb 28 08:22:07.865: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 61.740906ms)
Feb 28 08:22:07.866: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 60.180181ms)
Feb 28 08:22:07.866: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 62.345257ms)
Feb 28 08:22:07.866: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 62.904439ms)
Feb 28 08:22:07.866: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 60.489132ms)
Feb 28 08:22:07.866: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 60.431275ms)
Feb 28 08:22:07.866: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 62.337216ms)
Feb 28 08:22:07.866: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 60.471825ms)
Feb 28 08:22:07.867: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.382348ms)
Feb 28 08:22:07.927: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 59.744323ms)
Feb 28 08:22:07.928: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 60.659882ms)
Feb 28 08:22:07.928: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 60.414232ms)
Feb 28 08:22:07.928: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 60.554225ms)
Feb 28 08:22:07.928: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 60.204282ms)
Feb 28 08:22:07.929: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 60.386606ms)
Feb 28 08:22:07.929: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 60.89307ms)
Feb 28 08:22:07.929: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 61.103057ms)
Feb 28 08:22:07.929: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 60.2462ms)
Feb 28 08:22:07.929: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 61.335432ms)
Feb 28 08:22:07.929: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 60.705743ms)
Feb 28 08:22:07.929: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 60.605221ms)
Feb 28 08:22:07.930: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 61.510585ms)
Feb 28 08:22:07.930: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 62.361821ms)
Feb 28 08:22:07.930: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 62.838743ms)
Feb 28 08:22:07.930: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 62.249769ms)
Feb 28 08:22:07.991: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 61.028581ms)
Feb 28 08:22:07.991: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.16814ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.162018ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 61.051621ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.176887ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 61.145297ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 61.27758ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 61.325377ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 61.420413ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 61.47297ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 61.403335ms)
Feb 28 08:22:07.992: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 61.384349ms)
Feb 28 08:22:07.994: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 63.810097ms)
Feb 28 08:22:07.994: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 63.986377ms)
Feb 28 08:22:07.994: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 64.185289ms)
Feb 28 08:22:07.994: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 64.114251ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 60.155929ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 60.013766ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 60.081822ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 60.153209ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 60.075403ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 60.103421ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 60.116257ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 60.215739ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 60.181538ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 60.342483ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 60.175068ms)
Feb 28 08:22:08.055: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 60.133154ms)
Feb 28 08:22:08.056: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 61.062059ms)
Feb 28 08:22:08.056: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 61.235898ms)
Feb 28 08:22:08.056: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.181973ms)
Feb 28 08:22:08.056: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.193274ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 59.98976ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 60.016916ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 60.186263ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 60.152141ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 59.999017ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 60.198524ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 60.078926ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 60.22565ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 60.088744ms)
Feb 28 08:22:08.116: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 60.181895ms)
Feb 28 08:22:08.117: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 60.358792ms)
Feb 28 08:22:08.117: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 60.315071ms)
Feb 28 08:22:08.117: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 61.213939ms)
Feb 28 08:22:08.118: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 62.096353ms)
Feb 28 08:22:08.118: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 62.075798ms)
Feb 28 08:22:08.118: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 62.183641ms)
Feb 28 08:22:08.180: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 61.578501ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 61.980678ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 62.03151ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 62.048804ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 62.031725ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 62.272947ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 62.884142ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 62.872092ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 63.090139ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 63.035103ms)
Feb 28 08:22:08.181: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 62.959986ms)
Feb 28 08:22:08.182: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 63.025195ms)
Feb 28 08:22:08.182: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 63.390626ms)
Feb 28 08:22:08.183: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 64.244593ms)
Feb 28 08:22:08.183: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 64.273302ms)
Feb 28 08:22:08.183: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 64.293028ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 65.303536ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 65.309521ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 65.25638ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 65.393666ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 65.300326ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 65.439202ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 65.391917ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 65.373646ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 65.445892ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 65.4506ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 65.403825ms)
Feb 28 08:22:08.248: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 65.621521ms)
Feb 28 08:22:08.249: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 66.225094ms)
Feb 28 08:22:08.249: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 66.327466ms)
Feb 28 08:22:08.249: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 66.399542ms)
Feb 28 08:22:08.249: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 66.351101ms)
Feb 28 08:22:08.310: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 60.903663ms)
Feb 28 08:22:08.311: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 61.13111ms)
Feb 28 08:22:08.311: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 61.387876ms)
Feb 28 08:22:08.311: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.333612ms)
Feb 28 08:22:08.311: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 61.208723ms)
Feb 28 08:22:08.311: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 61.321844ms)
Feb 28 08:22:08.311: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.096909ms)
Feb 28 08:22:08.311: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 61.160137ms)
Feb 28 08:22:08.311: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.192614ms)
Feb 28 08:22:08.311: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 61.45308ms)
Feb 28 08:22:08.313: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 63.841229ms)
Feb 28 08:22:08.313: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 63.811422ms)
Feb 28 08:22:08.313: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 64.009866ms)
Feb 28 08:22:08.314: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 64.261674ms)
Feb 28 08:22:08.314: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 64.926916ms)
Feb 28 08:22:08.314: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 64.805827ms)
Feb 28 08:22:08.376: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 61.468662ms)
Feb 28 08:22:08.376: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.68126ms)
Feb 28 08:22:08.376: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 61.742462ms)
Feb 28 08:22:08.376: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.422381ms)
Feb 28 08:22:08.376: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 61.750782ms)
Feb 28 08:22:08.376: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 61.437641ms)
Feb 28 08:22:08.376: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.569701ms)
Feb 28 08:22:08.377: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 62.457948ms)
Feb 28 08:22:08.377: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 62.639401ms)
Feb 28 08:22:08.377: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 62.4618ms)
Feb 28 08:22:08.377: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 62.377719ms)
Feb 28 08:22:08.377: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 62.431477ms)
Feb 28 08:22:08.377: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 62.949644ms)
Feb 28 08:22:08.377: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 62.802449ms)
Feb 28 08:22:08.378: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 62.903452ms)
Feb 28 08:22:08.378: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 63.274192ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 59.493592ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 59.616214ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 59.735195ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 59.559309ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 59.710746ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 59.821882ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 59.608824ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 59.732447ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 59.589128ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 59.964539ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 59.860085ms)
Feb 28 08:22:08.438: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 59.976481ms)
Feb 28 08:22:08.439: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 61.122353ms)
Feb 28 08:22:08.439: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 61.229675ms)
Feb 28 08:22:08.439: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 61.353879ms)
Feb 28 08:22:08.439: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 61.315772ms)
Feb 28 08:22:08.500: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 60.427527ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 61.102745ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 61.271186ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.317367ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 61.294804ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 61.243794ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.63664ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.774809ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 61.751044ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.953447ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 61.896385ms)
Feb 28 08:22:08.501: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 61.810536ms)
Feb 28 08:22:08.502: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 62.995281ms)
Feb 28 08:22:08.502: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 62.956995ms)
Feb 28 08:22:08.502: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 62.996596ms)
Feb 28 08:22:08.502: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 62.905156ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 60.375673ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 60.551228ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 60.513755ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 60.544083ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 60.67607ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 60.601456ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 60.648467ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 60.592783ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 60.646512ms)
Feb 28 08:22:08.563: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 60.596207ms)
Feb 28 08:22:08.564: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 61.105848ms)
Feb 28 08:22:08.564: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 61.097526ms)
Feb 28 08:22:08.564: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 61.551916ms)
Feb 28 08:22:08.565: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 62.22152ms)
Feb 28 08:22:08.565: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 62.238488ms)
Feb 28 08:22:08.565: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 62.125372ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 63.925941ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 64.216398ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 64.146116ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 64.150556ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 63.748174ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 63.948716ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 64.577757ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 64.439315ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 64.243301ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 65.243414ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 65.089425ms)
Feb 28 08:22:08.630: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 64.886777ms)
Feb 28 08:22:08.631: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 65.167391ms)
Feb 28 08:22:08.631: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 65.859375ms)
Feb 28 08:22:08.631: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 66.073221ms)
Feb 28 08:22:08.631: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 66.02815ms)
Feb 28 08:22:08.693: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 61.77013ms)
Feb 28 08:22:08.693: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.713313ms)
Feb 28 08:22:08.693: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 61.606978ms)
Feb 28 08:22:08.693: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.751862ms)
Feb 28 08:22:08.693: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.699537ms)
Feb 28 08:22:08.693: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 61.747397ms)
Feb 28 08:22:08.693: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 61.84333ms)
Feb 28 08:22:08.694: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 62.71948ms)
Feb 28 08:22:08.694: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 62.827697ms)
Feb 28 08:22:08.694: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 62.719716ms)
Feb 28 08:22:08.694: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 62.753534ms)
Feb 28 08:22:08.694: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 62.81737ms)
Feb 28 08:22:08.694: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 62.799651ms)
Feb 28 08:22:08.694: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 63.008076ms)
Feb 28 08:22:08.694: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 63.130199ms)
Feb 28 08:22:08.696: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 64.6936ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 59.25822ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 59.341043ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 59.437803ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 59.577347ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 59.578319ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 59.645385ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 59.70017ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 59.669274ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 59.660271ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 59.85926ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 59.800553ms)
Feb 28 08:22:08.756: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 59.861492ms)
Feb 28 08:22:08.757: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 60.473625ms)
Feb 28 08:22:08.757: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 61.157141ms)
Feb 28 08:22:08.757: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 61.233612ms)
Feb 28 08:22:08.757: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 61.278503ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.029334ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.069434ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 61.225515ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 61.328083ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.374552ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 61.592279ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 61.43766ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 61.524771ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 61.498268ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 61.703929ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 61.609939ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 61.471763ms)
Feb 28 08:22:08.819: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.635104ms)
Feb 28 08:22:08.820: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 62.424955ms)
Feb 28 08:22:08.820: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 62.186537ms)
Feb 28 08:22:08.820: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 62.495399ms)
Feb 28 08:22:08.881: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.10466ms)
Feb 28 08:22:08.882: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 61.445462ms)
Feb 28 08:22:08.882: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 61.255206ms)
Feb 28 08:22:08.883: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 62.048392ms)
Feb 28 08:22:08.883: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 62.160011ms)
Feb 28 08:22:08.883: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 62.768891ms)
Feb 28 08:22:08.884: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 62.70941ms)
Feb 28 08:22:08.884: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 62.746035ms)
Feb 28 08:22:08.884: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 62.796256ms)
Feb 28 08:22:08.884: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 62.830138ms)
Feb 28 08:22:08.884: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 62.880459ms)
Feb 28 08:22:08.884: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 63.220577ms)
Feb 28 08:22:08.884: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 63.213679ms)
Feb 28 08:22:08.884: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 63.313736ms)
Feb 28 08:22:08.884: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 63.697637ms)
Feb 28 08:22:08.885: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 63.931325ms)
Feb 28 08:22:08.946: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 60.859771ms)
Feb 28 08:22:08.946: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.259794ms)
Feb 28 08:22:08.946: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 60.668998ms)
Feb 28 08:22:08.946: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.23148ms)
Feb 28 08:22:08.946: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.080959ms)
Feb 28 08:22:08.946: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 60.681204ms)
Feb 28 08:22:08.946: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 60.657589ms)
Feb 28 08:22:08.947: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 61.399278ms)
Feb 28 08:22:08.947: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 61.592604ms)
Feb 28 08:22:08.947: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 62.207159ms)
Feb 28 08:22:08.947: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 61.520906ms)
Feb 28 08:22:08.947: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 61.818259ms)
Feb 28 08:22:08.989: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 104.194099ms)
Feb 28 08:22:08.989: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 104.036833ms)
Feb 28 08:22:08.989: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 103.629566ms)
Feb 28 08:22:08.989: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 103.951476ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:460/proxy/: tls baz (200; 61.18692ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.674599ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname2/proxy/: tls qux (200; 61.916434ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:1080/proxy/rewri... (200; 61.833138ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:443/proxy/... (200; 61.771596ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/https:proxy-service-cvbdn:tlsportname1/proxy/: tls baz (200; 61.856177ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:1080/proxy/... (200; 61.806906ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:160/proxy/: foo (200; 61.786271ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.901564ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/https:proxy-service-cvbdn-zcxv5:462/proxy/: tls qux (200; 62.024132ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/http:proxy-service-cvbdn-zcxv5:162/proxy/: bar (200; 61.854361ms)
Feb 28 08:22:09.051: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-xcgtv/pods/proxy-service-cvbdn-zcxv5/proxy/rewriteme"... (200; 61.900302ms)
Feb 28 08:22:09.052: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname2/proxy/: bar (200; 62.673841ms)
Feb 28 08:22:09.052: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/http:proxy-service-cvbdn:portname1/proxy/: foo (200; 62.923478ms)
Feb 28 08:22:09.053: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname1/proxy/: foo (200; 63.643674ms)
Feb 28 08:22:09.053: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-xcgtv/services/proxy-service-cvbdn:portname2/proxy/: bar (200; 63.604057ms)
STEP: deleting { ReplicationController} proxy-service-cvbdn in namespace e2e-tests-proxy-xcgtv, will wait for the garbage collector to delete the pods
Feb 28 08:22:09.273: INFO: Deleting { ReplicationController} proxy-service-cvbdn took: 60.319738ms
Feb 28 08:22:09.373: INFO: Terminating { ReplicationController} proxy-service-cvbdn pods took: 100.304357ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:22:10.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-xcgtv" for this suite.
Feb 28 08:22:17.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:19.158: INFO: namespace: e2e-tests-proxy-xcgtv, resource: bindings, ignored listing per whitelist
Feb 28 08:22:19.573: INFO: namespace e2e-tests-proxy-xcgtv deletion completed in 8.536917177s

• [SLOW TEST:22.498 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:22:19.575: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-46gxv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-f8712b9e-3b31-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:22:22.033: INFO: Waiting up to 5m0s for pod "pod-configmaps-f87a5476-3b31-11e9-9760-9e5b40196308" in namespace "e2e-tests-configmap-46gxv" to be "success or failure"
Feb 28 08:22:22.094: INFO: Pod "pod-configmaps-f87a5476-3b31-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 60.91192ms
Feb 28 08:22:24.157: INFO: Pod "pod-configmaps-f87a5476-3b31-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.124399771s
STEP: Saw pod success
Feb 28 08:22:24.158: INFO: Pod "pod-configmaps-f87a5476-3b31-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:22:24.216: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-f87a5476-3b31-11e9-9760-9e5b40196308 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:22:24.341: INFO: Waiting for pod pod-configmaps-f87a5476-3b31-11e9-9760-9e5b40196308 to disappear
Feb 28 08:22:24.399: INFO: Pod pod-configmaps-f87a5476-3b31-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:22:24.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-46gxv" for this suite.
Feb 28 08:22:30.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:31.599: INFO: namespace: e2e-tests-configmap-46gxv, resource: bindings, ignored listing per whitelist
Feb 28 08:22:32.967: INFO: namespace e2e-tests-configmap-46gxv deletion completed in 8.506039802s

• [SLOW TEST:13.391 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:22:32.967: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8t76j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:22:35.306: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-8t76j'
Feb 28 08:22:35.994: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 08:22:35.994: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 28 08:22:36.110: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-vpzkp]
Feb 28 08:22:36.110: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-vpzkp" in namespace "e2e-tests-kubectl-8t76j" to be "running and ready"
Feb 28 08:22:36.168: INFO: Pod "e2e-test-nginx-rc-vpzkp": Phase="Pending", Reason="", readiness=false. Elapsed: 58.222886ms
Feb 28 08:22:38.226: INFO: Pod "e2e-test-nginx-rc-vpzkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.115864263s
Feb 28 08:22:38.226: INFO: Pod "e2e-test-nginx-rc-vpzkp" satisfied condition "running and ready"
Feb 28 08:22:38.226: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-vpzkp]
Feb 28 08:22:38.226: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-8t76j'
Feb 28 08:22:38.664: INFO: stderr: ""
Feb 28 08:22:38.664: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
Feb 28 08:22:38.664: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-8t76j'
Feb 28 08:22:39.043: INFO: stderr: ""
Feb 28 08:22:39.043: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:22:39.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8t76j" for this suite.
Feb 28 08:22:45.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:46.798: INFO: namespace: e2e-tests-kubectl-8t76j, resource: bindings, ignored listing per whitelist
Feb 28 08:22:47.568: INFO: namespace e2e-tests-kubectl-8t76j deletion completed in 8.467524174s

• [SLOW TEST:14.601 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:22:47.568: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-vv68w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 08:22:53.083: INFO: Successfully updated pod "pod-update-091eece1-3b32-11e9-9760-9e5b40196308"
STEP: verifying the updated pod is in kubernetes
Feb 28 08:22:53.203: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:22:53.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vv68w" for this suite.
Feb 28 08:23:15.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:23:17.380: INFO: namespace: e2e-tests-pods-vv68w, resource: bindings, ignored listing per whitelist
Feb 28 08:23:17.722: INFO: namespace e2e-tests-pods-vv68w deletion completed in 24.457085165s

• [SLOW TEST:30.154 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:23:17.723: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-26thp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-1b3dca2d-3b32-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:23:20.415: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b46bd3b-3b32-11e9-9760-9e5b40196308" in namespace "e2e-tests-configmap-26thp" to be "success or failure"
Feb 28 08:23:20.476: INFO: Pod "pod-configmaps-1b46bd3b-3b32-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 61.122394ms
Feb 28 08:23:22.537: INFO: Pod "pod-configmaps-1b46bd3b-3b32-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121796881s
STEP: Saw pod success
Feb 28 08:23:22.537: INFO: Pod "pod-configmaps-1b46bd3b-3b32-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:23:22.599: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-1b46bd3b-3b32-11e9-9760-9e5b40196308 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:23:22.848: INFO: Waiting for pod pod-configmaps-1b46bd3b-3b32-11e9-9760-9e5b40196308 to disappear
Feb 28 08:23:22.946: INFO: Pod pod-configmaps-1b46bd3b-3b32-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:23:22.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-26thp" for this suite.
Feb 28 08:23:29.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:23:29.808: INFO: namespace: e2e-tests-configmap-26thp, resource: bindings, ignored listing per whitelist
Feb 28 08:23:31.385: INFO: namespace e2e-tests-configmap-26thp deletion completed in 8.379463258s

• [SLOW TEST:13.663 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:23:31.386: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-j5nm6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:23:33.941: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2357145d-3b32-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-j5nm6" to be "success or failure"
Feb 28 08:23:34.000: INFO: Pod "downwardapi-volume-2357145d-3b32-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.947748ms
Feb 28 08:23:36.054: INFO: Pod "downwardapi-volume-2357145d-3b32-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.113368079s
STEP: Saw pod success
Feb 28 08:23:36.054: INFO: Pod "downwardapi-volume-2357145d-3b32-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:23:36.110: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-2357145d-3b32-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:23:36.229: INFO: Waiting for pod downwardapi-volume-2357145d-3b32-11e9-9760-9e5b40196308 to disappear
Feb 28 08:23:36.284: INFO: Pod downwardapi-volume-2357145d-3b32-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:23:36.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-j5nm6" for this suite.
Feb 28 08:23:42.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:23:44.115: INFO: namespace: e2e-tests-downward-api-j5nm6, resource: bindings, ignored listing per whitelist
Feb 28 08:23:44.715: INFO: namespace e2e-tests-downward-api-j5nm6 deletion completed in 8.375513306s

• [SLOW TEST:13.329 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:23:44.715: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-f8kdf
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-2b3f5af5-3b32-11e9-9760-9e5b40196308
STEP: Creating configMap with name cm-test-opt-upd-2b3f5b3e-3b32-11e9-9760-9e5b40196308
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2b3f5af5-3b32-11e9-9760-9e5b40196308
STEP: Updating configmap cm-test-opt-upd-2b3f5b3e-3b32-11e9-9760-9e5b40196308
STEP: Creating configMap with name cm-test-opt-create-2b3f5b56-3b32-11e9-9760-9e5b40196308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:24:54.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-f8kdf" for this suite.
Feb 28 08:25:16.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:17.089: INFO: namespace: e2e-tests-configmap-f8kdf, resource: bindings, ignored listing per whitelist
Feb 28 08:25:18.821: INFO: namespace e2e-tests-configmap-f8kdf deletion completed in 24.557841016s

• [SLOW TEST:94.105 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:25:18.821: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-v58k6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 28 08:25:21.222: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:25:21.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v58k6" for this suite.
Feb 28 08:25:28.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:29.811: INFO: namespace: e2e-tests-kubectl-v58k6, resource: bindings, ignored listing per whitelist
Feb 28 08:25:30.561: INFO: namespace e2e-tests-kubectl-v58k6 deletion completed in 8.565285118s

• [SLOW TEST:11.740 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:25:30.561: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-2zx58
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 08:25:32.989: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:25:36.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-2zx58" for this suite.
Feb 28 08:25:42.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:42.920: INFO: namespace: e2e-tests-init-container-2zx58, resource: bindings, ignored listing per whitelist
Feb 28 08:25:44.759: INFO: namespace e2e-tests-init-container-2zx58 deletion completed in 8.492931167s

• [SLOW TEST:14.199 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:25:44.760: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-5l7lp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 28 08:25:47.933: INFO: created pod pod-service-account-defaultsa
Feb 28 08:25:47.933: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 28 08:25:47.995: INFO: created pod pod-service-account-mountsa
Feb 28 08:25:47.995: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 28 08:25:48.059: INFO: created pod pod-service-account-nomountsa
Feb 28 08:25:48.060: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 28 08:25:48.123: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 28 08:25:48.123: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 28 08:25:48.183: INFO: created pod pod-service-account-mountsa-mountspec
Feb 28 08:25:48.183: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 28 08:25:48.245: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 28 08:25:48.245: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 28 08:25:48.310: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 28 08:25:48.310: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 28 08:25:48.369: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 28 08:25:48.369: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 28 08:25:48.428: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 28 08:25:48.428: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:25:48.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-5l7lp" for this suite.
Feb 28 08:25:56.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:57.279: INFO: namespace: e2e-tests-svcaccounts-5l7lp, resource: bindings, ignored listing per whitelist
Feb 28 08:25:59.027: INFO: namespace e2e-tests-svcaccounts-5l7lp deletion completed in 10.538692853s

• [SLOW TEST:14.267 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:25:59.027: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-f666j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 08:26:04.353: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7b4773ad-3b32-11e9-9760-9e5b40196308"
Feb 28 08:26:04.353: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7b4773ad-3b32-11e9-9760-9e5b40196308" in namespace "e2e-tests-pods-f666j" to be "terminated due to deadline exceeded"
Feb 28 08:26:04.413: INFO: Pod "pod-update-activedeadlineseconds-7b4773ad-3b32-11e9-9760-9e5b40196308": Phase="Running", Reason="", readiness=true. Elapsed: 59.535688ms
Feb 28 08:26:06.477: INFO: Pod "pod-update-activedeadlineseconds-7b4773ad-3b32-11e9-9760-9e5b40196308": Phase="Running", Reason="", readiness=true. Elapsed: 2.12356749s
Feb 28 08:26:08.537: INFO: Pod "pod-update-activedeadlineseconds-7b4773ad-3b32-11e9-9760-9e5b40196308": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.183399462s
Feb 28 08:26:08.537: INFO: Pod "pod-update-activedeadlineseconds-7b4773ad-3b32-11e9-9760-9e5b40196308" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:26:08.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-f666j" for this suite.
Feb 28 08:26:14.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:26:16.203: INFO: namespace: e2e-tests-pods-f666j, resource: bindings, ignored listing per whitelist
Feb 28 08:26:17.106: INFO: namespace e2e-tests-pods-f666j deletion completed in 8.506184792s

• [SLOW TEST:18.079 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:26:17.107: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-5w2tm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:26:19.801: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 28 08:26:19.921: INFO: Number of nodes with available pods: 0
Feb 28 08:26:19.921: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 28 08:26:20.169: INFO: Number of nodes with available pods: 0
Feb 28 08:26:20.169: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:21.227: INFO: Number of nodes with available pods: 1
Feb 28 08:26:21.227: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 28 08:26:21.465: INFO: Number of nodes with available pods: 0
Feb 28 08:26:21.465: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 28 08:26:21.586: INFO: Number of nodes with available pods: 0
Feb 28 08:26:21.586: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:22.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:22.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:23.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:23.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:24.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:24.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:25.645: INFO: Number of nodes with available pods: 0
Feb 28 08:26:25.645: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:26.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:26.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:27.646: INFO: Number of nodes with available pods: 0
Feb 28 08:26:27.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:28.650: INFO: Number of nodes with available pods: 0
Feb 28 08:26:28.650: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:29.646: INFO: Number of nodes with available pods: 0
Feb 28 08:26:29.646: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:30.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:30.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:31.645: INFO: Number of nodes with available pods: 0
Feb 28 08:26:31.645: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:32.645: INFO: Number of nodes with available pods: 0
Feb 28 08:26:32.645: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:33.645: INFO: Number of nodes with available pods: 0
Feb 28 08:26:33.645: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:34.648: INFO: Number of nodes with available pods: 0
Feb 28 08:26:34.648: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:35.645: INFO: Number of nodes with available pods: 0
Feb 28 08:26:35.645: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:36.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:36.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:37.648: INFO: Number of nodes with available pods: 0
Feb 28 08:26:37.648: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:38.650: INFO: Number of nodes with available pods: 0
Feb 28 08:26:38.650: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:39.679: INFO: Number of nodes with available pods: 0
Feb 28 08:26:39.679: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:40.646: INFO: Number of nodes with available pods: 0
Feb 28 08:26:40.646: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:41.644: INFO: Number of nodes with available pods: 0
Feb 28 08:26:41.644: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:42.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:42.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:43.649: INFO: Number of nodes with available pods: 0
Feb 28 08:26:43.649: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:44.649: INFO: Number of nodes with available pods: 0
Feb 28 08:26:44.649: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:45.646: INFO: Number of nodes with available pods: 0
Feb 28 08:26:45.646: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:46.648: INFO: Number of nodes with available pods: 0
Feb 28 08:26:46.648: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:47.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:47.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:48.644: INFO: Number of nodes with available pods: 0
Feb 28 08:26:48.644: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:49.643: INFO: Number of nodes with available pods: 0
Feb 28 08:26:49.643: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:50.646: INFO: Number of nodes with available pods: 0
Feb 28 08:26:50.646: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:51.644: INFO: Number of nodes with available pods: 0
Feb 28 08:26:51.644: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:52.648: INFO: Number of nodes with available pods: 0
Feb 28 08:26:52.648: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:53.645: INFO: Number of nodes with available pods: 0
Feb 28 08:26:53.646: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:54.645: INFO: Number of nodes with available pods: 0
Feb 28 08:26:54.645: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:55.645: INFO: Number of nodes with available pods: 0
Feb 28 08:26:55.645: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:56.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:56.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:57.649: INFO: Number of nodes with available pods: 0
Feb 28 08:26:57.649: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:58.643: INFO: Number of nodes with available pods: 0
Feb 28 08:26:58.643: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:26:59.647: INFO: Number of nodes with available pods: 0
Feb 28 08:26:59.647: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:27:00.642: INFO: Number of nodes with available pods: 0
Feb 28 08:27:00.642: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:27:01.644: INFO: Number of nodes with available pods: 0
Feb 28 08:27:01.644: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:27:02.644: INFO: Number of nodes with available pods: 1
Feb 28 08:27:02.644: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-5w2tm, will wait for the garbage collector to delete the pods
Feb 28 08:27:02.977: INFO: Deleting {extensions DaemonSet} daemon-set took: 55.584884ms
Feb 28 08:27:03.077: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.277371ms
Feb 28 08:27:40.736: INFO: Number of nodes with available pods: 0
Feb 28 08:27:40.736: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:27:40.792: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-5w2tm/daemonsets","resourceVersion":"14380"},"items":null}

Feb 28 08:27:40.846: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-5w2tm/pods","resourceVersion":"14380"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:27:41.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-5w2tm" for this suite.
Feb 28 08:27:47.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:27:48.687: INFO: namespace: e2e-tests-daemonsets-5w2tm, resource: bindings, ignored listing per whitelist
Feb 28 08:27:49.644: INFO: namespace e2e-tests-daemonsets-5w2tm deletion completed in 8.482339645s

• [SLOW TEST:92.538 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:27:49.644: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-jhfsm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-jhfsm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-jhfsm to expose endpoints map[]
Feb 28 08:27:52.030: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-jhfsm exposes endpoints map[] (53.717845ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-jhfsm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-jhfsm to expose endpoints map[pod1:[80]]
Feb 28 08:27:54.439: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-jhfsm exposes endpoints map[pod1:[80]] (2.352840745s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-jhfsm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-jhfsm to expose endpoints map[pod1:[80] pod2:[80]]
Feb 28 08:27:57.041: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-jhfsm exposes endpoints map[pod1:[80] pod2:[80]] (2.539092592s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-jhfsm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-jhfsm to expose endpoints map[pod2:[80]]
Feb 28 08:27:57.222: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-jhfsm exposes endpoints map[pod2:[80]] (118.805953ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-jhfsm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-jhfsm to expose endpoints map[]
Feb 28 08:27:57.341: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-jhfsm exposes endpoints map[] (56.27422ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:27:57.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-jhfsm" for this suite.
Feb 28 08:28:19.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:28:21.286: INFO: namespace: e2e-tests-services-jhfsm, resource: bindings, ignored listing per whitelist
Feb 28 08:28:21.943: INFO: namespace e2e-tests-services-jhfsm deletion completed in 24.48275062s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:32.299 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:28:21.943: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-jtq6r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-ktdr7
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 28 08:28:34.641: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-qkslg
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:28:52.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-jtq6r" for this suite.
Feb 28 08:28:58.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:28:59.189: INFO: namespace: e2e-tests-namespaces-jtq6r, resource: bindings, ignored listing per whitelist
Feb 28 08:29:00.627: INFO: namespace e2e-tests-namespaces-jtq6r deletion completed in 8.51180859s
STEP: Destroying namespace "e2e-tests-nsdeletetest-ktdr7" for this suite.
Feb 28 08:29:00.687: INFO: Namespace e2e-tests-nsdeletetest-ktdr7 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-qkslg" for this suite.
Feb 28 08:29:06.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:08.211: INFO: namespace: e2e-tests-nsdeletetest-qkslg, resource: bindings, ignored listing per whitelist
Feb 28 08:29:09.179: INFO: namespace e2e-tests-nsdeletetest-qkslg deletion completed in 8.491960918s

• [SLOW TEST:47.236 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:29:09.191: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gwhrd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:29:11.659: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eca2c81f-3b32-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-gwhrd" to be "success or failure"
Feb 28 08:29:11.718: INFO: Pod "downwardapi-volume-eca2c81f-3b32-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.058147ms
Feb 28 08:29:13.779: INFO: Pod "downwardapi-volume-eca2c81f-3b32-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.119120719s
STEP: Saw pod success
Feb 28 08:29:13.779: INFO: Pod "downwardapi-volume-eca2c81f-3b32-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:29:13.839: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-eca2c81f-3b32-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:29:13.972: INFO: Waiting for pod downwardapi-volume-eca2c81f-3b32-11e9-9760-9e5b40196308 to disappear
Feb 28 08:29:14.033: INFO: Pod downwardapi-volume-eca2c81f-3b32-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:29:14.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gwhrd" for this suite.
Feb 28 08:29:20.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:21.214: INFO: namespace: e2e-tests-projected-gwhrd, resource: bindings, ignored listing per whitelist
Feb 28 08:29:22.541: INFO: namespace e2e-tests-projected-gwhrd deletion completed in 8.445950831s

• [SLOW TEST:13.350 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:29:22.542: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-h4ftw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:29:24.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-h4ftw" for this suite.
Feb 28 08:29:31.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:32.528: INFO: namespace: e2e-tests-services-h4ftw, resource: bindings, ignored listing per whitelist
Feb 28 08:29:33.462: INFO: namespace e2e-tests-services-h4ftw deletion completed in 8.434193989s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:10.920 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:29:33.462: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-r92z8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-xn7p
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:29:36.016: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xn7p" in namespace "e2e-tests-subpath-r92z8" to be "success or failure"
Feb 28 08:29:36.075: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Pending", Reason="", readiness=false. Elapsed: 59.139673ms
Feb 28 08:29:38.137: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120655238s
Feb 28 08:29:40.197: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 4.181149862s
Feb 28 08:29:42.256: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 6.239733088s
Feb 28 08:29:44.318: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 8.301406593s
Feb 28 08:29:46.378: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 10.362019384s
Feb 28 08:29:48.439: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 12.422875799s
Feb 28 08:29:50.503: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 14.486704822s
Feb 28 08:29:52.566: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 16.549239885s
Feb 28 08:29:54.624: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 18.607459855s
Feb 28 08:29:56.683: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 20.666398306s
Feb 28 08:29:58.744: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Running", Reason="", readiness=false. Elapsed: 22.727691085s
Feb 28 08:30:00.802: INFO: Pod "pod-subpath-test-configmap-xn7p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.785743364s
STEP: Saw pod success
Feb 28 08:30:00.802: INFO: Pod "pod-subpath-test-configmap-xn7p" satisfied condition "success or failure"
Feb 28 08:30:00.862: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-subpath-test-configmap-xn7p container test-container-subpath-configmap-xn7p: <nil>
STEP: delete the pod
Feb 28 08:30:00.993: INFO: Waiting for pod pod-subpath-test-configmap-xn7p to disappear
Feb 28 08:30:01.055: INFO: Pod pod-subpath-test-configmap-xn7p no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xn7p
Feb 28 08:30:01.055: INFO: Deleting pod "pod-subpath-test-configmap-xn7p" in namespace "e2e-tests-subpath-r92z8"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:30:01.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-r92z8" for this suite.
Feb 28 08:30:07.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:30:07.896: INFO: namespace: e2e-tests-subpath-r92z8, resource: bindings, ignored listing per whitelist
Feb 28 08:30:09.699: INFO: namespace e2e-tests-subpath-r92z8 deletion completed in 8.523123789s

• [SLOW TEST:36.237 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:30:09.699: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-nnj2j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 08:30:15.128: INFO: Successfully updated pod "annotationupdate10b80e9f-3b33-11e9-9760-9e5b40196308"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:30:17.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nnj2j" for this suite.
Feb 28 08:30:35.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:30:37.606: INFO: namespace: e2e-tests-projected-nnj2j, resource: bindings, ignored listing per whitelist
Feb 28 08:30:37.837: INFO: namespace e2e-tests-projected-nnj2j deletion completed in 20.513904917s

• [SLOW TEST:28.138 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:30:37.838: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-bgf2j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-bgf2j
Feb 28 08:30:42.386: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-bgf2j
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:30:42.446: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:34:43.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-bgf2j" for this suite.
Feb 28 08:34:49.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:34:50.504: INFO: namespace: e2e-tests-container-probe-bgf2j, resource: bindings, ignored listing per whitelist
Feb 28 08:34:52.227: INFO: namespace e2e-tests-container-probe-bgf2j deletion completed in 8.439794067s

• [SLOW TEST:254.390 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:34:52.228: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-mxf2k
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-b914ba2d-3b33-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:34:54.727: INFO: Waiting up to 5m0s for pod "pod-configmaps-b91e0264-3b33-11e9-9760-9e5b40196308" in namespace "e2e-tests-configmap-mxf2k" to be "success or failure"
Feb 28 08:34:54.786: INFO: Pod "pod-configmaps-b91e0264-3b33-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.37448ms
Feb 28 08:34:56.848: INFO: Pod "pod-configmaps-b91e0264-3b33-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121301127s
STEP: Saw pod success
Feb 28 08:34:56.848: INFO: Pod "pod-configmaps-b91e0264-3b33-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:34:56.910: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-b91e0264-3b33-11e9-9760-9e5b40196308 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:34:57.044: INFO: Waiting for pod pod-configmaps-b91e0264-3b33-11e9-9760-9e5b40196308 to disappear
Feb 28 08:34:57.105: INFO: Pod pod-configmaps-b91e0264-3b33-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:34:57.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mxf2k" for this suite.
Feb 28 08:35:03.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:05.641: INFO: namespace: e2e-tests-configmap-mxf2k, resource: bindings, ignored listing per whitelist
Feb 28 08:35:05.699: INFO: namespace e2e-tests-configmap-mxf2k deletion completed in 8.530656672s

• [SLOW TEST:13.471 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:35:05.699: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-49ptn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:35:08.258: INFO: Waiting up to 5m0s for pod "downward-api-c12f2d9e-3b33-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-49ptn" to be "success or failure"
Feb 28 08:35:08.316: INFO: Pod "downward-api-c12f2d9e-3b33-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.077373ms
Feb 28 08:35:10.377: INFO: Pod "downward-api-c12f2d9e-3b33-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.119198131s
STEP: Saw pod success
Feb 28 08:35:10.378: INFO: Pod "downward-api-c12f2d9e-3b33-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:35:10.434: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downward-api-c12f2d9e-3b33-11e9-9760-9e5b40196308 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:35:10.563: INFO: Waiting for pod downward-api-c12f2d9e-3b33-11e9-9760-9e5b40196308 to disappear
Feb 28 08:35:10.621: INFO: Pod downward-api-c12f2d9e-3b33-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:35:10.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-49ptn" for this suite.
Feb 28 08:35:16.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:18.493: INFO: namespace: e2e-tests-downward-api-49ptn, resource: bindings, ignored listing per whitelist
Feb 28 08:35:19.159: INFO: namespace e2e-tests-downward-api-49ptn deletion completed in 8.477922905s

• [SLOW TEST:13.460 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:35:19.159: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ttwks
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:35:21.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c91c43c3-3b33-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-ttwks" to be "success or failure"
Feb 28 08:35:21.614: INFO: Pod "downwardapi-volume-c91c43c3-3b33-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.013779ms
Feb 28 08:35:23.671: INFO: Pod "downwardapi-volume-c91c43c3-3b33-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.115673526s
STEP: Saw pod success
Feb 28 08:35:23.671: INFO: Pod "downwardapi-volume-c91c43c3-3b33-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:35:23.728: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-c91c43c3-3b33-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:35:23.852: INFO: Waiting for pod downwardapi-volume-c91c43c3-3b33-11e9-9760-9e5b40196308 to disappear
Feb 28 08:35:23.952: INFO: Pod downwardapi-volume-c91c43c3-3b33-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:35:23.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ttwks" for this suite.
Feb 28 08:35:30.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:31.760: INFO: namespace: e2e-tests-downward-api-ttwks, resource: bindings, ignored listing per whitelist
Feb 28 08:35:32.602: INFO: namespace e2e-tests-downward-api-ttwks deletion completed in 8.551040358s

• [SLOW TEST:13.443 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:35:32.602: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-w8bmv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:35:35.119: INFO: (0) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 61.86587ms)
Feb 28 08:35:35.181: INFO: (1) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 62.111038ms)
Feb 28 08:35:35.241: INFO: (2) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 60.278988ms)
Feb 28 08:35:35.304: INFO: (3) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 62.592315ms)
Feb 28 08:35:35.367: INFO: (4) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 63.435094ms)
Feb 28 08:35:35.430: INFO: (5) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 62.711412ms)
Feb 28 08:35:35.493: INFO: (6) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 62.673261ms)
Feb 28 08:35:35.552: INFO: (7) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 59.304666ms)
Feb 28 08:35:35.612: INFO: (8) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 59.71768ms)
Feb 28 08:35:35.672: INFO: (9) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 59.839445ms)
Feb 28 08:35:35.732: INFO: (10) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 60.117993ms)
Feb 28 08:35:35.793: INFO: (11) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 60.708416ms)
Feb 28 08:35:35.855: INFO: (12) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 61.853286ms)
Feb 28 08:35:35.917: INFO: (13) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 61.670103ms)
Feb 28 08:35:35.978: INFO: (14) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 59.917051ms)
Feb 28 08:35:36.038: INFO: (15) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 57.935513ms)
Feb 28 08:35:36.097: INFO: (16) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 59.257834ms)
Feb 28 08:35:36.158: INFO: (17) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 60.414242ms)
Feb 28 08:35:36.221: INFO: (18) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 62.697761ms)
Feb 28 08:35:36.283: INFO: (19) /api/v1/nodes/ip-10-250-8-168.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 62.448465ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:35:36.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-w8bmv" for this suite.
Feb 28 08:35:42.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:44.782: INFO: namespace: e2e-tests-proxy-w8bmv, resource: bindings, ignored listing per whitelist
Feb 28 08:35:45.077: INFO: namespace e2e-tests-proxy-w8bmv deletion completed in 8.732253018s

• [SLOW TEST:12.475 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:35:45.077: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zvrnq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-d89c9951-3b33-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:35:47.625: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d8a5d6ef-3b33-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-zvrnq" to be "success or failure"
Feb 28 08:35:47.684: INFO: Pod "pod-projected-configmaps-d8a5d6ef-3b33-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.550744ms
Feb 28 08:35:49.745: INFO: Pod "pod-projected-configmaps-d8a5d6ef-3b33-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.119814891s
STEP: Saw pod success
Feb 28 08:35:49.745: INFO: Pod "pod-projected-configmaps-d8a5d6ef-3b33-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:35:49.808: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-configmaps-d8a5d6ef-3b33-11e9-9760-9e5b40196308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:35:49.937: INFO: Waiting for pod pod-projected-configmaps-d8a5d6ef-3b33-11e9-9760-9e5b40196308 to disappear
Feb 28 08:35:49.995: INFO: Pod pod-projected-configmaps-d8a5d6ef-3b33-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:35:49.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zvrnq" for this suite.
Feb 28 08:35:56.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:57.429: INFO: namespace: e2e-tests-projected-zvrnq, resource: bindings, ignored listing per whitelist
Feb 28 08:35:58.557: INFO: namespace e2e-tests-projected-zvrnq deletion completed in 8.498018883s

• [SLOW TEST:13.480 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:35:58.558: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8q2f9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:36:01.359: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0d56a47-3b33-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-8q2f9" to be "success or failure"
Feb 28 08:36:01.424: INFO: Pod "downwardapi-volume-e0d56a47-3b33-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 64.994824ms
Feb 28 08:36:03.487: INFO: Pod "downwardapi-volume-e0d56a47-3b33-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.127621198s
STEP: Saw pod success
Feb 28 08:36:03.487: INFO: Pod "downwardapi-volume-e0d56a47-3b33-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:36:03.545: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-e0d56a47-3b33-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:36:03.675: INFO: Waiting for pod downwardapi-volume-e0d56a47-3b33-11e9-9760-9e5b40196308 to disappear
Feb 28 08:36:03.735: INFO: Pod downwardapi-volume-e0d56a47-3b33-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:36:03.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8q2f9" for this suite.
Feb 28 08:36:10.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:36:11.583: INFO: namespace: e2e-tests-projected-8q2f9, resource: bindings, ignored listing per whitelist
Feb 28 08:36:12.394: INFO: namespace e2e-tests-projected-8q2f9 deletion completed in 8.543052116s

• [SLOW TEST:13.837 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:36:12.395: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2zc9j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:36:14.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8d64258-3b33-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-2zc9j" to be "success or failure"
Feb 28 08:36:14.843: INFO: Pod "downwardapi-volume-e8d64258-3b33-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 57.417968ms
Feb 28 08:36:16.918: INFO: Pod "downwardapi-volume-e8d64258-3b33-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.132282028s
STEP: Saw pod success
Feb 28 08:36:16.918: INFO: Pod "downwardapi-volume-e8d64258-3b33-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:36:16.979: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-e8d64258-3b33-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:36:17.108: INFO: Waiting for pod downwardapi-volume-e8d64258-3b33-11e9-9760-9e5b40196308 to disappear
Feb 28 08:36:17.170: INFO: Pod downwardapi-volume-e8d64258-3b33-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:36:17.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2zc9j" for this suite.
Feb 28 08:36:23.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:36:24.206: INFO: namespace: e2e-tests-projected-2zc9j, resource: bindings, ignored listing per whitelist
Feb 28 08:36:25.752: INFO: namespace e2e-tests-projected-2zc9j deletion completed in 8.521871162s

• [SLOW TEST:13.358 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:36:25.753: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-vxv6t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 28 08:36:28.180: INFO: Waiting up to 5m0s for pod "client-containers-f0d24b38-3b33-11e9-9760-9e5b40196308" in namespace "e2e-tests-containers-vxv6t" to be "success or failure"
Feb 28 08:36:28.241: INFO: Pod "client-containers-f0d24b38-3b33-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 60.444122ms
Feb 28 08:36:30.302: INFO: Pod "client-containers-f0d24b38-3b33-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121829293s
STEP: Saw pod success
Feb 28 08:36:30.303: INFO: Pod "client-containers-f0d24b38-3b33-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:36:30.363: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod client-containers-f0d24b38-3b33-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 08:36:30.494: INFO: Waiting for pod client-containers-f0d24b38-3b33-11e9-9760-9e5b40196308 to disappear
Feb 28 08:36:30.558: INFO: Pod client-containers-f0d24b38-3b33-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:36:30.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-vxv6t" for this suite.
Feb 28 08:36:36.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:36:38.097: INFO: namespace: e2e-tests-containers-vxv6t, resource: bindings, ignored listing per whitelist
Feb 28 08:36:39.110: INFO: namespace e2e-tests-containers-vxv6t deletion completed in 8.490521902s

• [SLOW TEST:13.357 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:36:39.110: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2sldz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:36:41.464: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8bc96b9-3b33-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-2sldz" to be "success or failure"
Feb 28 08:36:41.524: INFO: Pod "downwardapi-volume-f8bc96b9-3b33-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 60.432279ms
Feb 28 08:36:43.582: INFO: Pod "downwardapi-volume-f8bc96b9-3b33-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118022626s
STEP: Saw pod success
Feb 28 08:36:43.582: INFO: Pod "downwardapi-volume-f8bc96b9-3b33-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:36:43.640: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-f8bc96b9-3b33-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:36:43.771: INFO: Waiting for pod downwardapi-volume-f8bc96b9-3b33-11e9-9760-9e5b40196308 to disappear
Feb 28 08:36:43.832: INFO: Pod downwardapi-volume-f8bc96b9-3b33-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:36:43.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2sldz" for this suite.
Feb 28 08:36:50.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:36:52.081: INFO: namespace: e2e-tests-projected-2sldz, resource: bindings, ignored listing per whitelist
Feb 28 08:36:52.440: INFO: namespace e2e-tests-projected-2sldz deletion completed in 8.544563537s

• [SLOW TEST:13.330 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:36:52.440: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wszjd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 08:36:54.880: INFO: Waiting up to 5m0s for pod "pod-00bc2ea7-3b34-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-wszjd" to be "success or failure"
Feb 28 08:36:54.941: INFO: Pod "pod-00bc2ea7-3b34-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 60.423409ms
Feb 28 08:36:56.999: INFO: Pod "pod-00bc2ea7-3b34-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118797091s
STEP: Saw pod success
Feb 28 08:36:56.999: INFO: Pod "pod-00bc2ea7-3b34-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:36:57.056: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-00bc2ea7-3b34-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 08:36:57.181: INFO: Waiting for pod pod-00bc2ea7-3b34-11e9-9760-9e5b40196308 to disappear
Feb 28 08:36:57.239: INFO: Pod pod-00bc2ea7-3b34-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:36:57.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wszjd" for this suite.
Feb 28 08:37:03.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:37:04.264: INFO: namespace: e2e-tests-emptydir-wszjd, resource: bindings, ignored listing per whitelist
Feb 28 08:37:05.835: INFO: namespace e2e-tests-emptydir-wszjd deletion completed in 8.537509255s

• [SLOW TEST:13.395 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:37:05.835: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-zl9ch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-zl9ch
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-zl9ch
STEP: Deleting pre-stop pod
Feb 28 08:37:21.827: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:37:21.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-zl9ch" for this suite.
Feb 28 08:38:00.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:01.727: INFO: namespace: e2e-tests-prestop-zl9ch, resource: bindings, ignored listing per whitelist
Feb 28 08:38:02.609: INFO: namespace e2e-tests-prestop-zl9ch deletion completed in 40.656543307s

• [SLOW TEST:56.774 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:38:02.609: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-qvrzp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 08:38:05.065: INFO: Waiting up to 5m0s for pod "pod-2a91cda8-3b34-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-qvrzp" to be "success or failure"
Feb 28 08:38:05.121: INFO: Pod "pod-2a91cda8-3b34-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 56.409409ms
Feb 28 08:38:07.182: INFO: Pod "pod-2a91cda8-3b34-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117060009s
STEP: Saw pod success
Feb 28 08:38:07.183: INFO: Pod "pod-2a91cda8-3b34-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:38:07.240: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-2a91cda8-3b34-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 08:38:07.370: INFO: Waiting for pod pod-2a91cda8-3b34-11e9-9760-9e5b40196308 to disappear
Feb 28 08:38:07.429: INFO: Pod pod-2a91cda8-3b34-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:38:07.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qvrzp" for this suite.
Feb 28 08:38:13.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:14.152: INFO: namespace: e2e-tests-emptydir-qvrzp, resource: bindings, ignored listing per whitelist
Feb 28 08:38:15.998: INFO: namespace e2e-tests-emptydir-qvrzp deletion completed in 8.508272807s

• [SLOW TEST:13.389 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:38:15.998: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-8d2d7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-4j8l
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:38:18.585: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4j8l" in namespace "e2e-tests-subpath-8d2d7" to be "success or failure"
Feb 28 08:38:18.643: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Pending", Reason="", readiness=false. Elapsed: 57.706067ms
Feb 28 08:38:20.700: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.113978403s
Feb 28 08:38:22.761: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 4.175282284s
Feb 28 08:38:24.822: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 6.236510868s
Feb 28 08:38:26.892: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 8.305967346s
Feb 28 08:38:28.957: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 10.371479788s
Feb 28 08:38:31.020: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 12.434262943s
Feb 28 08:38:33.082: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 14.496639749s
Feb 28 08:38:35.167: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 16.581863091s
Feb 28 08:38:37.230: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 18.644013245s
Feb 28 08:38:39.289: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 20.703851959s
Feb 28 08:38:41.353: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Running", Reason="", readiness=false. Elapsed: 22.767054214s
Feb 28 08:38:43.413: INFO: Pod "pod-subpath-test-secret-4j8l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.827244835s
STEP: Saw pod success
Feb 28 08:38:43.413: INFO: Pod "pod-subpath-test-secret-4j8l" satisfied condition "success or failure"
Feb 28 08:38:43.473: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-subpath-test-secret-4j8l container test-container-subpath-secret-4j8l: <nil>
STEP: delete the pod
Feb 28 08:38:43.603: INFO: Waiting for pod pod-subpath-test-secret-4j8l to disappear
Feb 28 08:38:43.662: INFO: Pod pod-subpath-test-secret-4j8l no longer exists
STEP: Deleting pod pod-subpath-test-secret-4j8l
Feb 28 08:38:43.662: INFO: Deleting pod "pod-subpath-test-secret-4j8l" in namespace "e2e-tests-subpath-8d2d7"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:38:43.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-8d2d7" for this suite.
Feb 28 08:38:49.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:38:51.907: INFO: namespace: e2e-tests-subpath-8d2d7, resource: bindings, ignored listing per whitelist
Feb 28 08:38:52.264: INFO: namespace e2e-tests-subpath-8d2d7 deletion completed in 8.479556458s

• [SLOW TEST:36.266 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:38:52.282: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-d2v5h
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 28 08:38:54.769: INFO: Waiting up to 5m0s for pod "pod-4831e1be-3b34-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-d2v5h" to be "success or failure"
Feb 28 08:38:54.828: INFO: Pod "pod-4831e1be-3b34-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.914025ms
Feb 28 08:38:56.887: INFO: Pod "pod-4831e1be-3b34-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117640535s
STEP: Saw pod success
Feb 28 08:38:56.887: INFO: Pod "pod-4831e1be-3b34-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:38:56.946: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-4831e1be-3b34-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 08:38:57.078: INFO: Waiting for pod pod-4831e1be-3b34-11e9-9760-9e5b40196308 to disappear
Feb 28 08:38:57.137: INFO: Pod pod-4831e1be-3b34-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:38:57.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-d2v5h" for this suite.
Feb 28 08:39:03.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:39:03.742: INFO: namespace: e2e-tests-emptydir-d2v5h, resource: bindings, ignored listing per whitelist
Feb 28 08:39:05.694: INFO: namespace e2e-tests-emptydir-d2v5h deletion completed in 8.496432706s

• [SLOW TEST:13.411 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:39:05.694: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-whvgw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 08:39:08.427: INFO: Waiting up to 5m0s for pod "pod-5055c9b4-3b34-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-whvgw" to be "success or failure"
Feb 28 08:39:08.487: INFO: Pod "pod-5055c9b4-3b34-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.977298ms
Feb 28 08:39:10.549: INFO: Pod "pod-5055c9b4-3b34-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.12140492s
STEP: Saw pod success
Feb 28 08:39:10.549: INFO: Pod "pod-5055c9b4-3b34-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:39:10.610: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-5055c9b4-3b34-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 08:39:10.736: INFO: Waiting for pod pod-5055c9b4-3b34-11e9-9760-9e5b40196308 to disappear
Feb 28 08:39:10.793: INFO: Pod pod-5055c9b4-3b34-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:39:10.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-whvgw" for this suite.
Feb 28 08:39:17.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:39:17.702: INFO: namespace: e2e-tests-emptydir-whvgw, resource: bindings, ignored listing per whitelist
Feb 28 08:39:19.454: INFO: namespace e2e-tests-emptydir-whvgw deletion completed in 8.544896789s

• [SLOW TEST:13.760 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:39:19.454: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-djt57
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 28 08:39:21.879: INFO: Waiting up to 5m0s for pod "client-containers-585a88c1-3b34-11e9-9760-9e5b40196308" in namespace "e2e-tests-containers-djt57" to be "success or failure"
Feb 28 08:39:21.938: INFO: Pod "client-containers-585a88c1-3b34-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.26432ms
Feb 28 08:39:24.000: INFO: Pod "client-containers-585a88c1-3b34-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121110027s
STEP: Saw pod success
Feb 28 08:39:24.001: INFO: Pod "client-containers-585a88c1-3b34-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:39:24.060: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod client-containers-585a88c1-3b34-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 08:39:24.192: INFO: Waiting for pod client-containers-585a88c1-3b34-11e9-9760-9e5b40196308 to disappear
Feb 28 08:39:24.254: INFO: Pod client-containers-585a88c1-3b34-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:39:24.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-djt57" for this suite.
Feb 28 08:39:30.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:39:32.149: INFO: namespace: e2e-tests-containers-djt57, resource: bindings, ignored listing per whitelist
Feb 28 08:39:32.824: INFO: namespace e2e-tests-containers-djt57 deletion completed in 8.508411019s

• [SLOW TEST:13.370 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:39:32.824: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-bdbvn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-6057ff2d-3b34-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 08:39:35.346: INFO: Waiting up to 5m0s for pod "pod-secrets-606127c8-3b34-11e9-9760-9e5b40196308" in namespace "e2e-tests-secrets-bdbvn" to be "success or failure"
Feb 28 08:39:35.411: INFO: Pod "pod-secrets-606127c8-3b34-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 64.768276ms
Feb 28 08:39:37.473: INFO: Pod "pod-secrets-606127c8-3b34-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.127112664s
STEP: Saw pod success
Feb 28 08:39:37.474: INFO: Pod "pod-secrets-606127c8-3b34-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:39:37.534: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-secrets-606127c8-3b34-11e9-9760-9e5b40196308 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:39:37.658: INFO: Waiting for pod pod-secrets-606127c8-3b34-11e9-9760-9e5b40196308 to disappear
Feb 28 08:39:37.716: INFO: Pod pod-secrets-606127c8-3b34-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:39:37.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bdbvn" for this suite.
Feb 28 08:39:43.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:39:45.410: INFO: namespace: e2e-tests-secrets-bdbvn, resource: bindings, ignored listing per whitelist
Feb 28 08:39:46.297: INFO: namespace e2e-tests-secrets-bdbvn deletion completed in 8.522289178s

• [SLOW TEST:13.473 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:39:46.297: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5w55c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 28 08:39:48.710: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml --namespace=e2e-tests-kubectl-5w55c run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 28 08:39:55.188: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 28 08:39:55.188: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:39:57.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5w55c" for this suite.
Feb 28 08:40:03.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:40:05.175: INFO: namespace: e2e-tests-kubectl-5w55c, resource: bindings, ignored listing per whitelist
Feb 28 08:40:05.906: INFO: namespace e2e-tests-kubectl-5w55c deletion completed in 8.522394089s

• [SLOW TEST:19.608 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:40:05.906: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-hqzr9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-hqzr9
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-hqzr9
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-hqzr9
Feb 28 08:40:08.535: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 28 08:40:18.601: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 28 08:40:18.662: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:40:19.696: INFO: stderr: ""
Feb 28 08:40:19.696: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:40:19.696: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:40:19.756: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 28 08:40:29.818: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:40:29.818: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:40:30.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999292s
Feb 28 08:40:31.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.936164008s
Feb 28 08:40:32.194: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.874931465s
Feb 28 08:40:33.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.816377274s
Feb 28 08:40:34.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.756809097s
Feb 28 08:40:35.375: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.696105527s
Feb 28 08:40:36.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.63499091s
Feb 28 08:40:37.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.570818906s
Feb 28 08:40:38.563: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.509503844s
Feb 28 08:40:39.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 447.691511ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-hqzr9
Feb 28 08:40:40.689: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:40:42.023: INFO: stderr: ""
Feb 28 08:40:42.023: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:40:42.023: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:40:42.023: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:40:43.214: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 28 08:40:43.215: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:40:43.215: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:40:43.215: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:40:44.386: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 28 08:40:44.386: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:40:44.386: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:40:44.446: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:40:44.446: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:40:44.446: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 28 08:40:44.508: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:40:45.644: INFO: stderr: ""
Feb 28 08:40:45.644: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:40:45.644: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:40:45.644: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:40:46.793: INFO: stderr: ""
Feb 28 08:40:46.793: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:40:46.793: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:40:46.793: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:40:47.947: INFO: stderr: ""
Feb 28 08:40:47.947: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:40:47.947: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:40:47.947: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:40:48.005: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 28 08:40:58.135: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:40:58.136: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:40:58.136: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:40:58.315: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:40:58.315: INFO: ss-0  ip-10-250-9-188.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:08 +0000 UTC  }]
Feb 28 08:40:58.315: INFO: ss-1  ip-10-250-8-168.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:40:58.315: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:40:58.315: INFO: 
Feb 28 08:40:58.315: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 08:40:59.374: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:40:59.374: INFO: ss-0  ip-10-250-9-188.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:08 +0000 UTC  }]
Feb 28 08:40:59.374: INFO: ss-1  ip-10-250-8-168.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:40:59.374: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:40:59.374: INFO: 
Feb 28 08:40:59.374: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 08:41:00.433: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:41:00.433: INFO: ss-0  ip-10-250-9-188.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:08 +0000 UTC  }]
Feb 28 08:41:00.433: INFO: ss-1  ip-10-250-8-168.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:41:00.433: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:41:00.433: INFO: 
Feb 28 08:41:00.434: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 08:41:01.492: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:41:01.492: INFO: ss-0  ip-10-250-9-188.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:08 +0000 UTC  }]
Feb 28 08:41:01.492: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:41:01.492: INFO: 
Feb 28 08:41:01.492: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 08:41:02.551: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:41:02.551: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:41:02.551: INFO: 
Feb 28 08:41:02.551: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 28 08:41:03.611: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:41:03.611: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:41:03.611: INFO: 
Feb 28 08:41:03.611: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 28 08:41:04.669: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:41:04.669: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:41:04.669: INFO: 
Feb 28 08:41:04.669: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 28 08:41:05.731: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:41:05.731: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:41:05.731: INFO: 
Feb 28 08:41:05.731: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 28 08:41:06.793: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:41:06.793: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:41:06.793: INFO: 
Feb 28 08:41:06.793: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 28 08:41:07.854: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 28 08:41:07.854: INFO: ss-2  ip-10-250-9-188.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:40:29 +0000 UTC  }]
Feb 28 08:41:07.854: INFO: 
Feb 28 08:41:07.854: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-hqzr9
Feb 28 08:41:08.921: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:09.758: INFO: rc: 1
Feb 28 08:41:09.762: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0027608d0 exit status 1 <nil> <nil> true [0xc0000d4630 0xc0000d4778 0xc0000d49c0] [0xc0000d4630 0xc0000d4778 0xc0000d49c0] [0xc0000d4670 0xc0000d48c8] [0x932420 0x932420] 0xc000bf3020 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 28 08:41:19.779: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:20.105: INFO: rc: 1
Feb 28 08:41:20.105: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002766d20 exit status 1 <nil> <nil> true [0xc00000e0b8 0xc00000e118 0xc00000e138] [0xc00000e0b8 0xc00000e118 0xc00000e138] [0xc00000e0f0 0xc00000e128] [0x932420 0x932420] 0xc0027243c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:41:30.106: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:30.447: INFO: rc: 1
Feb 28 08:41:30.447: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002760c00 exit status 1 <nil> <nil> true [0xc0000d4a88 0xc0000d4f50 0xc0000d5058] [0xc0000d4a88 0xc0000d4f50 0xc0000d5058] [0xc0000d4e10 0xc0000d5028] [0x932420 0x932420] 0xc000bf3320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:41:40.448: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:40.807: INFO: rc: 1
Feb 28 08:41:40.808: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002760ea0 exit status 1 <nil> <nil> true [0xc0000d5060 0xc0000d5158 0xc0000d5200] [0xc0000d5060 0xc0000d5158 0xc0000d5200] [0xc0000d5138 0xc0000d51e8] [0x932420 0x932420] 0xc000bf3680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:41:50.808: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:51.320: INFO: rc: 1
Feb 28 08:41:51.322: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726a50 exit status 1 <nil> <nil> true [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac2d8 0xc0001ac440] [0x932420 0x932420] 0xc002822a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:42:01.322: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:01.663: INFO: rc: 1
Feb 28 08:42:01.664: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b6c270 exit status 1 <nil> <nil> true [0xc001478028 0xc0014780d8 0xc001478118] [0xc001478028 0xc0014780d8 0xc001478118] [0xc0014780b8 0xc001478100] [0x932420 0x932420] 0xc0016d98c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:42:11.664: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:11.991: INFO: rc: 1
Feb 28 08:42:11.991: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002766fc0 exit status 1 <nil> <nil> true [0xc00000e140 0xc00000e198 0xc00000e1e8] [0xc00000e140 0xc00000e198 0xc00000e1e8] [0xc00000e188 0xc00000e1c8] [0x932420 0x932420] 0xc0027246c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:42:21.991: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:22.316: INFO: rc: 1
Feb 28 08:42:22.316: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b6c7b0 exit status 1 <nil> <nil> true [0xc001478160 0xc0014781c8 0xc001478210] [0xc001478160 0xc0014781c8 0xc001478210] [0xc0014781b8 0xc0014781f8] [0x932420 0x932420] 0xc0016d9c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:42:32.317: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:32.638: INFO: rc: 1
Feb 28 08:42:32.638: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002761140 exit status 1 <nil> <nil> true [0xc0000d52b0 0xc0000d53b0 0xc0000d54a8] [0xc0000d52b0 0xc0000d53b0 0xc0000d54a8] [0xc0000d5330 0xc0000d5480] [0x932420 0x932420] 0xc000bf3980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:42:42.638: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:42.956: INFO: rc: 1
Feb 28 08:42:42.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0027613e0 exit status 1 <nil> <nil> true [0xc0000d54d0 0xc0000d55d8 0xc0000d5790] [0xc0000d54d0 0xc0000d55d8 0xc0000d5790] [0xc0000d5528 0xc0000d5740] [0x932420 0x932420] 0xc000bf3c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:42:52.957: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:53.342: INFO: rc: 1
Feb 28 08:42:53.342: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726d20 exit status 1 <nil> <nil> true [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac520 0xc0001acb48] [0x932420 0x932420] 0xc002822d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:43:03.343: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:43:03.683: INFO: rc: 1
Feb 28 08:43:03.683: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726fc0 exit status 1 <nil> <nil> true [0xc0001acb80 0xc0001acc68 0xc0001acd68] [0xc0001acb80 0xc0001acc68 0xc0001acd68] [0xc0001acc50 0xc0001acce0] [0x932420 0x932420] 0xc002823500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:43:13.684: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:43:14.033: INFO: rc: 1
Feb 28 08:43:14.033: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b6c2a0 exit status 1 <nil> <nil> true [0xc001478038 0xc0014780e8 0xc001478160] [0xc001478038 0xc0014780e8 0xc001478160] [0xc0014780d8 0xc001478118] [0x932420 0x932420] 0xc0016d98c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:43:24.034: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:43:24.451: INFO: rc: 1
Feb 28 08:43:24.451: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0027605a0 exit status 1 <nil> <nil> true [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac2d8 0xc0001ac440] [0x932420 0x932420] 0xc0028228a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:43:34.451: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:43:34.786: INFO: rc: 1
Feb 28 08:43:34.786: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0027609f0 exit status 1 <nil> <nil> true [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac520 0xc0001acb48] [0x932420 0x932420] 0xc002822ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:43:44.787: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:43:45.129: INFO: rc: 1
Feb 28 08:43:45.129: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b6c7e0 exit status 1 <nil> <nil> true [0xc001478198 0xc0014781e8 0xc001478248] [0xc001478198 0xc0014781e8 0xc001478248] [0xc0014781c8 0xc001478210] [0x932420 0x932420] 0xc0016d9c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:43:55.131: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:43:55.641: INFO: rc: 1
Feb 28 08:43:55.641: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726480 exit status 1 <nil> <nil> true [0xc0000d4040 0xc0000d4598 0xc0000d4638] [0xc0000d4040 0xc0000d4598 0xc0000d4638] [0xc0000d44b0 0xc0000d4630] [0x932420 0x932420] 0xc000bf2f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:44:05.642: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:44:05.967: INFO: rc: 1
Feb 28 08:44:05.967: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b6ca50 exit status 1 <nil> <nil> true [0xc001478258 0xc0014782b0 0xc0014782f8] [0xc001478258 0xc0014782b0 0xc0014782f8] [0xc001478298 0xc0014782e8] [0x932420 0x932420] 0xc002724120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:44:15.968: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:44:16.340: INFO: rc: 1
Feb 28 08:44:16.341: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726750 exit status 1 <nil> <nil> true [0xc0000d4670 0xc0000d48c8 0xc0000d4c30] [0xc0000d4670 0xc0000d48c8 0xc0000d4c30] [0xc0000d4840 0xc0000d4a88] [0x932420 0x932420] 0xc000bf3260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:44:26.341: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:44:26.677: INFO: rc: 1
Feb 28 08:44:26.677: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002760d20 exit status 1 <nil> <nil> true [0xc0001acb80 0xc0001acc68 0xc0001acd68] [0xc0001acb80 0xc0001acc68 0xc0001acd68] [0xc0001acc50 0xc0001acce0] [0x932420 0x932420] 0xc002823380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:44:36.677: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:44:37.065: INFO: rc: 1
Feb 28 08:44:37.066: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726a20 exit status 1 <nil> <nil> true [0xc0000d4e10 0xc0000d5028 0xc0000d50c0] [0xc0000d4e10 0xc0000d5028 0xc0000d50c0] [0xc0000d5010 0xc0000d5060] [0x932420 0x932420] 0xc000bf3560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:44:47.066: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:44:47.454: INFO: rc: 1
Feb 28 08:44:47.455: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002760ff0 exit status 1 <nil> <nil> true [0xc0001acda0 0xc0001aced8 0xc0001acf68] [0xc0001acda0 0xc0001aced8 0xc0001acf68] [0xc0001aceb0 0xc0001acf38] [0x932420 0x932420] 0xc002823680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:44:57.455: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:44:57.909: INFO: rc: 1
Feb 28 08:44:57.909: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726cf0 exit status 1 <nil> <nil> true [0xc0000d5138 0xc0000d51e8 0xc0000d52c8] [0xc0000d5138 0xc0000d51e8 0xc0000d52c8] [0xc0000d5188 0xc0000d52b0] [0x932420 0x932420] 0xc000bf38c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:45:07.910: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:45:08.312: INFO: rc: 1
Feb 28 08:45:08.313: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b6cd80 exit status 1 <nil> <nil> true [0xc001478338 0xc001478388 0xc0014783c8] [0xc001478338 0xc001478388 0xc0014783c8] [0xc001478370 0xc0014783b8] [0x932420 0x932420] 0xc002724420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:45:18.313: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:45:18.737: INFO: rc: 1
Feb 28 08:45:18.738: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002760570 exit status 1 <nil> <nil> true [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac000 0xc0001ac3b0 0xc0001ac478] [0xc0001ac2d8 0xc0001ac440] [0x932420 0x932420] 0xc0016d98c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:45:28.738: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:45:29.222: INFO: rc: 1
Feb 28 08:45:29.222: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726450 exit status 1 <nil> <nil> true [0xc001478028 0xc0014780d8 0xc001478118] [0xc001478028 0xc0014780d8 0xc001478118] [0xc0014780b8 0xc001478100] [0x932420 0x932420] 0xc002724540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:45:39.224: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:45:39.579: INFO: rc: 1
Feb 28 08:45:39.579: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726780 exit status 1 <nil> <nil> true [0xc001478160 0xc0014781c8 0xc001478210] [0xc001478160 0xc0014781c8 0xc001478210] [0xc0014781b8 0xc0014781f8] [0x932420 0x932420] 0xc002724840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:45:49.580: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:45:49.921: INFO: rc: 1
Feb 28 08:45:49.921: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002726a80 exit status 1 <nil> <nil> true [0xc001478248 0xc001478298 0xc0014782e8] [0xc001478248 0xc001478298 0xc0014782e8] [0xc001478288 0xc0014782c0] [0x932420 0x932420] 0xc002724b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:45:59.921: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:46:00.695: INFO: rc: 1
Feb 28 08:46:00.695: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0027609c0 exit status 1 <nil> <nil> true [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac480 0xc0001ac660 0xc0001acb68] [0xc0001ac520 0xc0001acb48] [0x932420 0x932420] 0xc0016d9c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 28 08:46:10.696: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-hqzr9 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:46:11.047: INFO: rc: 1
Feb 28 08:46:11.047: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 28 08:46:11.047: INFO: Scaling statefulset ss to 0
Feb 28 08:46:11.232: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:46:11.294: INFO: Deleting all statefulset in ns e2e-tests-statefulset-hqzr9
Feb 28 08:46:11.356: INFO: Scaling statefulset ss to 0
Feb 28 08:46:11.537: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:46:11.599: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:46:11.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-hqzr9" for this suite.
Feb 28 08:46:18.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:46:18.525: INFO: namespace: e2e-tests-statefulset-hqzr9, resource: bindings, ignored listing per whitelist
Feb 28 08:46:20.337: INFO: namespace e2e-tests-statefulset-hqzr9 deletion completed in 8.496151974s

• [SLOW TEST:374.431 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:46:20.338: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wsfzr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:46:22.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53393b26-3b35-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-wsfzr" to be "success or failure"
Feb 28 08:46:22.832: INFO: Pod "downwardapi-volume-53393b26-3b35-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 62.71181ms
Feb 28 08:46:24.894: INFO: Pod "downwardapi-volume-53393b26-3b35-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.12471775s
STEP: Saw pod success
Feb 28 08:46:24.894: INFO: Pod "downwardapi-volume-53393b26-3b35-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:46:24.956: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-53393b26-3b35-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:46:25.093: INFO: Waiting for pod downwardapi-volume-53393b26-3b35-11e9-9760-9e5b40196308 to disappear
Feb 28 08:46:25.153: INFO: Pod downwardapi-volume-53393b26-3b35-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:46:25.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wsfzr" for this suite.
Feb 28 08:46:31.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:46:33.550: INFO: namespace: e2e-tests-projected-wsfzr, resource: bindings, ignored listing per whitelist
Feb 28 08:46:33.735: INFO: namespace e2e-tests-projected-wsfzr deletion completed in 8.51876259s

• [SLOW TEST:13.397 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:46:33.735: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-b8c9h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:46:36.319: INFO: Waiting up to 5m0s for pod "downward-api-5b4c9f79-3b35-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-b8c9h" to be "success or failure"
Feb 28 08:46:36.382: INFO: Pod "downward-api-5b4c9f79-3b35-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 63.219114ms
Feb 28 08:46:38.443: INFO: Pod "downward-api-5b4c9f79-3b35-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.124584618s
STEP: Saw pod success
Feb 28 08:46:38.443: INFO: Pod "downward-api-5b4c9f79-3b35-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:46:38.505: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downward-api-5b4c9f79-3b35-11e9-9760-9e5b40196308 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:46:38.632: INFO: Waiting for pod downward-api-5b4c9f79-3b35-11e9-9760-9e5b40196308 to disappear
Feb 28 08:46:38.690: INFO: Pod downward-api-5b4c9f79-3b35-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:46:38.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-b8c9h" for this suite.
Feb 28 08:46:44.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:46:45.541: INFO: namespace: e2e-tests-downward-api-b8c9h, resource: bindings, ignored listing per whitelist
Feb 28 08:46:47.209: INFO: namespace e2e-tests-downward-api-b8c9h deletion completed in 8.458098536s

• [SLOW TEST:13.474 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:46:47.209: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6qxnt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-63505b44-3b35-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:46:49.827: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63598d5f-3b35-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-6qxnt" to be "success or failure"
Feb 28 08:46:49.886: INFO: Pod "pod-projected-configmaps-63598d5f-3b35-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.600393ms
Feb 28 08:46:51.948: INFO: Pod "pod-projected-configmaps-63598d5f-3b35-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120520083s
STEP: Saw pod success
Feb 28 08:46:51.948: INFO: Pod "pod-projected-configmaps-63598d5f-3b35-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:46:52.008: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-configmaps-63598d5f-3b35-11e9-9760-9e5b40196308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:46:52.144: INFO: Waiting for pod pod-projected-configmaps-63598d5f-3b35-11e9-9760-9e5b40196308 to disappear
Feb 28 08:46:52.204: INFO: Pod pod-projected-configmaps-63598d5f-3b35-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:46:52.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6qxnt" for this suite.
Feb 28 08:46:58.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:00.476: INFO: namespace: e2e-tests-projected-6qxnt, resource: bindings, ignored listing per whitelist
Feb 28 08:47:00.717: INFO: namespace e2e-tests-projected-6qxnt deletion completed in 8.453183345s

• [SLOW TEST:13.508 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:47:00.717: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-l9hhb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:47:03.112: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-l9hhb'
Feb 28 08:47:03.603: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 08:47:03.603: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
Feb 28 08:47:05.736: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-l9hhb'
Feb 28 08:47:06.187: INFO: stderr: ""
Feb 28 08:47:06.187: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:47:06.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l9hhb" for this suite.
Feb 28 08:47:12.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:13.786: INFO: namespace: e2e-tests-kubectl-l9hhb, resource: bindings, ignored listing per whitelist
Feb 28 08:47:14.738: INFO: namespace e2e-tests-kubectl-l9hhb deletion completed in 8.491361164s

• [SLOW TEST:14.021 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:47:14.739: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-84tbf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 08:47:17.109: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:47:20.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-84tbf" for this suite.
Feb 28 08:47:43.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:44.199: INFO: namespace: e2e-tests-init-container-84tbf, resource: bindings, ignored listing per whitelist
Feb 28 08:47:45.520: INFO: namespace e2e-tests-init-container-84tbf deletion completed in 24.509349416s

• [SLOW TEST:30.781 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:47:45.520: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-vbfcc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 28 08:47:47.969: INFO: Waiting up to 5m0s for pod "var-expansion-8601749a-3b35-11e9-9760-9e5b40196308" in namespace "e2e-tests-var-expansion-vbfcc" to be "success or failure"
Feb 28 08:47:48.029: INFO: Pod "var-expansion-8601749a-3b35-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 60.054268ms
Feb 28 08:47:50.089: INFO: Pod "var-expansion-8601749a-3b35-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120658469s
STEP: Saw pod success
Feb 28 08:47:50.090: INFO: Pod "var-expansion-8601749a-3b35-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:47:50.150: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod var-expansion-8601749a-3b35-11e9-9760-9e5b40196308 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:47:50.280: INFO: Waiting for pod var-expansion-8601749a-3b35-11e9-9760-9e5b40196308 to disappear
Feb 28 08:47:50.338: INFO: Pod var-expansion-8601749a-3b35-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:47:50.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-vbfcc" for this suite.
Feb 28 08:47:56.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:57.907: INFO: namespace: e2e-tests-var-expansion-vbfcc, resource: bindings, ignored listing per whitelist
Feb 28 08:47:58.880: INFO: namespace e2e-tests-var-expansion-vbfcc deletion completed in 8.481276348s

• [SLOW TEST:13.360 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:47:58.880: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-sln28
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 08:48:04.232: INFO: Successfully updated pod "labelsupdate8dfdeae3-3b35-11e9-9760-9e5b40196308"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:48:06.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sln28" for this suite.
Feb 28 08:48:28.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:29.885: INFO: namespace: e2e-tests-downward-api-sln28, resource: bindings, ignored listing per whitelist
Feb 28 08:48:30.883: INFO: namespace e2e-tests-downward-api-sln28 deletion completed in 24.459146152s

• [SLOW TEST:32.004 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:48:30.884: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-hnjpb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:48:33.217: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-hnjpb'
Feb 28 08:48:33.918: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 08:48:33.918: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
Feb 28 08:48:33.977: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-hnjpb'
Feb 28 08:48:34.379: INFO: stderr: ""
Feb 28 08:48:34.379: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:48:34.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hnjpb" for this suite.
Feb 28 08:48:56.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:56.920: INFO: namespace: e2e-tests-kubectl-hnjpb, resource: bindings, ignored listing per whitelist
Feb 28 08:48:58.884: INFO: namespace e2e-tests-kubectl-hnjpb deletion completed in 24.443868342s

• [SLOW TEST:28.001 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:48:58.885: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-jf2dw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:49:03.764: INFO: Waiting up to 5m0s for pod "client-envvars-b32ede60-3b35-11e9-9760-9e5b40196308" in namespace "e2e-tests-pods-jf2dw" to be "success or failure"
Feb 28 08:49:03.825: INFO: Pod "client-envvars-b32ede60-3b35-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 61.499736ms
Feb 28 08:49:05.886: INFO: Pod "client-envvars-b32ede60-3b35-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121933255s
STEP: Saw pod success
Feb 28 08:49:05.886: INFO: Pod "client-envvars-b32ede60-3b35-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:49:05.947: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod client-envvars-b32ede60-3b35-11e9-9760-9e5b40196308 container env3cont: <nil>
STEP: delete the pod
Feb 28 08:49:06.073: INFO: Waiting for pod client-envvars-b32ede60-3b35-11e9-9760-9e5b40196308 to disappear
Feb 28 08:49:06.131: INFO: Pod client-envvars-b32ede60-3b35-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:49:06.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-jf2dw" for this suite.
Feb 28 08:49:58.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:49:59.387: INFO: namespace: e2e-tests-pods-jf2dw, resource: bindings, ignored listing per whitelist
Feb 28 08:50:00.675: INFO: namespace e2e-tests-pods-jf2dw deletion completed in 54.483708808s

• [SLOW TEST:61.790 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:50:00.675: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-jtlh7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 08:50:03.914: INFO: Number of nodes with available pods: 0
Feb 28 08:50:03.915: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:05.040: INFO: Number of nodes with available pods: 0
Feb 28 08:50:05.041: INFO: Node ip-10-250-8-168.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:06.041: INFO: Number of nodes with available pods: 2
Feb 28 08:50:06.041: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 28 08:50:06.353: INFO: Number of nodes with available pods: 1
Feb 28 08:50:06.353: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:07.477: INFO: Number of nodes with available pods: 1
Feb 28 08:50:07.477: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:08.477: INFO: Number of nodes with available pods: 1
Feb 28 08:50:08.477: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:09.478: INFO: Number of nodes with available pods: 1
Feb 28 08:50:09.478: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:10.473: INFO: Number of nodes with available pods: 1
Feb 28 08:50:10.473: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:11.477: INFO: Number of nodes with available pods: 1
Feb 28 08:50:11.477: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:12.477: INFO: Number of nodes with available pods: 1
Feb 28 08:50:12.477: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:13.472: INFO: Number of nodes with available pods: 1
Feb 28 08:50:13.473: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:14.472: INFO: Number of nodes with available pods: 1
Feb 28 08:50:14.472: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:15.477: INFO: Number of nodes with available pods: 1
Feb 28 08:50:15.477: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:16.477: INFO: Number of nodes with available pods: 1
Feb 28 08:50:16.477: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:17.476: INFO: Number of nodes with available pods: 1
Feb 28 08:50:17.476: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:18.478: INFO: Number of nodes with available pods: 1
Feb 28 08:50:18.478: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:19.476: INFO: Number of nodes with available pods: 1
Feb 28 08:50:19.476: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:20.472: INFO: Number of nodes with available pods: 1
Feb 28 08:50:20.472: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:21.476: INFO: Number of nodes with available pods: 1
Feb 28 08:50:21.476: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:22.475: INFO: Number of nodes with available pods: 1
Feb 28 08:50:22.475: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:23.476: INFO: Number of nodes with available pods: 1
Feb 28 08:50:23.476: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:24.474: INFO: Number of nodes with available pods: 1
Feb 28 08:50:24.474: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:25.478: INFO: Number of nodes with available pods: 1
Feb 28 08:50:25.478: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:26.479: INFO: Number of nodes with available pods: 1
Feb 28 08:50:26.479: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:27.475: INFO: Number of nodes with available pods: 1
Feb 28 08:50:27.475: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:28.475: INFO: Number of nodes with available pods: 1
Feb 28 08:50:28.475: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:29.481: INFO: Number of nodes with available pods: 1
Feb 28 08:50:29.481: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:30.487: INFO: Number of nodes with available pods: 1
Feb 28 08:50:30.488: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:31.474: INFO: Number of nodes with available pods: 1
Feb 28 08:50:31.474: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:32.475: INFO: Number of nodes with available pods: 1
Feb 28 08:50:32.475: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:33.474: INFO: Number of nodes with available pods: 1
Feb 28 08:50:33.474: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:34.474: INFO: Number of nodes with available pods: 1
Feb 28 08:50:34.474: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:35.469: INFO: Number of nodes with available pods: 1
Feb 28 08:50:35.469: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:36.477: INFO: Number of nodes with available pods: 1
Feb 28 08:50:36.477: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:37.474: INFO: Number of nodes with available pods: 1
Feb 28 08:50:37.474: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:38.477: INFO: Number of nodes with available pods: 1
Feb 28 08:50:38.477: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:39.475: INFO: Number of nodes with available pods: 1
Feb 28 08:50:39.475: INFO: Node ip-10-250-9-188.eu-west-1.compute.internal is running more than one daemon pod
Feb 28 08:50:40.472: INFO: Number of nodes with available pods: 2
Feb 28 08:50:40.472: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-jtlh7, will wait for the garbage collector to delete the pods
Feb 28 08:50:40.754: INFO: Deleting {extensions DaemonSet} daemon-set took: 62.25592ms
Feb 28 08:50:40.855: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.364853ms
Feb 28 08:51:23.014: INFO: Number of nodes with available pods: 0
Feb 28 08:51:23.014: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:51:23.071: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-jtlh7/daemonsets","resourceVersion":"17877"},"items":null}

Feb 28 08:51:23.130: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-jtlh7/pods","resourceVersion":"17877"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:51:23.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-jtlh7" for this suite.
Feb 28 08:51:29.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:51:31.842: INFO: namespace: e2e-tests-daemonsets-jtlh7, resource: bindings, ignored listing per whitelist
Feb 28 08:51:31.964: INFO: namespace e2e-tests-daemonsets-jtlh7 deletion completed in 8.592234442s

• [SLOW TEST:91.289 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:51:31.964: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-z7km2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:51:34.365: INFO: Waiting up to 5m0s for pod "downward-api-0cf2f41d-3b36-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-z7km2" to be "success or failure"
Feb 28 08:51:34.426: INFO: Pod "downward-api-0cf2f41d-3b36-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 61.9025ms
Feb 28 08:51:36.488: INFO: Pod "downward-api-0cf2f41d-3b36-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.123708093s
STEP: Saw pod success
Feb 28 08:51:36.488: INFO: Pod "downward-api-0cf2f41d-3b36-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:51:36.550: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downward-api-0cf2f41d-3b36-11e9-9760-9e5b40196308 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:51:36.683: INFO: Waiting for pod downward-api-0cf2f41d-3b36-11e9-9760-9e5b40196308 to disappear
Feb 28 08:51:36.743: INFO: Pod downward-api-0cf2f41d-3b36-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:51:36.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-z7km2" for this suite.
Feb 28 08:51:42.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:51:45.150: INFO: namespace: e2e-tests-downward-api-z7km2, resource: bindings, ignored listing per whitelist
Feb 28 08:51:45.393: INFO: namespace e2e-tests-downward-api-z7km2 deletion completed in 8.590301464s

• [SLOW TEST:13.428 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:51:45.393: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ls6t2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:51:48.069: INFO: Waiting up to 5m0s for pod "downward-api-151e464f-3b36-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-ls6t2" to be "success or failure"
Feb 28 08:51:48.128: INFO: Pod "downward-api-151e464f-3b36-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.106135ms
Feb 28 08:51:50.188: INFO: Pod "downward-api-151e464f-3b36-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.119503072s
STEP: Saw pod success
Feb 28 08:51:50.188: INFO: Pod "downward-api-151e464f-3b36-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:51:50.249: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downward-api-151e464f-3b36-11e9-9760-9e5b40196308 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:51:50.378: INFO: Waiting for pod downward-api-151e464f-3b36-11e9-9760-9e5b40196308 to disappear
Feb 28 08:51:50.437: INFO: Pod downward-api-151e464f-3b36-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:51:50.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ls6t2" for this suite.
Feb 28 08:51:56.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:51:57.315: INFO: namespace: e2e-tests-downward-api-ls6t2, resource: bindings, ignored listing per whitelist
Feb 28 08:51:59.165: INFO: namespace e2e-tests-downward-api-ls6t2 deletion completed in 8.614070943s

• [SLOW TEST:13.772 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:51:59.165: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-xddcm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:52:01.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xddcm" for this suite.
Feb 28 08:52:24.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:25.206: INFO: namespace: e2e-tests-pods-xddcm, resource: bindings, ignored listing per whitelist
Feb 28 08:52:26.485: INFO: namespace e2e-tests-pods-xddcm deletion completed in 24.570125625s

• [SLOW TEST:27.320 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:52:26.486: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-lqdqc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 28 08:52:31.184: INFO: Pod pod-hostip-2d7cb171-3b36-11e9-9760-9e5b40196308 has hostIP: 10.250.9.188
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:52:31.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lqdqc" for this suite.
Feb 28 08:52:53.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:54.389: INFO: namespace: e2e-tests-pods-lqdqc, resource: bindings, ignored listing per whitelist
Feb 28 08:52:55.774: INFO: namespace e2e-tests-pods-lqdqc deletion completed in 24.532680878s

• [SLOW TEST:29.288 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:52:55.774: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-n9qns
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 08:53:02.742: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:53:02.800: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:53:04.800: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:53:04.862: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:53:06.801: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:53:06.862: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:53:08.800: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:53:08.863: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:53:10.800: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:53:10.869: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:53:12.800: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:53:12.863: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:53:14.807: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:53:14.866: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:53:14.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-n9qns" for this suite.
Feb 28 08:53:37.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:53:39.237: INFO: namespace: e2e-tests-container-lifecycle-hook-n9qns, resource: bindings, ignored listing per whitelist
Feb 28 08:53:39.419: INFO: namespace e2e-tests-container-lifecycle-hook-n9qns deletion completed in 24.493362423s

• [SLOW TEST:43.645 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:53:39.419: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mp4cv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:53:41.901: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-mp4cv'
Feb 28 08:53:44.692: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 08:53:44.692: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
Feb 28 08:53:44.752: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-mp4cv'
Feb 28 08:53:45.161: INFO: stderr: ""
Feb 28 08:53:45.161: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:53:45.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mp4cv" for this suite.
Feb 28 08:53:51.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:53:51.992: INFO: namespace: e2e-tests-kubectl-mp4cv, resource: bindings, ignored listing per whitelist
Feb 28 08:53:53.795: INFO: namespace e2e-tests-kubectl-mp4cv deletion completed in 8.51742058s

• [SLOW TEST:14.376 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:53:53.795: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-2x7wx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-2x7wx
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 28 08:53:56.451: INFO: Found 1 stateful pods, waiting for 3
Feb 28 08:54:06.509: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:54:06.509: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:54:06.509: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 08:54:06.812: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 28 08:54:07.060: INFO: Updating stateful set ss2
Feb 28 08:54:07.181: INFO: Waiting for Pod e2e-tests-statefulset-2x7wx/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 28 08:54:17.478: INFO: Found 2 stateful pods, waiting for 3
Feb 28 08:54:27.545: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:54:27.545: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:54:27.545: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 28 08:54:27.800: INFO: Updating stateful set ss2
Feb 28 08:54:27.931: INFO: Waiting for Pod e2e-tests-statefulset-2x7wx/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 28 08:54:38.193: INFO: Updating stateful set ss2
Feb 28 08:54:38.318: INFO: Waiting for StatefulSet e2e-tests-statefulset-2x7wx/ss2 to complete update
Feb 28 08:54:38.318: INFO: Waiting for Pod e2e-tests-statefulset-2x7wx/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:54:48.441: INFO: Deleting all statefulset in ns e2e-tests-statefulset-2x7wx
Feb 28 08:54:48.501: INFO: Scaling statefulset ss2 to 0
Feb 28 08:55:08.746: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:55:08.805: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:55:08.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-2x7wx" for this suite.
Feb 28 08:55:15.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:55:16.759: INFO: namespace: e2e-tests-statefulset-2x7wx, resource: bindings, ignored listing per whitelist
Feb 28 08:55:17.599: INFO: namespace e2e-tests-statefulset-2x7wx deletion completed in 8.48339242s

• [SLOW TEST:83.804 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:55:17.605: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-2vh4z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-2vh4z
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:55:20.101: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:55:41.188: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.215:8080/dial?request=hostName&protocol=udp&host=100.96.0.65&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-2vh4z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:55:41.188: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 08:55:42.074: INFO: Waiting for endpoints: map[]
Feb 28 08:55:42.132: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.215:8080/dial?request=hostName&protocol=udp&host=100.96.2.214&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-2vh4z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:55:42.132: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 08:55:42.808: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:55:42.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-2vh4z" for this suite.
Feb 28 08:56:05.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:56:07.185: INFO: namespace: e2e-tests-pod-network-test-2vh4z, resource: bindings, ignored listing per whitelist
Feb 28 08:56:07.413: INFO: namespace e2e-tests-pod-network-test-2vh4z deletion completed in 24.546691241s

• [SLOW TEST:49.808 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:56:07.413: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-lcqm4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-b128eb2e-3b36-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 08:56:09.921: INFO: Waiting up to 5m0s for pod "pod-secrets-b131d7d4-3b36-11e9-9760-9e5b40196308" in namespace "e2e-tests-secrets-lcqm4" to be "success or failure"
Feb 28 08:56:09.979: INFO: Pod "pod-secrets-b131d7d4-3b36-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 57.715363ms
Feb 28 08:56:12.041: INFO: Pod "pod-secrets-b131d7d4-3b36-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.119741242s
STEP: Saw pod success
Feb 28 08:56:12.041: INFO: Pod "pod-secrets-b131d7d4-3b36-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:56:12.101: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-secrets-b131d7d4-3b36-11e9-9760-9e5b40196308 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:56:12.234: INFO: Waiting for pod pod-secrets-b131d7d4-3b36-11e9-9760-9e5b40196308 to disappear
Feb 28 08:56:12.292: INFO: Pod pod-secrets-b131d7d4-3b36-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:56:12.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lcqm4" for this suite.
Feb 28 08:56:18.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:56:20.311: INFO: namespace: e2e-tests-secrets-lcqm4, resource: bindings, ignored listing per whitelist
Feb 28 08:56:20.840: INFO: namespace e2e-tests-secrets-lcqm4 deletion completed in 8.488958635s

• [SLOW TEST:13.428 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:56:20.841: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-t4b92
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 28 08:56:23.431: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-a,UID:b942cd0a-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18793,Generation:0,CreationTimestamp:2019-02-28 08:56:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:56:23.436: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-a,UID:b942cd0a-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18793,Generation:0,CreationTimestamp:2019-02-28 08:56:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 28 08:56:33.552: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-a,UID:b942cd0a-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18812,Generation:0,CreationTimestamp:2019-02-28 08:56:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 08:56:33.553: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-a,UID:b942cd0a-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18812,Generation:0,CreationTimestamp:2019-02-28 08:56:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 28 08:56:43.670: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-a,UID:b942cd0a-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18833,Generation:0,CreationTimestamp:2019-02-28 08:56:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:56:43.671: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-a,UID:b942cd0a-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18833,Generation:0,CreationTimestamp:2019-02-28 08:56:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 28 08:56:53.733: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-a,UID:b942cd0a-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18852,Generation:0,CreationTimestamp:2019-02-28 08:56:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:56:53.733: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-a,UID:b942cd0a-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18852,Generation:0,CreationTimestamp:2019-02-28 08:56:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 28 08:57:03.794: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-b,UID:d1519eb4-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18871,Generation:0,CreationTimestamp:2019-02-28 08:57:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:57:03.794: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-b,UID:d1519eb4-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18871,Generation:0,CreationTimestamp:2019-02-28 08:57:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 28 08:57:13.874: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-b,UID:d1519eb4-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18891,Generation:0,CreationTimestamp:2019-02-28 08:57:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:57:13.874: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-t4b92,SelfLink:/api/v1/namespaces/e2e-tests-watch-t4b92/configmaps/e2e-watch-test-configmap-b,UID:d1519eb4-3b36-11e9-8db9-7e40ccd56e85,ResourceVersion:18891,Generation:0,CreationTimestamp:2019-02-28 08:57:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:57:23.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-t4b92" for this suite.
Feb 28 08:57:30.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:57:31.222: INFO: namespace: e2e-tests-watch-t4b92, resource: bindings, ignored listing per whitelist
Feb 28 08:57:32.493: INFO: namespace e2e-tests-watch-t4b92 deletion completed in 8.555287756s

• [SLOW TEST:71.653 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:57:32.493: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-hv4m7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-hv4m7/configmap-test-e3ff77e2-3b36-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:57:35.223: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4094905-3b36-11e9-9760-9e5b40196308" in namespace "e2e-tests-configmap-hv4m7" to be "success or failure"
Feb 28 08:57:35.286: INFO: Pod "pod-configmaps-e4094905-3b36-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 62.223541ms
Feb 28 08:57:37.371: INFO: Pod "pod-configmaps-e4094905-3b36-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.147335467s
STEP: Saw pod success
Feb 28 08:57:37.371: INFO: Pod "pod-configmaps-e4094905-3b36-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:57:37.432: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-configmaps-e4094905-3b36-11e9-9760-9e5b40196308 container env-test: <nil>
STEP: delete the pod
Feb 28 08:57:37.571: INFO: Waiting for pod pod-configmaps-e4094905-3b36-11e9-9760-9e5b40196308 to disappear
Feb 28 08:57:37.634: INFO: Pod pod-configmaps-e4094905-3b36-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:57:37.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hv4m7" for this suite.
Feb 28 08:57:43.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:57:44.468: INFO: namespace: e2e-tests-configmap-hv4m7, resource: bindings, ignored listing per whitelist
Feb 28 08:57:46.272: INFO: namespace e2e-tests-configmap-hv4m7 deletion completed in 8.514769289s

• [SLOW TEST:13.779 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:57:46.272: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xc5h7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-ec1c1ea3-3b36-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 08:57:48.826: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ec254416-3b36-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-xc5h7" to be "success or failure"
Feb 28 08:57:48.882: INFO: Pod "pod-projected-secrets-ec254416-3b36-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 56.414978ms
Feb 28 08:57:50.942: INFO: Pod "pod-projected-secrets-ec254416-3b36-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11589949s
STEP: Saw pod success
Feb 28 08:57:50.942: INFO: Pod "pod-projected-secrets-ec254416-3b36-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:57:51.002: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-secrets-ec254416-3b36-11e9-9760-9e5b40196308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:57:51.129: INFO: Waiting for pod pod-projected-secrets-ec254416-3b36-11e9-9760-9e5b40196308 to disappear
Feb 28 08:57:51.190: INFO: Pod pod-projected-secrets-ec254416-3b36-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:57:51.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xc5h7" for this suite.
Feb 28 08:57:57.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:57:59.355: INFO: namespace: e2e-tests-projected-xc5h7, resource: bindings, ignored listing per whitelist
Feb 28 08:57:59.806: INFO: namespace e2e-tests-projected-xc5h7 deletion completed in 8.553289899s

• [SLOW TEST:13.534 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:57:59.806: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-8m8zb
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 28 08:58:02.271: INFO: Waiting up to 5m0s for pod "pod-f4289013-3b36-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-8m8zb" to be "success or failure"
Feb 28 08:58:02.333: INFO: Pod "pod-f4289013-3b36-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 61.465595ms
Feb 28 08:58:04.393: INFO: Pod "pod-f4289013-3b36-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121753795s
STEP: Saw pod success
Feb 28 08:58:04.394: INFO: Pod "pod-f4289013-3b36-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:58:04.455: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-f4289013-3b36-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 08:58:04.582: INFO: Waiting for pod pod-f4289013-3b36-11e9-9760-9e5b40196308 to disappear
Feb 28 08:58:04.642: INFO: Pod pod-f4289013-3b36-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:58:04.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8m8zb" for this suite.
Feb 28 08:58:10.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:58:12.509: INFO: namespace: e2e-tests-emptydir-8m8zb, resource: bindings, ignored listing per whitelist
Feb 28 08:58:13.327: INFO: namespace e2e-tests-emptydir-8m8zb deletion completed in 8.624042684s

• [SLOW TEST:13.521 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:58:13.327: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-dg9ck
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 28 08:58:15.793: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 08:58:16.273: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 08:58:16.332: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-8-168.eu-west-1.compute.internal before test
Feb 28 08:58:16.696: INFO: metrics-server-54fc54bd68-wflk4 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.696: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 08:58:16.696: INFO: coredns-5f4748c5f-k5h6g from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.696: INFO: 	Container coredns ready: true, restart count 0
Feb 28 08:58:16.696: INFO: blackbox-exporter-58fd9b8556-xjpfq from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.696: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 08:58:16.696: INFO: vpn-shoot-7bcff87c69-gp7q2 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.696: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 08:58:16.696: INFO: addons-nginx-ingress-controller-6b47c6c4cb-t86bf from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.696: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 08:58:16.696: INFO: addons-kubernetes-dashboard-5f64f76bd-ps8rt from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.696: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 08:58:16.696: INFO: calico-node-hjb94 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.696: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:58:16.696: INFO: kube-proxy-dkbnd from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.696: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:58:16.696: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-psl62 from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.697: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 08:58:16.697: INFO: node-exporter-pq7cp from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.697: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:58:16.697: INFO: addons-kube-lego-648f8c9f5c-jhthm from kube-system started at 2019-02-28 07:12:00 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.697: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 08:58:16.697: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-9-188.eu-west-1.compute.internal before test
Feb 28 08:58:16.822: INFO: calico-node-kjkhj from kube-system started at 2019-02-28 07:12:03 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.822: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:58:16.822: INFO: node-exporter-8nnqh from kube-system started at 2019-02-28 07:12:03 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.822: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:58:16.822: INFO: kube-proxy-4mpns from kube-system started at 2019-02-28 07:12:03 +0000 UTC (1 container statuses recorded)
Feb 28 08:58:16.822: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15877b6b3db60c91], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:58:18.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-dg9ck" for this suite.
Feb 28 08:58:24.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:58:26.632: INFO: namespace: e2e-tests-sched-pred-dg9ck, resource: bindings, ignored listing per whitelist
Feb 28 08:58:27.052: INFO: namespace e2e-tests-sched-pred-dg9ck deletion completed in 8.713798651s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:13.725 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:58:27.053: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kz4mh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-046d2b98-3b37-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:58:29.622: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-04769023-3b37-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-kz4mh" to be "success or failure"
Feb 28 08:58:29.681: INFO: Pod "pod-projected-configmaps-04769023-3b37-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.780976ms
Feb 28 08:58:31.742: INFO: Pod "pod-projected-configmaps-04769023-3b37-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.119327716s
STEP: Saw pod success
Feb 28 08:58:31.742: INFO: Pod "pod-projected-configmaps-04769023-3b37-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:58:31.803: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-configmaps-04769023-3b37-11e9-9760-9e5b40196308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:58:31.940: INFO: Waiting for pod pod-projected-configmaps-04769023-3b37-11e9-9760-9e5b40196308 to disappear
Feb 28 08:58:32.004: INFO: Pod pod-projected-configmaps-04769023-3b37-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:58:32.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kz4mh" for this suite.
Feb 28 08:58:38.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:58:39.049: INFO: namespace: e2e-tests-projected-kz4mh, resource: bindings, ignored listing per whitelist
Feb 28 08:58:40.564: INFO: namespace e2e-tests-projected-kz4mh deletion completed in 8.500380544s

• [SLOW TEST:13.512 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:58:40.564: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-p2p29
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 28 08:58:43.298: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-p2p29,SelfLink:/api/v1/namespaces/e2e-tests-watch-p2p29/configmaps/e2e-watch-test-label-changed,UID:0c7c2ead-3b37-11e9-8db9-7e40ccd56e85,ResourceVersion:19161,Generation:0,CreationTimestamp:2019-02-28 08:58:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:58:43.298: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-p2p29,SelfLink:/api/v1/namespaces/e2e-tests-watch-p2p29/configmaps/e2e-watch-test-label-changed,UID:0c7c2ead-3b37-11e9-8db9-7e40ccd56e85,ResourceVersion:19162,Generation:0,CreationTimestamp:2019-02-28 08:58:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 08:58:43.298: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-p2p29,SelfLink:/api/v1/namespaces/e2e-tests-watch-p2p29/configmaps/e2e-watch-test-label-changed,UID:0c7c2ead-3b37-11e9-8db9-7e40ccd56e85,ResourceVersion:19163,Generation:0,CreationTimestamp:2019-02-28 08:58:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 28 08:58:53.735: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-p2p29,SelfLink:/api/v1/namespaces/e2e-tests-watch-p2p29/configmaps/e2e-watch-test-label-changed,UID:0c7c2ead-3b37-11e9-8db9-7e40ccd56e85,ResourceVersion:19185,Generation:0,CreationTimestamp:2019-02-28 08:58:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:58:53.736: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-p2p29,SelfLink:/api/v1/namespaces/e2e-tests-watch-p2p29/configmaps/e2e-watch-test-label-changed,UID:0c7c2ead-3b37-11e9-8db9-7e40ccd56e85,ResourceVersion:19187,Generation:0,CreationTimestamp:2019-02-28 08:58:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 28 08:58:53.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-p2p29,SelfLink:/api/v1/namespaces/e2e-tests-watch-p2p29/configmaps/e2e-watch-test-label-changed,UID:0c7c2ead-3b37-11e9-8db9-7e40ccd56e85,ResourceVersion:19188,Generation:0,CreationTimestamp:2019-02-28 08:58:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:58:53.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-p2p29" for this suite.
Feb 28 08:58:59.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:59:01.803: INFO: namespace: e2e-tests-watch-p2p29, resource: bindings, ignored listing per whitelist
Feb 28 08:59:02.318: INFO: namespace e2e-tests-watch-p2p29 deletion completed in 8.523616858s

• [SLOW TEST:21.754 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:59:02.318: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-5vxdt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:59:04.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1969c5db-3b37-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-5vxdt" to be "success or failure"
Feb 28 08:59:04.830: INFO: Pod "downwardapi-volume-1969c5db-3b37-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.146577ms
Feb 28 08:59:06.893: INFO: Pod "downwardapi-volume-1969c5db-3b37-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.12080299s
STEP: Saw pod success
Feb 28 08:59:06.893: INFO: Pod "downwardapi-volume-1969c5db-3b37-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:59:06.955: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-1969c5db-3b37-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 08:59:07.088: INFO: Waiting for pod downwardapi-volume-1969c5db-3b37-11e9-9760-9e5b40196308 to disappear
Feb 28 08:59:07.149: INFO: Pod downwardapi-volume-1969c5db-3b37-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:59:07.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5vxdt" for this suite.
Feb 28 08:59:13.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:59:14.470: INFO: namespace: e2e-tests-downward-api-5vxdt, resource: bindings, ignored listing per whitelist
Feb 28 08:59:15.741: INFO: namespace e2e-tests-downward-api-5vxdt deletion completed in 8.529743189s

• [SLOW TEST:13.423 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:59:15.742: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-86n8h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-21761552-3b37-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume configMaps
Feb 28 08:59:18.337: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-217f2f0f-3b37-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-86n8h" to be "success or failure"
Feb 28 08:59:18.399: INFO: Pod "pod-projected-configmaps-217f2f0f-3b37-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 61.835039ms
Feb 28 08:59:20.459: INFO: Pod "pod-projected-configmaps-217f2f0f-3b37-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.121548309s
STEP: Saw pod success
Feb 28 08:59:20.459: INFO: Pod "pod-projected-configmaps-217f2f0f-3b37-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 08:59:20.519: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-configmaps-217f2f0f-3b37-11e9-9760-9e5b40196308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:59:20.649: INFO: Waiting for pod pod-projected-configmaps-217f2f0f-3b37-11e9-9760-9e5b40196308 to disappear
Feb 28 08:59:20.710: INFO: Pod pod-projected-configmaps-217f2f0f-3b37-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:59:20.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-86n8h" for this suite.
Feb 28 08:59:26.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:59:28.027: INFO: namespace: e2e-tests-projected-86n8h, resource: bindings, ignored listing per whitelist
Feb 28 08:59:29.357: INFO: namespace e2e-tests-projected-86n8h deletion completed in 8.585239309s

• [SLOW TEST:13.616 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:59:29.358: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-x29tt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-297f75b0-3b37-11e9-9760-9e5b40196308
Feb 28 08:59:31.817: INFO: Pod name my-hostname-basic-297f75b0-3b37-11e9-9760-9e5b40196308: Found 1 pods out of 1
Feb 28 08:59:31.817: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-297f75b0-3b37-11e9-9760-9e5b40196308" are running
Feb 28 08:59:33.938: INFO: Pod "my-hostname-basic-297f75b0-3b37-11e9-9760-9e5b40196308-w7l5w" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:59:31 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:59:31 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-297f75b0-3b37-11e9-9760-9e5b40196308]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:59:31 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-297f75b0-3b37-11e9-9760-9e5b40196308]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:59:31 +0000 UTC Reason: Message:}])
Feb 28 08:59:33.938: INFO: Trying to dial the pod
Feb 28 08:59:39.198: INFO: Controller my-hostname-basic-297f75b0-3b37-11e9-9760-9e5b40196308: Got expected result from replica 1 [my-hostname-basic-297f75b0-3b37-11e9-9760-9e5b40196308-w7l5w]: "my-hostname-basic-297f75b0-3b37-11e9-9760-9e5b40196308-w7l5w", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:59:39.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-x29tt" for this suite.
Feb 28 08:59:45.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:59:46.855: INFO: namespace: e2e-tests-replication-controller-x29tt, resource: bindings, ignored listing per whitelist
Feb 28 08:59:47.732: INFO: namespace e2e-tests-replication-controller-x29tt deletion completed in 8.472720784s

• [SLOW TEST:18.374 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:59:47.732: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-cjr4m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 09:00:14.443: INFO: Container started at 2019-02-28 08:59:51 +0000 UTC, pod became ready at 2019-02-28 09:00:13 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:00:14.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cjr4m" for this suite.
Feb 28 09:00:34.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:00:36.666: INFO: namespace: e2e-tests-container-probe-cjr4m, resource: bindings, ignored listing per whitelist
Feb 28 09:00:37.098: INFO: namespace e2e-tests-container-probe-cjr4m deletion completed in 22.591947069s

• [SLOW TEST:49.366 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:00:37.099: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-mpwcg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 09:00:39.554: INFO: Waiting up to 5m0s for pod "pod-51e89419-3b37-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-mpwcg" to be "success or failure"
Feb 28 09:00:39.611: INFO: Pod "pod-51e89419-3b37-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 56.464949ms
Feb 28 09:00:41.671: INFO: Pod "pod-51e89419-3b37-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.116920471s
STEP: Saw pod success
Feb 28 09:00:41.671: INFO: Pod "pod-51e89419-3b37-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 09:00:41.736: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-51e89419-3b37-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 09:00:41.869: INFO: Waiting for pod pod-51e89419-3b37-11e9-9760-9e5b40196308 to disappear
Feb 28 09:00:41.928: INFO: Pod pod-51e89419-3b37-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:00:41.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mpwcg" for this suite.
Feb 28 09:00:48.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:00:49.176: INFO: namespace: e2e-tests-emptydir-mpwcg, resource: bindings, ignored listing per whitelist
Feb 28 09:00:50.775: INFO: namespace e2e-tests-emptydir-mpwcg deletion completed in 8.782351104s

• [SLOW TEST:13.676 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:00:50.775: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-b6pk4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 09:00:53.366: INFO: Waiting up to 5m0s for pod "pod-5a237a1e-3b37-11e9-9760-9e5b40196308" in namespace "e2e-tests-emptydir-b6pk4" to be "success or failure"
Feb 28 09:00:53.426: INFO: Pod "pod-5a237a1e-3b37-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.817932ms
Feb 28 09:00:55.489: INFO: Pod "pod-5a237a1e-3b37-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.1227408s
STEP: Saw pod success
Feb 28 09:00:55.489: INFO: Pod "pod-5a237a1e-3b37-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 09:00:55.549: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-5a237a1e-3b37-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 09:00:55.691: INFO: Waiting for pod pod-5a237a1e-3b37-11e9-9760-9e5b40196308 to disappear
Feb 28 09:00:55.749: INFO: Pod pod-5a237a1e-3b37-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:00:55.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b6pk4" for this suite.
Feb 28 09:01:02.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:01:03.542: INFO: namespace: e2e-tests-emptydir-b6pk4, resource: bindings, ignored listing per whitelist
Feb 28 09:01:04.319: INFO: namespace e2e-tests-emptydir-b6pk4 deletion completed in 8.448854867s

• [SLOW TEST:13.544 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:01:04.319: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-gh94x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:02:07.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-gh94x" for this suite.
Feb 28 09:02:29.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:02:31.202: INFO: namespace: e2e-tests-container-probe-gh94x, resource: bindings, ignored listing per whitelist
Feb 28 09:02:31.909: INFO: namespace e2e-tests-container-probe-gh94x deletion completed in 24.505159096s

• [SLOW TEST:87.590 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:02:31.909: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-65hbv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 09:02:34.359: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96561f35-3b37-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-65hbv" to be "success or failure"
Feb 28 09:02:34.418: INFO: Pod "downwardapi-volume-96561f35-3b37-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 58.475304ms
Feb 28 09:02:36.476: INFO: Pod "downwardapi-volume-96561f35-3b37-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.116690796s
STEP: Saw pod success
Feb 28 09:02:36.476: INFO: Pod "downwardapi-volume-96561f35-3b37-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 09:02:36.537: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-96561f35-3b37-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 09:02:36.668: INFO: Waiting for pod downwardapi-volume-96561f35-3b37-11e9-9760-9e5b40196308 to disappear
Feb 28 09:02:36.730: INFO: Pod downwardapi-volume-96561f35-3b37-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:02:36.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-65hbv" for this suite.
Feb 28 09:02:43.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:02:43.861: INFO: namespace: e2e-tests-projected-65hbv, resource: bindings, ignored listing per whitelist
Feb 28 09:02:45.421: INFO: namespace e2e-tests-projected-65hbv deletion completed in 8.579825833s

• [SLOW TEST:13.512 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:02:45.421: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-jrwpt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-jrwpt
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 09:02:47.805: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 09:03:02.943: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.0.66 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-jrwpt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:03:02.943: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:03:05.016: INFO: Found all expected endpoints: [netserver-0]
Feb 28 09:03:05.078: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.2.229 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-jrwpt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:03:05.078: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:03:07.740: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:03:07.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-jrwpt" for this suite.
Feb 28 09:03:30.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:03:31.642: INFO: namespace: e2e-tests-pod-network-test-jrwpt, resource: bindings, ignored listing per whitelist
Feb 28 09:03:32.317: INFO: namespace e2e-tests-pod-network-test-jrwpt deletion completed in 24.45932773s

• [SLOW TEST:46.896 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:03:32.318: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-7zq97
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 09:03:34.706: INFO: Creating ReplicaSet my-hostname-basic-ba57c625-3b37-11e9-9760-9e5b40196308
Feb 28 09:03:34.823: INFO: Pod name my-hostname-basic-ba57c625-3b37-11e9-9760-9e5b40196308: Found 1 pods out of 1
Feb 28 09:03:34.823: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ba57c625-3b37-11e9-9760-9e5b40196308" is running
Feb 28 09:03:36.940: INFO: Pod "my-hostname-basic-ba57c625-3b37-11e9-9760-9e5b40196308-dgcrv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 09:03:34 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 09:03:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ba57c625-3b37-11e9-9760-9e5b40196308]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 09:03:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ba57c625-3b37-11e9-9760-9e5b40196308]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 09:03:34 +0000 UTC Reason: Message:}])
Feb 28 09:03:36.940: INFO: Trying to dial the pod
Feb 28 09:03:42.208: INFO: Controller my-hostname-basic-ba57c625-3b37-11e9-9760-9e5b40196308: Got expected result from replica 1 [my-hostname-basic-ba57c625-3b37-11e9-9760-9e5b40196308-dgcrv]: "my-hostname-basic-ba57c625-3b37-11e9-9760-9e5b40196308-dgcrv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:03:42.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-7zq97" for this suite.
Feb 28 09:03:48.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:03:49.392: INFO: namespace: e2e-tests-replicaset-7zq97, resource: bindings, ignored listing per whitelist
Feb 28 09:03:50.901: INFO: namespace e2e-tests-replicaset-7zq97 deletion completed in 8.63334426s

• [SLOW TEST:18.583 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:03:50.901: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dpv8v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 09:03:56.223: INFO: Successfully updated pod "labelsupdatec56d3ecb-3b37-11e9-9760-9e5b40196308"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:03:58.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dpv8v" for this suite.
Feb 28 09:04:20.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:04:22.304: INFO: namespace: e2e-tests-projected-dpv8v, resource: bindings, ignored listing per whitelist
Feb 28 09:04:22.893: INFO: namespace e2e-tests-projected-dpv8v deletion completed in 24.478089351s

• [SLOW TEST:31.992 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:04:22.893: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-2xvtn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0228 09:04:31.645496   29661 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 09:04:31.645: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:04:31.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2xvtn" for this suite.
Feb 28 09:04:37.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:04:39.413: INFO: namespace: e2e-tests-gc-2xvtn, resource: bindings, ignored listing per whitelist
Feb 28 09:04:40.194: INFO: namespace e2e-tests-gc-2xvtn deletion completed in 8.491473625s

• [SLOW TEST:17.301 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:04:40.195: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-wbbfx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 09:04:42.750: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e2d7ff17-3b37-11e9-8db9-7e40ccd56e85", Controller:(*bool)(0xc001823782), BlockOwnerDeletion:(*bool)(0xc001823783)}}
Feb 28 09:04:42.811: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e2c5c9c0-3b37-11e9-8db9-7e40ccd56e85", Controller:(*bool)(0xc002406246), BlockOwnerDeletion:(*bool)(0xc002406247)}}
Feb 28 09:04:42.872: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e2cef607-3b37-11e9-8db9-7e40ccd56e85", Controller:(*bool)(0xc00240642e), BlockOwnerDeletion:(*bool)(0xc00240642f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:04:47.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-wbbfx" for this suite.
Feb 28 09:04:54.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:04:55.725: INFO: namespace: e2e-tests-gc-wbbfx, resource: bindings, ignored listing per whitelist
Feb 28 09:04:56.512: INFO: namespace e2e-tests-gc-wbbfx deletion completed in 8.456315981s

• [SLOW TEST:16.318 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:04:56.513: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-njx49
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-ec9589b8-3b37-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 09:04:59.119: INFO: Waiting up to 5m0s for pod "pod-secrets-ec9ee34e-3b37-11e9-9760-9e5b40196308" in namespace "e2e-tests-secrets-njx49" to be "success or failure"
Feb 28 09:04:59.178: INFO: Pod "pod-secrets-ec9ee34e-3b37-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.242142ms
Feb 28 09:05:01.241: INFO: Pod "pod-secrets-ec9ee34e-3b37-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.122131046s
STEP: Saw pod success
Feb 28 09:05:01.242: INFO: Pod "pod-secrets-ec9ee34e-3b37-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 09:05:01.304: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-secrets-ec9ee34e-3b37-11e9-9760-9e5b40196308 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 09:05:01.437: INFO: Waiting for pod pod-secrets-ec9ee34e-3b37-11e9-9760-9e5b40196308 to disappear
Feb 28 09:05:01.502: INFO: Pod pod-secrets-ec9ee34e-3b37-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:05:01.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-njx49" for this suite.
Feb 28 09:05:07.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:05:10.104: INFO: namespace: e2e-tests-secrets-njx49, resource: bindings, ignored listing per whitelist
Feb 28 09:05:10.104: INFO: namespace e2e-tests-secrets-njx49 deletion completed in 8.540666457s

• [SLOW TEST:13.592 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:05:10.105: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-wrjvw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wrjvw A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wrjvw A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wrjvw.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wrjvw.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wrjvw.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wrjvw.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-wrjvw.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-wrjvw.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 176.117.66.100.in-addr.arpa. PTR)" && echo OK > /results/100.66.117.176_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 176.117.66.100.in-addr.arpa. PTR)" && echo OK > /results/100.66.117.176_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wrjvw A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-wrjvw;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wrjvw A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-wrjvw.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-wrjvw.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wrjvw.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-wrjvw.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-wrjvw.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-wrjvw.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-wrjvw.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 176.117.66.100.in-addr.arpa. PTR)" && echo OK > /results/100.66.117.176_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 176.117.66.100.in-addr.arpa. PTR)" && echo OK > /results/100.66.117.176_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 09:05:25.129: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:25.191: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:25.263: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:25.325: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:25.386: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:25.449: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:25.513: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:25.574: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:26.014: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:26.075: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:26.134: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wrjvw from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:26.194: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:26.254: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:26.318: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:26.386: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:26.449: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:26.855: INFO: Lookups using e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw.svc wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-wrjvw jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw jessie_udp@dns-test-service.e2e-tests-dns-wrjvw.svc jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc]

Feb 28 09:05:35.044: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:35.105: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:35.167: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:35.229: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:35.291: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:35.353: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:35.417: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:35.476: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:35.893: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:35.955: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:36.016: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wrjvw from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:36.079: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:36.140: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:36.202: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:36.265: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:36.327: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc from pod e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308: the server could not find the requested resource (get pods dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308)
Feb 28 09:05:36.697: INFO: Lookups using e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw wheezy_udp@dns-test-service.e2e-tests-dns-wrjvw.svc wheezy_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-wrjvw jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw jessie_udp@dns-test-service.e2e-tests-dns-wrjvw.svc jessie_tcp@dns-test-service.e2e-tests-dns-wrjvw.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-wrjvw.svc]

Feb 28 09:05:46.764: INFO: DNS probes using e2e-tests-dns-wrjvw/dns-test-f4c5f0d4-3b37-11e9-9760-9e5b40196308 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:05:46.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-wrjvw" for this suite.
Feb 28 09:05:53.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:05:54.642: INFO: namespace: e2e-tests-dns-wrjvw, resource: bindings, ignored listing per whitelist
Feb 28 09:05:55.535: INFO: namespace e2e-tests-dns-wrjvw deletion completed in 8.50888322s

• [SLOW TEST:45.430 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:05:55.535: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-5zrg8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 09:05:57.967: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fb27903-3b38-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-5zrg8" to be "success or failure"
Feb 28 09:05:58.024: INFO: Pod "downwardapi-volume-0fb27903-3b38-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 57.557369ms
Feb 28 09:06:00.083: INFO: Pod "downwardapi-volume-0fb27903-3b38-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.116486574s
STEP: Saw pod success
Feb 28 09:06:00.083: INFO: Pod "downwardapi-volume-0fb27903-3b38-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 09:06:00.140: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-0fb27903-3b38-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 09:06:00.266: INFO: Waiting for pod downwardapi-volume-0fb27903-3b38-11e9-9760-9e5b40196308 to disappear
Feb 28 09:06:00.324: INFO: Pod downwardapi-volume-0fb27903-3b38-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:06:00.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5zrg8" for this suite.
Feb 28 09:06:06.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:08.554: INFO: namespace: e2e-tests-downward-api-5zrg8, resource: bindings, ignored listing per whitelist
Feb 28 09:06:08.855: INFO: namespace e2e-tests-downward-api-5zrg8 deletion completed in 8.471312591s

• [SLOW TEST:13.320 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:06:08.855: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-g9w5j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
Feb 28 09:06:11.218: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-g9w5j'
Feb 28 09:06:14.384: INFO: stderr: ""
Feb 28 09:06:14.384: INFO: stdout: "pod/pause created\n"
Feb 28 09:06:14.384: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 28 09:06:14.384: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-g9w5j" to be "running and ready"
Feb 28 09:06:14.444: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 60.292334ms
Feb 28 09:06:16.505: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.120383555s
Feb 28 09:06:16.505: INFO: Pod "pause" satisfied condition "running and ready"
Feb 28 09:06:16.505: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 28 09:06:16.505: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-g9w5j'
Feb 28 09:06:16.912: INFO: stderr: ""
Feb 28 09:06:16.912: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 28 09:06:16.912: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-g9w5j'
Feb 28 09:06:17.262: INFO: stderr: ""
Feb 28 09:06:17.262: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 28 09:06:17.262: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-g9w5j'
Feb 28 09:06:17.664: INFO: stderr: ""
Feb 28 09:06:17.664: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 28 09:06:17.664: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-g9w5j'
Feb 28 09:06:18.011: INFO: stderr: ""
Feb 28 09:06:18.011: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
Feb 28 09:06:18.011: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-g9w5j'
Feb 28 09:06:18.398: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 09:06:18.398: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 28 09:06:18.398: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-g9w5j'
Feb 28 09:06:18.835: INFO: stderr: "No resources found.\n"
Feb 28 09:06:18.835: INFO: stdout: ""
Feb 28 09:06:18.835: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-g9w5j -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 09:06:19.168: INFO: stderr: ""
Feb 28 09:06:19.168: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:06:19.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-g9w5j" for this suite.
Feb 28 09:06:25.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:26.910: INFO: namespace: e2e-tests-kubectl-g9w5j, resource: bindings, ignored listing per whitelist
Feb 28 09:06:27.680: INFO: namespace e2e-tests-kubectl-g9w5j deletion completed in 8.453528964s

• [SLOW TEST:18.825 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:06:27.680: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tw968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 09:06:30.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22d413a1-3b38-11e9-9760-9e5b40196308" in namespace "e2e-tests-downward-api-tw968" to be "success or failure"
Feb 28 09:06:30.124: INFO: Pod "downwardapi-volume-22d413a1-3b38-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.038899ms
Feb 28 09:06:32.188: INFO: Pod "downwardapi-volume-22d413a1-3b38-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.123450648s
STEP: Saw pod success
Feb 28 09:06:32.188: INFO: Pod "downwardapi-volume-22d413a1-3b38-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 09:06:32.247: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod downwardapi-volume-22d413a1-3b38-11e9-9760-9e5b40196308 container client-container: <nil>
STEP: delete the pod
Feb 28 09:06:32.375: INFO: Waiting for pod downwardapi-volume-22d413a1-3b38-11e9-9760-9e5b40196308 to disappear
Feb 28 09:06:32.435: INFO: Pod downwardapi-volume-22d413a1-3b38-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:06:32.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tw968" for this suite.
Feb 28 09:06:38.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:38.912: INFO: namespace: e2e-tests-downward-api-tw968, resource: bindings, ignored listing per whitelist
Feb 28 09:06:41.289: INFO: namespace e2e-tests-downward-api-tw968 deletion completed in 8.787974557s

• [SLOW TEST:13.609 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:06:41.290: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qkl5p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-2aeef5fb-3b38-11e9-9760-9e5b40196308
STEP: Creating a pod to test consume secrets
Feb 28 09:06:43.718: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2af772e4-3b38-11e9-9760-9e5b40196308" in namespace "e2e-tests-projected-qkl5p" to be "success or failure"
Feb 28 09:06:43.778: INFO: Pod "pod-projected-secrets-2af772e4-3b38-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 59.752061ms
Feb 28 09:06:45.837: INFO: Pod "pod-projected-secrets-2af772e4-3b38-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11878006s
STEP: Saw pod success
Feb 28 09:06:45.837: INFO: Pod "pod-projected-secrets-2af772e4-3b38-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 09:06:45.897: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod pod-projected-secrets-2af772e4-3b38-11e9-9760-9e5b40196308 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 09:06:46.025: INFO: Waiting for pod pod-projected-secrets-2af772e4-3b38-11e9-9760-9e5b40196308 to disappear
Feb 28 09:06:46.084: INFO: Pod pod-projected-secrets-2af772e4-3b38-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:06:46.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qkl5p" for this suite.
Feb 28 09:06:52.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:06:52.940: INFO: namespace: e2e-tests-projected-qkl5p, resource: bindings, ignored listing per whitelist
Feb 28 09:06:54.608: INFO: namespace e2e-tests-projected-qkl5p deletion completed in 8.466110556s

• [SLOW TEST:13.319 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:06:54.609: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-wgrtr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 28 09:07:01.457: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:01.458: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:02.221: INFO: Exec stderr: ""
Feb 28 09:07:02.221: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:02.221: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:02.934: INFO: Exec stderr: ""
Feb 28 09:07:02.934: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:02.934: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:03.607: INFO: Exec stderr: ""
Feb 28 09:07:03.607: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:03.607: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:04.345: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 28 09:07:04.345: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:04.345: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:05.047: INFO: Exec stderr: ""
Feb 28 09:07:05.047: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:05.047: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:05.731: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 28 09:07:05.731: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:05.731: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:06.441: INFO: Exec stderr: ""
Feb 28 09:07:06.441: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:06.442: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:07.087: INFO: Exec stderr: ""
Feb 28 09:07:07.087: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:07.087: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:07.789: INFO: Exec stderr: ""
Feb 28 09:07:07.789: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-wgrtr PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 09:07:07.789: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
Feb 28 09:07:08.507: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:07:08.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-wgrtr" for this suite.
Feb 28 09:07:54.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:07:55.877: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-wgrtr, resource: bindings, ignored listing per whitelist
Feb 28 09:07:56.998: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-wgrtr deletion completed in 48.433366639s

• [SLOW TEST:62.390 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:07:56.999: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cvzs8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 28 09:07:59.308: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:07:59.879: INFO: stderr: ""
Feb 28 09:07:59.879: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 09:07:59.879: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:00.209: INFO: stderr: ""
Feb 28 09:08:00.209: INFO: stdout: "update-demo-nautilus-c29bs update-demo-nautilus-hpmnm "
Feb 28 09:08:00.209: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-c29bs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:00.538: INFO: stderr: ""
Feb 28 09:08:00.538: INFO: stdout: ""
Feb 28 09:08:00.538: INFO: update-demo-nautilus-c29bs is created but not running
Feb 28 09:08:05.539: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:05.867: INFO: stderr: ""
Feb 28 09:08:05.867: INFO: stdout: "update-demo-nautilus-c29bs update-demo-nautilus-hpmnm "
Feb 28 09:08:05.867: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-c29bs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:06.202: INFO: stderr: ""
Feb 28 09:08:06.202: INFO: stdout: "true"
Feb 28 09:08:06.202: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-c29bs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:06.542: INFO: stderr: ""
Feb 28 09:08:06.542: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 09:08:06.542: INFO: validating pod update-demo-nautilus-c29bs
Feb 28 09:08:06.690: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 09:08:06.690: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 09:08:06.690: INFO: update-demo-nautilus-c29bs is verified up and running
Feb 28 09:08:06.691: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-hpmnm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:07.012: INFO: stderr: ""
Feb 28 09:08:07.012: INFO: stdout: "true"
Feb 28 09:08:07.012: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-nautilus-hpmnm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:07.336: INFO: stderr: ""
Feb 28 09:08:07.336: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 09:08:07.337: INFO: validating pod update-demo-nautilus-hpmnm
Feb 28 09:08:07.484: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 09:08:07.484: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 09:08:07.484: INFO: update-demo-nautilus-hpmnm is verified up and running
STEP: rolling-update to new replication controller
Feb 28 09:08:07.492: INFO: scanned /root for discovery docs: <nil>
Feb 28 09:08:07.492: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:23.347: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 09:08:23.347: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 09:08:23.347: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:23.788: INFO: stderr: ""
Feb 28 09:08:23.788: INFO: stdout: "update-demo-kitten-gfqwj update-demo-kitten-xsh5d "
Feb 28 09:08:23.788: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-kitten-gfqwj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:24.257: INFO: stderr: ""
Feb 28 09:08:24.257: INFO: stdout: "true"
Feb 28 09:08:24.257: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-kitten-gfqwj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:24.592: INFO: stderr: ""
Feb 28 09:08:24.592: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 09:08:24.592: INFO: validating pod update-demo-kitten-gfqwj
Feb 28 09:08:24.736: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 09:08:24.736: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 09:08:24.736: INFO: update-demo-kitten-gfqwj is verified up and running
Feb 28 09:08:24.736: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-kitten-xsh5d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:25.076: INFO: stderr: ""
Feb 28 09:08:25.076: INFO: stdout: "true"
Feb 28 09:08:25.077: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-aws-sxtla.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml get pods update-demo-kitten-xsh5d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvzs8'
Feb 28 09:08:25.474: INFO: stderr: ""
Feb 28 09:08:25.474: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 09:08:25.474: INFO: validating pod update-demo-kitten-xsh5d
Feb 28 09:08:25.640: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 09:08:25.640: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 09:08:25.640: INFO: update-demo-kitten-xsh5d is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:08:25.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cvzs8" for this suite.
Feb 28 09:08:47.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:08:48.219: INFO: namespace: e2e-tests-kubectl-cvzs8, resource: bindings, ignored listing per whitelist
Feb 28 09:08:50.176: INFO: namespace e2e-tests-kubectl-cvzs8 deletion completed in 24.471993903s

• [SLOW TEST:53.178 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 09:08:50.177: INFO: >>> kubeConfig: /tmp/build/418d26f1/git-kubernetes_publish_conf_test_results-master_master/scripts/aws_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-j8g75
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 28 09:08:52.543: INFO: Waiting up to 5m0s for pod "client-containers-77c0e5db-3b38-11e9-9760-9e5b40196308" in namespace "e2e-tests-containers-j8g75" to be "success or failure"
Feb 28 09:08:52.599: INFO: Pod "client-containers-77c0e5db-3b38-11e9-9760-9e5b40196308": Phase="Pending", Reason="", readiness=false. Elapsed: 55.736459ms
Feb 28 09:08:54.663: INFO: Pod "client-containers-77c0e5db-3b38-11e9-9760-9e5b40196308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.119363254s
STEP: Saw pod success
Feb 28 09:08:54.663: INFO: Pod "client-containers-77c0e5db-3b38-11e9-9760-9e5b40196308" satisfied condition "success or failure"
Feb 28 09:08:54.725: INFO: Trying to get logs from node ip-10-250-9-188.eu-west-1.compute.internal pod client-containers-77c0e5db-3b38-11e9-9760-9e5b40196308 container test-container: <nil>
STEP: delete the pod
Feb 28 09:08:54.863: INFO: Waiting for pod client-containers-77c0e5db-3b38-11e9-9760-9e5b40196308 to disappear
Feb 28 09:08:54.925: INFO: Pod client-containers-77c0e5db-3b38-11e9-9760-9e5b40196308 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 09:08:54.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-j8g75" for this suite.
Feb 28 09:09:01.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 09:09:03.348: INFO: namespace: e2e-tests-containers-j8g75, resource: bindings, ignored listing per whitelist
Feb 28 09:09:03.514: INFO: namespace e2e-tests-containers-j8g75 deletion completed in 8.529545142s

• [SLOW TEST:13.338 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSFeb 28 09:09:03.515: INFO: Running AfterSuite actions on all node
Feb 28 09:09:03.515: INFO: Running AfterSuite actions on node 1
Feb 28 09:09:03.515: INFO: Skipping dumping logs from cluster

Ran 187 of 2011 Specs in 6342.860 seconds
SUCCESS! -- 187 Passed | 0 Failed | 0 Flaked | 0 Pending | 1824 Skipped PASS

Ginkgo ran 1 suite in 1h45m44.106150139s
Test Suite Passed
