Conformance test: not doing test setup.
Feb 28 07:34:06.286: INFO: Overriding default scale value of zero to 1
Feb 28 07:34:06.291: INFO: Overriding default milliseconds value of zero to 5000
I0228 07:34:07.019095   30646 e2e.go:304] Starting e2e run "3a72f277-3b2b-11e9-b1ab-fe0431d33f49" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1551339245 - Will randomize all specs
Will run 188 of 2011 specs

Feb 28 07:34:07.387: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 07:34:07.389: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 28 07:34:07.422: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 28 07:34:07.472: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 28 07:34:07.472: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 28 07:34:07.472: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 28 07:34:07.487: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 28 07:34:07.487: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 28 07:34:07.487: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 28 07:34:07.487: INFO: e2e test version: v1.12.5
Feb 28 07:34:07.491: INFO: kube-apiserver version: v1.12.5
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:34:07.492: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
Feb 28 07:34:07.802: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 28 07:34:07.827: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-pm2fl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 28 07:34:07.947: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:08.483: INFO: stderr: ""
Feb 28 07:34:08.483: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 07:34:08.483: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:08.731: INFO: stderr: ""
Feb 28 07:34:08.731: INFO: stdout: "update-demo-nautilus-nqmn5 update-demo-nautilus-zdmcm "
Feb 28 07:34:08.731: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-nqmn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:09.038: INFO: stderr: ""
Feb 28 07:34:09.038: INFO: stdout: ""
Feb 28 07:34:09.038: INFO: update-demo-nautilus-nqmn5 is created but not running
Feb 28 07:34:14.038: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:14.257: INFO: stderr: ""
Feb 28 07:34:14.257: INFO: stdout: "update-demo-nautilus-nqmn5 update-demo-nautilus-zdmcm "
Feb 28 07:34:14.257: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-nqmn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:14.543: INFO: stderr: ""
Feb 28 07:34:14.543: INFO: stdout: "true"
Feb 28 07:34:14.543: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-nqmn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:14.760: INFO: stderr: ""
Feb 28 07:34:14.760: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:34:14.760: INFO: validating pod update-demo-nautilus-nqmn5
Feb 28 07:34:14.883: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:34:14.883: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:34:14.883: INFO: update-demo-nautilus-nqmn5 is verified up and running
Feb 28 07:34:14.883: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-zdmcm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:15.134: INFO: stderr: ""
Feb 28 07:34:15.134: INFO: stdout: "true"
Feb 28 07:34:15.134: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-zdmcm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:15.380: INFO: stderr: ""
Feb 28 07:34:15.380: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:34:15.380: INFO: validating pod update-demo-nautilus-zdmcm
Feb 28 07:34:15.464: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:34:15.464: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:34:15.464: INFO: update-demo-nautilus-zdmcm is verified up and running
STEP: scaling down the replication controller
Feb 28 07:34:15.471: INFO: scanned /root for discovery docs: <nil>
Feb 28 07:34:15.471: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:16.861: INFO: stderr: ""
Feb 28 07:34:16.861: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 07:34:16.861: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:17.291: INFO: stderr: ""
Feb 28 07:34:17.291: INFO: stdout: "update-demo-nautilus-nqmn5 update-demo-nautilus-zdmcm "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 07:34:22.291: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:22.588: INFO: stderr: ""
Feb 28 07:34:22.588: INFO: stdout: "update-demo-nautilus-nqmn5 update-demo-nautilus-zdmcm "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 07:34:27.588: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:27.833: INFO: stderr: ""
Feb 28 07:34:27.833: INFO: stdout: "update-demo-nautilus-nqmn5 "
Feb 28 07:34:27.833: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-nqmn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:28.165: INFO: stderr: ""
Feb 28 07:34:28.165: INFO: stdout: "true"
Feb 28 07:34:28.165: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-nqmn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:28.433: INFO: stderr: ""
Feb 28 07:34:28.433: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:34:28.433: INFO: validating pod update-demo-nautilus-nqmn5
Feb 28 07:34:28.440: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:34:28.440: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:34:28.440: INFO: update-demo-nautilus-nqmn5 is verified up and running
STEP: scaling up the replication controller
Feb 28 07:34:28.473: INFO: scanned /root for discovery docs: <nil>
Feb 28 07:34:28.473: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:29.791: INFO: stderr: ""
Feb 28 07:34:29.791: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 07:34:29.791: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:30.140: INFO: stderr: ""
Feb 28 07:34:30.140: INFO: stdout: "update-demo-nautilus-54txf update-demo-nautilus-nqmn5 "
Feb 28 07:34:30.140: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-54txf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:30.581: INFO: stderr: ""
Feb 28 07:34:30.581: INFO: stdout: "true"
Feb 28 07:34:30.581: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-54txf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:30.832: INFO: stderr: ""
Feb 28 07:34:30.832: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:34:30.832: INFO: validating pod update-demo-nautilus-54txf
Feb 28 07:34:30.926: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:34:30.926: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:34:30.926: INFO: update-demo-nautilus-54txf is verified up and running
Feb 28 07:34:30.926: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-nqmn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:31.169: INFO: stderr: ""
Feb 28 07:34:31.169: INFO: stdout: "true"
Feb 28 07:34:31.169: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-nqmn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:31.398: INFO: stderr: ""
Feb 28 07:34:31.398: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:34:31.398: INFO: validating pod update-demo-nautilus-nqmn5
Feb 28 07:34:31.411: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:34:31.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:34:31.411: INFO: update-demo-nautilus-nqmn5 is verified up and running
STEP: using delete to clean up resources
Feb 28 07:34:31.411: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:31.679: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:34:31.679: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 07:34:31.680: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-pm2fl'
Feb 28 07:34:31.927: INFO: stderr: "No resources found.\n"
Feb 28 07:34:31.927: INFO: stdout: ""
Feb 28 07:34:31.927: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-pm2fl -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 07:34:32.253: INFO: stderr: ""
Feb 28 07:34:32.253: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:34:32.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pm2fl" for this suite.
Feb 28 07:34:54.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:34:54.462: INFO: namespace: e2e-tests-kubectl-pm2fl, resource: bindings, ignored listing per whitelist
Feb 28 07:34:54.562: INFO: namespace e2e-tests-kubectl-pm2fl deletion completed in 22.303929521s

• [SLOW TEST:47.071 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:34:54.562: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fkdrx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 07:34:55.019: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-fkdrx'
Feb 28 07:34:55.240: INFO: stderr: ""
Feb 28 07:34:55.240: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
Feb 28 07:34:55.245: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-fkdrx'
Feb 28 07:34:57.047: INFO: stderr: ""
Feb 28 07:34:57.048: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:34:57.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fkdrx" for this suite.
Feb 28 07:35:03.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:35:03.130: INFO: namespace: e2e-tests-kubectl-fkdrx, resource: bindings, ignored listing per whitelist
Feb 28 07:35:03.218: INFO: namespace e2e-tests-kubectl-fkdrx deletion completed in 6.165441362s

• [SLOW TEST:8.656 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:35:03.218: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-4mhk8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 07:35:03.522: INFO: Waiting up to 5m0s for pod "downward-api-5ca053da-3b2b-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-4mhk8" to be "success or failure"
Feb 28 07:35:03.531: INFO: Pod "downward-api-5ca053da-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 8.65643ms
Feb 28 07:35:05.537: INFO: Pod "downward-api-5ca053da-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014903903s
Feb 28 07:35:07.544: INFO: Pod "downward-api-5ca053da-3b2b-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022307162s
STEP: Saw pod success
Feb 28 07:35:07.544: INFO: Pod "downward-api-5ca053da-3b2b-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:35:07.559: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downward-api-5ca053da-3b2b-11e9-b1ab-fe0431d33f49 container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:35:07.691: INFO: Waiting for pod downward-api-5ca053da-3b2b-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:35:07.694: INFO: Pod downward-api-5ca053da-3b2b-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:35:07.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-4mhk8" for this suite.
Feb 28 07:35:13.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:35:13.893: INFO: namespace: e2e-tests-downward-api-4mhk8, resource: bindings, ignored listing per whitelist
Feb 28 07:35:13.923: INFO: namespace e2e-tests-downward-api-4mhk8 deletion completed in 6.22560742s

• [SLOW TEST:10.705 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:35:13.924: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-dpzp8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 28 07:35:14.435: INFO: namespace e2e-tests-kubectl-dpzp8
Feb 28 07:35:14.435: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-dpzp8'
Feb 28 07:35:14.924: INFO: stderr: ""
Feb 28 07:35:14.924: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 07:35:15.931: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:35:15.931: INFO: Found 0 / 1
Feb 28 07:35:16.932: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:35:16.932: INFO: Found 0 / 1
Feb 28 07:35:17.930: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:35:17.930: INFO: Found 1 / 1
Feb 28 07:35:17.930: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 07:35:17.934: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:35:17.934: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 07:35:17.934: INFO: wait on redis-master startup in e2e-tests-kubectl-dpzp8 
Feb 28 07:35:17.934: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs redis-master-b5fbp redis-master --namespace=e2e-tests-kubectl-dpzp8'
Feb 28 07:35:18.259: INFO: stderr: ""
Feb 28 07:35:18.259: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 07:35:17.177 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 07:35:17.177 # Server started, Redis version 3.2.12\n1:M 28 Feb 07:35:17.177 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 07:35:17.177 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 28 07:35:18.259: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-dpzp8'
Feb 28 07:35:18.653: INFO: stderr: ""
Feb 28 07:35:18.653: INFO: stdout: "service/rm2 exposed\n"
Feb 28 07:35:18.666: INFO: Service rm2 in namespace e2e-tests-kubectl-dpzp8 found.
STEP: exposing service
Feb 28 07:35:20.683: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-dpzp8'
Feb 28 07:35:20.958: INFO: stderr: ""
Feb 28 07:35:20.958: INFO: stdout: "service/rm3 exposed\n"
Feb 28 07:35:20.964: INFO: Service rm3 in namespace e2e-tests-kubectl-dpzp8 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:35:22.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dpzp8" for this suite.
Feb 28 07:35:45.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:35:45.369: INFO: namespace: e2e-tests-kubectl-dpzp8, resource: bindings, ignored listing per whitelist
Feb 28 07:35:45.611: INFO: namespace e2e-tests-kubectl-dpzp8 deletion completed in 22.603514275s

• [SLOW TEST:31.687 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:35:45.611: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-wtmpq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 07:35:46.245: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:35:50.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-wtmpq" for this suite.
Feb 28 07:36:12.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:36:12.730: INFO: namespace: e2e-tests-init-container-wtmpq, resource: bindings, ignored listing per whitelist
Feb 28 07:36:12.803: INFO: namespace e2e-tests-init-container-wtmpq deletion completed in 22.219993511s

• [SLOW TEST:27.192 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:36:12.803: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-6pxx7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 07:36:13.144: INFO: Number of nodes with available pods: 0
Feb 28 07:36:13.144: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:36:14.161: INFO: Number of nodes with available pods: 0
Feb 28 07:36:14.161: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:36:15.153: INFO: Number of nodes with available pods: 0
Feb 28 07:36:15.153: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:36:16.153: INFO: Number of nodes with available pods: 2
Feb 28 07:36:16.153: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 28 07:36:16.175: INFO: Number of nodes with available pods: 1
Feb 28 07:36:16.175: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 07:36:17.197: INFO: Number of nodes with available pods: 1
Feb 28 07:36:17.197: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 07:36:18.186: INFO: Number of nodes with available pods: 2
Feb 28 07:36:18.186: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-6pxx7, will wait for the garbage collector to delete the pods
Feb 28 07:36:18.254: INFO: Deleting {extensions DaemonSet} daemon-set took: 6.826023ms
Feb 28 07:36:18.354: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.232782ms
Feb 28 07:36:54.759: INFO: Number of nodes with available pods: 0
Feb 28 07:36:54.759: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 07:36:54.765: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-6pxx7/daemonsets","resourceVersion":"3114"},"items":null}

Feb 28 07:36:54.770: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-6pxx7/pods","resourceVersion":"3114"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:36:54.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-6pxx7" for this suite.
Feb 28 07:37:00.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:37:00.994: INFO: namespace: e2e-tests-daemonsets-6pxx7, resource: bindings, ignored listing per whitelist
Feb 28 07:37:01.063: INFO: namespace e2e-tests-daemonsets-6pxx7 deletion completed in 6.272585286s

• [SLOW TEST:48.260 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:37:01.063: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-bbnck
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 07:37:01.830: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-bbnck'
Feb 28 07:37:02.116: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 07:37:02.116: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
Feb 28 07:37:04.129: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-bbnck'
Feb 28 07:37:04.328: INFO: stderr: ""
Feb 28 07:37:04.328: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:37:04.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bbnck" for this suite.
Feb 28 07:37:10.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:37:10.625: INFO: namespace: e2e-tests-kubectl-bbnck, resource: bindings, ignored listing per whitelist
Feb 28 07:37:11.241: INFO: namespace e2e-tests-kubectl-bbnck deletion completed in 6.90318358s

• [SLOW TEST:10.177 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:37:11.241: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-tst7w
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:37:11.750: INFO: (0) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 107.291429ms)
Feb 28 07:37:11.763: INFO: (1) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.866755ms)
Feb 28 07:37:11.775: INFO: (2) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.208013ms)
Feb 28 07:37:11.786: INFO: (3) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.818502ms)
Feb 28 07:37:11.797: INFO: (4) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.217945ms)
Feb 28 07:37:11.807: INFO: (5) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.775761ms)
Feb 28 07:37:11.818: INFO: (6) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.90878ms)
Feb 28 07:37:11.829: INFO: (7) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.146155ms)
Feb 28 07:37:11.840: INFO: (8) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 11.208738ms)
Feb 28 07:37:11.849: INFO: (9) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.931794ms)
Feb 28 07:37:11.858: INFO: (10) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.258921ms)
Feb 28 07:37:11.868: INFO: (11) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.102288ms)
Feb 28 07:37:11.881: INFO: (12) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.195219ms)
Feb 28 07:37:11.891: INFO: (13) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.881167ms)
Feb 28 07:37:11.902: INFO: (14) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.976297ms)
Feb 28 07:37:11.913: INFO: (15) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.278538ms)
Feb 28 07:37:11.923: INFO: (16) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.183699ms)
Feb 28 07:37:11.933: INFO: (17) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.078913ms)
Feb 28 07:37:11.944: INFO: (18) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.325056ms)
Feb 28 07:37:11.956: INFO: (19) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.624008ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:37:11.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-tst7w" for this suite.
Feb 28 07:37:18.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:37:18.056: INFO: namespace: e2e-tests-proxy-tst7w, resource: bindings, ignored listing per whitelist
Feb 28 07:37:18.149: INFO: namespace e2e-tests-proxy-tst7w deletion completed in 6.174476026s

• [SLOW TEST:6.908 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:37:18.149: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mg28j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-ad0822dd-3b2b-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 07:37:18.424: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ad08c1a8-3b2b-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-mg28j" to be "success or failure"
Feb 28 07:37:18.427: INFO: Pod "pod-projected-configmaps-ad08c1a8-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324393ms
Feb 28 07:37:20.437: INFO: Pod "pod-projected-configmaps-ad08c1a8-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013080016s
Feb 28 07:37:22.458: INFO: Pod "pod-projected-configmaps-ad08c1a8-3b2b-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034043766s
STEP: Saw pod success
Feb 28 07:37:22.458: INFO: Pod "pod-projected-configmaps-ad08c1a8-3b2b-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:37:22.465: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-configmaps-ad08c1a8-3b2b-11e9-b1ab-fe0431d33f49 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:37:22.488: INFO: Waiting for pod pod-projected-configmaps-ad08c1a8-3b2b-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:37:22.493: INFO: Pod pod-projected-configmaps-ad08c1a8-3b2b-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:37:22.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mg28j" for this suite.
Feb 28 07:37:28.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:37:28.794: INFO: namespace: e2e-tests-projected-mg28j, resource: bindings, ignored listing per whitelist
Feb 28 07:37:28.821: INFO: namespace e2e-tests-projected-mg28j deletion completed in 6.318883962s

• [SLOW TEST:10.672 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:37:28.821: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-sdtcc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:37:29.121: INFO: (0) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.276525ms)
Feb 28 07:37:29.159: INFO: (1) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 38.482393ms)
Feb 28 07:37:29.164: INFO: (2) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.843575ms)
Feb 28 07:37:29.169: INFO: (3) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.643672ms)
Feb 28 07:37:29.174: INFO: (4) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.921775ms)
Feb 28 07:37:29.178: INFO: (5) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.446964ms)
Feb 28 07:37:29.183: INFO: (6) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.616373ms)
Feb 28 07:37:29.188: INFO: (7) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.642274ms)
Feb 28 07:37:29.193: INFO: (8) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.126914ms)
Feb 28 07:37:29.198: INFO: (9) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.203449ms)
Feb 28 07:37:29.203: INFO: (10) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.567082ms)
Feb 28 07:37:29.207: INFO: (11) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.755262ms)
Feb 28 07:37:29.212: INFO: (12) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.761139ms)
Feb 28 07:37:29.217: INFO: (13) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.618828ms)
Feb 28 07:37:29.223: INFO: (14) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.606003ms)
Feb 28 07:37:29.227: INFO: (15) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.63032ms)
Feb 28 07:37:29.239: INFO: (16) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 11.580381ms)
Feb 28 07:37:29.244: INFO: (17) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.329345ms)
Feb 28 07:37:29.255: INFO: (18) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.870687ms)
Feb 28 07:37:29.261: INFO: (19) /api/v1/nodes/shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.667446ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:37:29.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-sdtcc" for this suite.
Feb 28 07:37:35.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:37:35.581: INFO: namespace: e2e-tests-proxy-sdtcc, resource: bindings, ignored listing per whitelist
Feb 28 07:37:35.634: INFO: namespace e2e-tests-proxy-sdtcc deletion completed in 6.369530521s

• [SLOW TEST:6.813 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:37:35.635: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-fssrj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:37:59.950: INFO: Container started at 2019-02-28 07:37:37 +0000 UTC, pod became ready at 2019-02-28 07:37:58 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:37:59.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fssrj" for this suite.
Feb 28 07:38:21.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:38:22.667: INFO: namespace: e2e-tests-container-probe-fssrj, resource: bindings, ignored listing per whitelist
Feb 28 07:38:22.695: INFO: namespace e2e-tests-container-probe-fssrj deletion completed in 22.731089602s

• [SLOW TEST:47.060 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:38:22.695: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-m6sn8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 07:38:25.677: INFO: Successfully updated pod "labelsupdated399eaab-3b2b-11e9-b1ab-fe0431d33f49"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:38:27.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-m6sn8" for this suite.
Feb 28 07:38:49.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:38:49.909: INFO: namespace: e2e-tests-downward-api-m6sn8, resource: bindings, ignored listing per whitelist
Feb 28 07:38:50.247: INFO: namespace e2e-tests-downward-api-m6sn8 deletion completed in 22.541344434s

• [SLOW TEST:27.552 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:38:50.247: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-sgqzp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:38:50.622: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3fcecb2-3b2b-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-sgqzp" to be "success or failure"
Feb 28 07:38:50.627: INFO: Pod "downwardapi-volume-e3fcecb2-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.169972ms
Feb 28 07:38:52.633: INFO: Pod "downwardapi-volume-e3fcecb2-3b2b-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011081667s
STEP: Saw pod success
Feb 28 07:38:52.633: INFO: Pod "downwardapi-volume-e3fcecb2-3b2b-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:38:52.638: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-e3fcecb2-3b2b-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 07:38:52.666: INFO: Waiting for pod downwardapi-volume-e3fcecb2-3b2b-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:38:52.671: INFO: Pod downwardapi-volume-e3fcecb2-3b2b-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:38:52.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sgqzp" for this suite.
Feb 28 07:38:58.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:38:59.261: INFO: namespace: e2e-tests-downward-api-sgqzp, resource: bindings, ignored listing per whitelist
Feb 28 07:38:59.340: INFO: namespace e2e-tests-downward-api-sgqzp deletion completed in 6.659381654s

• [SLOW TEST:9.092 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:38:59.340: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-9l6gz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-e969995b-3b2b-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 07:38:59.734: INFO: Waiting up to 5m0s for pod "pod-configmaps-e96ac3ff-3b2b-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-configmap-9l6gz" to be "success or failure"
Feb 28 07:38:59.739: INFO: Pod "pod-configmaps-e96ac3ff-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.925965ms
Feb 28 07:39:01.748: INFO: Pod "pod-configmaps-e96ac3ff-3b2b-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013981587s
STEP: Saw pod success
Feb 28 07:39:01.748: INFO: Pod "pod-configmaps-e96ac3ff-3b2b-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:39:01.754: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-e96ac3ff-3b2b-11e9-b1ab-fe0431d33f49 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:39:01.780: INFO: Waiting for pod pod-configmaps-e96ac3ff-3b2b-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:39:01.789: INFO: Pod pod-configmaps-e96ac3ff-3b2b-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:39:01.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9l6gz" for this suite.
Feb 28 07:39:07.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:39:08.113: INFO: namespace: e2e-tests-configmap-9l6gz, resource: bindings, ignored listing per whitelist
Feb 28 07:39:08.544: INFO: namespace e2e-tests-configmap-9l6gz deletion completed in 6.744963262s

• [SLOW TEST:9.204 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:39:08.544: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-4bk2f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 28 07:39:09.262: INFO: Waiting up to 5m0s for pod "client-containers-ef17c120-3b2b-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-containers-4bk2f" to be "success or failure"
Feb 28 07:39:09.267: INFO: Pod "client-containers-ef17c120-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.087691ms
Feb 28 07:39:11.280: INFO: Pod "client-containers-ef17c120-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018370285s
Feb 28 07:39:13.287: INFO: Pod "client-containers-ef17c120-3b2b-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025388425s
STEP: Saw pod success
Feb 28 07:39:13.287: INFO: Pod "client-containers-ef17c120-3b2b-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:39:13.294: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod client-containers-ef17c120-3b2b-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 07:39:13.319: INFO: Waiting for pod client-containers-ef17c120-3b2b-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:39:13.323: INFO: Pod client-containers-ef17c120-3b2b-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:39:13.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-4bk2f" for this suite.
Feb 28 07:39:19.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:39:19.755: INFO: namespace: e2e-tests-containers-4bk2f, resource: bindings, ignored listing per whitelist
Feb 28 07:39:19.813: INFO: namespace e2e-tests-containers-4bk2f deletion completed in 6.479897313s

• [SLOW TEST:11.269 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:39:19.813: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-52z2q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 28 07:39:20.441: INFO: Waiting up to 5m0s for pod "var-expansion-f5c2a671-3b2b-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-var-expansion-52z2q" to be "success or failure"
Feb 28 07:39:20.448: INFO: Pod "var-expansion-f5c2a671-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 7.503394ms
Feb 28 07:39:22.460: INFO: Pod "var-expansion-f5c2a671-3b2b-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018591061s
STEP: Saw pod success
Feb 28 07:39:22.460: INFO: Pod "var-expansion-f5c2a671-3b2b-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:39:22.472: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod var-expansion-f5c2a671-3b2b-11e9-b1ab-fe0431d33f49 container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:39:22.528: INFO: Waiting for pod var-expansion-f5c2a671-3b2b-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:39:22.536: INFO: Pod var-expansion-f5c2a671-3b2b-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:39:22.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-52z2q" for this suite.
Feb 28 07:39:28.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:39:28.618: INFO: namespace: e2e-tests-var-expansion-52z2q, resource: bindings, ignored listing per whitelist
Feb 28 07:39:28.841: INFO: namespace e2e-tests-var-expansion-52z2q deletion completed in 6.298684787s

• [SLOW TEST:9.028 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:39:28.842: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-q2bbt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:39:29.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fafe4099-3b2b-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-q2bbt" to be "success or failure"
Feb 28 07:39:29.222: INFO: Pod "downwardapi-volume-fafe4099-3b2b-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.642842ms
Feb 28 07:39:31.228: INFO: Pod "downwardapi-volume-fafe4099-3b2b-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009189275s
STEP: Saw pod success
Feb 28 07:39:31.228: INFO: Pod "downwardapi-volume-fafe4099-3b2b-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:39:31.236: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-fafe4099-3b2b-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 07:39:31.260: INFO: Waiting for pod downwardapi-volume-fafe4099-3b2b-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:39:31.265: INFO: Pod downwardapi-volume-fafe4099-3b2b-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:39:31.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q2bbt" for this suite.
Feb 28 07:39:37.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:39:37.347: INFO: namespace: e2e-tests-projected-q2bbt, resource: bindings, ignored listing per whitelist
Feb 28 07:39:37.512: INFO: namespace e2e-tests-projected-q2bbt deletion completed in 6.23990923s

• [SLOW TEST:8.670 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:39:37.512: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-k6zgk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-k6zgk
Feb 28 07:39:41.928: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-k6zgk
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 07:39:41.933: INFO: Initial restart count of pod liveness-http is 0
Feb 28 07:39:56.222: INFO: Restart count of pod e2e-tests-container-probe-k6zgk/liveness-http is now 1 (14.288283698s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:39:56.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-k6zgk" for this suite.
Feb 28 07:40:02.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:40:02.443: INFO: namespace: e2e-tests-container-probe-k6zgk, resource: bindings, ignored listing per whitelist
Feb 28 07:40:02.567: INFO: namespace e2e-tests-container-probe-k6zgk deletion completed in 6.327455924s

• [SLOW TEST:25.056 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:40:02.568: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-9nl26
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 28 07:40:02.916: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml cluster-info'
Feb 28 07:40:03.195: INFO: stderr: ""
Feb 28 07:40:03.195: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:40:03.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9nl26" for this suite.
Feb 28 07:40:09.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:40:09.397: INFO: namespace: e2e-tests-kubectl-9nl26, resource: bindings, ignored listing per whitelist
Feb 28 07:40:09.607: INFO: namespace e2e-tests-kubectl-9nl26 deletion completed in 6.405348556s

• [SLOW TEST:7.039 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:40:09.607: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-sfl8w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 28 07:40:10.119: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:10.638: INFO: stderr: ""
Feb 28 07:40:10.638: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 07:40:10.638: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:10.889: INFO: stderr: ""
Feb 28 07:40:10.889: INFO: stdout: "update-demo-nautilus-psqp5 update-demo-nautilus-zz68g "
Feb 28 07:40:10.890: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-psqp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:11.129: INFO: stderr: ""
Feb 28 07:40:11.129: INFO: stdout: ""
Feb 28 07:40:11.129: INFO: update-demo-nautilus-psqp5 is created but not running
Feb 28 07:40:16.129: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:16.346: INFO: stderr: ""
Feb 28 07:40:16.346: INFO: stdout: "update-demo-nautilus-psqp5 update-demo-nautilus-zz68g "
Feb 28 07:40:16.346: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-psqp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:16.504: INFO: stderr: ""
Feb 28 07:40:16.504: INFO: stdout: "true"
Feb 28 07:40:16.504: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-psqp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:16.704: INFO: stderr: ""
Feb 28 07:40:16.704: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:40:16.704: INFO: validating pod update-demo-nautilus-psqp5
Feb 28 07:40:16.796: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:40:16.796: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:40:16.796: INFO: update-demo-nautilus-psqp5 is verified up and running
Feb 28 07:40:16.796: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-zz68g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:16.960: INFO: stderr: ""
Feb 28 07:40:16.960: INFO: stdout: "true"
Feb 28 07:40:16.960: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-zz68g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:17.156: INFO: stderr: ""
Feb 28 07:40:17.156: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 07:40:17.156: INFO: validating pod update-demo-nautilus-zz68g
Feb 28 07:40:17.242: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 07:40:17.242: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 07:40:17.242: INFO: update-demo-nautilus-zz68g is verified up and running
STEP: using delete to clean up resources
Feb 28 07:40:17.242: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:17.364: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 07:40:17.364: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 07:40:17.364: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-sfl8w'
Feb 28 07:40:18.591: INFO: stderr: "No resources found.\n"
Feb 28 07:40:18.591: INFO: stdout: ""
Feb 28 07:40:18.591: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-sfl8w -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 07:40:18.725: INFO: stderr: ""
Feb 28 07:40:18.725: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:40:18.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-sfl8w" for this suite.
Feb 28 07:40:24.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:40:24.777: INFO: namespace: e2e-tests-kubectl-sfl8w, resource: bindings, ignored listing per whitelist
Feb 28 07:40:24.936: INFO: namespace e2e-tests-kubectl-sfl8w deletion completed in 6.200443045s

• [SLOW TEST:15.329 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:40:24.936: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-dml68
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-1c91a214-3b2c-11e9-b1ab-fe0431d33f49
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-1c91a214-3b2c-11e9-b1ab-fe0431d33f49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:40:29.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-dml68" for this suite.
Feb 28 07:40:51.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:40:51.940: INFO: namespace: e2e-tests-configmap-dml68, resource: bindings, ignored listing per whitelist
Feb 28 07:40:52.023: INFO: namespace e2e-tests-configmap-dml68 deletion completed in 22.278765764s

• [SLOW TEST:27.087 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:40:52.023: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hnzjs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-2cb76dfb-3b2c-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 07:40:52.652: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2cb8bf26-3b2c-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-hnzjs" to be "success or failure"
Feb 28 07:40:52.656: INFO: Pod "pod-projected-secrets-2cb8bf26-3b2c-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189773ms
Feb 28 07:40:54.661: INFO: Pod "pod-projected-secrets-2cb8bf26-3b2c-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008776523s
STEP: Saw pod success
Feb 28 07:40:54.661: INFO: Pod "pod-projected-secrets-2cb8bf26-3b2c-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:40:54.664: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-secrets-2cb8bf26-3b2c-11e9-b1ab-fe0431d33f49 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:40:54.894: INFO: Waiting for pod pod-projected-secrets-2cb8bf26-3b2c-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:40:54.899: INFO: Pod pod-projected-secrets-2cb8bf26-3b2c-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:40:54.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hnzjs" for this suite.
Feb 28 07:41:00.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:01.368: INFO: namespace: e2e-tests-projected-hnzjs, resource: bindings, ignored listing per whitelist
Feb 28 07:41:01.436: INFO: namespace e2e-tests-projected-hnzjs deletion completed in 6.523589356s

• [SLOW TEST:9.413 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:41:01.436: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-r2dwr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 28 07:41:01.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-r2dwr,SelfLink:/api/v1/namespaces/e2e-tests-watch-r2dwr/configmaps/e2e-watch-test-watch-closed,UID:32315574-3b2c-11e9-a4f3-1a76b5824175,ResourceVersion:3929,Generation:0,CreationTimestamp:2019-02-28 07:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 07:41:01.835: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-r2dwr,SelfLink:/api/v1/namespaces/e2e-tests-watch-r2dwr/configmaps/e2e-watch-test-watch-closed,UID:32315574-3b2c-11e9-a4f3-1a76b5824175,ResourceVersion:3930,Generation:0,CreationTimestamp:2019-02-28 07:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 28 07:41:01.855: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-r2dwr,SelfLink:/api/v1/namespaces/e2e-tests-watch-r2dwr/configmaps/e2e-watch-test-watch-closed,UID:32315574-3b2c-11e9-a4f3-1a76b5824175,ResourceVersion:3931,Generation:0,CreationTimestamp:2019-02-28 07:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 07:41:01.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-r2dwr,SelfLink:/api/v1/namespaces/e2e-tests-watch-r2dwr/configmaps/e2e-watch-test-watch-closed,UID:32315574-3b2c-11e9-a4f3-1a76b5824175,ResourceVersion:3932,Generation:0,CreationTimestamp:2019-02-28 07:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:41:01.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-r2dwr" for this suite.
Feb 28 07:41:07.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:07.943: INFO: namespace: e2e-tests-watch-r2dwr, resource: bindings, ignored listing per whitelist
Feb 28 07:41:08.094: INFO: namespace e2e-tests-watch-r2dwr deletion completed in 6.23235885s

• [SLOW TEST:6.658 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:41:08.095: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-2zzxz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 28 07:41:08.425: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 07:41:08.437: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 07:41:08.441: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc before test
Feb 28 07:41:08.465: INFO: vpn-shoot-c4d998c9f-45b9w from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 07:41:08.465: INFO: addons-kubernetes-dashboard-5f64f76bd-rd55c from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 07:41:08.465: INFO: coredns-5f4748c5f-lk8wm from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container coredns ready: true, restart count 0
Feb 28 07:41:08.465: INFO: addons-nginx-ingress-controller-8f975c795-fn6qg from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 07:41:08.465: INFO: kube-proxy-26xfb from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 07:41:08.465: INFO: calico-node-b4l87 from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 07:41:08.465: INFO: node-exporter-rm79f from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 07:41:08.465: INFO: blackbox-exporter-58fd9b8556-6kz6h from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 07:41:08.465: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-g2mjq from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 07:41:08.465: INFO: metrics-server-85674d74c8-6gl2l from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 07:41:08.465: INFO: addons-kube-lego-648f8c9f5c-r9z47 from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.465: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 07:41:08.465: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr before test
Feb 28 07:41:08.513: INFO: kube-proxy-2n298 from kube-system started at 2019-02-28 07:18:56 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.513: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 07:41:08.513: INFO: node-exporter-t6qsh from kube-system started at 2019-02-28 07:18:56 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.513: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 07:41:08.513: INFO: calico-node-stfff from kube-system started at 2019-02-28 07:18:56 +0000 UTC (1 container statuses recorded)
Feb 28 07:41:08.513: INFO: 	Container calico-node ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
STEP: verifying the node has the label node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr
Feb 28 07:41:08.571: INFO: Pod addons-kube-lego-648f8c9f5c-r9z47 requesting resource cpu=20m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod addons-kubernetes-dashboard-5f64f76bd-rd55c requesting resource cpu=50m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod addons-nginx-ingress-controller-8f975c795-fn6qg requesting resource cpu=100m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-g2mjq requesting resource cpu=0m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod blackbox-exporter-58fd9b8556-6kz6h requesting resource cpu=5m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod calico-node-b4l87 requesting resource cpu=100m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod calico-node-stfff requesting resource cpu=100m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr
Feb 28 07:41:08.571: INFO: Pod coredns-5f4748c5f-lk8wm requesting resource cpu=50m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod kube-proxy-26xfb requesting resource cpu=20m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod kube-proxy-2n298 requesting resource cpu=20m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr
Feb 28 07:41:08.571: INFO: Pod metrics-server-85674d74c8-6gl2l requesting resource cpu=20m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod node-exporter-rm79f requesting resource cpu=5m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
Feb 28 07:41:08.571: INFO: Pod node-exporter-t6qsh requesting resource cpu=5m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr
Feb 28 07:41:08.571: INFO: Pod vpn-shoot-c4d998c9f-45b9w requesting resource cpu=50m on Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-36377c80-3b2c-11e9-b1ab-fe0431d33f49.158777358f47516b], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-2zzxz/filler-pod-36377c80-3b2c-11e9-b1ab-fe0431d33f49 to shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-36377c80-3b2c-11e9-b1ab-fe0431d33f49.15877735b945ab16], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-36377c80-3b2c-11e9-b1ab-fe0431d33f49.15877735e97173a6], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-36377c80-3b2c-11e9-b1ab-fe0431d33f49.15877735ecfc8137], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-36377c80-3b2c-11e9-b1ab-fe0431d33f49.15877735f45b91a0], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3639022d-3b2c-11e9-b1ab-fe0431d33f49.158777358fc7d05a], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-2zzxz/filler-pod-3639022d-3b2c-11e9-b1ab-fe0431d33f49 to shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3639022d-3b2c-11e9-b1ab-fe0431d33f49.15877735b76198c7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3639022d-3b2c-11e9-b1ab-fe0431d33f49.15877735ba2aba92], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3639022d-3b2c-11e9-b1ab-fe0431d33f49.15877735c066c36a], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158777367fdd7d57], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:41:13.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-2zzxz" for this suite.
Feb 28 07:41:19.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:19.931: INFO: namespace: e2e-tests-sched-pred-2zzxz, resource: bindings, ignored listing per whitelist
Feb 28 07:41:19.937: INFO: namespace e2e-tests-sched-pred-2zzxz deletion completed in 6.261695694s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:11.843 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:41:19.937: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-2xt4d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-2xt4d
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-2xt4d to expose endpoints map[]
Feb 28 07:41:20.248: INFO: Get endpoints failed (5.272481ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 28 07:41:21.252: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-2xt4d exposes endpoints map[] (1.009927538s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-2xt4d
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-2xt4d to expose endpoints map[pod1:[80]]
Feb 28 07:41:22.285: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-2xt4d exposes endpoints map[pod1:[80]] (1.02262193s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-2xt4d
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-2xt4d to expose endpoints map[pod1:[80] pod2:[80]]
Feb 28 07:41:23.316: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-2xt4d exposes endpoints map[pod1:[80] pod2:[80]] (1.025303827s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-2xt4d
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-2xt4d to expose endpoints map[pod2:[80]]
Feb 28 07:41:23.329: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-2xt4d exposes endpoints map[pod2:[80]] (7.446019ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-2xt4d
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-2xt4d to expose endpoints map[]
Feb 28 07:41:24.358: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-2xt4d exposes endpoints map[] (1.024026119s elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:41:24.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-2xt4d" for this suite.
Feb 28 07:41:46.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:47.122: INFO: namespace: e2e-tests-services-2xt4d, resource: bindings, ignored listing per whitelist
Feb 28 07:41:47.143: INFO: namespace e2e-tests-services-2xt4d deletion completed in 22.75657422s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:27.206 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:41:47.143: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6cpn6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:41:47.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4da9d50a-3b2c-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-6cpn6" to be "success or failure"
Feb 28 07:41:47.922: INFO: Pod "downwardapi-volume-4da9d50a-3b2c-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.854098ms
Feb 28 07:41:49.930: INFO: Pod "downwardapi-volume-4da9d50a-3b2c-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012704669s
STEP: Saw pod success
Feb 28 07:41:49.930: INFO: Pod "downwardapi-volume-4da9d50a-3b2c-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:41:49.936: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-4da9d50a-3b2c-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 07:41:49.961: INFO: Waiting for pod downwardapi-volume-4da9d50a-3b2c-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:41:49.968: INFO: Pod downwardapi-volume-4da9d50a-3b2c-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:41:49.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6cpn6" for this suite.
Feb 28 07:41:56.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:41:56.109: INFO: namespace: e2e-tests-projected-6cpn6, resource: bindings, ignored listing per whitelist
Feb 28 07:41:56.214: INFO: namespace e2e-tests-projected-6cpn6 deletion completed in 6.238098836s

• [SLOW TEST:9.071 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:41:56.214: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-s52cq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 07:41:56.528: INFO: Waiting up to 5m0s for pod "pod-52cba495-3b2c-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-s52cq" to be "success or failure"
Feb 28 07:41:56.532: INFO: Pod "pod-52cba495-3b2c-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.957307ms
Feb 28 07:41:58.542: INFO: Pod "pod-52cba495-3b2c-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014175044s
STEP: Saw pod success
Feb 28 07:41:58.542: INFO: Pod "pod-52cba495-3b2c-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:41:58.554: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-52cba495-3b2c-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 07:41:58.581: INFO: Waiting for pod pod-52cba495-3b2c-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:41:58.588: INFO: Pod pod-52cba495-3b2c-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:41:58.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-s52cq" for this suite.
Feb 28 07:42:04.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:42:04.945: INFO: namespace: e2e-tests-emptydir-s52cq, resource: bindings, ignored listing per whitelist
Feb 28 07:42:04.999: INFO: namespace e2e-tests-emptydir-s52cq deletion completed in 6.399327241s

• [SLOW TEST:8.785 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:42:04.999: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jsj59
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-583e7810-3b2c-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 07:42:05.670: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-583f0f0a-3b2c-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-jsj59" to be "success or failure"
Feb 28 07:42:05.681: INFO: Pod "pod-projected-secrets-583f0f0a-3b2c-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 11.364255ms
Feb 28 07:42:07.693: INFO: Pod "pod-projected-secrets-583f0f0a-3b2c-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023372406s
STEP: Saw pod success
Feb 28 07:42:07.693: INFO: Pod "pod-projected-secrets-583f0f0a-3b2c-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:42:07.704: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-secrets-583f0f0a-3b2c-11e9-b1ab-fe0431d33f49 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:42:07.735: INFO: Waiting for pod pod-projected-secrets-583f0f0a-3b2c-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:42:07.740: INFO: Pod pod-projected-secrets-583f0f0a-3b2c-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:42:07.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jsj59" for this suite.
Feb 28 07:42:13.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:42:14.309: INFO: namespace: e2e-tests-projected-jsj59, resource: bindings, ignored listing per whitelist
Feb 28 07:42:14.329: INFO: namespace e2e-tests-projected-jsj59 deletion completed in 6.577122671s

• [SLOW TEST:9.329 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:42:14.329: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-n5j29
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 07:42:14.748: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-n5j29'
Feb 28 07:42:14.995: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 07:42:14.995: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 28 07:42:15.233: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-fmzjt]
Feb 28 07:42:15.233: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-fmzjt" in namespace "e2e-tests-kubectl-n5j29" to be "running and ready"
Feb 28 07:42:15.239: INFO: Pod "e2e-test-nginx-rc-fmzjt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082237ms
Feb 28 07:42:17.248: INFO: Pod "e2e-test-nginx-rc-fmzjt": Phase="Running", Reason="", readiness=true. Elapsed: 2.014723065s
Feb 28 07:42:17.248: INFO: Pod "e2e-test-nginx-rc-fmzjt" satisfied condition "running and ready"
Feb 28 07:42:17.248: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-fmzjt]
Feb 28 07:42:17.248: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-n5j29'
Feb 28 07:42:17.392: INFO: stderr: ""
Feb 28 07:42:17.392: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
Feb 28 07:42:17.392: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-n5j29'
Feb 28 07:42:18.767: INFO: stderr: ""
Feb 28 07:42:18.767: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:42:18.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n5j29" for this suite.
Feb 28 07:42:40.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:42:40.939: INFO: namespace: e2e-tests-kubectl-n5j29, resource: bindings, ignored listing per whitelist
Feb 28 07:42:41.081: INFO: namespace e2e-tests-kubectl-n5j29 deletion completed in 22.306403907s

• [SLOW TEST:26.752 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:42:41.082: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-brcpc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 07:42:41.423: INFO: Waiting up to 5m0s for pod "pod-6d8e0afb-3b2c-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-brcpc" to be "success or failure"
Feb 28 07:42:41.429: INFO: Pod "pod-6d8e0afb-3b2c-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.189141ms
Feb 28 07:42:43.439: INFO: Pod "pod-6d8e0afb-3b2c-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016185616s
STEP: Saw pod success
Feb 28 07:42:43.439: INFO: Pod "pod-6d8e0afb-3b2c-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:42:43.444: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-6d8e0afb-3b2c-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 07:42:43.509: INFO: Waiting for pod pod-6d8e0afb-3b2c-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:42:43.516: INFO: Pod pod-6d8e0afb-3b2c-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:42:43.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-brcpc" for this suite.
Feb 28 07:42:49.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:42:49.956: INFO: namespace: e2e-tests-emptydir-brcpc, resource: bindings, ignored listing per whitelist
Feb 28 07:42:49.974: INFO: namespace e2e-tests-emptydir-brcpc deletion completed in 6.45213005s

• [SLOW TEST:8.892 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:42:49.975: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-v4f55
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 28 07:42:50.330: INFO: Waiting up to 5m0s for pod "client-containers-72dd229f-3b2c-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-containers-v4f55" to be "success or failure"
Feb 28 07:42:50.339: INFO: Pod "client-containers-72dd229f-3b2c-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 8.807729ms
Feb 28 07:42:52.346: INFO: Pod "client-containers-72dd229f-3b2c-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016316774s
STEP: Saw pod success
Feb 28 07:42:52.346: INFO: Pod "client-containers-72dd229f-3b2c-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:42:52.354: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod client-containers-72dd229f-3b2c-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 07:42:52.380: INFO: Waiting for pod client-containers-72dd229f-3b2c-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:42:52.384: INFO: Pod client-containers-72dd229f-3b2c-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:42:52.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-v4f55" for this suite.
Feb 28 07:42:58.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:42:58.745: INFO: namespace: e2e-tests-containers-v4f55, resource: bindings, ignored listing per whitelist
Feb 28 07:42:58.771: INFO: namespace e2e-tests-containers-v4f55 deletion completed in 6.379638689s

• [SLOW TEST:8.796 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:42:58.771: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-t4r7n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-t4r7n.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-t4r7n.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-t4r7n.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-t4r7n.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-t4r7n.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-t4r7n.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 07:43:23.121: INFO: DNS probes using e2e-tests-dns-t4r7n/dns-test-7848b254-3b2c-11e9-b1ab-fe0431d33f49 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:43:23.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-t4r7n" for this suite.
Feb 28 07:43:29.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:43:29.263: INFO: namespace: e2e-tests-dns-t4r7n, resource: bindings, ignored listing per whitelist
Feb 28 07:43:29.320: INFO: namespace e2e-tests-dns-t4r7n deletion completed in 6.181183372s

• [SLOW TEST:30.549 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:43:29.320: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zrmkh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:43:29.605: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml version'
Feb 28 07:43:29.914: INFO: stderr: ""
Feb 28 07:43:29.914: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-28T07:30:42Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"clean\", BuildDate:\"2019-01-16T18:14:49Z\", GoVersion:\"go1.10.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:43:29.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zrmkh" for this suite.
Feb 28 07:43:35.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:43:36.025: INFO: namespace: e2e-tests-kubectl-zrmkh, resource: bindings, ignored listing per whitelist
Feb 28 07:43:36.161: INFO: namespace e2e-tests-kubectl-zrmkh deletion completed in 6.241741964s

• [SLOW TEST:6.841 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:43:36.161: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-sqh57
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 07:43:44.459: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:43:44.463: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:43:46.463: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:43:46.469: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:43:48.463: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:43:48.468: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:43:50.463: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:43:50.468: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:43:52.463: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:43:52.468: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:43:54.463: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:43:54.469: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:43:56.463: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:43:56.469: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:43:58.463: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:43:58.468: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:44:00.464: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:44:00.470: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:44:02.463: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:44:02.468: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:44:04.464: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:44:04.469: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 07:44:06.464: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 07:44:06.471: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:44:06.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-sqh57" for this suite.
Feb 28 07:44:28.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:44:28.670: INFO: namespace: e2e-tests-container-lifecycle-hook-sqh57, resource: bindings, ignored listing per whitelist
Feb 28 07:44:28.723: INFO: namespace e2e-tests-container-lifecycle-hook-sqh57 deletion completed in 22.236631454s

• [SLOW TEST:52.562 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:44:28.723: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pkzbj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-adb05747-3b2c-11e9-b1ab-fe0431d33f49
STEP: Creating secret with name secret-projected-all-test-volume-adb0572b-3b2c-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 28 07:44:29.027: INFO: Waiting up to 5m0s for pod "projected-volume-adb056d9-3b2c-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-pkzbj" to be "success or failure"
Feb 28 07:44:29.031: INFO: Pod "projected-volume-adb056d9-3b2c-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.444177ms
Feb 28 07:44:31.040: INFO: Pod "projected-volume-adb056d9-3b2c-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012414634s
STEP: Saw pod success
Feb 28 07:44:31.040: INFO: Pod "projected-volume-adb056d9-3b2c-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:44:31.044: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod projected-volume-adb056d9-3b2c-11e9-b1ab-fe0431d33f49 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 28 07:44:31.062: INFO: Waiting for pod projected-volume-adb056d9-3b2c-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:44:31.065: INFO: Pod projected-volume-adb056d9-3b2c-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:44:31.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pkzbj" for this suite.
Feb 28 07:44:37.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:44:37.310: INFO: namespace: e2e-tests-projected-pkzbj, resource: bindings, ignored listing per whitelist
Feb 28 07:44:37.334: INFO: namespace e2e-tests-projected-pkzbj deletion completed in 6.265692303s

• [SLOW TEST:8.611 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:44:37.335: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-cvfd4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:44:37.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2de9b44-3b2c-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-cvfd4" to be "success or failure"
Feb 28 07:44:37.718: INFO: Pod "downwardapi-volume-b2de9b44-3b2c-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538684ms
Feb 28 07:44:39.722: INFO: Pod "downwardapi-volume-b2de9b44-3b2c-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010465092s
STEP: Saw pod success
Feb 28 07:44:39.723: INFO: Pod "downwardapi-volume-b2de9b44-3b2c-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:44:39.726: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-b2de9b44-3b2c-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 07:44:39.744: INFO: Waiting for pod downwardapi-volume-b2de9b44-3b2c-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:44:39.747: INFO: Pod downwardapi-volume-b2de9b44-3b2c-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:44:39.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cvfd4" for this suite.
Feb 28 07:44:45.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:44:45.829: INFO: namespace: e2e-tests-projected-cvfd4, resource: bindings, ignored listing per whitelist
Feb 28 07:44:45.969: INFO: namespace e2e-tests-projected-cvfd4 deletion completed in 6.217993387s

• [SLOW TEST:8.634 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:44:45.970: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-g9q9w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-g9q9w
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 28 07:44:46.319: INFO: Found 0 stateful pods, waiting for 3
Feb 28 07:44:56.326: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:44:56.326: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:44:56.326: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 07:44:56.339: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-g9q9w ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:44:56.917: INFO: stderr: ""
Feb 28 07:44:56.917: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:44:56.917: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 07:45:06.954: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 28 07:45:16.980: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-g9q9w ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:45:17.522: INFO: stderr: ""
Feb 28 07:45:17.522: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:45:17.522: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:45:27.554: INFO: Waiting for StatefulSet e2e-tests-statefulset-g9q9w/ss2 to complete update
Feb 28 07:45:27.554: INFO: Waiting for Pod e2e-tests-statefulset-g9q9w/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 28 07:45:27.554: INFO: Waiting for Pod e2e-tests-statefulset-g9q9w/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 28 07:45:37.563: INFO: Waiting for StatefulSet e2e-tests-statefulset-g9q9w/ss2 to complete update
Feb 28 07:45:37.563: INFO: Waiting for Pod e2e-tests-statefulset-g9q9w/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Feb 28 07:45:47.564: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-g9q9w ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 07:45:48.074: INFO: stderr: ""
Feb 28 07:45:48.074: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 07:45:48.074: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 07:45:58.116: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 28 07:46:08.135: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-g9q9w ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 07:46:08.648: INFO: stderr: ""
Feb 28 07:46:08.648: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 07:46:08.648: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 07:46:28.675: INFO: Waiting for StatefulSet e2e-tests-statefulset-g9q9w/ss2 to complete update
Feb 28 07:46:28.675: INFO: Waiting for Pod e2e-tests-statefulset-g9q9w/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 07:46:38.688: INFO: Deleting all statefulset in ns e2e-tests-statefulset-g9q9w
Feb 28 07:46:38.694: INFO: Scaling statefulset ss2 to 0
Feb 28 07:46:58.726: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 07:46:58.730: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:46:58.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-g9q9w" for this suite.
Feb 28 07:47:04.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:47:04.921: INFO: namespace: e2e-tests-statefulset-g9q9w, resource: bindings, ignored listing per whitelist
Feb 28 07:47:04.959: INFO: namespace e2e-tests-statefulset-g9q9w deletion completed in 6.20879531s

• [SLOW TEST:138.989 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:47:04.959: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-75k5k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:47:05.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0aca2bea-3b2d-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-75k5k" to be "success or failure"
Feb 28 07:47:05.221: INFO: Pod "downwardapi-volume-0aca2bea-3b2d-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.460712ms
Feb 28 07:47:07.226: INFO: Pod "downwardapi-volume-0aca2bea-3b2d-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008357385s
STEP: Saw pod success
Feb 28 07:47:07.226: INFO: Pod "downwardapi-volume-0aca2bea-3b2d-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:47:07.230: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-0aca2bea-3b2d-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 07:47:07.255: INFO: Waiting for pod downwardapi-volume-0aca2bea-3b2d-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:47:07.258: INFO: Pod downwardapi-volume-0aca2bea-3b2d-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:47:07.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-75k5k" for this suite.
Feb 28 07:47:13.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:47:13.483: INFO: namespace: e2e-tests-projected-75k5k, resource: bindings, ignored listing per whitelist
Feb 28 07:47:13.515: INFO: namespace e2e-tests-projected-75k5k deletion completed in 6.251944042s

• [SLOW TEST:8.556 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:47:13.515: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-6ttn4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 28 07:47:15.838: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-0fea924c-3b2d-11e9-b1ab-fe0431d33f49,GenerateName:,Namespace:e2e-tests-events-6ttn4,SelfLink:/api/v1/namespaces/e2e-tests-events-6ttn4/pods/send-events-0fea924c-3b2d-11e9-b1ab-fe0431d33f49,UID:0feac873-3b2d-11e9-a4f3-1a76b5824175,ResourceVersion:5175,Generation:0,CreationTimestamp:2019-02-28 07:47:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 810905584,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.43/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wpldw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wpldw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-wpldw true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00226ea90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00226eab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 07:47:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.1.43,StartTime:2019-02-28 07:47:13 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-28 07:47:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://5805c3be33528027e024d63c015d926d743e2d9dfed2f73acaf34c3e965f095f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 28 07:47:17.844: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 28 07:47:19.849: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:47:19.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-6ttn4" for this suite.
Feb 28 07:47:57.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:47:58.052: INFO: namespace: e2e-tests-events-6ttn4, resource: bindings, ignored listing per whitelist
Feb 28 07:47:58.083: INFO: namespace e2e-tests-events-6ttn4 deletion completed in 38.222524578s

• [SLOW TEST:44.568 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:47:58.084: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-t227s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:47:58.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-t227s" for this suite.
Feb 28 07:48:04.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:04.419: INFO: namespace: e2e-tests-services-t227s, resource: bindings, ignored listing per whitelist
Feb 28 07:48:04.466: INFO: namespace e2e-tests-services-t227s deletion completed in 6.141694566s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:6.382 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:48:04.466: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5k8w9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:48:04.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e4125ea-3b2d-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-5k8w9" to be "success or failure"
Feb 28 07:48:04.720: INFO: Pod "downwardapi-volume-2e4125ea-3b2d-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.578358ms
Feb 28 07:48:06.725: INFO: Pod "downwardapi-volume-2e4125ea-3b2d-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008695303s
STEP: Saw pod success
Feb 28 07:48:06.725: INFO: Pod "downwardapi-volume-2e4125ea-3b2d-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:48:06.728: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-2e4125ea-3b2d-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 07:48:06.751: INFO: Waiting for pod downwardapi-volume-2e4125ea-3b2d-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:48:06.755: INFO: Pod downwardapi-volume-2e4125ea-3b2d-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:48:06.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5k8w9" for this suite.
Feb 28 07:48:12.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:48:12.928: INFO: namespace: e2e-tests-projected-5k8w9, resource: bindings, ignored listing per whitelist
Feb 28 07:48:12.937: INFO: namespace e2e-tests-projected-5k8w9 deletion completed in 6.177449324s

• [SLOW TEST:8.472 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:48:12.938: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-586mm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 07:48:17.262: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:17.267: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:19.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:19.273: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:21.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:21.272: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:23.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:23.273: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:25.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:25.275: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:27.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:27.272: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:29.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:29.272: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:31.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:31.273: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:33.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:33.273: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:35.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:35.275: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:37.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:37.273: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:39.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:39.272: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:41.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:41.273: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:43.268: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:43.273: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 07:48:45.267: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 07:48:45.274: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:48:45.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-586mm" for this suite.
Feb 28 07:49:07.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:07.458: INFO: namespace: e2e-tests-container-lifecycle-hook-586mm, resource: bindings, ignored listing per whitelist
Feb 28 07:49:07.470: INFO: namespace e2e-tests-container-lifecycle-hook-586mm deletion completed in 22.190596994s

• [SLOW TEST:54.533 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:49:07.471: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-v79kx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-v79kx
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 07:49:07.807: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 07:49:31.899: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.1.47:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-v79kx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:49:31.901: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 07:49:32.421: INFO: Found all expected endpoints: [netserver-0]
Feb 28 07:49:32.426: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.0.18:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-v79kx PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:49:32.426: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 07:49:32.793: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:49:32.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-v79kx" for this suite.
Feb 28 07:49:54.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:49:54.871: INFO: namespace: e2e-tests-pod-network-test-v79kx, resource: bindings, ignored listing per whitelist
Feb 28 07:49:54.952: INFO: namespace e2e-tests-pod-network-test-v79kx deletion completed in 22.154131522s

• [SLOW TEST:47.482 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:49:54.953: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-4lgnn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-4lgnn
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4lgnn to expose endpoints map[]
Feb 28 07:49:55.222: INFO: Get endpoints failed (3.710969ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 28 07:49:56.227: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4lgnn exposes endpoints map[] (1.008503257s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-4lgnn
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4lgnn to expose endpoints map[pod1:[100]]
Feb 28 07:49:57.461: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4lgnn exposes endpoints map[pod1:[100]] (1.22568554s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-4lgnn
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4lgnn to expose endpoints map[pod1:[100] pod2:[101]]
Feb 28 07:49:59.504: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4lgnn exposes endpoints map[pod1:[100] pod2:[101]] (2.038495912s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-4lgnn
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4lgnn to expose endpoints map[pod2:[101]]
Feb 28 07:49:59.516: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4lgnn exposes endpoints map[pod2:[101]] (6.823868ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-4lgnn
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-4lgnn to expose endpoints map[]
Feb 28 07:50:00.528: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-4lgnn exposes endpoints map[] (1.007309427s elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:50:00.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-4lgnn" for this suite.
Feb 28 07:50:06.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:06.692: INFO: namespace: e2e-tests-services-4lgnn, resource: bindings, ignored listing per whitelist
Feb 28 07:50:06.758: INFO: namespace e2e-tests-services-4lgnn deletion completed in 6.207180424s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:11.805 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:50:06.758: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-8nmbs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-7727887d-3b2d-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 07:50:07.026: INFO: Waiting up to 5m0s for pod "pod-secrets-77281be9-3b2d-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-secrets-8nmbs" to be "success or failure"
Feb 28 07:50:07.030: INFO: Pod "pod-secrets-77281be9-3b2d-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.842207ms
Feb 28 07:50:09.035: INFO: Pod "pod-secrets-77281be9-3b2d-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009312875s
STEP: Saw pod success
Feb 28 07:50:09.035: INFO: Pod "pod-secrets-77281be9-3b2d-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:50:09.039: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-secrets-77281be9-3b2d-11e9-b1ab-fe0431d33f49 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:50:09.060: INFO: Waiting for pod pod-secrets-77281be9-3b2d-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:50:09.063: INFO: Pod pod-secrets-77281be9-3b2d-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:50:09.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-8nmbs" for this suite.
Feb 28 07:50:15.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:15.263: INFO: namespace: e2e-tests-secrets-8nmbs, resource: bindings, ignored listing per whitelist
Feb 28 07:50:15.374: INFO: namespace e2e-tests-secrets-8nmbs deletion completed in 6.306763916s

• [SLOW TEST:8.615 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:50:15.374: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-pgw5x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 28 07:50:16.204: INFO: created pod pod-service-account-defaultsa
Feb 28 07:50:16.204: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 28 07:50:16.208: INFO: created pod pod-service-account-mountsa
Feb 28 07:50:16.208: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 28 07:50:16.212: INFO: created pod pod-service-account-nomountsa
Feb 28 07:50:16.212: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 28 07:50:16.217: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 28 07:50:16.217: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 28 07:50:16.220: INFO: created pod pod-service-account-mountsa-mountspec
Feb 28 07:50:16.220: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 28 07:50:16.225: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 28 07:50:16.225: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 28 07:50:16.230: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 28 07:50:16.230: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 28 07:50:16.235: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 28 07:50:16.235: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 28 07:50:16.240: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 28 07:50:16.240: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:50:16.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-pgw5x" for this suite.
Feb 28 07:50:38.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:50:38.537: INFO: namespace: e2e-tests-svcaccounts-pgw5x, resource: bindings, ignored listing per whitelist
Feb 28 07:50:38.644: INFO: namespace e2e-tests-svcaccounts-pgw5x deletion completed in 22.399132828s

• [SLOW TEST:23.270 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:50:38.645: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-hbq55
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-hbq55
Feb 28 07:50:40.926: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-hbq55
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 07:50:40.929: INFO: Initial restart count of pod liveness-http is 0
Feb 28 07:50:54.979: INFO: Restart count of pod e2e-tests-container-probe-hbq55/liveness-http is now 1 (14.050407637s elapsed)
Feb 28 07:51:15.032: INFO: Restart count of pod e2e-tests-container-probe-hbq55/liveness-http is now 2 (34.10271069s elapsed)
Feb 28 07:51:35.085: INFO: Restart count of pod e2e-tests-container-probe-hbq55/liveness-http is now 3 (54.155744591s elapsed)
Feb 28 07:51:55.136: INFO: Restart count of pod e2e-tests-container-probe-hbq55/liveness-http is now 4 (1m14.206886979s elapsed)
Feb 28 07:52:55.296: INFO: Restart count of pod e2e-tests-container-probe-hbq55/liveness-http is now 5 (2m14.366773264s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:52:55.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-hbq55" for this suite.
Feb 28 07:53:01.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:01.449: INFO: namespace: e2e-tests-container-probe-hbq55, resource: bindings, ignored listing per whitelist
Feb 28 07:53:01.505: INFO: namespace e2e-tests-container-probe-hbq55 deletion completed in 6.188381415s

• [SLOW TEST:142.861 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:53:01.507: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-zqm4q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-df56eb7e-3b2d-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 07:53:01.819: INFO: Waiting up to 5m0s for pod "pod-configmaps-df577968-3b2d-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-configmap-zqm4q" to be "success or failure"
Feb 28 07:53:01.823: INFO: Pod "pod-configmaps-df577968-3b2d-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064976ms
Feb 28 07:53:03.829: INFO: Pod "pod-configmaps-df577968-3b2d-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009357544s
STEP: Saw pod success
Feb 28 07:53:03.829: INFO: Pod "pod-configmaps-df577968-3b2d-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:53:03.832: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-df577968-3b2d-11e9-b1ab-fe0431d33f49 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:53:03.854: INFO: Waiting for pod pod-configmaps-df577968-3b2d-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:53:03.858: INFO: Pod pod-configmaps-df577968-3b2d-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:53:03.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zqm4q" for this suite.
Feb 28 07:53:09.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:10.046: INFO: namespace: e2e-tests-configmap-zqm4q, resource: bindings, ignored listing per whitelist
Feb 28 07:53:10.083: INFO: namespace e2e-tests-configmap-zqm4q deletion completed in 6.220374107s

• [SLOW TEST:8.575 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:53:10.083: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-fvwbl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-fvwbl
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 07:53:10.402: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 07:53:26.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.59:8080/dial?request=hostName&protocol=http&host=100.96.0.24&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-fvwbl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:53:26.488: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 07:53:26.909: INFO: Waiting for endpoints: map[]
Feb 28 07:53:26.938: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.59:8080/dial?request=hostName&protocol=http&host=100.96.1.58&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-fvwbl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 07:53:26.938: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 07:53:27.335: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:53:27.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-fvwbl" for this suite.
Feb 28 07:53:49.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:53:49.471: INFO: namespace: e2e-tests-pod-network-test-fvwbl, resource: bindings, ignored listing per whitelist
Feb 28 07:53:49.495: INFO: namespace e2e-tests-pod-network-test-fvwbl deletion completed in 22.152818687s

• [SLOW TEST:39.411 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:53:49.495: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-x662c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 07:53:49.809: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-x662c'
Feb 28 07:53:50.538: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 07:53:50.538: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
Feb 28 07:53:52.548: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-x662c'
Feb 28 07:53:52.656: INFO: stderr: ""
Feb 28 07:53:52.656: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:53:52.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x662c" for this suite.
Feb 28 07:54:14.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:14.765: INFO: namespace: e2e-tests-kubectl-x662c, resource: bindings, ignored listing per whitelist
Feb 28 07:54:14.816: INFO: namespace e2e-tests-kubectl-x662c deletion completed in 22.154788036s

• [SLOW TEST:25.321 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:54:14.816: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-r4ppq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:54:15.130: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 28 07:54:15.141: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-r4ppq/daemonsets","resourceVersion":"6367"},"items":null}

Feb 28 07:54:15.144: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-r4ppq/pods","resourceVersion":"6367"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:54:15.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-r4ppq" for this suite.
Feb 28 07:54:21.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:21.227: INFO: namespace: e2e-tests-daemonsets-r4ppq, resource: bindings, ignored listing per whitelist
Feb 28 07:54:21.385: INFO: namespace e2e-tests-daemonsets-r4ppq deletion completed in 6.225929518s

S [SKIPPING] [6.569 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 28 07:54:15.130: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:54:21.386: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-29b88
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 07:54:24.248: INFO: Successfully updated pod "annotationupdate0ef5e0c6-3b2e-11e9-b1ab-fe0431d33f49"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:54:26.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-29b88" for this suite.
Feb 28 07:54:48.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:54:48.344: INFO: namespace: e2e-tests-downward-api-29b88, resource: bindings, ignored listing per whitelist
Feb 28 07:54:48.475: INFO: namespace e2e-tests-downward-api-29b88 deletion completed in 22.199870647s

• [SLOW TEST:27.089 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:54:48.475: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5tgmh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:54:48.906: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml version --client'
Feb 28 07:54:48.989: INFO: stderr: ""
Feb 28 07:54:48.989: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-28T07:30:42Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 28 07:54:48.992: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-5tgmh'
Feb 28 07:54:49.251: INFO: stderr: ""
Feb 28 07:54:49.251: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 28 07:54:49.251: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-5tgmh'
Feb 28 07:54:49.507: INFO: stderr: ""
Feb 28 07:54:49.507: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 07:54:50.513: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:54:50.513: INFO: Found 1 / 1
Feb 28 07:54:50.513: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 07:54:50.518: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 07:54:50.518: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 07:54:50.518: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe pod redis-master-5th75 --namespace=e2e-tests-kubectl-5tgmh'
Feb 28 07:54:50.645: INFO: stderr: ""
Feb 28 07:54:50.645: INFO: stdout: "Name:               redis-master-5th75\nNamespace:          e2e-tests-kubectl-5tgmh\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr/10.250.0.18\nStart Time:         Thu, 28 Feb 2019 07:54:49 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.62/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.62\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://688d5f811f9a9bb7147f20bccb18cdd9e2953bc75accebba5b788efaeb20aac9\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 28 Feb 2019 07:54:50 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8c9rs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-8c9rs:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-8c9rs\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                             Message\n  ----    ------     ----  ----                                                             -------\n  Normal  Scheduled  1s    default-scheduler                                                Successfully assigned e2e-tests-kubectl-5tgmh/redis-master-5th75 to shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr\n  Normal  Pulled     1s    kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Created container\n  Normal  Started    0s    kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Started container\n"
Feb 28 07:54:50.645: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-5tgmh'
Feb 28 07:54:50.824: INFO: stderr: ""
Feb 28 07:54:50.824: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-5tgmh\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: redis-master-5th75\n"
Feb 28 07:54:50.824: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-5tgmh'
Feb 28 07:54:50.943: INFO: stderr: ""
Feb 28 07:54:50.943: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-5tgmh\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.69.164.187\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.62:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 28 07:54:50.948: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc'
Feb 28 07:54:51.110: INFO: stderr: ""
Feb 28 07:54:51.110: INFO: stdout: "Name:               shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecf69421-6238-4433-a0a0-70bb24198e09\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/zone=rot_1_1\n                    kubernetes.io/hostname=shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.11/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 28 Feb 2019 07:18:53 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  OutOfDisk        False   Thu, 28 Feb 2019 07:54:47 +0000   Thu, 28 Feb 2019 07:18:53 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure   False   Thu, 28 Feb 2019 07:54:47 +0000   Thu, 28 Feb 2019 07:18:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 28 Feb 2019 07:54:47 +0000   Thu, 28 Feb 2019 07:18:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 28 Feb 2019 07:54:47 +0000   Thu, 28 Feb 2019 07:18:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 28 Feb 2019 07:54:47 +0000   Thu, 28 Feb 2019 07:19:23 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.0.11\nCapacity:\n cpu:                2\n ephemeral-storage:  38216108Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             4042360Ki\n pods:               110\nAllocatable:\n cpu:                1920m\n ephemeral-storage:  37176629834\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             2858665981\n pods:               110\nSystem Info:\n Machine ID:                 54bab336129c4370949473049a366d75\n System UUID:                54BAB336-129C-4370-9494-73049A366D75\n Boot ID:                    1b94b523-4507-45c2-a6b6-80c54ff6d65c\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.12.5\n Kube-Proxy Version:         v1.12.5\nPodCIDR:                     100.96.0.0/24\nProviderID:                  openstack:///54bab336-129c-4370-9494-73049a366d75\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------\n  kube-system                addons-kube-lego-648f8c9f5c-r9z47                                  20m (1%)      50m (2%)    8Mi (0%)         32Mi (1%)\n  kube-system                addons-kubernetes-dashboard-5f64f76bd-rd55c                        50m (2%)      100m (5%)   50Mi (1%)        256Mi (9%)\n  kube-system                addons-nginx-ingress-controller-8f975c795-fn6qg                    100m (5%)     2 (104%)    100Mi (3%)       800Mi (29%)\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-g2mjq    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                blackbox-exporter-58fd9b8556-6kz6h                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (1%)\n  kube-system                calico-node-b4l87                                                  100m (5%)     500m (26%)  100Mi (3%)       700Mi (25%)\n  kube-system                coredns-5f4748c5f-lk8wm                                            50m (2%)      100m (5%)   15Mi (0%)        100Mi (3%)\n  kube-system                kube-proxy-26xfb                                                   20m (1%)      900m (46%)  64Mi (2%)        200Mi (7%)\n  kube-system                metrics-server-85674d74c8-6gl2l                                    20m (1%)      80m (4%)    100Mi (3%)       400Mi (14%)\n  kube-system                node-exporter-rm79f                                                5m (0%)       15m (0%)    10Mi (0%)        50Mi (1%)\n  kube-system                vpn-shoot-c4d998c9f-45b9w                                          50m (2%)      100m (5%)   50Mi (1%)        100Mi (3%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource  Requests     Limits\n  --------  --------     ------\n  cpu       420m (21%)   3855m (200%)\n  memory    502Mi (18%)  2673Mi (98%)\nEvents:\n  Type    Reason                   Age                From                                                                Message\n  ----    ------                   ----               ----                                                                -------\n  Normal  Starting                 35m                kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc     Starting kubelet.\n  Normal  NodeHasSufficientDisk    35m (x2 over 35m)  kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc     Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc status is now: NodeHasSufficientDisk\n  Normal  NodeHasSufficientMemory  35m (x2 over 35m)  kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc     Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    35m (x2 over 35m)  kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc     Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     35m (x2 over 35m)  kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc     Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  35m                kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc     Updated Node Allocatable limit across pods\n  Normal  Starting                 35m                kube-proxy, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Starting kube-proxy.\n  Normal  NodeReady                35m                kubelet, shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc     Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc status is now: NodeReady\n"
Feb 28 07:54:51.110: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml describe namespace e2e-tests-kubectl-5tgmh'
Feb 28 07:54:51.302: INFO: stderr: ""
Feb 28 07:54:51.302: INFO: stdout: "Name:         e2e-tests-kubectl-5tgmh\nLabels:       e2e-framework=kubectl\n              e2e-run=3a72f277-3b2b-11e9-b1ab-fe0431d33f49\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:54:51.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5tgmh" for this suite.
Feb 28 07:55:13.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:13.683: INFO: namespace: e2e-tests-kubectl-5tgmh, resource: bindings, ignored listing per whitelist
Feb 28 07:55:13.756: INFO: namespace e2e-tests-kubectl-5tgmh deletion completed in 22.449942391s

• [SLOW TEST:25.281 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:55:13.756: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-mc72c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:55:14.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e444fcb-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-mc72c" to be "success or failure"
Feb 28 07:55:14.243: INFO: Pod "downwardapi-volume-2e444fcb-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.954643ms
Feb 28 07:55:16.249: INFO: Pod "downwardapi-volume-2e444fcb-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011924622s
STEP: Saw pod success
Feb 28 07:55:16.249: INFO: Pod "downwardapi-volume-2e444fcb-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:55:16.254: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-2e444fcb-3b2e-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 07:55:16.272: INFO: Waiting for pod downwardapi-volume-2e444fcb-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:55:16.275: INFO: Pod downwardapi-volume-2e444fcb-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:55:16.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mc72c" for this suite.
Feb 28 07:55:22.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:22.430: INFO: namespace: e2e-tests-downward-api-mc72c, resource: bindings, ignored listing per whitelist
Feb 28 07:55:22.436: INFO: namespace e2e-tests-downward-api-mc72c deletion completed in 6.155461298s

• [SLOW TEST:8.680 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:55:22.436: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-nqd26
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-33524c56-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 07:55:22.720: INFO: Waiting up to 5m0s for pod "pod-secrets-33533168-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-secrets-nqd26" to be "success or failure"
Feb 28 07:55:22.724: INFO: Pod "pod-secrets-33533168-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.176068ms
Feb 28 07:55:24.729: INFO: Pod "pod-secrets-33533168-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009007417s
STEP: Saw pod success
Feb 28 07:55:24.729: INFO: Pod "pod-secrets-33533168-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:55:24.734: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-secrets-33533168-3b2e-11e9-b1ab-fe0431d33f49 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:55:24.821: INFO: Waiting for pod pod-secrets-33533168-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:55:24.823: INFO: Pod pod-secrets-33533168-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:55:24.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nqd26" for this suite.
Feb 28 07:55:30.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:30.886: INFO: namespace: e2e-tests-secrets-nqd26, resource: bindings, ignored listing per whitelist
Feb 28 07:55:31.017: INFO: namespace e2e-tests-secrets-nqd26 deletion completed in 6.188495156s

• [SLOW TEST:8.581 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:55:31.018: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-57pcg
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 28 07:55:31.314: INFO: Waiting up to 5m0s for pod "pod-3872a432-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-57pcg" to be "success or failure"
Feb 28 07:55:31.318: INFO: Pod "pod-3872a432-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060004ms
Feb 28 07:55:33.323: INFO: Pod "pod-3872a432-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008601211s
STEP: Saw pod success
Feb 28 07:55:33.323: INFO: Pod "pod-3872a432-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:55:33.326: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-3872a432-3b2e-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 07:55:33.343: INFO: Waiting for pod pod-3872a432-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:55:33.347: INFO: Pod pod-3872a432-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:55:33.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-57pcg" for this suite.
Feb 28 07:55:39.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:39.607: INFO: namespace: e2e-tests-emptydir-57pcg, resource: bindings, ignored listing per whitelist
Feb 28 07:55:39.691: INFO: namespace e2e-tests-emptydir-57pcg deletion completed in 6.3401253s

• [SLOW TEST:8.673 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:55:39.691: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2m7wd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 07:55:40.236: INFO: Waiting up to 5m0s for pod "pod-3dc326b5-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-2m7wd" to be "success or failure"
Feb 28 07:55:40.243: INFO: Pod "pod-3dc326b5-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.91943ms
Feb 28 07:55:42.253: INFO: Pod "pod-3dc326b5-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017266571s
STEP: Saw pod success
Feb 28 07:55:42.254: INFO: Pod "pod-3dc326b5-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:55:42.264: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-3dc326b5-3b2e-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 07:55:42.295: INFO: Waiting for pod pod-3dc326b5-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:55:42.304: INFO: Pod pod-3dc326b5-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:55:42.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2m7wd" for this suite.
Feb 28 07:55:48.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:48.548: INFO: namespace: e2e-tests-emptydir-2m7wd, resource: bindings, ignored listing per whitelist
Feb 28 07:55:48.664: INFO: namespace e2e-tests-emptydir-2m7wd deletion completed in 6.35052287s

• [SLOW TEST:8.973 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:55:48.665: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-6l98n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-43334395-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 07:55:49.370: INFO: Waiting up to 5m0s for pod "pod-configmaps-4334af54-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-configmap-6l98n" to be "success or failure"
Feb 28 07:55:49.379: INFO: Pod "pod-configmaps-4334af54-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 9.035316ms
Feb 28 07:55:51.383: INFO: Pod "pod-configmaps-4334af54-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01303578s
STEP: Saw pod success
Feb 28 07:55:51.383: INFO: Pod "pod-configmaps-4334af54-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:55:51.387: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-4334af54-3b2e-11e9-b1ab-fe0431d33f49 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 07:55:51.410: INFO: Waiting for pod pod-configmaps-4334af54-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:55:51.414: INFO: Pod pod-configmaps-4334af54-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:55:51.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-6l98n" for this suite.
Feb 28 07:55:57.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:55:57.607: INFO: namespace: e2e-tests-configmap-6l98n, resource: bindings, ignored listing per whitelist
Feb 28 07:55:57.632: INFO: namespace e2e-tests-configmap-6l98n deletion completed in 6.212236282s

• [SLOW TEST:8.968 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:55:57.633: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-7ds7m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 28 07:55:58.139: INFO: Waiting up to 5m0s for pod "var-expansion-486fad28-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-var-expansion-7ds7m" to be "success or failure"
Feb 28 07:55:58.143: INFO: Pod "var-expansion-486fad28-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.279207ms
Feb 28 07:56:00.148: INFO: Pod "var-expansion-486fad28-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008746687s
STEP: Saw pod success
Feb 28 07:56:00.148: INFO: Pod "var-expansion-486fad28-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:56:00.155: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod var-expansion-486fad28-3b2e-11e9-b1ab-fe0431d33f49 container dapi-container: <nil>
STEP: delete the pod
Feb 28 07:56:00.173: INFO: Waiting for pod var-expansion-486fad28-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:56:00.175: INFO: Pod var-expansion-486fad28-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:56:00.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-7ds7m" for this suite.
Feb 28 07:56:06.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:06.303: INFO: namespace: e2e-tests-var-expansion-7ds7m, resource: bindings, ignored listing per whitelist
Feb 28 07:56:06.405: INFO: namespace e2e-tests-var-expansion-7ds7m deletion completed in 6.225652466s

• [SLOW TEST:8.772 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:56:06.405: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2prwf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 07:56:06.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d8bf5cb-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-2prwf" to be "success or failure"
Feb 28 07:56:06.717: INFO: Pod "downwardapi-volume-4d8bf5cb-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.609605ms
Feb 28 07:56:08.724: INFO: Pod "downwardapi-volume-4d8bf5cb-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010078331s
STEP: Saw pod success
Feb 28 07:56:08.724: INFO: Pod "downwardapi-volume-4d8bf5cb-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:56:08.729: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-4d8bf5cb-3b2e-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 07:56:08.750: INFO: Waiting for pod downwardapi-volume-4d8bf5cb-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:56:08.755: INFO: Pod downwardapi-volume-4d8bf5cb-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:56:08.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2prwf" for this suite.
Feb 28 07:56:14.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:15.080: INFO: namespace: e2e-tests-downward-api-2prwf, resource: bindings, ignored listing per whitelist
Feb 28 07:56:15.170: INFO: namespace e2e-tests-downward-api-2prwf deletion completed in 6.409937608s

• [SLOW TEST:8.765 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:56:15.170: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-jjr2h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 07:56:15.517: INFO: Waiting up to 5m0s for pod "pod-52cb5cb3-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-jjr2h" to be "success or failure"
Feb 28 07:56:15.521: INFO: Pod "pod-52cb5cb3-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02051ms
Feb 28 07:56:17.526: INFO: Pod "pod-52cb5cb3-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009250823s
Feb 28 07:56:19.533: INFO: Pod "pod-52cb5cb3-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016256264s
STEP: Saw pod success
Feb 28 07:56:19.533: INFO: Pod "pod-52cb5cb3-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:56:19.537: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-52cb5cb3-3b2e-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 07:56:19.557: INFO: Waiting for pod pod-52cb5cb3-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:56:19.561: INFO: Pod pod-52cb5cb3-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:56:19.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jjr2h" for this suite.
Feb 28 07:56:25.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:25.694: INFO: namespace: e2e-tests-emptydir-jjr2h, resource: bindings, ignored listing per whitelist
Feb 28 07:56:25.858: INFO: namespace e2e-tests-emptydir-jjr2h deletion completed in 6.291556499s

• [SLOW TEST:10.688 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:56:25.859: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-zvd8r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-592ae8a1-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 07:56:26.213: INFO: Waiting up to 5m0s for pod "pod-secrets-592b93c9-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-secrets-zvd8r" to be "success or failure"
Feb 28 07:56:26.218: INFO: Pod "pod-secrets-592b93c9-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.106346ms
Feb 28 07:56:28.223: INFO: Pod "pod-secrets-592b93c9-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009892286s
STEP: Saw pod success
Feb 28 07:56:28.223: INFO: Pod "pod-secrets-592b93c9-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 07:56:28.227: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-secrets-592b93c9-3b2e-11e9-b1ab-fe0431d33f49 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 07:56:28.246: INFO: Waiting for pod pod-secrets-592b93c9-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 07:56:28.250: INFO: Pod pod-secrets-592b93c9-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:56:28.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-zvd8r" for this suite.
Feb 28 07:56:34.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:56:34.285: INFO: namespace: e2e-tests-secrets-zvd8r, resource: bindings, ignored listing per whitelist
Feb 28 07:56:34.489: INFO: namespace e2e-tests-secrets-zvd8r deletion completed in 6.235236206s

• [SLOW TEST:8.630 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:56:34.490: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-tppfs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 07:56:34.939: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 28 07:56:34.957: INFO: Number of nodes with available pods: 0
Feb 28 07:56:34.957: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 28 07:56:34.983: INFO: Number of nodes with available pods: 0
Feb 28 07:56:34.983: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:35.988: INFO: Number of nodes with available pods: 0
Feb 28 07:56:35.988: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:36.994: INFO: Number of nodes with available pods: 1
Feb 28 07:56:36.994: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 28 07:56:37.031: INFO: Number of nodes with available pods: 0
Feb 28 07:56:37.031: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 28 07:56:37.046: INFO: Number of nodes with available pods: 0
Feb 28 07:56:37.047: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:38.054: INFO: Number of nodes with available pods: 0
Feb 28 07:56:38.054: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:39.053: INFO: Number of nodes with available pods: 0
Feb 28 07:56:39.053: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:40.055: INFO: Number of nodes with available pods: 0
Feb 28 07:56:40.055: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:41.056: INFO: Number of nodes with available pods: 0
Feb 28 07:56:41.057: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:42.053: INFO: Number of nodes with available pods: 0
Feb 28 07:56:42.053: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:43.054: INFO: Number of nodes with available pods: 0
Feb 28 07:56:43.054: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:44.053: INFO: Number of nodes with available pods: 0
Feb 28 07:56:44.053: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:45.054: INFO: Number of nodes with available pods: 0
Feb 28 07:56:45.054: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:46.052: INFO: Number of nodes with available pods: 0
Feb 28 07:56:46.052: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:47.056: INFO: Number of nodes with available pods: 0
Feb 28 07:56:47.056: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:48.052: INFO: Number of nodes with available pods: 0
Feb 28 07:56:48.052: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:49.052: INFO: Number of nodes with available pods: 0
Feb 28 07:56:49.052: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:50.054: INFO: Number of nodes with available pods: 0
Feb 28 07:56:50.054: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:51.052: INFO: Number of nodes with available pods: 0
Feb 28 07:56:51.052: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:52.056: INFO: Number of nodes with available pods: 0
Feb 28 07:56:52.056: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:53.052: INFO: Number of nodes with available pods: 0
Feb 28 07:56:53.053: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:54.055: INFO: Number of nodes with available pods: 0
Feb 28 07:56:54.055: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:55.052: INFO: Number of nodes with available pods: 0
Feb 28 07:56:55.052: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:56.055: INFO: Number of nodes with available pods: 0
Feb 28 07:56:56.055: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:57.055: INFO: Number of nodes with available pods: 0
Feb 28 07:56:57.055: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:58.054: INFO: Number of nodes with available pods: 0
Feb 28 07:56:58.055: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:56:59.054: INFO: Number of nodes with available pods: 0
Feb 28 07:56:59.054: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:00.051: INFO: Number of nodes with available pods: 0
Feb 28 07:57:00.051: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:01.052: INFO: Number of nodes with available pods: 0
Feb 28 07:57:01.053: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:02.052: INFO: Number of nodes with available pods: 0
Feb 28 07:57:02.052: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:03.052: INFO: Number of nodes with available pods: 0
Feb 28 07:57:03.052: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:04.053: INFO: Number of nodes with available pods: 0
Feb 28 07:57:04.053: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:05.051: INFO: Number of nodes with available pods: 0
Feb 28 07:57:05.051: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:06.062: INFO: Number of nodes with available pods: 0
Feb 28 07:57:06.063: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:07.054: INFO: Number of nodes with available pods: 0
Feb 28 07:57:07.054: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:08.051: INFO: Number of nodes with available pods: 0
Feb 28 07:57:08.052: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:09.056: INFO: Number of nodes with available pods: 0
Feb 28 07:57:09.056: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:10.051: INFO: Number of nodes with available pods: 0
Feb 28 07:57:10.051: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:11.051: INFO: Number of nodes with available pods: 0
Feb 28 07:57:11.051: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:12.053: INFO: Number of nodes with available pods: 0
Feb 28 07:57:12.053: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:13.054: INFO: Number of nodes with available pods: 0
Feb 28 07:57:13.054: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 07:57:14.061: INFO: Number of nodes with available pods: 1
Feb 28 07:57:14.061: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-tppfs, will wait for the garbage collector to delete the pods
Feb 28 07:57:14.164: INFO: Deleting {extensions DaemonSet} daemon-set took: 15.876735ms
Feb 28 07:57:14.264: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.326456ms
Feb 28 07:57:52.469: INFO: Number of nodes with available pods: 0
Feb 28 07:57:52.469: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 07:57:52.473: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tppfs/daemonsets","resourceVersion":"7065"},"items":null}

Feb 28 07:57:52.477: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tppfs/pods","resourceVersion":"7065"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:57:52.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tppfs" for this suite.
Feb 28 07:57:58.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:57:58.546: INFO: namespace: e2e-tests-daemonsets-tppfs, resource: bindings, ignored listing per whitelist
Feb 28 07:57:58.639: INFO: namespace e2e-tests-daemonsets-tppfs deletion completed in 6.140861661s

• [SLOW TEST:84.150 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:57:58.640: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-jvmf8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0228 07:58:29.489034   30646 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 07:58:29.489: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:58:29.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-jvmf8" for this suite.
Feb 28 07:58:35.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:58:35.569: INFO: namespace: e2e-tests-gc-jvmf8, resource: bindings, ignored listing per whitelist
Feb 28 07:58:35.739: INFO: namespace e2e-tests-gc-jvmf8 deletion completed in 6.23957935s

• [SLOW TEST:37.099 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:58:35.740: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-zlpqf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-5wrt
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 07:58:36.159: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5wrt" in namespace "e2e-tests-subpath-zlpqf" to be "success or failure"
Feb 28 07:58:36.163: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16402ms
Feb 28 07:58:38.172: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013326094s
Feb 28 07:58:40.184: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 4.024855336s
Feb 28 07:58:42.196: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 6.036946942s
Feb 28 07:58:44.206: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 8.046469299s
Feb 28 07:58:46.216: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 10.056446511s
Feb 28 07:58:48.224: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 12.064759683s
Feb 28 07:58:50.235: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 14.075751673s
Feb 28 07:58:52.245: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 16.085698563s
Feb 28 07:58:54.253: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 18.094094568s
Feb 28 07:58:56.262: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 20.102824892s
Feb 28 07:58:58.268: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Running", Reason="", readiness=false. Elapsed: 22.109267895s
Feb 28 07:59:00.274: INFO: Pod "pod-subpath-test-configmap-5wrt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.114582335s
STEP: Saw pod success
Feb 28 07:59:00.274: INFO: Pod "pod-subpath-test-configmap-5wrt" satisfied condition "success or failure"
Feb 28 07:59:00.278: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-subpath-test-configmap-5wrt container test-container-subpath-configmap-5wrt: <nil>
STEP: delete the pod
Feb 28 07:59:00.297: INFO: Waiting for pod pod-subpath-test-configmap-5wrt to disappear
Feb 28 07:59:00.300: INFO: Pod pod-subpath-test-configmap-5wrt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5wrt
Feb 28 07:59:00.300: INFO: Deleting pod "pod-subpath-test-configmap-5wrt" in namespace "e2e-tests-subpath-zlpqf"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:59:00.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-zlpqf" for this suite.
Feb 28 07:59:06.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:59:06.587: INFO: namespace: e2e-tests-subpath-zlpqf, resource: bindings, ignored listing per whitelist
Feb 28 07:59:06.606: INFO: namespace e2e-tests-subpath-zlpqf deletion completed in 6.298366545s

• [SLOW TEST:30.866 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:59:06.607: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-w458g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 07:59:09.457: INFO: Successfully updated pod "pod-update-b8f6fe22-3b2e-11e9-b1ab-fe0431d33f49"
STEP: verifying the updated pod is in kubernetes
Feb 28 07:59:09.466: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:59:09.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-w458g" for this suite.
Feb 28 07:59:31.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:59:31.542: INFO: namespace: e2e-tests-pods-w458g, resource: bindings, ignored listing per whitelist
Feb 28 07:59:31.816: INFO: namespace e2e-tests-pods-w458g deletion completed in 22.345996374s

• [SLOW TEST:25.210 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:59:31.817: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-4nxkm
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-c81d1723-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating secret with name s-test-opt-upd-c81d18db-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c81d1723-3b2e-11e9-b1ab-fe0431d33f49
STEP: Updating secret s-test-opt-upd-c81d18db-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating secret with name s-test-opt-create-c81d1903-3b2e-11e9-b1ab-fe0431d33f49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 07:59:36.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4nxkm" for this suite.
Feb 28 07:59:58.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 07:59:59.042: INFO: namespace: e2e-tests-secrets-4nxkm, resource: bindings, ignored listing per whitelist
Feb 28 07:59:59.050: INFO: namespace e2e-tests-secrets-4nxkm deletion completed in 22.199904735s

• [SLOW TEST:27.233 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 07:59:59.050: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-txrwn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-d83eadb7-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 07:59:59.413: INFO: Waiting up to 5m0s for pod "pod-secrets-d83f677c-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-secrets-txrwn" to be "success or failure"
Feb 28 07:59:59.417: INFO: Pod "pod-secrets-d83f677c-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.617509ms
Feb 28 08:00:01.423: INFO: Pod "pod-secrets-d83f677c-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010124042s
STEP: Saw pod success
Feb 28 08:00:01.423: INFO: Pod "pod-secrets-d83f677c-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:00:01.428: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-secrets-d83f677c-3b2e-11e9-b1ab-fe0431d33f49 container secret-env-test: <nil>
STEP: delete the pod
Feb 28 08:00:01.658: INFO: Waiting for pod pod-secrets-d83f677c-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:00:01.666: INFO: Pod pod-secrets-d83f677c-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:00:01.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-txrwn" for this suite.
Feb 28 08:00:07.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:00:07.715: INFO: namespace: e2e-tests-secrets-txrwn, resource: bindings, ignored listing per whitelist
Feb 28 08:00:07.861: INFO: namespace e2e-tests-secrets-txrwn deletion completed in 6.188850882s

• [SLOW TEST:8.811 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:00:07.862: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-nv8xr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:00:08.119: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd6f8026-3b2e-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-nv8xr" to be "success or failure"
Feb 28 08:00:08.125: INFO: Pod "downwardapi-volume-dd6f8026-3b2e-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.780659ms
Feb 28 08:00:10.131: INFO: Pod "downwardapi-volume-dd6f8026-3b2e-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011476318s
STEP: Saw pod success
Feb 28 08:00:10.131: INFO: Pod "downwardapi-volume-dd6f8026-3b2e-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:00:10.137: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-dd6f8026-3b2e-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:00:10.161: INFO: Waiting for pod downwardapi-volume-dd6f8026-3b2e-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:00:10.167: INFO: Pod downwardapi-volume-dd6f8026-3b2e-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:00:10.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nv8xr" for this suite.
Feb 28 08:00:16.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:00:16.407: INFO: namespace: e2e-tests-downward-api-nv8xr, resource: bindings, ignored listing per whitelist
Feb 28 08:00:16.412: INFO: namespace e2e-tests-downward-api-nv8xr deletion completed in 6.236040079s

• [SLOW TEST:8.551 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:00:16.412: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hs228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-e2afe2de-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating secret with name s-test-opt-upd-e2afe3b5-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e2afe2de-3b2e-11e9-b1ab-fe0431d33f49
STEP: Updating secret s-test-opt-upd-e2afe3b5-3b2e-11e9-b1ab-fe0431d33f49
STEP: Creating secret with name s-test-opt-create-e2afe3d0-3b2e-11e9-b1ab-fe0431d33f49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:00:21.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hs228" for this suite.
Feb 28 08:00:43.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:00:43.551: INFO: namespace: e2e-tests-projected-hs228, resource: bindings, ignored listing per whitelist
Feb 28 08:00:43.672: INFO: namespace e2e-tests-projected-hs228 deletion completed in 22.275186488s

• [SLOW TEST:27.260 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:00:43.673: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-xkjj7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:00:44.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xkjj7" for this suite.
Feb 28 08:01:06.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:01:06.333: INFO: namespace: e2e-tests-pods-xkjj7, resource: bindings, ignored listing per whitelist
Feb 28 08:01:06.387: INFO: namespace e2e-tests-pods-xkjj7 deletion completed in 22.151399877s

• [SLOW TEST:22.715 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:01:06.388: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-xrd8l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 08:01:06.701: INFO: PodSpec: initContainers in spec.initContainers
Feb 28 08:01:51.273: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-005ba1d0-3b2f-11e9-b1ab-fe0431d33f49", GenerateName:"", Namespace:"e2e-tests-init-container-xrd8l", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-xrd8l/pods/pod-init-005ba1d0-3b2f-11e9-b1ab-fe0431d33f49", UID:"005c4fe3-3b2f-11e9-a4f3-1a76b5824175", ResourceVersion:"7757", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686937666, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"701772432"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.80/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-6dp86", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001acdd00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6dp86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6dp86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6dp86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000dc8f08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000da3c20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000dc8f80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000dc8fa0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000dc8fa8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937666, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937666, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937666, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686937666, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.18", PodIP:"100.96.1.80", StartTime:(*v1.Time)(0xc00195a2e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006eaa10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006eaa80)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://0572c42b6204b6ff88ae593cb74f94f5435fdcea19df931c2518239a22c5fd60"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00195a320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00195a300), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:01:51.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-xrd8l" for this suite.
Feb 28 08:02:13.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:02:13.592: INFO: namespace: e2e-tests-init-container-xrd8l, resource: bindings, ignored listing per whitelist
Feb 28 08:02:13.670: INFO: namespace e2e-tests-init-container-xrd8l deletion completed in 22.377482414s

• [SLOW TEST:67.283 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:02:13.671: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vml2j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-287c0958-3b2f-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:02:14.037: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-287cf322-3b2f-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-vml2j" to be "success or failure"
Feb 28 08:02:14.042: INFO: Pod "pod-projected-configmaps-287cf322-3b2f-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.678853ms
Feb 28 08:02:16.057: INFO: Pod "pod-projected-configmaps-287cf322-3b2f-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019316625s
STEP: Saw pod success
Feb 28 08:02:16.057: INFO: Pod "pod-projected-configmaps-287cf322-3b2f-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:02:16.065: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-configmaps-287cf322-3b2f-11e9-b1ab-fe0431d33f49 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:02:16.131: INFO: Waiting for pod pod-projected-configmaps-287cf322-3b2f-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:02:16.371: INFO: Pod pod-projected-configmaps-287cf322-3b2f-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:02:16.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vml2j" for this suite.
Feb 28 08:02:22.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:02:22.775: INFO: namespace: e2e-tests-projected-vml2j, resource: bindings, ignored listing per whitelist
Feb 28 08:02:22.785: INFO: namespace e2e-tests-projected-vml2j deletion completed in 6.397277505s

• [SLOW TEST:9.114 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:02:22.785: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-s5qdg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-2df8f485-3b2f-11e9-b1ab-fe0431d33f49
Feb 28 08:02:23.256: INFO: Pod name my-hostname-basic-2df8f485-3b2f-11e9-b1ab-fe0431d33f49: Found 1 pods out of 1
Feb 28 08:02:23.256: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2df8f485-3b2f-11e9-b1ab-fe0431d33f49" are running
Feb 28 08:02:25.274: INFO: Pod "my-hostname-basic-2df8f485-3b2f-11e9-b1ab-fe0431d33f49-vzsh9" is running (conditions: [])
Feb 28 08:02:25.274: INFO: Trying to dial the pod
Feb 28 08:02:30.597: INFO: Controller my-hostname-basic-2df8f485-3b2f-11e9-b1ab-fe0431d33f49: Got expected result from replica 1 [my-hostname-basic-2df8f485-3b2f-11e9-b1ab-fe0431d33f49-vzsh9]: "my-hostname-basic-2df8f485-3b2f-11e9-b1ab-fe0431d33f49-vzsh9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:02:30.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-s5qdg" for this suite.
Feb 28 08:02:36.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:02:37.483: INFO: namespace: e2e-tests-replication-controller-s5qdg, resource: bindings, ignored listing per whitelist
Feb 28 08:02:37.489: INFO: namespace e2e-tests-replication-controller-s5qdg deletion completed in 6.876194724s

• [SLOW TEST:14.704 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:02:37.489: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-gj854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 28 08:02:38.150: INFO: Waiting up to 5m0s for pod "client-containers-36db88f2-3b2f-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-containers-gj854" to be "success or failure"
Feb 28 08:02:38.159: INFO: Pod "client-containers-36db88f2-3b2f-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 9.495447ms
Feb 28 08:02:40.167: INFO: Pod "client-containers-36db88f2-3b2f-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017521857s
STEP: Saw pod success
Feb 28 08:02:40.168: INFO: Pod "client-containers-36db88f2-3b2f-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:02:40.176: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod client-containers-36db88f2-3b2f-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:02:40.209: INFO: Waiting for pod client-containers-36db88f2-3b2f-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:02:40.218: INFO: Pod client-containers-36db88f2-3b2f-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:02:40.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-gj854" for this suite.
Feb 28 08:02:46.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:02:46.538: INFO: namespace: e2e-tests-containers-gj854, resource: bindings, ignored listing per whitelist
Feb 28 08:02:46.541: INFO: namespace e2e-tests-containers-gj854 deletion completed in 6.308127204s

• [SLOW TEST:9.053 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:02:46.542: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-pv2lm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0228 08:03:26.986306   30646 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:03:26.986: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:03:26.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-pv2lm" for this suite.
Feb 28 08:03:33.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:33.343: INFO: namespace: e2e-tests-gc-pv2lm, resource: bindings, ignored listing per whitelist
Feb 28 08:03:33.509: INFO: namespace e2e-tests-gc-pv2lm deletion completed in 6.508094884s

• [SLOW TEST:46.967 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:03:33.509: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-4ldmn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-581b9f5d-3b2f-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:03:33.944: INFO: Waiting up to 5m0s for pod "pod-configmaps-581d0156-3b2f-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-configmap-4ldmn" to be "success or failure"
Feb 28 08:03:33.951: INFO: Pod "pod-configmaps-581d0156-3b2f-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 7.891207ms
Feb 28 08:03:35.960: INFO: Pod "pod-configmaps-581d0156-3b2f-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016675108s
Feb 28 08:03:37.970: INFO: Pod "pod-configmaps-581d0156-3b2f-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02680915s
STEP: Saw pod success
Feb 28 08:03:37.971: INFO: Pod "pod-configmaps-581d0156-3b2f-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:03:37.982: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-581d0156-3b2f-11e9-b1ab-fe0431d33f49 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:03:38.017: INFO: Waiting for pod pod-configmaps-581d0156-3b2f-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:03:38.026: INFO: Pod pod-configmaps-581d0156-3b2f-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:03:38.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-4ldmn" for this suite.
Feb 28 08:03:44.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:44.116: INFO: namespace: e2e-tests-configmap-4ldmn, resource: bindings, ignored listing per whitelist
Feb 28 08:03:44.239: INFO: namespace e2e-tests-configmap-4ldmn deletion completed in 6.197351749s

• [SLOW TEST:10.730 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:03:44.239: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-csrcd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 08:03:44.515: INFO: Waiting up to 5m0s for pod "pod-5e6aee22-3b2f-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-csrcd" to be "success or failure"
Feb 28 08:03:44.519: INFO: Pod "pod-5e6aee22-3b2f-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146256ms
Feb 28 08:03:46.527: INFO: Pod "pod-5e6aee22-3b2f-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011623645s
STEP: Saw pod success
Feb 28 08:03:46.527: INFO: Pod "pod-5e6aee22-3b2f-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:03:46.533: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-5e6aee22-3b2f-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:03:46.558: INFO: Waiting for pod pod-5e6aee22-3b2f-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:03:46.565: INFO: Pod pod-5e6aee22-3b2f-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:03:46.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-csrcd" for this suite.
Feb 28 08:03:52.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:03:52.722: INFO: namespace: e2e-tests-emptydir-csrcd, resource: bindings, ignored listing per whitelist
Feb 28 08:03:52.757: INFO: namespace e2e-tests-emptydir-csrcd deletion completed in 6.185411609s

• [SLOW TEST:8.518 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:03:52.757: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-8p78d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-8p78d
I0228 08:03:53.111394   30646 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-8p78d, replica count: 1
I0228 08:03:54.162196   30646 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 08:03:55.162518   30646 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 08:03:55.277: INFO: Created: latency-svc-t6vh8
Feb 28 08:03:55.283: INFO: Got endpoints: latency-svc-t6vh8 [19.652964ms]
Feb 28 08:03:55.296: INFO: Created: latency-svc-88kmk
Feb 28 08:03:55.298: INFO: Got endpoints: latency-svc-88kmk [14.247454ms]
Feb 28 08:03:55.304: INFO: Created: latency-svc-82xrp
Feb 28 08:03:55.305: INFO: Got endpoints: latency-svc-82xrp [21.294122ms]
Feb 28 08:03:55.310: INFO: Created: latency-svc-5chxn
Feb 28 08:03:55.313: INFO: Got endpoints: latency-svc-5chxn [28.766609ms]
Feb 28 08:03:55.319: INFO: Created: latency-svc-slz9m
Feb 28 08:03:55.322: INFO: Got endpoints: latency-svc-slz9m [37.621623ms]
Feb 28 08:03:55.326: INFO: Created: latency-svc-2ncqt
Feb 28 08:03:55.330: INFO: Got endpoints: latency-svc-2ncqt [46.030186ms]
Feb 28 08:03:55.332: INFO: Created: latency-svc-92cfc
Feb 28 08:03:55.335: INFO: Got endpoints: latency-svc-92cfc [50.796775ms]
Feb 28 08:03:55.350: INFO: Created: latency-svc-5zvp9
Feb 28 08:03:55.352: INFO: Got endpoints: latency-svc-5zvp9 [66.98956ms]
Feb 28 08:03:55.356: INFO: Created: latency-svc-cfcht
Feb 28 08:03:55.358: INFO: Got endpoints: latency-svc-cfcht [73.804912ms]
Feb 28 08:03:55.365: INFO: Created: latency-svc-8ng8s
Feb 28 08:03:55.368: INFO: Got endpoints: latency-svc-8ng8s [82.818833ms]
Feb 28 08:03:55.373: INFO: Created: latency-svc-x7sbc
Feb 28 08:03:55.373: INFO: Got endpoints: latency-svc-x7sbc [88.181662ms]
Feb 28 08:03:55.377: INFO: Created: latency-svc-rw4sk
Feb 28 08:03:55.379: INFO: Got endpoints: latency-svc-rw4sk [93.657491ms]
Feb 28 08:03:55.383: INFO: Created: latency-svc-bwzvn
Feb 28 08:03:55.386: INFO: Got endpoints: latency-svc-bwzvn [101.207436ms]
Feb 28 08:03:55.389: INFO: Created: latency-svc-4k9ck
Feb 28 08:03:55.393: INFO: Got endpoints: latency-svc-4k9ck [107.205451ms]
Feb 28 08:03:55.395: INFO: Created: latency-svc-6786g
Feb 28 08:03:55.399: INFO: Got endpoints: latency-svc-6786g [113.524705ms]
Feb 28 08:03:55.402: INFO: Created: latency-svc-5t625
Feb 28 08:03:55.405: INFO: Got endpoints: latency-svc-5t625 [119.362881ms]
Feb 28 08:03:55.408: INFO: Created: latency-svc-vk5xz
Feb 28 08:03:55.410: INFO: Got endpoints: latency-svc-vk5xz [112.101896ms]
Feb 28 08:03:55.414: INFO: Created: latency-svc-5jhn7
Feb 28 08:03:55.418: INFO: Got endpoints: latency-svc-5jhn7 [113.131052ms]
Feb 28 08:03:55.425: INFO: Created: latency-svc-cxqm4
Feb 28 08:03:55.427: INFO: Got endpoints: latency-svc-cxqm4 [113.986811ms]
Feb 28 08:03:55.432: INFO: Created: latency-svc-pnwwx
Feb 28 08:03:55.435: INFO: Got endpoints: latency-svc-pnwwx [112.911934ms]
Feb 28 08:03:55.438: INFO: Created: latency-svc-fmcfj
Feb 28 08:03:55.443: INFO: Got endpoints: latency-svc-fmcfj [112.764877ms]
Feb 28 08:03:55.446: INFO: Created: latency-svc-k4pgv
Feb 28 08:03:55.449: INFO: Got endpoints: latency-svc-k4pgv [113.479122ms]
Feb 28 08:03:55.451: INFO: Created: latency-svc-c9svk
Feb 28 08:03:55.455: INFO: Got endpoints: latency-svc-c9svk [103.387833ms]
Feb 28 08:03:55.460: INFO: Created: latency-svc-59nfq
Feb 28 08:03:55.461: INFO: Got endpoints: latency-svc-59nfq [102.488703ms]
Feb 28 08:03:55.476: INFO: Created: latency-svc-lc8f4
Feb 28 08:03:55.480: INFO: Got endpoints: latency-svc-lc8f4 [111.798862ms]
Feb 28 08:03:55.483: INFO: Created: latency-svc-qs8rt
Feb 28 08:03:55.486: INFO: Got endpoints: latency-svc-qs8rt [113.290911ms]
Feb 28 08:03:55.490: INFO: Created: latency-svc-rxt5g
Feb 28 08:03:55.492: INFO: Got endpoints: latency-svc-rxt5g [113.446102ms]
Feb 28 08:03:55.499: INFO: Created: latency-svc-6rtzp
Feb 28 08:03:55.500: INFO: Got endpoints: latency-svc-6rtzp [113.482099ms]
Feb 28 08:03:55.506: INFO: Created: latency-svc-tj57w
Feb 28 08:03:55.508: INFO: Got endpoints: latency-svc-tj57w [114.956243ms]
Feb 28 08:03:55.512: INFO: Created: latency-svc-d8kx2
Feb 28 08:03:55.515: INFO: Got endpoints: latency-svc-d8kx2 [115.43317ms]
Feb 28 08:03:55.519: INFO: Created: latency-svc-2d5vl
Feb 28 08:03:55.524: INFO: Got endpoints: latency-svc-2d5vl [119.052913ms]
Feb 28 08:03:55.536: INFO: Created: latency-svc-zm6bv
Feb 28 08:03:55.539: INFO: Got endpoints: latency-svc-zm6bv [128.736865ms]
Feb 28 08:03:55.544: INFO: Created: latency-svc-mht8t
Feb 28 08:03:55.547: INFO: Got endpoints: latency-svc-mht8t [128.654047ms]
Feb 28 08:03:55.551: INFO: Created: latency-svc-qhbx9
Feb 28 08:03:55.554: INFO: Got endpoints: latency-svc-qhbx9 [126.839966ms]
Feb 28 08:03:55.561: INFO: Created: latency-svc-wg8rs
Feb 28 08:03:55.565: INFO: Got endpoints: latency-svc-wg8rs [130.432506ms]
Feb 28 08:03:55.566: INFO: Created: latency-svc-pdxvs
Feb 28 08:03:55.574: INFO: Created: latency-svc-9d85z
Feb 28 08:03:55.583: INFO: Got endpoints: latency-svc-pdxvs [139.208631ms]
Feb 28 08:03:55.584: INFO: Created: latency-svc-xp47t
Feb 28 08:03:55.593: INFO: Created: latency-svc-khprl
Feb 28 08:03:55.598: INFO: Created: latency-svc-ljk4c
Feb 28 08:03:55.605: INFO: Created: latency-svc-nldpf
Feb 28 08:03:55.614: INFO: Created: latency-svc-tndr2
Feb 28 08:03:55.627: INFO: Created: latency-svc-2vgtv
Feb 28 08:03:55.631: INFO: Created: latency-svc-m6bbn
Feb 28 08:03:55.631: INFO: Got endpoints: latency-svc-9d85z [182.443283ms]
Feb 28 08:03:55.642: INFO: Created: latency-svc-pnstl
Feb 28 08:03:55.650: INFO: Created: latency-svc-5wkbx
Feb 28 08:03:55.658: INFO: Created: latency-svc-d4k8b
Feb 28 08:03:55.668: INFO: Created: latency-svc-g6r54
Feb 28 08:03:55.674: INFO: Created: latency-svc-6fk29
Feb 28 08:03:55.683: INFO: Created: latency-svc-t9z9c
Feb 28 08:03:55.683: INFO: Got endpoints: latency-svc-xp47t [227.455277ms]
Feb 28 08:03:55.688: INFO: Created: latency-svc-gxrgh
Feb 28 08:03:55.695: INFO: Created: latency-svc-cvr97
Feb 28 08:03:55.701: INFO: Created: latency-svc-c5fts
Feb 28 08:03:55.730: INFO: Got endpoints: latency-svc-khprl [269.248114ms]
Feb 28 08:03:55.747: INFO: Created: latency-svc-ddbbr
Feb 28 08:03:55.780: INFO: Got endpoints: latency-svc-ljk4c [300.589718ms]
Feb 28 08:03:55.795: INFO: Created: latency-svc-7cnsw
Feb 28 08:03:55.829: INFO: Got endpoints: latency-svc-nldpf [342.663301ms]
Feb 28 08:03:55.842: INFO: Created: latency-svc-zsjpd
Feb 28 08:03:55.880: INFO: Got endpoints: latency-svc-tndr2 [388.138748ms]
Feb 28 08:03:55.900: INFO: Created: latency-svc-5zvd4
Feb 28 08:03:55.930: INFO: Got endpoints: latency-svc-2vgtv [430.07795ms]
Feb 28 08:03:55.942: INFO: Created: latency-svc-t6gl2
Feb 28 08:03:55.981: INFO: Got endpoints: latency-svc-m6bbn [472.962993ms]
Feb 28 08:03:55.997: INFO: Created: latency-svc-pwt4f
Feb 28 08:03:56.030: INFO: Got endpoints: latency-svc-pnstl [514.987808ms]
Feb 28 08:03:56.042: INFO: Created: latency-svc-kvbg5
Feb 28 08:03:56.081: INFO: Got endpoints: latency-svc-5wkbx [556.766411ms]
Feb 28 08:03:56.092: INFO: Created: latency-svc-8dccl
Feb 28 08:03:56.130: INFO: Got endpoints: latency-svc-d4k8b [591.077907ms]
Feb 28 08:03:56.142: INFO: Created: latency-svc-p88p5
Feb 28 08:03:56.182: INFO: Got endpoints: latency-svc-g6r54 [634.973824ms]
Feb 28 08:03:56.196: INFO: Created: latency-svc-dvwlk
Feb 28 08:03:56.231: INFO: Got endpoints: latency-svc-6fk29 [677.396215ms]
Feb 28 08:03:56.247: INFO: Created: latency-svc-wxjqb
Feb 28 08:03:56.280: INFO: Got endpoints: latency-svc-t9z9c [714.843549ms]
Feb 28 08:03:56.291: INFO: Created: latency-svc-vtzqz
Feb 28 08:03:56.333: INFO: Got endpoints: latency-svc-gxrgh [750.190136ms]
Feb 28 08:03:56.348: INFO: Created: latency-svc-scb5w
Feb 28 08:03:56.381: INFO: Got endpoints: latency-svc-cvr97 [749.599425ms]
Feb 28 08:03:56.393: INFO: Created: latency-svc-66l57
Feb 28 08:03:56.430: INFO: Got endpoints: latency-svc-c5fts [747.435397ms]
Feb 28 08:03:56.443: INFO: Created: latency-svc-kl9cq
Feb 28 08:03:56.481: INFO: Got endpoints: latency-svc-ddbbr [750.919002ms]
Feb 28 08:03:56.492: INFO: Created: latency-svc-r5fkf
Feb 28 08:03:56.530: INFO: Got endpoints: latency-svc-7cnsw [749.600204ms]
Feb 28 08:03:56.541: INFO: Created: latency-svc-s66wp
Feb 28 08:03:56.581: INFO: Got endpoints: latency-svc-zsjpd [752.104136ms]
Feb 28 08:03:56.596: INFO: Created: latency-svc-d4ztw
Feb 28 08:03:56.632: INFO: Got endpoints: latency-svc-5zvd4 [751.750831ms]
Feb 28 08:03:56.646: INFO: Created: latency-svc-g66jb
Feb 28 08:03:56.680: INFO: Got endpoints: latency-svc-t6gl2 [750.201728ms]
Feb 28 08:03:56.696: INFO: Created: latency-svc-lm6td
Feb 28 08:03:56.731: INFO: Got endpoints: latency-svc-pwt4f [750.076961ms]
Feb 28 08:03:56.745: INFO: Created: latency-svc-ssclz
Feb 28 08:03:56.780: INFO: Got endpoints: latency-svc-kvbg5 [750.354578ms]
Feb 28 08:03:56.795: INFO: Created: latency-svc-v7r98
Feb 28 08:03:56.982: INFO: Got endpoints: latency-svc-wxjqb [750.422351ms]
Feb 28 08:03:56.982: INFO: Got endpoints: latency-svc-p88p5 [851.540229ms]
Feb 28 08:03:56.982: INFO: Got endpoints: latency-svc-8dccl [901.090618ms]
Feb 28 08:03:56.982: INFO: Got endpoints: latency-svc-dvwlk [799.969191ms]
Feb 28 08:03:56.994: INFO: Created: latency-svc-2p87r
Feb 28 08:03:57.006: INFO: Created: latency-svc-qrzkm
Feb 28 08:03:57.008: INFO: Created: latency-svc-ss527
Feb 28 08:03:57.018: INFO: Created: latency-svc-g47x5
Feb 28 08:03:57.031: INFO: Got endpoints: latency-svc-vtzqz [750.707338ms]
Feb 28 08:03:57.041: INFO: Created: latency-svc-rvlzz
Feb 28 08:03:57.080: INFO: Got endpoints: latency-svc-scb5w [746.930903ms]
Feb 28 08:03:57.096: INFO: Created: latency-svc-xvfdq
Feb 28 08:03:57.282: INFO: Got endpoints: latency-svc-r5fkf [800.669963ms]
Feb 28 08:03:57.282: INFO: Got endpoints: latency-svc-66l57 [901.0297ms]
Feb 28 08:03:57.282: INFO: Got endpoints: latency-svc-kl9cq [851.712359ms]
Feb 28 08:03:57.283: INFO: Got endpoints: latency-svc-s66wp [752.537604ms]
Feb 28 08:03:57.296: INFO: Created: latency-svc-5w8j4
Feb 28 08:03:57.302: INFO: Created: latency-svc-77dfb
Feb 28 08:03:57.309: INFO: Created: latency-svc-t5jqq
Feb 28 08:03:57.317: INFO: Created: latency-svc-nmjpv
Feb 28 08:03:57.330: INFO: Got endpoints: latency-svc-d4ztw [748.043874ms]
Feb 28 08:03:57.340: INFO: Created: latency-svc-crzns
Feb 28 08:03:57.532: INFO: Got endpoints: latency-svc-v7r98 [751.213696ms]
Feb 28 08:03:57.532: INFO: Got endpoints: latency-svc-lm6td [851.336123ms]
Feb 28 08:03:57.532: INFO: Got endpoints: latency-svc-g66jb [899.458782ms]
Feb 28 08:03:57.532: INFO: Got endpoints: latency-svc-ssclz [800.702474ms]
Feb 28 08:03:57.545: INFO: Created: latency-svc-kg2vb
Feb 28 08:03:57.551: INFO: Created: latency-svc-lh8bp
Feb 28 08:03:57.558: INFO: Created: latency-svc-5bvj7
Feb 28 08:03:57.566: INFO: Created: latency-svc-zthrb
Feb 28 08:03:57.581: INFO: Got endpoints: latency-svc-2p87r [598.72021ms]
Feb 28 08:03:57.593: INFO: Created: latency-svc-8j9cl
Feb 28 08:03:57.631: INFO: Got endpoints: latency-svc-qrzkm [648.853305ms]
Feb 28 08:03:57.644: INFO: Created: latency-svc-h7kr2
Feb 28 08:03:57.680: INFO: Got endpoints: latency-svc-ss527 [698.067891ms]
Feb 28 08:03:57.692: INFO: Created: latency-svc-d9sp8
Feb 28 08:03:57.733: INFO: Got endpoints: latency-svc-g47x5 [751.07912ms]
Feb 28 08:03:57.744: INFO: Created: latency-svc-t4l5k
Feb 28 08:03:57.780: INFO: Got endpoints: latency-svc-rvlzz [749.328577ms]
Feb 28 08:03:57.792: INFO: Created: latency-svc-27bzf
Feb 28 08:03:57.831: INFO: Got endpoints: latency-svc-xvfdq [750.528941ms]
Feb 28 08:03:57.845: INFO: Created: latency-svc-csnmf
Feb 28 08:03:57.946: INFO: Got endpoints: latency-svc-77dfb [663.919309ms]
Feb 28 08:03:57.946: INFO: Got endpoints: latency-svc-5w8j4 [663.795914ms]
Feb 28 08:03:57.969: INFO: Created: latency-svc-wldvx
Feb 28 08:03:57.969: INFO: Created: latency-svc-r2mjw
Feb 28 08:03:57.980: INFO: Got endpoints: latency-svc-t5jqq [697.756174ms]
Feb 28 08:03:57.993: INFO: Created: latency-svc-tbsrw
Feb 28 08:03:58.033: INFO: Got endpoints: latency-svc-nmjpv [750.57055ms]
Feb 28 08:03:58.052: INFO: Created: latency-svc-9gs82
Feb 28 08:03:58.080: INFO: Got endpoints: latency-svc-crzns [750.109416ms]
Feb 28 08:03:58.092: INFO: Created: latency-svc-f6hcj
Feb 28 08:03:58.131: INFO: Got endpoints: latency-svc-kg2vb [599.010383ms]
Feb 28 08:03:58.149: INFO: Created: latency-svc-4dsxb
Feb 28 08:03:58.181: INFO: Got endpoints: latency-svc-lh8bp [649.327337ms]
Feb 28 08:03:58.204: INFO: Created: latency-svc-xgbsw
Feb 28 08:03:58.236: INFO: Got endpoints: latency-svc-5bvj7 [703.79366ms]
Feb 28 08:03:58.281: INFO: Got endpoints: latency-svc-zthrb [749.071868ms]
Feb 28 08:03:58.287: INFO: Created: latency-svc-684jf
Feb 28 08:03:58.298: INFO: Created: latency-svc-zprxl
Feb 28 08:03:58.332: INFO: Got endpoints: latency-svc-8j9cl [751.266526ms]
Feb 28 08:03:58.345: INFO: Created: latency-svc-t77qh
Feb 28 08:03:58.390: INFO: Got endpoints: latency-svc-h7kr2 [759.166845ms]
Feb 28 08:03:58.402: INFO: Created: latency-svc-ng4ht
Feb 28 08:03:58.431: INFO: Got endpoints: latency-svc-d9sp8 [750.977605ms]
Feb 28 08:03:58.443: INFO: Created: latency-svc-dmtqj
Feb 28 08:03:58.484: INFO: Got endpoints: latency-svc-t4l5k [750.637145ms]
Feb 28 08:03:58.497: INFO: Created: latency-svc-rfb5k
Feb 28 08:03:58.531: INFO: Got endpoints: latency-svc-27bzf [750.883313ms]
Feb 28 08:03:58.545: INFO: Created: latency-svc-bvkrs
Feb 28 08:03:58.581: INFO: Got endpoints: latency-svc-csnmf [750.639816ms]
Feb 28 08:03:58.593: INFO: Created: latency-svc-xcwvl
Feb 28 08:03:58.631: INFO: Got endpoints: latency-svc-r2mjw [684.970262ms]
Feb 28 08:03:58.643: INFO: Created: latency-svc-96ccp
Feb 28 08:03:58.680: INFO: Got endpoints: latency-svc-wldvx [733.677659ms]
Feb 28 08:03:58.689: INFO: Created: latency-svc-4rpwz
Feb 28 08:03:58.731: INFO: Got endpoints: latency-svc-tbsrw [750.391437ms]
Feb 28 08:03:58.743: INFO: Created: latency-svc-9m6w5
Feb 28 08:03:58.782: INFO: Got endpoints: latency-svc-9gs82 [749.281498ms]
Feb 28 08:03:58.793: INFO: Created: latency-svc-r6phz
Feb 28 08:03:58.831: INFO: Got endpoints: latency-svc-f6hcj [751.012484ms]
Feb 28 08:03:58.842: INFO: Created: latency-svc-6tdnc
Feb 28 08:03:58.880: INFO: Got endpoints: latency-svc-4dsxb [749.562944ms]
Feb 28 08:03:58.895: INFO: Created: latency-svc-22fdh
Feb 28 08:03:58.930: INFO: Got endpoints: latency-svc-xgbsw [748.923213ms]
Feb 28 08:03:58.942: INFO: Created: latency-svc-nqnw7
Feb 28 08:03:58.982: INFO: Got endpoints: latency-svc-684jf [746.598906ms]
Feb 28 08:03:58.995: INFO: Created: latency-svc-57hn4
Feb 28 08:03:59.183: INFO: Got endpoints: latency-svc-t77qh [851.226464ms]
Feb 28 08:03:59.183: INFO: Got endpoints: latency-svc-zprxl [902.518926ms]
Feb 28 08:03:59.183: INFO: Got endpoints: latency-svc-dmtqj [752.273387ms]
Feb 28 08:03:59.183: INFO: Got endpoints: latency-svc-ng4ht [793.087881ms]
Feb 28 08:03:59.197: INFO: Created: latency-svc-b7m74
Feb 28 08:03:59.203: INFO: Created: latency-svc-r8d7k
Feb 28 08:03:59.210: INFO: Created: latency-svc-vkw8b
Feb 28 08:03:59.215: INFO: Created: latency-svc-9dxw9
Feb 28 08:03:59.230: INFO: Got endpoints: latency-svc-rfb5k [746.159339ms]
Feb 28 08:03:59.242: INFO: Created: latency-svc-wnkhc
Feb 28 08:03:59.282: INFO: Got endpoints: latency-svc-bvkrs [750.644465ms]
Feb 28 08:03:59.298: INFO: Created: latency-svc-w4q2l
Feb 28 08:03:59.330: INFO: Got endpoints: latency-svc-xcwvl [748.581709ms]
Feb 28 08:03:59.343: INFO: Created: latency-svc-xsr2f
Feb 28 08:03:59.381: INFO: Got endpoints: latency-svc-96ccp [750.282693ms]
Feb 28 08:03:59.393: INFO: Created: latency-svc-nnf5m
Feb 28 08:03:59.430: INFO: Got endpoints: latency-svc-4rpwz [749.652006ms]
Feb 28 08:03:59.441: INFO: Created: latency-svc-ns4pm
Feb 28 08:03:59.481: INFO: Got endpoints: latency-svc-9m6w5 [749.594411ms]
Feb 28 08:03:59.492: INFO: Created: latency-svc-r5w9j
Feb 28 08:03:59.531: INFO: Got endpoints: latency-svc-r6phz [748.957168ms]
Feb 28 08:03:59.543: INFO: Created: latency-svc-gf99v
Feb 28 08:03:59.581: INFO: Got endpoints: latency-svc-6tdnc [749.606413ms]
Feb 28 08:03:59.594: INFO: Created: latency-svc-2wmxp
Feb 28 08:03:59.632: INFO: Got endpoints: latency-svc-22fdh [751.017578ms]
Feb 28 08:03:59.645: INFO: Created: latency-svc-hd7xg
Feb 28 08:03:59.681: INFO: Got endpoints: latency-svc-nqnw7 [750.518566ms]
Feb 28 08:03:59.695: INFO: Created: latency-svc-mxnr2
Feb 28 08:03:59.730: INFO: Got endpoints: latency-svc-57hn4 [747.741536ms]
Feb 28 08:03:59.741: INFO: Created: latency-svc-fh78v
Feb 28 08:03:59.780: INFO: Got endpoints: latency-svc-b7m74 [596.818002ms]
Feb 28 08:03:59.793: INFO: Created: latency-svc-hxrfz
Feb 28 08:03:59.830: INFO: Got endpoints: latency-svc-r8d7k [646.800794ms]
Feb 28 08:03:59.841: INFO: Created: latency-svc-87zzh
Feb 28 08:03:59.881: INFO: Got endpoints: latency-svc-vkw8b [697.544569ms]
Feb 28 08:03:59.899: INFO: Created: latency-svc-wntqw
Feb 28 08:03:59.930: INFO: Got endpoints: latency-svc-9dxw9 [746.721326ms]
Feb 28 08:03:59.943: INFO: Created: latency-svc-v7tf5
Feb 28 08:03:59.980: INFO: Got endpoints: latency-svc-wnkhc [749.787144ms]
Feb 28 08:03:59.991: INFO: Created: latency-svc-qmffr
Feb 28 08:04:00.031: INFO: Got endpoints: latency-svc-w4q2l [749.330146ms]
Feb 28 08:04:00.042: INFO: Created: latency-svc-pvjrz
Feb 28 08:04:00.080: INFO: Got endpoints: latency-svc-xsr2f [749.839783ms]
Feb 28 08:04:00.092: INFO: Created: latency-svc-r5tgc
Feb 28 08:04:00.130: INFO: Got endpoints: latency-svc-nnf5m [748.552056ms]
Feb 28 08:04:00.143: INFO: Created: latency-svc-gk46t
Feb 28 08:04:00.181: INFO: Got endpoints: latency-svc-ns4pm [751.427215ms]
Feb 28 08:04:00.194: INFO: Created: latency-svc-dr76r
Feb 28 08:04:00.232: INFO: Got endpoints: latency-svc-r5w9j [750.876566ms]
Feb 28 08:04:00.248: INFO: Created: latency-svc-vq4bx
Feb 28 08:04:00.282: INFO: Got endpoints: latency-svc-gf99v [751.0514ms]
Feb 28 08:04:00.295: INFO: Created: latency-svc-c972q
Feb 28 08:04:00.330: INFO: Got endpoints: latency-svc-2wmxp [748.886686ms]
Feb 28 08:04:00.351: INFO: Created: latency-svc-qg8kl
Feb 28 08:04:00.380: INFO: Got endpoints: latency-svc-hd7xg [748.399045ms]
Feb 28 08:04:00.393: INFO: Created: latency-svc-cjjl8
Feb 28 08:04:00.430: INFO: Got endpoints: latency-svc-mxnr2 [748.80881ms]
Feb 28 08:04:00.451: INFO: Created: latency-svc-f4q4f
Feb 28 08:04:00.481: INFO: Got endpoints: latency-svc-fh78v [751.222235ms]
Feb 28 08:04:00.504: INFO: Created: latency-svc-rgqzr
Feb 28 08:04:00.531: INFO: Got endpoints: latency-svc-hxrfz [750.658888ms]
Feb 28 08:04:00.545: INFO: Created: latency-svc-w56l6
Feb 28 08:04:00.580: INFO: Got endpoints: latency-svc-87zzh [749.534066ms]
Feb 28 08:04:00.592: INFO: Created: latency-svc-d7pdg
Feb 28 08:04:00.630: INFO: Got endpoints: latency-svc-wntqw [749.438879ms]
Feb 28 08:04:00.651: INFO: Created: latency-svc-wr54n
Feb 28 08:04:00.680: INFO: Got endpoints: latency-svc-v7tf5 [749.617307ms]
Feb 28 08:04:00.691: INFO: Created: latency-svc-kkvxr
Feb 28 08:04:00.730: INFO: Got endpoints: latency-svc-qmffr [749.828131ms]
Feb 28 08:04:00.740: INFO: Created: latency-svc-9zm9s
Feb 28 08:04:00.781: INFO: Got endpoints: latency-svc-pvjrz [749.023207ms]
Feb 28 08:04:00.794: INFO: Created: latency-svc-s9zkb
Feb 28 08:04:00.831: INFO: Got endpoints: latency-svc-r5tgc [750.625565ms]
Feb 28 08:04:00.846: INFO: Created: latency-svc-z8bm2
Feb 28 08:04:00.881: INFO: Got endpoints: latency-svc-gk46t [750.593009ms]
Feb 28 08:04:00.893: INFO: Created: latency-svc-kdjpw
Feb 28 08:04:00.931: INFO: Got endpoints: latency-svc-dr76r [750.160648ms]
Feb 28 08:04:00.944: INFO: Created: latency-svc-pfxc6
Feb 28 08:04:00.981: INFO: Got endpoints: latency-svc-vq4bx [749.113259ms]
Feb 28 08:04:00.995: INFO: Created: latency-svc-2z48c
Feb 28 08:04:01.030: INFO: Got endpoints: latency-svc-c972q [747.776533ms]
Feb 28 08:04:01.042: INFO: Created: latency-svc-lmzfw
Feb 28 08:04:01.080: INFO: Got endpoints: latency-svc-qg8kl [749.755712ms]
Feb 28 08:04:01.091: INFO: Created: latency-svc-mw5pk
Feb 28 08:04:01.131: INFO: Got endpoints: latency-svc-cjjl8 [750.752405ms]
Feb 28 08:04:01.146: INFO: Created: latency-svc-2kk8p
Feb 28 08:04:01.182: INFO: Got endpoints: latency-svc-f4q4f [752.018371ms]
Feb 28 08:04:01.193: INFO: Created: latency-svc-hbpml
Feb 28 08:04:01.230: INFO: Got endpoints: latency-svc-rgqzr [748.56082ms]
Feb 28 08:04:01.242: INFO: Created: latency-svc-9qr64
Feb 28 08:04:01.281: INFO: Got endpoints: latency-svc-w56l6 [749.418961ms]
Feb 28 08:04:01.291: INFO: Created: latency-svc-bbqtr
Feb 28 08:04:01.330: INFO: Got endpoints: latency-svc-d7pdg [749.916483ms]
Feb 28 08:04:01.341: INFO: Created: latency-svc-5pbft
Feb 28 08:04:01.381: INFO: Got endpoints: latency-svc-wr54n [750.11256ms]
Feb 28 08:04:01.392: INFO: Created: latency-svc-vw55k
Feb 28 08:04:01.430: INFO: Got endpoints: latency-svc-kkvxr [749.739272ms]
Feb 28 08:04:01.442: INFO: Created: latency-svc-5p2p4
Feb 28 08:04:01.481: INFO: Got endpoints: latency-svc-9zm9s [750.90691ms]
Feb 28 08:04:01.495: INFO: Created: latency-svc-5qhpl
Feb 28 08:04:01.530: INFO: Got endpoints: latency-svc-s9zkb [749.782701ms]
Feb 28 08:04:01.541: INFO: Created: latency-svc-6qnkw
Feb 28 08:04:01.580: INFO: Got endpoints: latency-svc-z8bm2 [749.32844ms]
Feb 28 08:04:01.593: INFO: Created: latency-svc-xtv7q
Feb 28 08:04:01.630: INFO: Got endpoints: latency-svc-kdjpw [749.160825ms]
Feb 28 08:04:01.644: INFO: Created: latency-svc-6chp5
Feb 28 08:04:01.680: INFO: Got endpoints: latency-svc-pfxc6 [748.857188ms]
Feb 28 08:04:01.692: INFO: Created: latency-svc-9r9ds
Feb 28 08:04:01.731: INFO: Got endpoints: latency-svc-2z48c [749.890978ms]
Feb 28 08:04:01.742: INFO: Created: latency-svc-qgqtl
Feb 28 08:04:01.781: INFO: Got endpoints: latency-svc-lmzfw [751.057319ms]
Feb 28 08:04:01.793: INFO: Created: latency-svc-hgkdx
Feb 28 08:04:01.830: INFO: Got endpoints: latency-svc-mw5pk [750.124063ms]
Feb 28 08:04:01.842: INFO: Created: latency-svc-ng4js
Feb 28 08:04:01.880: INFO: Got endpoints: latency-svc-2kk8p [749.162899ms]
Feb 28 08:04:01.894: INFO: Created: latency-svc-qc278
Feb 28 08:04:01.930: INFO: Got endpoints: latency-svc-hbpml [748.489088ms]
Feb 28 08:04:01.942: INFO: Created: latency-svc-lfw85
Feb 28 08:04:01.981: INFO: Got endpoints: latency-svc-9qr64 [750.480531ms]
Feb 28 08:04:01.992: INFO: Created: latency-svc-qvgpx
Feb 28 08:04:02.031: INFO: Got endpoints: latency-svc-bbqtr [749.847586ms]
Feb 28 08:04:02.042: INFO: Created: latency-svc-lvkx6
Feb 28 08:04:02.081: INFO: Got endpoints: latency-svc-5pbft [751.214418ms]
Feb 28 08:04:02.093: INFO: Created: latency-svc-b72rn
Feb 28 08:04:02.131: INFO: Got endpoints: latency-svc-vw55k [750.124256ms]
Feb 28 08:04:02.144: INFO: Created: latency-svc-hvz5f
Feb 28 08:04:02.181: INFO: Got endpoints: latency-svc-5p2p4 [750.824362ms]
Feb 28 08:04:02.192: INFO: Created: latency-svc-q9sdr
Feb 28 08:04:02.230: INFO: Got endpoints: latency-svc-5qhpl [749.505445ms]
Feb 28 08:04:02.243: INFO: Created: latency-svc-nkv95
Feb 28 08:04:02.280: INFO: Got endpoints: latency-svc-6qnkw [749.770002ms]
Feb 28 08:04:02.292: INFO: Created: latency-svc-fhpqp
Feb 28 08:04:02.330: INFO: Got endpoints: latency-svc-xtv7q [750.329617ms]
Feb 28 08:04:02.349: INFO: Created: latency-svc-dsmmx
Feb 28 08:04:02.380: INFO: Got endpoints: latency-svc-6chp5 [749.879768ms]
Feb 28 08:04:02.391: INFO: Created: latency-svc-hrnck
Feb 28 08:04:02.433: INFO: Got endpoints: latency-svc-9r9ds [753.14501ms]
Feb 28 08:04:02.447: INFO: Created: latency-svc-s5n2h
Feb 28 08:04:02.480: INFO: Got endpoints: latency-svc-qgqtl [749.289873ms]
Feb 28 08:04:02.494: INFO: Created: latency-svc-k5jk9
Feb 28 08:04:02.530: INFO: Got endpoints: latency-svc-hgkdx [748.905266ms]
Feb 28 08:04:02.562: INFO: Created: latency-svc-lzsrr
Feb 28 08:04:02.580: INFO: Got endpoints: latency-svc-ng4js [750.198172ms]
Feb 28 08:04:02.592: INFO: Created: latency-svc-6nq8p
Feb 28 08:04:02.629: INFO: Got endpoints: latency-svc-qc278 [748.906637ms]
Feb 28 08:04:02.644: INFO: Created: latency-svc-vhv2r
Feb 28 08:04:02.681: INFO: Got endpoints: latency-svc-lfw85 [750.931059ms]
Feb 28 08:04:02.697: INFO: Created: latency-svc-bvnzk
Feb 28 08:04:02.730: INFO: Got endpoints: latency-svc-qvgpx [749.127691ms]
Feb 28 08:04:02.747: INFO: Created: latency-svc-jtmpt
Feb 28 08:04:02.780: INFO: Got endpoints: latency-svc-lvkx6 [749.504559ms]
Feb 28 08:04:02.799: INFO: Created: latency-svc-6h5mw
Feb 28 08:04:02.831: INFO: Got endpoints: latency-svc-b72rn [749.464549ms]
Feb 28 08:04:02.844: INFO: Created: latency-svc-8wfft
Feb 28 08:04:02.880: INFO: Got endpoints: latency-svc-hvz5f [749.415941ms]
Feb 28 08:04:02.891: INFO: Created: latency-svc-2l54c
Feb 28 08:04:02.930: INFO: Got endpoints: latency-svc-q9sdr [749.172843ms]
Feb 28 08:04:02.943: INFO: Created: latency-svc-fh459
Feb 28 08:04:02.980: INFO: Got endpoints: latency-svc-nkv95 [749.686085ms]
Feb 28 08:04:02.997: INFO: Created: latency-svc-bffpn
Feb 28 08:04:03.032: INFO: Got endpoints: latency-svc-fhpqp [752.072034ms]
Feb 28 08:04:03.045: INFO: Created: latency-svc-dtcgj
Feb 28 08:04:03.081: INFO: Got endpoints: latency-svc-dsmmx [750.17636ms]
Feb 28 08:04:03.094: INFO: Created: latency-svc-8jngf
Feb 28 08:04:03.130: INFO: Got endpoints: latency-svc-hrnck [749.831198ms]
Feb 28 08:04:03.180: INFO: Got endpoints: latency-svc-s5n2h [746.202593ms]
Feb 28 08:04:03.231: INFO: Got endpoints: latency-svc-k5jk9 [750.337803ms]
Feb 28 08:04:03.281: INFO: Got endpoints: latency-svc-lzsrr [750.240957ms]
Feb 28 08:04:03.330: INFO: Got endpoints: latency-svc-6nq8p [750.01633ms]
Feb 28 08:04:03.381: INFO: Got endpoints: latency-svc-vhv2r [751.265798ms]
Feb 28 08:04:03.430: INFO: Got endpoints: latency-svc-bvnzk [748.735999ms]
Feb 28 08:04:03.480: INFO: Got endpoints: latency-svc-jtmpt [749.870103ms]
Feb 28 08:04:03.532: INFO: Got endpoints: latency-svc-6h5mw [751.370802ms]
Feb 28 08:04:03.581: INFO: Got endpoints: latency-svc-8wfft [749.937466ms]
Feb 28 08:04:03.630: INFO: Got endpoints: latency-svc-2l54c [749.750467ms]
Feb 28 08:04:03.681: INFO: Got endpoints: latency-svc-fh459 [750.371171ms]
Feb 28 08:04:03.730: INFO: Got endpoints: latency-svc-bffpn [749.616715ms]
Feb 28 08:04:03.780: INFO: Got endpoints: latency-svc-dtcgj [747.789751ms]
Feb 28 08:04:03.830: INFO: Got endpoints: latency-svc-8jngf [749.357705ms]
Feb 28 08:04:03.830: INFO: Latencies: [14.247454ms 21.294122ms 28.766609ms 37.621623ms 46.030186ms 50.796775ms 66.98956ms 73.804912ms 82.818833ms 88.181662ms 93.657491ms 101.207436ms 102.488703ms 103.387833ms 107.205451ms 111.798862ms 112.101896ms 112.764877ms 112.911934ms 113.131052ms 113.290911ms 113.446102ms 113.479122ms 113.482099ms 113.524705ms 113.986811ms 114.956243ms 115.43317ms 119.052913ms 119.362881ms 126.839966ms 128.654047ms 128.736865ms 130.432506ms 139.208631ms 182.443283ms 227.455277ms 269.248114ms 300.589718ms 342.663301ms 388.138748ms 430.07795ms 472.962993ms 514.987808ms 556.766411ms 591.077907ms 596.818002ms 598.72021ms 599.010383ms 634.973824ms 646.800794ms 648.853305ms 649.327337ms 663.795914ms 663.919309ms 677.396215ms 684.970262ms 697.544569ms 697.756174ms 698.067891ms 703.79366ms 714.843549ms 733.677659ms 746.159339ms 746.202593ms 746.598906ms 746.721326ms 746.930903ms 747.435397ms 747.741536ms 747.776533ms 747.789751ms 748.043874ms 748.399045ms 748.489088ms 748.552056ms 748.56082ms 748.581709ms 748.735999ms 748.80881ms 748.857188ms 748.886686ms 748.905266ms 748.906637ms 748.923213ms 748.957168ms 749.023207ms 749.071868ms 749.113259ms 749.127691ms 749.160825ms 749.162899ms 749.172843ms 749.281498ms 749.289873ms 749.32844ms 749.328577ms 749.330146ms 749.357705ms 749.415941ms 749.418961ms 749.438879ms 749.464549ms 749.504559ms 749.505445ms 749.534066ms 749.562944ms 749.594411ms 749.599425ms 749.600204ms 749.606413ms 749.616715ms 749.617307ms 749.652006ms 749.686085ms 749.739272ms 749.750467ms 749.755712ms 749.770002ms 749.782701ms 749.787144ms 749.828131ms 749.831198ms 749.839783ms 749.847586ms 749.870103ms 749.879768ms 749.890978ms 749.916483ms 749.937466ms 750.01633ms 750.076961ms 750.109416ms 750.11256ms 750.124063ms 750.124256ms 750.160648ms 750.17636ms 750.190136ms 750.198172ms 750.201728ms 750.240957ms 750.282693ms 750.329617ms 750.337803ms 750.354578ms 750.371171ms 750.391437ms 750.422351ms 750.480531ms 750.518566ms 750.528941ms 750.57055ms 750.593009ms 750.625565ms 750.637145ms 750.639816ms 750.644465ms 750.658888ms 750.707338ms 750.752405ms 750.824362ms 750.876566ms 750.883313ms 750.90691ms 750.919002ms 750.931059ms 750.977605ms 751.012484ms 751.017578ms 751.0514ms 751.057319ms 751.07912ms 751.213696ms 751.214418ms 751.222235ms 751.265798ms 751.266526ms 751.370802ms 751.427215ms 751.750831ms 752.018371ms 752.072034ms 752.104136ms 752.273387ms 752.537604ms 753.14501ms 759.166845ms 793.087881ms 799.969191ms 800.669963ms 800.702474ms 851.226464ms 851.336123ms 851.540229ms 851.712359ms 899.458782ms 901.0297ms 901.090618ms 902.518926ms]
Feb 28 08:04:03.830: INFO: 50 %ile: 749.418961ms
Feb 28 08:04:03.831: INFO: 90 %ile: 751.750831ms
Feb 28 08:04:03.831: INFO: 99 %ile: 901.090618ms
Feb 28 08:04:03.831: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:04:03.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-8p78d" for this suite.
Feb 28 08:04:15.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:04:15.977: INFO: namespace: e2e-tests-svc-latency-8p78d, resource: bindings, ignored listing per whitelist
Feb 28 08:04:16.053: INFO: namespace e2e-tests-svc-latency-8p78d deletion completed in 12.216379549s

• [SLOW TEST:23.296 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:04:16.054: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-4pfsx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 28 08:04:16.306: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:17.351: INFO: stderr: ""
Feb 28 08:04:17.351: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:04:17.351: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:17.452: INFO: stderr: ""
Feb 28 08:04:17.453: INFO: stdout: "update-demo-nautilus-5mwgc update-demo-nautilus-68z79 "
Feb 28 08:04:17.453: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5mwgc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:17.562: INFO: stderr: ""
Feb 28 08:04:17.562: INFO: stdout: ""
Feb 28 08:04:17.562: INFO: update-demo-nautilus-5mwgc is created but not running
Feb 28 08:04:22.562: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:22.671: INFO: stderr: ""
Feb 28 08:04:22.671: INFO: stdout: "update-demo-nautilus-5mwgc update-demo-nautilus-68z79 "
Feb 28 08:04:22.672: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5mwgc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:22.772: INFO: stderr: ""
Feb 28 08:04:22.772: INFO: stdout: "true"
Feb 28 08:04:22.772: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-5mwgc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:22.914: INFO: stderr: ""
Feb 28 08:04:22.914: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:04:22.914: INFO: validating pod update-demo-nautilus-5mwgc
Feb 28 08:04:22.999: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:04:22.999: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:04:22.999: INFO: update-demo-nautilus-5mwgc is verified up and running
Feb 28 08:04:22.999: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-68z79 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:23.104: INFO: stderr: ""
Feb 28 08:04:23.104: INFO: stdout: "true"
Feb 28 08:04:23.104: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-nautilus-68z79 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:23.226: INFO: stderr: ""
Feb 28 08:04:23.226: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 08:04:23.226: INFO: validating pod update-demo-nautilus-68z79
Feb 28 08:04:23.312: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 08:04:23.312: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 08:04:23.312: INFO: update-demo-nautilus-68z79 is verified up and running
STEP: rolling-update to new replication controller
Feb 28 08:04:23.320: INFO: scanned /root for discovery docs: <nil>
Feb 28 08:04:23.320: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:41.991: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 08:04:41.991: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 08:04:41.991: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:42.100: INFO: stderr: ""
Feb 28 08:04:42.100: INFO: stdout: "update-demo-kitten-96pz2 update-demo-kitten-cx8lj "
Feb 28 08:04:42.100: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-96pz2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:42.195: INFO: stderr: ""
Feb 28 08:04:42.195: INFO: stdout: "true"
Feb 28 08:04:42.195: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-96pz2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:42.292: INFO: stderr: ""
Feb 28 08:04:42.292: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 08:04:42.292: INFO: validating pod update-demo-kitten-96pz2
Feb 28 08:04:42.375: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 08:04:42.375: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 08:04:42.375: INFO: update-demo-kitten-96pz2 is verified up and running
Feb 28 08:04:42.375: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-cx8lj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:42.485: INFO: stderr: ""
Feb 28 08:04:42.485: INFO: stdout: "true"
Feb 28 08:04:42.485: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods update-demo-kitten-cx8lj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-4pfsx'
Feb 28 08:04:42.588: INFO: stderr: ""
Feb 28 08:04:42.588: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 08:04:42.588: INFO: validating pod update-demo-kitten-cx8lj
Feb 28 08:04:42.679: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 08:04:42.679: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 08:04:42.679: INFO: update-demo-kitten-cx8lj is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:04:42.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4pfsx" for this suite.
Feb 28 08:05:04.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:05:04.791: INFO: namespace: e2e-tests-kubectl-4pfsx, resource: bindings, ignored listing per whitelist
Feb 28 08:05:04.836: INFO: namespace e2e-tests-kubectl-4pfsx deletion completed in 22.151045556s

• [SLOW TEST:48.782 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:05:04.836: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-2r4xl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:06:05.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-2r4xl" for this suite.
Feb 28 08:06:27.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:06:27.207: INFO: namespace: e2e-tests-container-probe-2r4xl, resource: bindings, ignored listing per whitelist
Feb 28 08:06:27.406: INFO: namespace e2e-tests-container-probe-2r4xl deletion completed in 22.274775094s

• [SLOW TEST:82.570 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:06:27.406: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-l7w2v
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-bfb21044-3b2f-11e9-b1ab-fe0431d33f49
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:06:29.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-l7w2v" for this suite.
Feb 28 08:06:51.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:06:52.257: INFO: namespace: e2e-tests-configmap-l7w2v, resource: bindings, ignored listing per whitelist
Feb 28 08:06:52.334: INFO: namespace e2e-tests-configmap-l7w2v deletion completed in 22.492253457s

• [SLOW TEST:24.928 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:06:52.334: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-nfl6r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 28 08:06:52.610: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:06:52.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nfl6r" for this suite.
Feb 28 08:06:58.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:06:58.903: INFO: namespace: e2e-tests-kubectl-nfl6r, resource: bindings, ignored listing per whitelist
Feb 28 08:06:59.011: INFO: namespace e2e-tests-kubectl-nfl6r deletion completed in 6.201285347s

• [SLOW TEST:6.677 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:06:59.012: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-k8s5c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:06:59.306: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-k8s5c'
Feb 28 08:06:59.428: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 08:06:59.428: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 28 08:06:59.436: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 28 08:06:59.441: INFO: scanned /root for discovery docs: <nil>
Feb 28 08:06:59.441: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-k8s5c'
Feb 28 08:07:15.270: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 08:07:15.270: INFO: stdout: "Created e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7\nScaling up e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 28 08:07:15.270: INFO: stdout: "Created e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7\nScaling up e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 28 08:07:15.270: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-k8s5c'
Feb 28 08:07:15.386: INFO: stderr: ""
Feb 28 08:07:15.386: INFO: stdout: "e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7-t2t6b "
Feb 28 08:07:15.386: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7-t2t6b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8s5c'
Feb 28 08:07:15.492: INFO: stderr: ""
Feb 28 08:07:15.492: INFO: stdout: "true"
Feb 28 08:07:15.492: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7-t2t6b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8s5c'
Feb 28 08:07:15.595: INFO: stderr: ""
Feb 28 08:07:15.595: INFO: stdout: "nginx:1.14-alpine"
Feb 28 08:07:15.595: INFO: e2e-test-nginx-rc-531025779b4fcc02485e92bac4766bb7-t2t6b is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
Feb 28 08:07:15.595: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-k8s5c'
Feb 28 08:07:15.721: INFO: stderr: ""
Feb 28 08:07:15.721: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:07:15.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-k8s5c" for this suite.
Feb 28 08:07:37.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:07:37.894: INFO: namespace: e2e-tests-kubectl-k8s5c, resource: bindings, ignored listing per whitelist
Feb 28 08:07:37.928: INFO: namespace e2e-tests-kubectl-k8s5c deletion completed in 22.198831186s

• [SLOW TEST:38.916 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:07:37.928: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-x9w8z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0228 08:07:48.446564   30646 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:07:48.446: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:07:48.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-x9w8z" for this suite.
Feb 28 08:07:54.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:07:54.556: INFO: namespace: e2e-tests-gc-x9w8z, resource: bindings, ignored listing per whitelist
Feb 28 08:07:54.647: INFO: namespace e2e-tests-gc-x9w8z deletion completed in 6.193788839s

• [SLOW TEST:16.719 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:07:54.647: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rvgdm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 28 08:07:54.907: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix896109140/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:07:54.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rvgdm" for this suite.
Feb 28 08:08:00.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:01.120: INFO: namespace: e2e-tests-kubectl-rvgdm, resource: bindings, ignored listing per whitelist
Feb 28 08:08:01.144: INFO: namespace e2e-tests-kubectl-rvgdm deletion completed in 6.164460336s

• [SLOW TEST:6.497 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:08:01.145: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-cw7gz
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 28 08:08:01.512: INFO: Waiting up to 5m0s for pod "pod-f7999423-3b2f-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-cw7gz" to be "success or failure"
Feb 28 08:08:01.519: INFO: Pod "pod-f7999423-3b2f-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.654525ms
Feb 28 08:08:03.524: INFO: Pod "pod-f7999423-3b2f-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011924461s
STEP: Saw pod success
Feb 28 08:08:03.524: INFO: Pod "pod-f7999423-3b2f-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:08:03.527: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-f7999423-3b2f-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:08:03.547: INFO: Waiting for pod pod-f7999423-3b2f-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:08:03.550: INFO: Pod pod-f7999423-3b2f-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:08:03.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cw7gz" for this suite.
Feb 28 08:08:09.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:09.706: INFO: namespace: e2e-tests-emptydir-cw7gz, resource: bindings, ignored listing per whitelist
Feb 28 08:08:09.755: INFO: namespace e2e-tests-emptydir-cw7gz deletion completed in 6.200072324s

• [SLOW TEST:8.611 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:08:09.756: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-k2pz5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fcacc762-3b2f-11e9-b1ab-fe0431d33f49
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fcacc762-3b2f-11e9-b1ab-fe0431d33f49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:08:14.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k2pz5" for this suite.
Feb 28 08:08:36.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:36.493: INFO: namespace: e2e-tests-projected-k2pz5, resource: bindings, ignored listing per whitelist
Feb 28 08:08:36.575: INFO: namespace e2e-tests-projected-k2pz5 deletion completed in 22.19770028s

• [SLOW TEST:26.819 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:08:36.575: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-j7ng4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 08:08:36.916: INFO: Waiting up to 5m0s for pod "pod-0cb3925e-3b30-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-j7ng4" to be "success or failure"
Feb 28 08:08:36.920: INFO: Pod "pod-0cb3925e-3b30-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.419976ms
Feb 28 08:08:38.925: INFO: Pod "pod-0cb3925e-3b30-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009049782s
STEP: Saw pod success
Feb 28 08:08:38.925: INFO: Pod "pod-0cb3925e-3b30-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:08:38.929: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-0cb3925e-3b30-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:08:38.946: INFO: Waiting for pod pod-0cb3925e-3b30-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:08:38.950: INFO: Pod pod-0cb3925e-3b30-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:08:38.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-j7ng4" for this suite.
Feb 28 08:08:44.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:08:45.118: INFO: namespace: e2e-tests-emptydir-j7ng4, resource: bindings, ignored listing per whitelist
Feb 28 08:08:45.246: INFO: namespace e2e-tests-emptydir-j7ng4 deletion completed in 6.291258234s

• [SLOW TEST:8.672 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:08:45.247: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-4k66q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4k66q
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:08:45.506: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:09:05.582: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.1.103 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4k66q PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:09:05.582: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:09:07.069: INFO: Found all expected endpoints: [netserver-0]
Feb 28 08:09:07.079: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.0.35 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4k66q PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:09:07.079: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:09:08.499: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:09:08.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4k66q" for this suite.
Feb 28 08:09:30.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:09:30.597: INFO: namespace: e2e-tests-pod-network-test-4k66q, resource: bindings, ignored listing per whitelist
Feb 28 08:09:30.697: INFO: namespace e2e-tests-pod-network-test-4k66q deletion completed in 22.188397617s

• [SLOW TEST:45.451 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:09:30.698: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-db22t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:09:31.250: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-db22t'
Feb 28 08:09:31.528: INFO: stderr: ""
Feb 28 08:09:31.529: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 28 08:09:36.580: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-db22t -o json'
Feb 28 08:09:36.693: INFO: stderr: ""
Feb 28 08:09:36.693: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.105/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-28T08:09:31Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-db22t\",\n        \"resourceVersion\": \"10506\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-db22t/pods/e2e-test-nginx-pod\",\n        \"uid\": \"2d3f6be2-3b30-11e9-a4f3-1a76b5824175\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zltds\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"nodeName\": \"shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zltds\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zltds\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T08:09:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T08:09:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T08:09:33Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-28T08:09:31Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9de2d96d47b182b58e5d4c48d8183b7f4b28ac490c84ffec76d4a16d89e30a41\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-28T08:09:32Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.18\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.105\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-28T08:09:31Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 28 08:09:36.693: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-db22t'
Feb 28 08:09:37.006: INFO: stderr: ""
Feb 28 08:09:37.006: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Feb 28 08:09:37.014: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-db22t'
Feb 28 08:09:39.312: INFO: stderr: ""
Feb 28 08:09:39.312: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:09:39.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-db22t" for this suite.
Feb 28 08:09:45.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:09:45.530: INFO: namespace: e2e-tests-kubectl-db22t, resource: bindings, ignored listing per whitelist
Feb 28 08:09:45.533: INFO: namespace e2e-tests-kubectl-db22t deletion completed in 6.21439111s

• [SLOW TEST:14.835 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:09:45.533: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-xfnqp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0228 08:09:46.955840   30646 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:09:46.955: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:09:46.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-xfnqp" for this suite.
Feb 28 08:09:52.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:09:53.156: INFO: namespace: e2e-tests-gc-xfnqp, resource: bindings, ignored listing per whitelist
Feb 28 08:09:53.170: INFO: namespace e2e-tests-gc-xfnqp deletion completed in 6.208820244s

• [SLOW TEST:7.637 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:09:53.170: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-khhvg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-mltgv
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 28 08:10:03.007: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-vjqvj
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:10:20.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-khhvg" for this suite.
Feb 28 08:10:26.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:26.392: INFO: namespace: e2e-tests-namespaces-khhvg, resource: bindings, ignored listing per whitelist
Feb 28 08:10:26.415: INFO: namespace e2e-tests-namespaces-khhvg deletion completed in 6.178030176s
STEP: Destroying namespace "e2e-tests-nsdeletetest-mltgv" for this suite.
Feb 28 08:10:26.418: INFO: Namespace e2e-tests-nsdeletetest-mltgv was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-vjqvj" for this suite.
Feb 28 08:10:32.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:32.468: INFO: namespace: e2e-tests-nsdeletetest-vjqvj, resource: bindings, ignored listing per whitelist
Feb 28 08:10:32.578: INFO: namespace e2e-tests-nsdeletetest-vjqvj deletion completed in 6.160013833s

• [SLOW TEST:39.408 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:10:32.578: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-4z8bn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 28 08:10:32.937: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-4z8bn,SelfLink:/api/v1/namespaces/e2e-tests-watch-4z8bn/configmaps/e2e-watch-test-resource-version,UID:51d7b5fe-3b30-11e9-a4f3-1a76b5824175,ResourceVersion:10716,Generation:0,CreationTimestamp:2019-02-28 08:10:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:10:32.937: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-4z8bn,SelfLink:/api/v1/namespaces/e2e-tests-watch-4z8bn/configmaps/e2e-watch-test-resource-version,UID:51d7b5fe-3b30-11e9-a4f3-1a76b5824175,ResourceVersion:10717,Generation:0,CreationTimestamp:2019-02-28 08:10:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:10:32.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-4z8bn" for this suite.
Feb 28 08:10:38.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:39.126: INFO: namespace: e2e-tests-watch-4z8bn, resource: bindings, ignored listing per whitelist
Feb 28 08:10:39.151: INFO: namespace e2e-tests-watch-4z8bn deletion completed in 6.209185427s

• [SLOW TEST:6.573 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:10:39.151: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-znhhk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:10:39.437: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55ba82c7-3b30-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-znhhk" to be "success or failure"
Feb 28 08:10:39.467: INFO: Pod "downwardapi-volume-55ba82c7-3b30-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.829364ms
Feb 28 08:10:41.477: INFO: Pod "downwardapi-volume-55ba82c7-3b30-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015180545s
STEP: Saw pod success
Feb 28 08:10:41.478: INFO: Pod "downwardapi-volume-55ba82c7-3b30-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:10:41.481: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-55ba82c7-3b30-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:10:41.514: INFO: Waiting for pod downwardapi-volume-55ba82c7-3b30-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:10:41.520: INFO: Pod downwardapi-volume-55ba82c7-3b30-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:10:41.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-znhhk" for this suite.
Feb 28 08:10:47.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:47.686: INFO: namespace: e2e-tests-downward-api-znhhk, resource: bindings, ignored listing per whitelist
Feb 28 08:10:47.686: INFO: namespace e2e-tests-downward-api-znhhk deletion completed in 6.161053194s

• [SLOW TEST:8.535 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:10:47.687: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-r2fdt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:10:47.921: INFO: Creating deployment "test-recreate-deployment"
Feb 28 08:10:47.927: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 28 08:10:47.936: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 28 08:10:49.949: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 28 08:10:50.173: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 28 08:10:50.189: INFO: Updating deployment test-recreate-deployment
Feb 28 08:10:50.189: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:10:50.248: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-r2fdt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-r2fdt/deployments/test-recreate-deployment,UID:5acb11e9-3b30-11e9-a4f3-1a76b5824175,ResourceVersion:10812,Generation:2,CreationTimestamp:2019-02-28 08:10:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-28 08:10:50 +0000 UTC 2019-02-28 08:10:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-28 08:10:50 +0000 UTC 2019-02-28 08:10:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 28 08:10:50.269: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-r2fdt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-r2fdt/replicasets/test-recreate-deployment-7cf749666b,UID:5c2818c1-3b30-11e9-a4f3-1a76b5824175,ResourceVersion:10810,Generation:1,CreationTimestamp:2019-02-28 08:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5acb11e9-3b30-11e9-a4f3-1a76b5824175 0xc00287c297 0xc00287c298}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:10:50.270: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 28 08:10:50.270: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-r2fdt,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-r2fdt/replicasets/test-recreate-deployment-79f694ff59,UID:5acc14e9-3b30-11e9-a4f3-1a76b5824175,ResourceVersion:10804,Generation:2,CreationTimestamp:2019-02-28 08:10:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5acb11e9-3b30-11e9-a4f3-1a76b5824175 0xc00287c1c7 0xc00287c1c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:10:50.274: INFO: Pod "test-recreate-deployment-7cf749666b-mkvs5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-mkvs5,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-r2fdt,SelfLink:/api/v1/namespaces/e2e-tests-deployment-r2fdt/pods/test-recreate-deployment-7cf749666b-mkvs5,UID:5c28b76f-3b30-11e9-a4f3-1a76b5824175,ResourceVersion:10813,Generation:0,CreationTimestamp:2019-02-28 08:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b 5c2818c1-3b30-11e9-a4f3-1a76b5824175 0xc00292c147 0xc00292c148}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p96qx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p96qx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p96qx true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00292c1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00292c1d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:10:50.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-r2fdt" for this suite.
Feb 28 08:10:56.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:10:56.414: INFO: namespace: e2e-tests-deployment-r2fdt, resource: bindings, ignored listing per whitelist
Feb 28 08:10:56.721: INFO: namespace e2e-tests-deployment-r2fdt deletion completed in 6.441007386s

• [SLOW TEST:9.034 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:10:56.721: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-vs9nc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-vs9nc/configmap-test-60459dd5-3b30-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:10:57.128: INFO: Waiting up to 5m0s for pod "pod-configmaps-6046c1ce-3b30-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-configmap-vs9nc" to be "success or failure"
Feb 28 08:10:57.131: INFO: Pod "pod-configmaps-6046c1ce-3b30-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.274301ms
Feb 28 08:10:59.136: INFO: Pod "pod-configmaps-6046c1ce-3b30-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007914142s
STEP: Saw pod success
Feb 28 08:10:59.136: INFO: Pod "pod-configmaps-6046c1ce-3b30-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:10:59.140: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-6046c1ce-3b30-11e9-b1ab-fe0431d33f49 container env-test: <nil>
STEP: delete the pod
Feb 28 08:10:59.161: INFO: Waiting for pod pod-configmaps-6046c1ce-3b30-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:10:59.164: INFO: Pod pod-configmaps-6046c1ce-3b30-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:10:59.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vs9nc" for this suite.
Feb 28 08:11:05.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:11:05.255: INFO: namespace: e2e-tests-configmap-vs9nc, resource: bindings, ignored listing per whitelist
Feb 28 08:11:05.380: INFO: namespace e2e-tests-configmap-vs9nc deletion completed in 6.210989055s

• [SLOW TEST:8.659 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:11:05.380: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-2hj9h
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-65642cd7-3b30-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:11:05.715: INFO: Waiting up to 5m0s for pod "pod-configmaps-6564cb8a-3b30-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-configmap-2hj9h" to be "success or failure"
Feb 28 08:11:05.721: INFO: Pod "pod-configmaps-6564cb8a-3b30-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064182ms
Feb 28 08:11:07.726: INFO: Pod "pod-configmaps-6564cb8a-3b30-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011198912s
STEP: Saw pod success
Feb 28 08:11:07.726: INFO: Pod "pod-configmaps-6564cb8a-3b30-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:11:07.729: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-6564cb8a-3b30-11e9-b1ab-fe0431d33f49 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:11:07.748: INFO: Waiting for pod pod-configmaps-6564cb8a-3b30-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:11:07.751: INFO: Pod pod-configmaps-6564cb8a-3b30-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:11:07.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-2hj9h" for this suite.
Feb 28 08:11:13.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:11:13.851: INFO: namespace: e2e-tests-configmap-2hj9h, resource: bindings, ignored listing per whitelist
Feb 28 08:11:13.911: INFO: namespace e2e-tests-configmap-2hj9h deletion completed in 6.156059321s

• [SLOW TEST:8.531 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:11:13.912: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-mccs4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-mccs4
Feb 28 08:11:16.223: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-mccs4
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:11:16.227: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:15:17.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-mccs4" for this suite.
Feb 28 08:15:23.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:15:23.260: INFO: namespace: e2e-tests-container-probe-mccs4, resource: bindings, ignored listing per whitelist
Feb 28 08:15:23.359: INFO: namespace e2e-tests-container-probe-mccs4 deletion completed in 6.192303099s

• [SLOW TEST:249.447 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:15:23.359: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-6lfms
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:15:23.711: INFO: Creating ReplicaSet my-hostname-basic-ff2ce537-3b30-11e9-b1ab-fe0431d33f49
Feb 28 08:15:23.721: INFO: Pod name my-hostname-basic-ff2ce537-3b30-11e9-b1ab-fe0431d33f49: Found 0 pods out of 1
Feb 28 08:15:28.726: INFO: Pod name my-hostname-basic-ff2ce537-3b30-11e9-b1ab-fe0431d33f49: Found 1 pods out of 1
Feb 28 08:15:28.726: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ff2ce537-3b30-11e9-b1ab-fe0431d33f49" is running
Feb 28 08:15:28.730: INFO: Pod "my-hostname-basic-ff2ce537-3b30-11e9-b1ab-fe0431d33f49-tln2v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:15:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:15:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:15:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-28 08:15:23 +0000 UTC Reason: Message:}])
Feb 28 08:15:28.730: INFO: Trying to dial the pod
Feb 28 08:15:33.830: INFO: Controller my-hostname-basic-ff2ce537-3b30-11e9-b1ab-fe0431d33f49: Got expected result from replica 1 [my-hostname-basic-ff2ce537-3b30-11e9-b1ab-fe0431d33f49-tln2v]: "my-hostname-basic-ff2ce537-3b30-11e9-b1ab-fe0431d33f49-tln2v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:15:33.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-6lfms" for this suite.
Feb 28 08:15:39.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:15:40.170: INFO: namespace: e2e-tests-replicaset-6lfms, resource: bindings, ignored listing per whitelist
Feb 28 08:15:40.206: INFO: namespace e2e-tests-replicaset-6lfms deletion completed in 6.370410273s

• [SLOW TEST:16.847 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:15:40.206: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-tcbb8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 28 08:15:40.507: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 08:15:40.515: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 08:15:40.518: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc before test
Feb 28 08:15:40.535: INFO: vpn-shoot-c4d998c9f-45b9w from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 08:15:40.535: INFO: addons-kubernetes-dashboard-5f64f76bd-rd55c from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 08:15:40.535: INFO: blackbox-exporter-58fd9b8556-6kz6h from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 08:15:40.535: INFO: addons-kube-lego-648f8c9f5c-r9z47 from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 08:15:40.535: INFO: addons-nginx-ingress-controller-8f975c795-fn6qg from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 08:15:40.535: INFO: kube-proxy-26xfb from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:15:40.535: INFO: coredns-5f4748c5f-lk8wm from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container coredns ready: true, restart count 0
Feb 28 08:15:40.535: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-g2mjq from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 08:15:40.535: INFO: metrics-server-85674d74c8-6gl2l from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 08:15:40.535: INFO: calico-node-b4l87 from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:15:40.535: INFO: node-exporter-rm79f from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.535: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:15:40.535: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr before test
Feb 28 08:15:40.580: INFO: calico-node-stfff from kube-system started at 2019-02-28 07:18:56 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.580: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:15:40.580: INFO: kube-proxy-2n298 from kube-system started at 2019-02-28 07:18:56 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.580: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:15:40.580: INFO: node-exporter-t6qsh from kube-system started at 2019-02-28 07:18:56 +0000 UTC (1 container statuses recorded)
Feb 28 08:15:40.580: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15877917fdafb1e0], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:15:41.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-tcbb8" for this suite.
Feb 28 08:15:47.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:15:47.722: INFO: namespace: e2e-tests-sched-pred-tcbb8, resource: bindings, ignored listing per whitelist
Feb 28 08:15:47.797: INFO: namespace e2e-tests-sched-pred-tcbb8 deletion completed in 6.182812182s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:7.591 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:15:47.797: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fllz9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 28 08:15:48.106: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-fllz9'
Feb 28 08:15:49.020: INFO: stderr: ""
Feb 28 08:15:49.020: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 08:15:50.026: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:15:50.026: INFO: Found 0 / 1
Feb 28 08:15:51.025: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:15:51.025: INFO: Found 1 / 1
Feb 28 08:15:51.025: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 28 08:15:51.029: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:15:51.029: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 08:15:51.029: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml patch pod redis-master-dfwm7 --namespace=e2e-tests-kubectl-fllz9 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 28 08:15:51.151: INFO: stderr: ""
Feb 28 08:15:51.151: INFO: stdout: "pod/redis-master-dfwm7 patched\n"
STEP: checking annotations
Feb 28 08:15:51.154: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:15:51.154: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:15:51.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fllz9" for this suite.
Feb 28 08:16:13.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:16:13.262: INFO: namespace: e2e-tests-kubectl-fllz9, resource: bindings, ignored listing per whitelist
Feb 28 08:16:13.380: INFO: namespace e2e-tests-kubectl-fllz9 deletion completed in 22.220730688s

• [SLOW TEST:25.583 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:16:13.381: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-lqrqj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-lqrqj/secret-test-1cf95b69-3b31-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 08:16:13.716: INFO: Waiting up to 5m0s for pod "pod-configmaps-1cfa20c8-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-secrets-lqrqj" to be "success or failure"
Feb 28 08:16:13.721: INFO: Pod "pod-configmaps-1cfa20c8-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.690521ms
Feb 28 08:16:15.726: INFO: Pod "pod-configmaps-1cfa20c8-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009687437s
STEP: Saw pod success
Feb 28 08:16:15.726: INFO: Pod "pod-configmaps-1cfa20c8-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:16:15.731: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-1cfa20c8-3b31-11e9-b1ab-fe0431d33f49 container env-test: <nil>
STEP: delete the pod
Feb 28 08:16:15.758: INFO: Waiting for pod pod-configmaps-1cfa20c8-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:16:15.762: INFO: Pod pod-configmaps-1cfa20c8-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:16:15.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-lqrqj" for this suite.
Feb 28 08:16:21.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:16:21.908: INFO: namespace: e2e-tests-secrets-lqrqj, resource: bindings, ignored listing per whitelist
Feb 28 08:16:21.945: INFO: namespace e2e-tests-secrets-lqrqj deletion completed in 6.17851165s

• [SLOW TEST:8.565 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:16:21.945: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-q7pqz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 08:16:22.313: INFO: Waiting up to 5m0s for pod "pod-2219afb0-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-q7pqz" to be "success or failure"
Feb 28 08:16:22.317: INFO: Pod "pod-2219afb0-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.691945ms
Feb 28 08:16:24.325: INFO: Pod "pod-2219afb0-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012496437s
STEP: Saw pod success
Feb 28 08:16:24.325: INFO: Pod "pod-2219afb0-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:16:24.333: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-2219afb0-3b31-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:16:24.359: INFO: Waiting for pod pod-2219afb0-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:16:24.366: INFO: Pod pod-2219afb0-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:16:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-q7pqz" for this suite.
Feb 28 08:16:30.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:16:30.439: INFO: namespace: e2e-tests-emptydir-q7pqz, resource: bindings, ignored listing per whitelist
Feb 28 08:16:30.539: INFO: namespace e2e-tests-emptydir-q7pqz deletion completed in 6.164292564s

• [SLOW TEST:8.594 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:16:30.539: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-rj7nk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-272b33e4-3b31-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 08:16:30.822: INFO: Waiting up to 5m0s for pod "pod-secrets-272beb00-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-secrets-rj7nk" to be "success or failure"
Feb 28 08:16:30.825: INFO: Pod "pod-secrets-272beb00-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258608ms
Feb 28 08:16:32.833: INFO: Pod "pod-secrets-272beb00-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011103965s
STEP: Saw pod success
Feb 28 08:16:32.833: INFO: Pod "pod-secrets-272beb00-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:16:32.837: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-secrets-272beb00-3b31-11e9-b1ab-fe0431d33f49 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:16:32.857: INFO: Waiting for pod pod-secrets-272beb00-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:16:32.862: INFO: Pod pod-secrets-272beb00-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:16:32.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rj7nk" for this suite.
Feb 28 08:16:38.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:16:38.939: INFO: namespace: e2e-tests-secrets-rj7nk, resource: bindings, ignored listing per whitelist
Feb 28 08:16:39.024: INFO: namespace e2e-tests-secrets-rj7nk deletion completed in 6.157877632s

• [SLOW TEST:8.485 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:16:39.024: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-zz8bf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-zz8bf A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-zz8bf A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-zz8bf.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-zz8bf.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-zz8bf.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-zz8bf.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-zz8bf.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-zz8bf.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 105.223.67.100.in-addr.arpa. PTR)" && echo OK > /results/100.67.223.105_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 105.223.67.100.in-addr.arpa. PTR)" && echo OK > /results/100.67.223.105_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-zz8bf A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-zz8bf;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-zz8bf A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-zz8bf.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-zz8bf.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-zz8bf.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-zz8bf.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-zz8bf.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-zz8bf.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-zz8bf.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 105.223.67.100.in-addr.arpa. PTR)" && echo OK > /results/100.67.223.105_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 105.223.67.100.in-addr.arpa. PTR)" && echo OK > /results/100.67.223.105_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 08:16:51.436: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:51.475: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:51.481: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:51.492: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:51.499: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:51.505: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:51.510: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:51.516: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:52.013: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:52.018: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:52.023: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-zz8bf from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:52.028: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:52.036: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:52.041: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:52.046: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:52.051: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:16:52.494: INFO: Lookups using e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf.svc wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-zz8bf jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf jessie_udp@dns-test-service.e2e-tests-dns-zz8bf.svc jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc]

Feb 28 08:17:01.355: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.361: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.368: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.375: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.381: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.388: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.393: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.399: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.888: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.894: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.902: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-zz8bf from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.908: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.914: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.923: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.928: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:01.935: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc from pod e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49: the server could not find the requested resource (get pods dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49)
Feb 28 08:17:02.383: INFO: Lookups using e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf wheezy_udp@dns-test-service.e2e-tests-dns-zz8bf.svc wheezy_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-zz8bf jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf jessie_udp@dns-test-service.e2e-tests-dns-zz8bf.svc jessie_tcp@dns-test-service.e2e-tests-dns-zz8bf.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-zz8bf.svc]

Feb 28 08:17:13.534: INFO: DNS probes using e2e-tests-dns-zz8bf/dns-test-2c3e2012-3b31-11e9-b1ab-fe0431d33f49 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:17:13.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-zz8bf" for this suite.
Feb 28 08:17:19.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:17:19.711: INFO: namespace: e2e-tests-dns-zz8bf, resource: bindings, ignored listing per whitelist
Feb 28 08:17:19.748: INFO: namespace e2e-tests-dns-zz8bf deletion completed in 6.171753473s

• [SLOW TEST:40.723 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:17:19.748: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-kjcr8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 08:17:20.121: INFO: Waiting up to 5m0s for pod "pod-448e34d3-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-kjcr8" to be "success or failure"
Feb 28 08:17:20.127: INFO: Pod "pod-448e34d3-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909157ms
Feb 28 08:17:22.133: INFO: Pod "pod-448e34d3-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012019412s
STEP: Saw pod success
Feb 28 08:17:22.133: INFO: Pod "pod-448e34d3-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:17:22.137: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-448e34d3-3b31-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:17:22.165: INFO: Waiting for pod pod-448e34d3-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:17:22.168: INFO: Pod pod-448e34d3-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:17:22.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kjcr8" for this suite.
Feb 28 08:17:28.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:17:28.354: INFO: namespace: e2e-tests-emptydir-kjcr8, resource: bindings, ignored listing per whitelist
Feb 28 08:17:28.363: INFO: namespace e2e-tests-emptydir-kjcr8 deletion completed in 6.190441773s

• [SLOW TEST:8.615 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:17:28.363: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-ffhgj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 08:17:28.715: INFO: Waiting up to 5m0s for pod "pod-49adde33-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-ffhgj" to be "success or failure"
Feb 28 08:17:28.718: INFO: Pod "pod-49adde33-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.393752ms
Feb 28 08:17:30.723: INFO: Pod "pod-49adde33-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008032378s
STEP: Saw pod success
Feb 28 08:17:30.723: INFO: Pod "pod-49adde33-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:17:30.727: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-49adde33-3b31-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:17:30.744: INFO: Waiting for pod pod-49adde33-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:17:30.748: INFO: Pod pod-49adde33-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:17:30.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ffhgj" for this suite.
Feb 28 08:17:36.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:17:36.894: INFO: namespace: e2e-tests-emptydir-ffhgj, resource: bindings, ignored listing per whitelist
Feb 28 08:17:36.920: INFO: namespace e2e-tests-emptydir-ffhgj deletion completed in 6.166760254s

• [SLOW TEST:8.557 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:17:36.920: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-8fp5g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:17:37.425: INFO: Creating deployment "nginx-deployment"
Feb 28 08:17:37.430: INFO: Waiting for observed generation 1
Feb 28 08:17:39.440: INFO: Waiting for all required pods to come up
Feb 28 08:17:39.447: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 28 08:17:41.460: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 28 08:17:41.468: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 28 08:17:41.477: INFO: Updating deployment nginx-deployment
Feb 28 08:17:41.477: INFO: Waiting for observed generation 2
Feb 28 08:17:43.488: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 28 08:17:43.491: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 28 08:17:43.494: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 08:17:43.505: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 28 08:17:43.505: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 28 08:17:43.508: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 08:17:43.516: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 28 08:17:43.516: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 28 08:17:43.523: INFO: Updating deployment nginx-deployment
Feb 28 08:17:43.523: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 28 08:17:43.532: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 28 08:17:45.543: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:17:45.550: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-8fp5g/deployments/nginx-deployment,UID:4ee0551e-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12059,Generation:3,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-28 08:17:43 +0000 UTC 2019-02-28 08:17:43 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-28 08:17:43 +0000 UTC 2019-02-28 08:17:37 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 28 08:17:45.553: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-8fp5g/replicasets/nginx-deployment-7dc8f79789,UID:514a7826-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12056,Generation:3,CreationTimestamp:2019-02-28 08:17:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4ee0551e-3b31-11e9-a4f3-1a76b5824175 0xc0008557d7 0xc0008557d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:17:45.553: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 28 08:17:45.554: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-8fp5g/replicasets/nginx-deployment-7f9675fb8b,UID:4ee1852b-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12051,Generation:3,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4ee0551e-3b31-11e9-a4f3-1a76b5824175 0xc0008558c7 0xc0008558c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 28 08:17:45.565: INFO: Pod "nginx-deployment-7dc8f79789-58vpf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-58vpf,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-58vpf,UID:5283b08f-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12080,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.44/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc001cb33d7 0xc001cb33d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001cb3580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001cb3890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.565: INFO: Pod "nginx-deployment-7dc8f79789-6wffn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-6wffn,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-6wffn,UID:514edf54-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11999,Generation:0,CreationTimestamp:2019-02-28 08:17:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.128/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc001cb3ba0 0xc001cb3ba1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001cb3dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001cb3de0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.565: INFO: Pod "nginx-deployment-7dc8f79789-9rbpm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-9rbpm,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-9rbpm,UID:514f8ffc-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11997,Generation:0,CreationTimestamp:2019-02-28 08:17:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.41/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc001cb3ee0 0xc001cb3ee1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001cb3fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001cb3fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.565: INFO: Pod "nginx-deployment-7dc8f79789-jf2tj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-jf2tj,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-jf2tj,UID:5285dbcf-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12066,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc00180a340 0xc00180a341}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00180a3b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00180a3d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.565: INFO: Pod "nginx-deployment-7dc8f79789-mc2kr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-mc2kr,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-mc2kr,UID:52863694-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12085,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.46/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc00180a8b0 0xc00180a8b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00180a980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00180a9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.565: INFO: Pod "nginx-deployment-7dc8f79789-mzxbz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-mzxbz,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-mzxbz,UID:52866859-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12065,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc00180b130 0xc00180b131}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00180b2b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00180b2d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.565: INFO: Pod "nginx-deployment-7dc8f79789-pk877" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-pk877,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-pk877,UID:514be044-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12001,Generation:0,CreationTimestamp:2019-02-28 08:17:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.129/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc00180b3a0 0xc00180b3a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00180b580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00180b5a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.565: INFO: Pod "nginx-deployment-7dc8f79789-qmtpk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-qmtpk,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-qmtpk,UID:528cb537-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12079,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.43/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc00180b740 0xc00180b741}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00180b830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00180b850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.566: INFO: Pod "nginx-deployment-7dc8f79789-qrfr8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-qrfr8,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-qrfr8,UID:52845b30-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12081,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.132/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc00180ba90 0xc00180ba91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00180bb40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00180bb60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.566: INFO: Pod "nginx-deployment-7dc8f79789-rwt7r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-rwt7r,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-rwt7r,UID:5284cd5d-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12076,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.42/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc00180bca0 0xc00180bca1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00180bd10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00180bd30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.566: INFO: Pod "nginx-deployment-7dc8f79789-t4lj2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-t4lj2,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-t4lj2,UID:514bc1ba-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11996,Generation:0,CreationTimestamp:2019-02-28 08:17:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.40/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc00180beb0 0xc00180beb1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00180bf20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00180bf40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.566: INFO: Pod "nginx-deployment-7dc8f79789-wg4tt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-wg4tt,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-wg4tt,UID:52860dd2-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12061,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc001b9e390 0xc001b9e391}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b9e470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b9e490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.566: INFO: Pod "nginx-deployment-7dc8f79789-xxlc4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-xxlc4,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7dc8f79789-xxlc4,UID:514b0ab9-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11998,Generation:0,CreationTimestamp:2019-02-28 08:17:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.127/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 514a7826-3b31-11e9-a4f3-1a76b5824175 0xc001b9e720 0xc001b9e721}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b9e800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b9e850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.566: INFO: Pod "nginx-deployment-7f9675fb8b-4fjrz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-4fjrz,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-4fjrz,UID:5285002c-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12054,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b9ea70 0xc001b9ea71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b9eb40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b9eb60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.566: INFO: Pod "nginx-deployment-7f9675fb8b-4q982" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-4q982,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-4q982,UID:4ee6924d-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11947,Generation:0,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.126/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b9f1f0 0xc001b9f1f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b9f300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b9f370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.1.126,StartTime:2019-02-28 08:17:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:17:39 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://adfa76899a5bb4df6155a8b9f0fae166d3a7db6b20d1d0fdef690e96e1fa0a44}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.567: INFO: Pod "nginx-deployment-7f9675fb8b-57h6n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-57h6n,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-57h6n,UID:528320b5-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12078,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.131/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b9f7a0 0xc001b9f7a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b9f800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b9f820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.567: INFO: Pod "nginx-deployment-7f9675fb8b-7h92m" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-7h92m,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-7h92m,UID:4ee39647-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11936,Generation:0,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.39/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b9f8e0 0xc001b9f8e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b9f970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b9f990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.96.0.39,StartTime:2019-02-28 08:17:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:17:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://a3720f859efc65ce67612f9a6394f43de726cacf8f20a104fef91dd57315ff7f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.567: INFO: Pod "nginx-deployment-7f9675fb8b-7jxgr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-7jxgr,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-7jxgr,UID:5283cfd3-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12082,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.45/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b9fa60 0xc001b9fa61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b9fb40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b9fb60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.567: INFO: Pod "nginx-deployment-7f9675fb8b-8g7b9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-8g7b9,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-8g7b9,UID:528759b1-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12068,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b9fc10 0xc001b9fc11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b9fd00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b9fe80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.567: INFO: Pod "nginx-deployment-7f9675fb8b-8tgzc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-8tgzc,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-8tgzc,UID:4ee4ff47-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11950,Generation:0,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.125/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b6c030 0xc001b6c031}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b6c110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b6c190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.1.125,StartTime:2019-02-28 08:17:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:17:39 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://e2cd820e8fb6d84bcac4ab7037e33f4ac3b586069bad84cedbc85d7e00209668}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.567: INFO: Pod "nginx-deployment-7f9675fb8b-8vncj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-8vncj,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-8vncj,UID:4ee688b3-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11933,Generation:0,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.38/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b6c320 0xc001b6c321}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b6c4b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b6c4d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.96.0.38,StartTime:2019-02-28 08:17:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:17:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://dccb36f54c2ba9bdd53c6682517b8baaa509e2902b7395a0086f118aba24e58d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.567: INFO: Pod "nginx-deployment-7f9675fb8b-g6m8n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-g6m8n,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-g6m8n,UID:5283d8c6-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12077,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.130/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b6c5e0 0xc001b6c5e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b6c650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b6d150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.567: INFO: Pod "nginx-deployment-7f9675fb8b-h78hs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-h78hs,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-h78hs,UID:528582b7-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12087,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.134/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b6d530 0xc001b6d531}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b6d590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b6d5b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.568: INFO: Pod "nginx-deployment-7f9675fb8b-hms5w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-hms5w,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-hms5w,UID:5287776c-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12086,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.47/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b6dbf0 0xc001b6dbf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b6dc50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b6dc70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.568: INFO: Pod "nginx-deployment-7f9675fb8b-hql7x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-hql7x,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-hql7x,UID:5287d45b-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12072,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc001b6de00 0xc001b6de01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b6dee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b6df00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.568: INFO: Pod "nginx-deployment-7f9675fb8b-km954" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-km954,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-km954,UID:4ee4bc84-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11928,Generation:0,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.121/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc000f58400 0xc000f58401}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f584f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f58510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.1.121,StartTime:2019-02-28 08:17:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:17:39 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://22476655c378a6c4cc2b8bb59e5da7a7017f4c4b57df4671c7af909dc13fbb35}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.568: INFO: Pod "nginx-deployment-7f9675fb8b-l7f9s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-l7f9s,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-l7f9s,UID:4ee32247-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11925,Generation:0,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.123/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc000f585e0 0xc000f585e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f587e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f58830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.1.123,StartTime:2019-02-28 08:17:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:17:39 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://e7506775dc81ab82e0f34771c9b0e78c0bc6788ddb56de3eeef6aa2d7640252a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.568: INFO: Pod "nginx-deployment-7f9675fb8b-l9grp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-l9grp,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-l9grp,UID:52855457-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12060,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc000f588f0 0xc000f588f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f58a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f58a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.568: INFO: Pod "nginx-deployment-7f9675fb8b-m85hb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-m85hb,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-m85hb,UID:5286850f-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12071,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc000f58ef0 0xc000f58ef1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f59040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f59060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.569: INFO: Pod "nginx-deployment-7f9675fb8b-r9kh5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-r9kh5,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-r9kh5,UID:4ee4d2f3-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11942,Generation:0,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.36/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc000f591a0 0xc000f591a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f598a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f598c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.96.0.36,StartTime:2019-02-28 08:17:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:17:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://86f5437940ff38f494ad491d28e478c547b8b48f8732e28790e5ecc58b342a46}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.569: INFO: Pod "nginx-deployment-7f9675fb8b-tstmb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-tstmb,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-tstmb,UID:52856132-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12062,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc000f59990 0xc000f59991}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f59a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f59aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.569: INFO: Pod "nginx-deployment-7f9675fb8b-x7tkj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-x7tkj,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-x7tkj,UID:5287a46b-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:12084,Generation:0,CreationTimestamp:2019-02-28 08:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.133/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc000f59b60 0xc000f59b61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f59bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f59c30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:,StartTime:2019-02-28 08:17:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:17:45.569: INFO: Pod "nginx-deployment-7f9675fb8b-xklkf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-xklkf,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-8fp5g,SelfLink:/api/v1/namespaces/e2e-tests-deployment-8fp5g/pods/nginx-deployment-7f9675fb8b-xklkf,UID:4ee5909b-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:11939,Generation:0,CreationTimestamp:2019-02-28 08:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.37/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 4ee1852b-3b31-11e9-a4f3-1a76b5824175 0xc000f59cf0 0xc000f59cf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-98h7g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98h7g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-98h7g true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00178ed30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00178ed50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:17:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.96.0.37,StartTime:2019-02-28 08:17:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:17:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://c15aad40efdde44288bb708110990f4634d63f96247aeab77bf4d86f1e4728ba}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:17:45.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-8fp5g" for this suite.
Feb 28 08:17:51.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:17:51.953: INFO: namespace: e2e-tests-deployment-8fp5g, resource: bindings, ignored listing per whitelist
Feb 28 08:17:52.000: INFO: namespace e2e-tests-deployment-8fp5g deletion completed in 6.425402767s

• [SLOW TEST:15.080 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:17:52.000: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-pmmhr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-pmmhr
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-pmmhr
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-pmmhr
Feb 28 08:17:52.528: INFO: Found 0 stateful pods, waiting for 1
Feb 28 08:18:02.536: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 28 08:18:02.542: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-pmmhr ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:18:03.122: INFO: stderr: ""
Feb 28 08:18:03.122: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:18:03.122: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:18:03.127: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:18:03.127: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:18:03.146: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999996s
Feb 28 08:18:04.151: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995678568s
Feb 28 08:18:05.157: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990514828s
Feb 28 08:18:06.162: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985152758s
Feb 28 08:18:07.168: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979950943s
Feb 28 08:18:08.174: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973937124s
Feb 28 08:18:09.179: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.967729952s
Feb 28 08:18:10.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.962885036s
Feb 28 08:18:11.191: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.95772317s
Feb 28 08:18:12.196: INFO: Verifying statefulset ss doesn't scale past 1 for another 950.674543ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-pmmhr
Feb 28 08:18:13.202: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-pmmhr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:18:13.948: INFO: stderr: ""
Feb 28 08:18:13.948: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:18:13.948: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:18:13.953: INFO: Found 1 stateful pods, waiting for 3
Feb 28 08:18:23.960: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:18:23.961: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:18:23.961: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 28 08:18:23.970: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-pmmhr ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:18:24.518: INFO: stderr: ""
Feb 28 08:18:24.518: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:18:24.518: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:18:24.518: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-pmmhr ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:18:25.272: INFO: stderr: ""
Feb 28 08:18:25.272: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:18:25.272: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:18:25.273: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-pmmhr ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:18:25.810: INFO: stderr: ""
Feb 28 08:18:25.810: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:18:25.810: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:18:25.810: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:18:25.814: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 28 08:18:35.826: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:18:35.827: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:18:35.827: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:18:35.840: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999699s
Feb 28 08:18:36.850: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996016854s
Feb 28 08:18:37.855: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985598334s
Feb 28 08:18:38.860: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980311409s
Feb 28 08:18:39.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97531024s
Feb 28 08:18:40.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968812705s
Feb 28 08:18:41.878: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963214318s
Feb 28 08:18:42.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957052198s
Feb 28 08:18:43.912: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951241453s
Feb 28 08:18:44.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 923.494412ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-pmmhr
Feb 28 08:18:45.924: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-pmmhr ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:18:46.460: INFO: stderr: ""
Feb 28 08:18:46.460: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:18:46.460: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:18:46.460: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-pmmhr ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:18:47.001: INFO: stderr: ""
Feb 28 08:18:47.001: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:18:47.001: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:18:47.001: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-pmmhr ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:18:47.516: INFO: stderr: ""
Feb 28 08:18:47.516: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:18:47.516: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:18:47.516: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:19:07.536: INFO: Deleting all statefulset in ns e2e-tests-statefulset-pmmhr
Feb 28 08:19:07.540: INFO: Scaling statefulset ss to 0
Feb 28 08:19:07.551: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:19:07.554: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:19:07.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-pmmhr" for this suite.
Feb 28 08:19:13.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:13.944: INFO: namespace: e2e-tests-statefulset-pmmhr, resource: bindings, ignored listing per whitelist
Feb 28 08:19:14.067: INFO: namespace e2e-tests-statefulset-pmmhr deletion completed in 6.490363287s

• [SLOW TEST:82.067 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:19:14.067: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-lxwmm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-88a00e8a-3b31-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 08:19:14.325: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-88a0c246-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-lxwmm" to be "success or failure"
Feb 28 08:19:14.336: INFO: Pod "pod-projected-secrets-88a0c246-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 10.97291ms
Feb 28 08:19:16.341: INFO: Pod "pod-projected-secrets-88a0c246-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015839574s
STEP: Saw pod success
Feb 28 08:19:16.341: INFO: Pod "pod-projected-secrets-88a0c246-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:19:16.346: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-secrets-88a0c246-3b31-11e9-b1ab-fe0431d33f49 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:19:16.368: INFO: Waiting for pod pod-projected-secrets-88a0c246-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:19:16.374: INFO: Pod pod-projected-secrets-88a0c246-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:19:16.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lxwmm" for this suite.
Feb 28 08:19:22.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:22.554: INFO: namespace: e2e-tests-projected-lxwmm, resource: bindings, ignored listing per whitelist
Feb 28 08:19:22.575: INFO: namespace e2e-tests-projected-lxwmm deletion completed in 6.195353016s

• [SLOW TEST:8.508 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:19:22.576: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-6h5mc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 28 08:19:24.838: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-8db02a9f-3b31-11e9-b1ab-fe0431d33f49", GenerateName:"", Namespace:"e2e-tests-pods-6h5mc", SelfLink:"/api/v1/namespaces/e2e-tests-pods-6h5mc/pods/pod-submit-remove-8db02a9f-3b31-11e9-b1ab-fe0431d33f49", UID:"8db1d190-3b31-11e9-a4f3-1a76b5824175", ResourceVersion:"12542", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686938762, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"807982544"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp", "cni.projectcalico.org/podIP":"100.96.1.144/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-b4fqf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00204adc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b4fqf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002348298), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001634b40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0023482d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0023482f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0023482f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686938762, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686938764, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686938764, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686938762, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.18", PodIP:"100.96.1.144", StartTime:(*v1.Time)(0xc0013fb820), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0013fb840), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://9e4660117953ad1c01d551021ad1b9a4a1a668826e09f3da24ed95360e164ab9"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 28 08:19:29.859: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:19:29.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6h5mc" for this suite.
Feb 28 08:19:35.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:35.936: INFO: namespace: e2e-tests-pods-6h5mc, resource: bindings, ignored listing per whitelist
Feb 28 08:19:36.071: INFO: namespace e2e-tests-pods-6h5mc deletion completed in 6.2023273s

• [SLOW TEST:13.495 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:19:36.071: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-rt5bs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-rt5bs/configmap-test-95bc4b92-3b31-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:19:36.319: INFO: Waiting up to 5m0s for pod "pod-configmaps-95bcebbd-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-configmap-rt5bs" to be "success or failure"
Feb 28 08:19:36.324: INFO: Pod "pod-configmaps-95bcebbd-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.409967ms
Feb 28 08:19:38.331: INFO: Pod "pod-configmaps-95bcebbd-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011568396s
STEP: Saw pod success
Feb 28 08:19:38.331: INFO: Pod "pod-configmaps-95bcebbd-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:19:38.334: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-95bcebbd-3b31-11e9-b1ab-fe0431d33f49 container env-test: <nil>
STEP: delete the pod
Feb 28 08:19:38.351: INFO: Waiting for pod pod-configmaps-95bcebbd-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:19:38.355: INFO: Pod pod-configmaps-95bcebbd-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:19:38.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rt5bs" for this suite.
Feb 28 08:19:44.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:44.408: INFO: namespace: e2e-tests-configmap-rt5bs, resource: bindings, ignored listing per whitelist
Feb 28 08:19:44.547: INFO: namespace e2e-tests-configmap-rt5bs deletion completed in 6.188226722s

• [SLOW TEST:8.476 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:19:44.548: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-4htbg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 28 08:19:44.805: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml api-versions'
Feb 28 08:19:44.991: INFO: stderr: ""
Feb 28 08:19:44.991: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:19:44.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4htbg" for this suite.
Feb 28 08:19:51.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:51.230: INFO: namespace: e2e-tests-kubectl-4htbg, resource: bindings, ignored listing per whitelist
Feb 28 08:19:51.362: INFO: namespace e2e-tests-kubectl-4htbg deletion completed in 6.366251868s

• [SLOW TEST:6.815 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:19:51.362: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qvp7h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9eda86c0-3b31-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:19:51.620: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9edb2fab-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-qvp7h" to be "success or failure"
Feb 28 08:19:51.623: INFO: Pod "pod-projected-configmaps-9edb2fab-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472752ms
Feb 28 08:19:53.629: INFO: Pod "pod-projected-configmaps-9edb2fab-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00926348s
STEP: Saw pod success
Feb 28 08:19:53.629: INFO: Pod "pod-projected-configmaps-9edb2fab-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:19:53.633: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-configmaps-9edb2fab-3b31-11e9-b1ab-fe0431d33f49 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:19:53.651: INFO: Waiting for pod pod-projected-configmaps-9edb2fab-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:19:53.654: INFO: Pod pod-projected-configmaps-9edb2fab-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:19:53.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qvp7h" for this suite.
Feb 28 08:19:59.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:19:59.721: INFO: namespace: e2e-tests-projected-qvp7h, resource: bindings, ignored listing per whitelist
Feb 28 08:19:59.849: INFO: namespace e2e-tests-projected-qvp7h deletion completed in 6.189926753s

• [SLOW TEST:8.487 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:19:59.849: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4fjqw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-a3f9bf15-3b31-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:20:00.210: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a3fa59ef-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-4fjqw" to be "success or failure"
Feb 28 08:20:00.215: INFO: Pod "pod-projected-configmaps-a3fa59ef-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042129ms
Feb 28 08:20:02.458: INFO: Pod "pod-projected-configmaps-a3fa59ef-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.248200793s
STEP: Saw pod success
Feb 28 08:20:02.459: INFO: Pod "pod-projected-configmaps-a3fa59ef-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:20:02.462: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-configmaps-a3fa59ef-3b31-11e9-b1ab-fe0431d33f49 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:20:02.487: INFO: Waiting for pod pod-projected-configmaps-a3fa59ef-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:20:02.491: INFO: Pod pod-projected-configmaps-a3fa59ef-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:20:02.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4fjqw" for this suite.
Feb 28 08:20:08.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:08.600: INFO: namespace: e2e-tests-projected-4fjqw, resource: bindings, ignored listing per whitelist
Feb 28 08:20:08.705: INFO: namespace e2e-tests-projected-4fjqw deletion completed in 6.209066303s

• [SLOW TEST:8.856 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:20:08.705: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6hk76
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:20:09.021: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a938e4e3-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-6hk76" to be "success or failure"
Feb 28 08:20:09.026: INFO: Pod "downwardapi-volume-a938e4e3-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.335484ms
Feb 28 08:20:11.031: INFO: Pod "downwardapi-volume-a938e4e3-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010295013s
STEP: Saw pod success
Feb 28 08:20:11.032: INFO: Pod "downwardapi-volume-a938e4e3-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:20:11.036: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-a938e4e3-3b31-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:20:11.055: INFO: Waiting for pod downwardapi-volume-a938e4e3-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:20:11.058: INFO: Pod downwardapi-volume-a938e4e3-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:20:11.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6hk76" for this suite.
Feb 28 08:20:17.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:17.194: INFO: namespace: e2e-tests-projected-6hk76, resource: bindings, ignored listing per whitelist
Feb 28 08:20:17.210: INFO: namespace e2e-tests-projected-6hk76 deletion completed in 6.146263399s

• [SLOW TEST:8.504 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:20:17.210: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-b2sxw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:20:17.513: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae4a66ee-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-b2sxw" to be "success or failure"
Feb 28 08:20:17.518: INFO: Pod "downwardapi-volume-ae4a66ee-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58096ms
Feb 28 08:20:19.523: INFO: Pod "downwardapi-volume-ae4a66ee-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009497637s
STEP: Saw pod success
Feb 28 08:20:19.523: INFO: Pod "downwardapi-volume-ae4a66ee-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:20:19.526: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-ae4a66ee-3b31-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:20:19.560: INFO: Waiting for pod downwardapi-volume-ae4a66ee-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:20:19.564: INFO: Pod downwardapi-volume-ae4a66ee-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:20:19.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-b2sxw" for this suite.
Feb 28 08:20:25.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:25.638: INFO: namespace: e2e-tests-downward-api-b2sxw, resource: bindings, ignored listing per whitelist
Feb 28 08:20:25.744: INFO: namespace e2e-tests-downward-api-b2sxw deletion completed in 6.175344738s

• [SLOW TEST:8.533 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:20:25.744: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-trdcw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 08:20:26.314: INFO: Waiting up to 5m0s for pod "pod-b36af86b-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-trdcw" to be "success or failure"
Feb 28 08:20:26.322: INFO: Pod "pod-b36af86b-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 7.477638ms
Feb 28 08:20:28.328: INFO: Pod "pod-b36af86b-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012948979s
STEP: Saw pod success
Feb 28 08:20:28.328: INFO: Pod "pod-b36af86b-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:20:28.331: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-b36af86b-3b31-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:20:28.351: INFO: Waiting for pod pod-b36af86b-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:20:28.354: INFO: Pod pod-b36af86b-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:20:28.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-trdcw" for this suite.
Feb 28 08:20:34.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:34.681: INFO: namespace: e2e-tests-emptydir-trdcw, resource: bindings, ignored listing per whitelist
Feb 28 08:20:34.739: INFO: namespace e2e-tests-emptydir-trdcw deletion completed in 6.378745858s

• [SLOW TEST:8.995 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:20:34.739: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7mnpx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-b8d74d36-3b31-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:20:35.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b8d7d5d3-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-7mnpx" to be "success or failure"
Feb 28 08:20:35.223: INFO: Pod "pod-projected-configmaps-b8d7d5d3-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.331689ms
Feb 28 08:20:37.236: INFO: Pod "pod-projected-configmaps-b8d7d5d3-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018191978s
STEP: Saw pod success
Feb 28 08:20:37.236: INFO: Pod "pod-projected-configmaps-b8d7d5d3-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:20:37.241: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-configmaps-b8d7d5d3-3b31-11e9-b1ab-fe0431d33f49 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:20:37.260: INFO: Waiting for pod pod-projected-configmaps-b8d7d5d3-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:20:37.265: INFO: Pod pod-projected-configmaps-b8d7d5d3-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:20:37.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7mnpx" for this suite.
Feb 28 08:20:43.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:43.313: INFO: namespace: e2e-tests-projected-7mnpx, resource: bindings, ignored listing per whitelist
Feb 28 08:20:43.442: INFO: namespace e2e-tests-projected-7mnpx deletion completed in 6.1699721s

• [SLOW TEST:8.703 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:20:43.442: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-qx2pj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0228 08:20:53.777841   30646 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:20:53.777: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:20:53.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-qx2pj" for this suite.
Feb 28 08:20:59.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:20:59.813: INFO: namespace: e2e-tests-gc-qx2pj, resource: bindings, ignored listing per whitelist
Feb 28 08:20:59.941: INFO: namespace e2e-tests-gc-qx2pj deletion completed in 6.157961665s

• [SLOW TEST:16.499 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:20:59.942: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-dc5cz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
Feb 28 08:21:00.305: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-dc5cz'
Feb 28 08:21:00.670: INFO: stderr: ""
Feb 28 08:21:00.670: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 28 08:21:01.675: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:21:01.676: INFO: Found 0 / 1
Feb 28 08:21:02.675: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:21:02.675: INFO: Found 1 / 1
Feb 28 08:21:02.675: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 08:21:02.680: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 08:21:02.680: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 28 08:21:02.680: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml logs redis-master-ww9pp redis-master --namespace=e2e-tests-kubectl-dc5cz'
Feb 28 08:21:02.813: INFO: stderr: ""
Feb 28 08:21:02.814: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 08:21:01.577 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 08:21:01.577 # Server started, Redis version 3.2.12\n1:M 28 Feb 08:21:01.577 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 08:21:01.577 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 28 08:21:02.814: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-ww9pp redis-master --namespace=e2e-tests-kubectl-dc5cz --tail=1'
Feb 28 08:21:02.927: INFO: stderr: ""
Feb 28 08:21:02.927: INFO: stdout: "1:M 28 Feb 08:21:01.577 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 28 08:21:02.927: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-ww9pp redis-master --namespace=e2e-tests-kubectl-dc5cz --limit-bytes=1'
Feb 28 08:21:03.053: INFO: stderr: ""
Feb 28 08:21:03.053: INFO: stdout: " "
STEP: exposing timestamps
Feb 28 08:21:03.053: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-ww9pp redis-master --namespace=e2e-tests-kubectl-dc5cz --tail=1 --timestamps'
Feb 28 08:21:03.171: INFO: stderr: ""
Feb 28 08:21:03.171: INFO: stdout: "2019-02-28T08:21:01.577341613Z 1:M 28 Feb 08:21:01.577 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 28 08:21:05.671: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-ww9pp redis-master --namespace=e2e-tests-kubectl-dc5cz --since=1s'
Feb 28 08:21:05.803: INFO: stderr: ""
Feb 28 08:21:05.803: INFO: stdout: ""
Feb 28 08:21:05.803: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml log redis-master-ww9pp redis-master --namespace=e2e-tests-kubectl-dc5cz --since=24h'
Feb 28 08:21:06.977: INFO: stderr: ""
Feb 28 08:21:06.977: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 08:21:01.577 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 08:21:01.577 # Server started, Redis version 3.2.12\n1:M 28 Feb 08:21:01.577 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 08:21:01.577 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
Feb 28 08:21:06.977: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dc5cz'
Feb 28 08:21:07.149: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:21:07.149: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 28 08:21:07.149: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-dc5cz'
Feb 28 08:21:07.286: INFO: stderr: "No resources found.\n"
Feb 28 08:21:07.286: INFO: stdout: ""
Feb 28 08:21:07.286: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-dc5cz -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 08:21:07.398: INFO: stderr: ""
Feb 28 08:21:07.398: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:21:07.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dc5cz" for this suite.
Feb 28 08:21:29.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:21:29.570: INFO: namespace: e2e-tests-kubectl-dc5cz, resource: bindings, ignored listing per whitelist
Feb 28 08:21:29.616: INFO: namespace e2e-tests-kubectl-dc5cz deletion completed in 22.212423467s

• [SLOW TEST:29.674 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:21:29.616: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qgq99
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:21:29.917: INFO: Waiting up to 5m0s for pod "downward-api-d9723b21-3b31-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-qgq99" to be "success or failure"
Feb 28 08:21:29.920: INFO: Pod "downward-api-d9723b21-3b31-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.560505ms
Feb 28 08:21:31.925: INFO: Pod "downward-api-d9723b21-3b31-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008359551s
STEP: Saw pod success
Feb 28 08:21:31.925: INFO: Pod "downward-api-d9723b21-3b31-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:21:31.929: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downward-api-d9723b21-3b31-11e9-b1ab-fe0431d33f49 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:21:31.947: INFO: Waiting for pod downward-api-d9723b21-3b31-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:21:31.953: INFO: Pod downward-api-d9723b21-3b31-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:21:31.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qgq99" for this suite.
Feb 28 08:21:38.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:21:38.210: INFO: namespace: e2e-tests-downward-api-qgq99, resource: bindings, ignored listing per whitelist
Feb 28 08:21:38.377: INFO: namespace e2e-tests-downward-api-qgq99 deletion completed in 6.419370089s

• [SLOW TEST:8.761 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:21:38.377: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-d7dz9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:21:38.640: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"dea489c2-3b31-11e9-a4f3-1a76b5824175", Controller:(*bool)(0xc000fcd882), BlockOwnerDeletion:(*bool)(0xc000fcd883)}}
Feb 28 08:21:38.644: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"dea2e9f6-3b31-11e9-a4f3-1a76b5824175", Controller:(*bool)(0xc0014342a6), BlockOwnerDeletion:(*bool)(0xc0014342a7)}}
Feb 28 08:21:38.650: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dea3d87f-3b31-11e9-a4f3-1a76b5824175", Controller:(*bool)(0xc000ddd1de), BlockOwnerDeletion:(*bool)(0xc000ddd1df)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:21:43.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-d7dz9" for this suite.
Feb 28 08:21:49.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:21:49.785: INFO: namespace: e2e-tests-gc-d7dz9, resource: bindings, ignored listing per whitelist
Feb 28 08:21:50.093: INFO: namespace e2e-tests-gc-d7dz9 deletion completed in 6.429295248s

• [SLOW TEST:11.716 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:21:50.093: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-jfqmm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 28 08:21:50.436: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-jfqmm,SelfLink:/api/v1/namespaces/e2e-tests-watch-jfqmm/configmaps/e2e-watch-test-label-changed,UID:e5ab7383-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:13189,Generation:0,CreationTimestamp:2019-02-28 08:21:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:21:50.436: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-jfqmm,SelfLink:/api/v1/namespaces/e2e-tests-watch-jfqmm/configmaps/e2e-watch-test-label-changed,UID:e5ab7383-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:13190,Generation:0,CreationTimestamp:2019-02-28 08:21:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 08:21:50.436: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-jfqmm,SelfLink:/api/v1/namespaces/e2e-tests-watch-jfqmm/configmaps/e2e-watch-test-label-changed,UID:e5ab7383-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:13191,Generation:0,CreationTimestamp:2019-02-28 08:21:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 28 08:22:00.482: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-jfqmm,SelfLink:/api/v1/namespaces/e2e-tests-watch-jfqmm/configmaps/e2e-watch-test-label-changed,UID:e5ab7383-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:13213,Generation:0,CreationTimestamp:2019-02-28 08:21:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:22:00.482: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-jfqmm,SelfLink:/api/v1/namespaces/e2e-tests-watch-jfqmm/configmaps/e2e-watch-test-label-changed,UID:e5ab7383-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:13214,Generation:0,CreationTimestamp:2019-02-28 08:21:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 28 08:22:00.483: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-jfqmm,SelfLink:/api/v1/namespaces/e2e-tests-watch-jfqmm/configmaps/e2e-watch-test-label-changed,UID:e5ab7383-3b31-11e9-a4f3-1a76b5824175,ResourceVersion:13215,Generation:0,CreationTimestamp:2019-02-28 08:21:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:22:00.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-jfqmm" for this suite.
Feb 28 08:22:06.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:06.561: INFO: namespace: e2e-tests-watch-jfqmm, resource: bindings, ignored listing per whitelist
Feb 28 08:22:06.744: INFO: namespace e2e-tests-watch-jfqmm deletion completed in 6.246202386s

• [SLOW TEST:16.651 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:22:06.745: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-nkdx6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
Feb 28 08:22:07.160: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-nkdx6'
Feb 28 08:22:07.423: INFO: stderr: ""
Feb 28 08:22:07.423: INFO: stdout: "pod/pause created\n"
Feb 28 08:22:07.423: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 28 08:22:07.423: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-nkdx6" to be "running and ready"
Feb 28 08:22:07.427: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154327ms
Feb 28 08:22:09.432: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009037606s
Feb 28 08:22:09.432: INFO: Pod "pause" satisfied condition "running and ready"
Feb 28 08:22:09.432: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 28 08:22:09.433: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-nkdx6'
Feb 28 08:22:09.550: INFO: stderr: ""
Feb 28 08:22:09.550: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 28 08:22:09.550: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-nkdx6'
Feb 28 08:22:09.675: INFO: stderr: ""
Feb 28 08:22:09.675: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 28 08:22:09.675: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-nkdx6'
Feb 28 08:22:09.799: INFO: stderr: ""
Feb 28 08:22:09.799: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 28 08:22:09.799: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-nkdx6'
Feb 28 08:22:09.958: INFO: stderr: ""
Feb 28 08:22:09.958: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
Feb 28 08:22:09.959: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-nkdx6'
Feb 28 08:22:10.107: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:22:10.108: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 28 08:22:10.108: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-nkdx6'
Feb 28 08:22:10.233: INFO: stderr: "No resources found.\n"
Feb 28 08:22:10.233: INFO: stdout: ""
Feb 28 08:22:10.233: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-nkdx6 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 08:22:10.354: INFO: stderr: ""
Feb 28 08:22:10.354: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:22:10.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nkdx6" for this suite.
Feb 28 08:22:16.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:16.448: INFO: namespace: e2e-tests-kubectl-nkdx6, resource: bindings, ignored listing per whitelist
Feb 28 08:22:16.818: INFO: namespace e2e-tests-kubectl-nkdx6 deletion completed in 6.458054822s

• [SLOW TEST:10.074 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:22:16.819: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-q7vc4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-znwj
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:22:17.128: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-znwj" in namespace "e2e-tests-subpath-q7vc4" to be "success or failure"
Feb 28 08:22:17.132: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.608711ms
Feb 28 08:22:19.138: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 2.009332842s
Feb 28 08:22:21.145: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 4.016488205s
Feb 28 08:22:23.152: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 6.023697438s
Feb 28 08:22:25.158: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 8.029644776s
Feb 28 08:22:27.169: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 10.040648798s
Feb 28 08:22:29.176: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 12.047950364s
Feb 28 08:22:31.181: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 14.052854173s
Feb 28 08:22:33.189: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 16.06016865s
Feb 28 08:22:35.193: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 18.065008875s
Feb 28 08:22:37.199: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 20.070528456s
Feb 28 08:22:39.204: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Running", Reason="", readiness=false. Elapsed: 22.07595637s
Feb 28 08:22:41.220: INFO: Pod "pod-subpath-test-configmap-znwj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.091416833s
STEP: Saw pod success
Feb 28 08:22:41.220: INFO: Pod "pod-subpath-test-configmap-znwj" satisfied condition "success or failure"
Feb 28 08:22:41.232: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-subpath-test-configmap-znwj container test-container-subpath-configmap-znwj: <nil>
STEP: delete the pod
Feb 28 08:22:41.271: INFO: Waiting for pod pod-subpath-test-configmap-znwj to disappear
Feb 28 08:22:41.283: INFO: Pod pod-subpath-test-configmap-znwj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-znwj
Feb 28 08:22:41.283: INFO: Deleting pod "pod-subpath-test-configmap-znwj" in namespace "e2e-tests-subpath-q7vc4"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:22:41.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-q7vc4" for this suite.
Feb 28 08:22:47.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:22:47.425: INFO: namespace: e2e-tests-subpath-q7vc4, resource: bindings, ignored listing per whitelist
Feb 28 08:22:47.481: INFO: namespace e2e-tests-subpath-q7vc4 deletion completed in 6.170693581s

• [SLOW TEST:30.662 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:22:47.481: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-k72q4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 28 08:22:47.827: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-a,UID:07e31884-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13362,Generation:0,CreationTimestamp:2019-02-28 08:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:22:47.827: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-a,UID:07e31884-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13362,Generation:0,CreationTimestamp:2019-02-28 08:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 28 08:22:57.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-a,UID:07e31884-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13384,Generation:0,CreationTimestamp:2019-02-28 08:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 08:22:57.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-a,UID:07e31884-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13384,Generation:0,CreationTimestamp:2019-02-28 08:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 28 08:23:07.849: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-a,UID:07e31884-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13404,Generation:0,CreationTimestamp:2019-02-28 08:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:23:07.849: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-a,UID:07e31884-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13404,Generation:0,CreationTimestamp:2019-02-28 08:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 28 08:23:17.857: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-a,UID:07e31884-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13424,Generation:0,CreationTimestamp:2019-02-28 08:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 08:23:17.857: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-a,UID:07e31884-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13424,Generation:0,CreationTimestamp:2019-02-28 08:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 28 08:23:27.865: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-b,UID:1fc022a1-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13445,Generation:0,CreationTimestamp:2019-02-28 08:23:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:23:27.865: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-b,UID:1fc022a1-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13445,Generation:0,CreationTimestamp:2019-02-28 08:23:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 28 08:23:37.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-b,UID:1fc022a1-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13465,Generation:0,CreationTimestamp:2019-02-28 08:23:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 08:23:37.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-k72q4,SelfLink:/api/v1/namespaces/e2e-tests-watch-k72q4/configmaps/e2e-watch-test-configmap-b,UID:1fc022a1-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:13465,Generation:0,CreationTimestamp:2019-02-28 08:23:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:23:47.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-k72q4" for this suite.
Feb 28 08:23:53.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:23:54.015: INFO: namespace: e2e-tests-watch-k72q4, resource: bindings, ignored listing per whitelist
Feb 28 08:23:54.077: INFO: namespace e2e-tests-watch-k72q4 deletion completed in 6.199289497s

• [SLOW TEST:66.596 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:23:54.077: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-68f4k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-2f84409c-3b32-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:23:54.324: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2f8544d0-3b32-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-68f4k" to be "success or failure"
Feb 28 08:23:54.329: INFO: Pod "pod-projected-configmaps-2f8544d0-3b32-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.365669ms
Feb 28 08:23:56.333: INFO: Pod "pod-projected-configmaps-2f8544d0-3b32-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00876801s
STEP: Saw pod success
Feb 28 08:23:56.333: INFO: Pod "pod-projected-configmaps-2f8544d0-3b32-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:23:56.338: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-configmaps-2f8544d0-3b32-11e9-b1ab-fe0431d33f49 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:23:56.361: INFO: Waiting for pod pod-projected-configmaps-2f8544d0-3b32-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:23:56.365: INFO: Pod pod-projected-configmaps-2f8544d0-3b32-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:23:56.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-68f4k" for this suite.
Feb 28 08:24:02.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:02.542: INFO: namespace: e2e-tests-projected-68f4k, resource: bindings, ignored listing per whitelist
Feb 28 08:24:02.608: INFO: namespace e2e-tests-projected-68f4k deletion completed in 6.238430808s

• [SLOW TEST:8.530 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:24:02.608: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-md29l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-34d23220-3b32-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 08:24:03.223: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-34d31dba-3b32-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-md29l" to be "success or failure"
Feb 28 08:24:03.227: INFO: Pod "pod-projected-secrets-34d31dba-3b32-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.238025ms
Feb 28 08:24:05.233: INFO: Pod "pod-projected-secrets-34d31dba-3b32-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009467384s
STEP: Saw pod success
Feb 28 08:24:05.233: INFO: Pod "pod-projected-secrets-34d31dba-3b32-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:24:05.236: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-secrets-34d31dba-3b32-11e9-b1ab-fe0431d33f49 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:24:05.255: INFO: Waiting for pod pod-projected-secrets-34d31dba-3b32-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:24:05.259: INFO: Pod pod-projected-secrets-34d31dba-3b32-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:24:05.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-md29l" for this suite.
Feb 28 08:24:11.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:11.567: INFO: namespace: e2e-tests-projected-md29l, resource: bindings, ignored listing per whitelist
Feb 28 08:24:11.628: INFO: namespace e2e-tests-projected-md29l deletion completed in 6.362179057s

• [SLOW TEST:9.021 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:24:11.629: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-q88sl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-3a1272fd-3b32-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:24:12.031: INFO: Waiting up to 5m0s for pod "pod-configmaps-3a132020-3b32-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-configmap-q88sl" to be "success or failure"
Feb 28 08:24:12.034: INFO: Pod "pod-configmaps-3a132020-3b32-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490445ms
Feb 28 08:24:14.042: INFO: Pod "pod-configmaps-3a132020-3b32-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011692715s
STEP: Saw pod success
Feb 28 08:24:14.042: INFO: Pod "pod-configmaps-3a132020-3b32-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:24:14.046: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-3a132020-3b32-11e9-b1ab-fe0431d33f49 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:24:14.065: INFO: Waiting for pod pod-configmaps-3a132020-3b32-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:24:14.068: INFO: Pod pod-configmaps-3a132020-3b32-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:24:14.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-q88sl" for this suite.
Feb 28 08:24:20.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:20.312: INFO: namespace: e2e-tests-configmap-q88sl, resource: bindings, ignored listing per whitelist
Feb 28 08:24:20.342: INFO: namespace e2e-tests-configmap-q88sl deletion completed in 6.267236795s

• [SLOW TEST:8.712 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:24:20.342: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vlhn9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:24:20.623: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f321e24-3b32-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-vlhn9" to be "success or failure"
Feb 28 08:24:20.629: INFO: Pod "downwardapi-volume-3f321e24-3b32-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.237521ms
Feb 28 08:24:22.634: INFO: Pod "downwardapi-volume-3f321e24-3b32-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011141614s
STEP: Saw pod success
Feb 28 08:24:22.634: INFO: Pod "downwardapi-volume-3f321e24-3b32-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:24:22.639: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-3f321e24-3b32-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:24:22.656: INFO: Waiting for pod downwardapi-volume-3f321e24-3b32-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:24:22.659: INFO: Pod downwardapi-volume-3f321e24-3b32-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:24:22.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vlhn9" for this suite.
Feb 28 08:24:28.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:28.726: INFO: namespace: e2e-tests-projected-vlhn9, resource: bindings, ignored listing per whitelist
Feb 28 08:24:28.898: INFO: namespace e2e-tests-projected-vlhn9 deletion completed in 6.233107983s

• [SLOW TEST:8.556 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:24:28.898: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-l78vn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-44525785-3b32-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:24:29.229: INFO: Waiting up to 5m0s for pod "pod-configmaps-4453286d-3b32-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-configmap-l78vn" to be "success or failure"
Feb 28 08:24:29.234: INFO: Pod "pod-configmaps-4453286d-3b32-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.944033ms
Feb 28 08:24:31.241: INFO: Pod "pod-configmaps-4453286d-3b32-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011927565s
STEP: Saw pod success
Feb 28 08:24:31.241: INFO: Pod "pod-configmaps-4453286d-3b32-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:24:31.245: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-configmaps-4453286d-3b32-11e9-b1ab-fe0431d33f49 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:24:31.265: INFO: Waiting for pod pod-configmaps-4453286d-3b32-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:24:31.268: INFO: Pod pod-configmaps-4453286d-3b32-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:24:31.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-l78vn" for this suite.
Feb 28 08:24:37.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:37.693: INFO: namespace: e2e-tests-configmap-l78vn, resource: bindings, ignored listing per whitelist
Feb 28 08:24:37.719: INFO: namespace e2e-tests-configmap-l78vn deletion completed in 6.44589574s

• [SLOW TEST:8.821 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:24:37.719: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-7s8gg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 28 08:24:38.529: INFO: Waiting up to 5m0s for pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-zkhkv" in namespace "e2e-tests-svcaccounts-7s8gg" to be "success or failure"
Feb 28 08:24:38.535: INFO: Pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-zkhkv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.704372ms
Feb 28 08:24:40.539: INFO: Pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-zkhkv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01049882s
STEP: Saw pod success
Feb 28 08:24:40.539: INFO: Pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-zkhkv" satisfied condition "success or failure"
Feb 28 08:24:40.542: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-zkhkv container token-test: <nil>
STEP: delete the pod
Feb 28 08:24:40.562: INFO: Waiting for pod pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-zkhkv to disappear
Feb 28 08:24:40.565: INFO: Pod pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-zkhkv no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 28 08:24:40.570: INFO: Waiting up to 5m0s for pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-p85n9" in namespace "e2e-tests-svcaccounts-7s8gg" to be "success or failure"
Feb 28 08:24:40.575: INFO: Pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-p85n9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.727372ms
Feb 28 08:24:42.580: INFO: Pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-p85n9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009846385s
STEP: Saw pod success
Feb 28 08:24:42.580: INFO: Pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-p85n9" satisfied condition "success or failure"
Feb 28 08:24:42.584: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-p85n9 container root-ca-test: <nil>
STEP: delete the pod
Feb 28 08:24:42.603: INFO: Waiting for pod pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-p85n9 to disappear
Feb 28 08:24:42.605: INFO: Pod pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-p85n9 no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 28 08:24:42.611: INFO: Waiting up to 5m0s for pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-svmft" in namespace "e2e-tests-svcaccounts-7s8gg" to be "success or failure"
Feb 28 08:24:42.614: INFO: Pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-svmft": Phase="Pending", Reason="", readiness=false. Elapsed: 3.675427ms
Feb 28 08:24:44.625: INFO: Pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-svmft": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013947376s
STEP: Saw pod success
Feb 28 08:24:44.628: INFO: Pod "pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-svmft" satisfied condition "success or failure"
Feb 28 08:24:44.634: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-svmft container namespace-test: <nil>
STEP: delete the pod
Feb 28 08:24:44.655: INFO: Waiting for pod pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-svmft to disappear
Feb 28 08:24:44.658: INFO: Pod pod-service-account-49de51ac-3b32-11e9-b1ab-fe0431d33f49-svmft no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:24:44.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-7s8gg" for this suite.
Feb 28 08:24:50.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:50.734: INFO: namespace: e2e-tests-svcaccounts-7s8gg, resource: bindings, ignored listing per whitelist
Feb 28 08:24:50.899: INFO: namespace e2e-tests-svcaccounts-7s8gg deletion completed in 6.231272457s

• [SLOW TEST:13.180 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:24:50.900: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-sxdzw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:24:51.313: INFO: Waiting up to 5m0s for pod "downwardapi-volume-517cd4f1-3b32-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-sxdzw" to be "success or failure"
Feb 28 08:24:51.316: INFO: Pod "downwardapi-volume-517cd4f1-3b32-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.475094ms
Feb 28 08:24:53.321: INFO: Pod "downwardapi-volume-517cd4f1-3b32-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008428275s
STEP: Saw pod success
Feb 28 08:24:53.321: INFO: Pod "downwardapi-volume-517cd4f1-3b32-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:24:53.325: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-517cd4f1-3b32-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:24:53.344: INFO: Waiting for pod downwardapi-volume-517cd4f1-3b32-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:24:53.347: INFO: Pod downwardapi-volume-517cd4f1-3b32-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:24:53.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-sxdzw" for this suite.
Feb 28 08:24:59.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:24:59.537: INFO: namespace: e2e-tests-downward-api-sxdzw, resource: bindings, ignored listing per whitelist
Feb 28 08:24:59.544: INFO: namespace e2e-tests-downward-api-sxdzw deletion completed in 6.193160959s

• [SLOW TEST:8.645 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:24:59.545: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-kf8zp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-kf8zp
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-kf8zp
STEP: Deleting pre-stop pod
Feb 28 08:25:11.358: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:25:11.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-kf8zp" for this suite.
Feb 28 08:25:49.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:49.502: INFO: namespace: e2e-tests-prestop-kf8zp, resource: bindings, ignored listing per whitelist
Feb 28 08:25:49.562: INFO: namespace e2e-tests-prestop-kf8zp deletion completed in 38.189112841s

• [SLOW TEST:50.017 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:25:49.562: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-cntrp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-746b5b24-3b32-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 08:25:49.923: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-746c5b90-3b32-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-cntrp" to be "success or failure"
Feb 28 08:25:49.927: INFO: Pod "pod-projected-secrets-746c5b90-3b32-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267887ms
Feb 28 08:25:51.931: INFO: Pod "pod-projected-secrets-746c5b90-3b32-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007870103s
STEP: Saw pod success
Feb 28 08:25:51.931: INFO: Pod "pod-projected-secrets-746c5b90-3b32-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:25:51.935: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-secrets-746c5b90-3b32-11e9-b1ab-fe0431d33f49 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:25:51.956: INFO: Waiting for pod pod-projected-secrets-746c5b90-3b32-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:25:51.960: INFO: Pod pod-projected-secrets-746c5b90-3b32-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:25:51.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cntrp" for this suite.
Feb 28 08:25:57.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:25:58.084: INFO: namespace: e2e-tests-projected-cntrp, resource: bindings, ignored listing per whitelist
Feb 28 08:25:58.191: INFO: namespace e2e-tests-projected-cntrp deletion completed in 6.226320654s

• [SLOW TEST:8.629 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:25:58.192: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-lhc7r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 08:26:02.569: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:26:02.574: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:26:04.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:26:04.579: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:26:06.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:26:06.583: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:26:08.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:26:08.580: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:26:10.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:26:10.581: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:26:12.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:26:12.580: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:26:14.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:26:14.580: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 08:26:16.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 08:26:16.580: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:26:16.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-lhc7r" for this suite.
Feb 28 08:26:38.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:26:38.626: INFO: namespace: e2e-tests-container-lifecycle-hook-lhc7r, resource: bindings, ignored listing per whitelist
Feb 28 08:26:38.813: INFO: namespace e2e-tests-container-lifecycle-hook-lhc7r deletion completed in 22.228632408s

• [SLOW TEST:40.621 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:26:38.813: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-gqb5t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 08:26:39.220: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:26:41.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-gqb5t" for this suite.
Feb 28 08:26:47.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:26:47.639: INFO: namespace: e2e-tests-init-container-gqb5t, resource: bindings, ignored listing per whitelist
Feb 28 08:26:47.672: INFO: namespace e2e-tests-init-container-gqb5t deletion completed in 6.222545976s

• [SLOW TEST:8.859 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:26:47.672: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-xr25b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 28 08:26:52.055: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:52.055: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:52.500: INFO: Exec stderr: ""
Feb 28 08:26:52.500: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:52.500: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:52.781: INFO: Exec stderr: ""
Feb 28 08:26:52.782: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:52.782: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:53.255: INFO: Exec stderr: ""
Feb 28 08:26:53.255: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:53.255: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:53.695: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 28 08:26:53.695: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:53.695: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:54.123: INFO: Exec stderr: ""
Feb 28 08:26:54.123: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:54.123: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:54.592: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 28 08:26:54.592: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:54.592: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:54.837: INFO: Exec stderr: ""
Feb 28 08:26:54.837: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:54.837: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:55.351: INFO: Exec stderr: ""
Feb 28 08:26:55.351: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:55.351: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:55.809: INFO: Exec stderr: ""
Feb 28 08:26:55.809: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-xr25b PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:26:55.809: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:26:56.246: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:26:56.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-xr25b" for this suite.
Feb 28 08:27:46.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:27:46.342: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-xr25b, resource: bindings, ignored listing per whitelist
Feb 28 08:27:46.487: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-xr25b deletion completed in 50.235548327s

• [SLOW TEST:58.815 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:27:46.487: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-ggr87
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4qg8n in namespace e2e-tests-proxy-ggr87
I0228 08:27:47.039882   30646 runners.go:180] Created replication controller with name: proxy-service-4qg8n, namespace: e2e-tests-proxy-ggr87, replica count: 1
I0228 08:27:48.090402   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 08:27:49.091403   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 08:27:50.091791   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:27:51.091988   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:27:52.092200   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:27:53.092377   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:27:54.092670   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:27:55.092902   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:27:56.093078   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 08:27:57.093260   30646 runners.go:180] proxy-service-4qg8n Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 08:27:57.096: INFO: setup took 10.073230226s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 28 08:27:57.111: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 13.025445ms)
Feb 28 08:27:57.111: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 13.23744ms)
Feb 28 08:27:57.111: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 12.423441ms)
Feb 28 08:27:57.111: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 13.603617ms)
Feb 28 08:27:57.111: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 13.400194ms)
Feb 28 08:27:57.111: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 13.182141ms)
Feb 28 08:27:57.114: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 16.478307ms)
Feb 28 08:27:57.114: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 16.79292ms)
Feb 28 08:27:57.115: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 17.423995ms)
Feb 28 08:27:57.115: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 17.05859ms)
Feb 28 08:27:57.115: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 17.025615ms)
Feb 28 08:27:57.119: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 21.301686ms)
Feb 28 08:27:57.119: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 21.104834ms)
Feb 28 08:27:57.121: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 22.827353ms)
Feb 28 08:27:57.122: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 24.414722ms)
Feb 28 08:27:57.124: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 25.060258ms)
Feb 28 08:27:57.130: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 6.33016ms)
Feb 28 08:27:57.131: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 5.961175ms)
Feb 28 08:27:57.131: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 6.641918ms)
Feb 28 08:27:57.131: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 6.959978ms)
Feb 28 08:27:57.131: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.326301ms)
Feb 28 08:27:57.131: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.260248ms)
Feb 28 08:27:57.132: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 7.248385ms)
Feb 28 08:27:57.132: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 6.935057ms)
Feb 28 08:27:57.132: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 7.130056ms)
Feb 28 08:27:57.132: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 7.413663ms)
Feb 28 08:27:57.132: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 7.229163ms)
Feb 28 08:27:57.140: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 14.744408ms)
Feb 28 08:27:57.140: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 15.576405ms)
Feb 28 08:27:57.140: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 15.252528ms)
Feb 28 08:27:57.140: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 14.739465ms)
Feb 28 08:27:57.140: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 15.730194ms)
Feb 28 08:27:57.146: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 6.021276ms)
Feb 28 08:27:57.146: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 6.280617ms)
Feb 28 08:27:57.146: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 5.994175ms)
Feb 28 08:27:57.146: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 5.845733ms)
Feb 28 08:27:57.146: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 5.948964ms)
Feb 28 08:27:57.146: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 5.975541ms)
Feb 28 08:27:57.146: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 6.088692ms)
Feb 28 08:27:57.153: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 10.91386ms)
Feb 28 08:27:57.153: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 12.581989ms)
Feb 28 08:27:57.153: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 11.057403ms)
Feb 28 08:27:57.153: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 13.049411ms)
Feb 28 08:27:57.153: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 11.13313ms)
Feb 28 08:27:57.153: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 12.583464ms)
Feb 28 08:27:57.153: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 12.364575ms)
Feb 28 08:27:57.153: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 11.159882ms)
Feb 28 08:27:57.153: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 11.129011ms)
Feb 28 08:27:57.161: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.830614ms)
Feb 28 08:27:57.161: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 7.696403ms)
Feb 28 08:27:57.161: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 6.931657ms)
Feb 28 08:27:57.161: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 7.610021ms)
Feb 28 08:27:57.161: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 7.723901ms)
Feb 28 08:27:57.161: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 7.416261ms)
Feb 28 08:27:57.161: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 8.316346ms)
Feb 28 08:27:57.161: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 7.374573ms)
Feb 28 08:27:57.161: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 8.299521ms)
Feb 28 08:27:57.162: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 8.103149ms)
Feb 28 08:27:57.162: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 8.461635ms)
Feb 28 08:27:57.162: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 8.230823ms)
Feb 28 08:27:57.164: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 9.593341ms)
Feb 28 08:27:57.164: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 10.60076ms)
Feb 28 08:27:57.164: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 9.813083ms)
Feb 28 08:27:57.164: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 10.22004ms)
Feb 28 08:27:57.173: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 8.894775ms)
Feb 28 08:27:57.174: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 9.61958ms)
Feb 28 08:27:57.174: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 9.747005ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 9.735384ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 9.653965ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 10.176266ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 11.018203ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 10.091095ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.880035ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 10.868223ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 10.816291ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 10.668283ms)
Feb 28 08:27:57.175: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 10.772658ms)
Feb 28 08:27:57.176: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 11.368363ms)
Feb 28 08:27:57.176: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 11.853521ms)
Feb 28 08:27:57.176: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 11.510483ms)
Feb 28 08:27:57.183: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 5.436476ms)
Feb 28 08:27:57.183: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 6.118654ms)
Feb 28 08:27:57.183: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 6.095776ms)
Feb 28 08:27:57.183: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 6.135335ms)
Feb 28 08:27:57.183: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 6.296839ms)
Feb 28 08:27:57.185: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 8.37636ms)
Feb 28 08:27:57.185: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.464153ms)
Feb 28 08:27:57.185: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 7.920738ms)
Feb 28 08:27:57.185: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 8.173903ms)
Feb 28 08:27:57.185: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 8.293972ms)
Feb 28 08:27:57.185: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 8.053328ms)
Feb 28 08:27:57.185: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 8.056905ms)
Feb 28 08:27:57.185: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 8.775102ms)
Feb 28 08:27:57.186: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.939512ms)
Feb 28 08:27:57.186: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 8.395013ms)
Feb 28 08:27:57.187: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 9.408804ms)
Feb 28 08:27:57.197: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 9.822212ms)
Feb 28 08:27:57.197: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.769216ms)
Feb 28 08:27:57.197: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 9.855807ms)
Feb 28 08:27:57.197: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 9.856442ms)
Feb 28 08:27:57.197: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 10.058797ms)
Feb 28 08:27:57.197: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 10.170535ms)
Feb 28 08:27:57.198: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 10.587764ms)
Feb 28 08:27:57.198: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 10.770229ms)
Feb 28 08:27:57.198: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 10.87787ms)
Feb 28 08:27:57.198: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 10.781602ms)
Feb 28 08:27:57.198: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 10.858183ms)
Feb 28 08:27:57.198: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 10.735485ms)
Feb 28 08:27:57.198: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 10.905107ms)
Feb 28 08:27:57.198: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 10.990006ms)
Feb 28 08:27:57.198: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 11.168546ms)
Feb 28 08:27:57.199: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 11.664771ms)
Feb 28 08:27:57.206: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 7.512025ms)
Feb 28 08:27:57.206: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 7.26995ms)
Feb 28 08:27:57.206: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 7.353203ms)
Feb 28 08:27:57.206: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 7.581159ms)
Feb 28 08:27:57.206: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 7.317816ms)
Feb 28 08:27:57.206: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 7.395561ms)
Feb 28 08:27:57.207: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 7.883069ms)
Feb 28 08:27:57.208: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 8.323822ms)
Feb 28 08:27:57.208: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 8.85575ms)
Feb 28 08:27:57.208: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 8.253498ms)
Feb 28 08:27:57.208: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 8.212873ms)
Feb 28 08:27:57.208: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 8.9111ms)
Feb 28 08:27:57.208: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 9.304672ms)
Feb 28 08:27:57.209: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.450373ms)
Feb 28 08:27:57.209: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.613272ms)
Feb 28 08:27:57.210: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 10.607882ms)
Feb 28 08:27:57.216: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 5.785824ms)
Feb 28 08:27:57.217: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 6.467752ms)
Feb 28 08:27:57.217: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 6.341788ms)
Feb 28 08:27:57.217: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 6.137438ms)
Feb 28 08:27:57.217: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 6.7394ms)
Feb 28 08:27:57.217: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 5.969268ms)
Feb 28 08:27:57.217: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 6.257916ms)
Feb 28 08:27:57.217: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 6.54235ms)
Feb 28 08:27:57.218: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 6.981288ms)
Feb 28 08:27:57.218: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 6.947513ms)
Feb 28 08:27:57.218: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 6.7993ms)
Feb 28 08:27:57.218: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 7.676318ms)
Feb 28 08:27:57.218: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 7.540032ms)
Feb 28 08:27:57.218: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 7.213013ms)
Feb 28 08:27:57.218: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 8.103933ms)
Feb 28 08:27:57.219: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 7.511721ms)
Feb 28 08:27:57.226: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 6.698646ms)
Feb 28 08:27:57.226: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 7.014378ms)
Feb 28 08:27:57.226: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 7.210559ms)
Feb 28 08:27:57.226: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 7.39442ms)
Feb 28 08:27:57.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 7.951122ms)
Feb 28 08:27:57.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.376625ms)
Feb 28 08:27:57.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.485261ms)
Feb 28 08:27:57.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 7.684192ms)
Feb 28 08:27:57.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 7.944855ms)
Feb 28 08:27:57.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 8.024323ms)
Feb 28 08:27:57.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 8.382762ms)
Feb 28 08:27:57.227: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 8.276336ms)
Feb 28 08:27:57.266: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 47.112647ms)
Feb 28 08:27:57.266: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 47.004255ms)
Feb 28 08:27:57.267: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 47.350626ms)
Feb 28 08:27:57.267: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 46.987276ms)
Feb 28 08:27:57.276: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 8.124691ms)
Feb 28 08:27:57.276: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 8.75575ms)
Feb 28 08:27:57.277: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.380852ms)
Feb 28 08:27:57.277: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 9.767732ms)
Feb 28 08:27:57.277: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 10.525974ms)
Feb 28 08:27:57.277: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 10.01506ms)
Feb 28 08:27:57.277: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 9.414908ms)
Feb 28 08:27:57.277: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 9.185347ms)
Feb 28 08:27:57.277: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 10.450971ms)
Feb 28 08:27:57.278: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 11.146476ms)
Feb 28 08:27:57.279: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 11.106865ms)
Feb 28 08:27:57.279: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 10.880495ms)
Feb 28 08:27:57.279: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 10.984104ms)
Feb 28 08:27:57.279: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 11.644101ms)
Feb 28 08:27:57.279: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 11.584149ms)
Feb 28 08:27:57.279: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 11.673534ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 7.290987ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 7.186114ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 6.873928ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 7.685715ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 6.993427ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 7.263446ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.358168ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 7.124601ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 6.960375ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 7.550204ms)
Feb 28 08:27:57.287: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 7.69138ms)
Feb 28 08:27:57.288: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 7.302117ms)
Feb 28 08:27:57.288: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 7.833464ms)
Feb 28 08:27:57.288: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 7.426889ms)
Feb 28 08:27:57.288: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 7.524627ms)
Feb 28 08:27:57.288: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 8.609085ms)
Feb 28 08:27:57.298: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 9.744641ms)
Feb 28 08:27:57.298: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.849197ms)
Feb 28 08:27:57.298: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 9.848112ms)
Feb 28 08:27:57.298: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 9.901966ms)
Feb 28 08:27:57.298: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 9.97262ms)
Feb 28 08:27:57.298: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 9.985541ms)
Feb 28 08:27:57.298: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 10.276664ms)
Feb 28 08:27:57.299: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 10.59706ms)
Feb 28 08:27:57.300: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 11.662246ms)
Feb 28 08:27:57.300: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 11.710535ms)
Feb 28 08:27:57.300: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 12.015685ms)
Feb 28 08:27:57.300: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 12.106954ms)
Feb 28 08:27:57.300: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 12.199745ms)
Feb 28 08:27:57.300: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 12.105886ms)
Feb 28 08:27:57.300: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 12.126172ms)
Feb 28 08:27:57.300: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 12.276123ms)
Feb 28 08:27:57.313: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 11.596489ms)
Feb 28 08:27:57.313: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 12.35563ms)
Feb 28 08:27:57.313: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 12.133842ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 12.99926ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 12.10832ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 12.312225ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 12.231164ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 13.004394ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 12.479564ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 12.991581ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 13.397745ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 13.298666ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 12.067032ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 12.164286ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 12.250453ms)
Feb 28 08:27:57.314: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 12.343196ms)
Feb 28 08:27:57.324: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 9.837627ms)
Feb 28 08:27:57.324: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.397446ms)
Feb 28 08:27:57.324: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 9.772511ms)
Feb 28 08:27:57.324: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 10.172165ms)
Feb 28 08:27:57.324: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 10.101817ms)
Feb 28 08:27:57.324: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 10.455248ms)
Feb 28 08:27:57.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 9.524533ms)
Feb 28 08:27:57.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 9.227863ms)
Feb 28 08:27:57.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 10.151362ms)
Feb 28 08:27:57.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 9.793938ms)
Feb 28 08:27:57.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 9.659807ms)
Feb 28 08:27:57.325: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 9.788742ms)
Feb 28 08:27:57.326: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 11.566172ms)
Feb 28 08:27:57.326: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 11.366334ms)
Feb 28 08:27:57.326: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 11.69617ms)
Feb 28 08:27:57.326: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 11.906818ms)
Feb 28 08:27:57.334: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 6.745623ms)
Feb 28 08:27:57.334: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 7.467779ms)
Feb 28 08:27:57.334: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 6.453929ms)
Feb 28 08:27:57.334: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 6.680427ms)
Feb 28 08:27:57.336: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 8.246763ms)
Feb 28 08:27:57.336: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 8.084209ms)
Feb 28 08:27:57.336: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 8.879544ms)
Feb 28 08:27:57.336: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 9.065234ms)
Feb 28 08:27:57.336: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 8.828803ms)
Feb 28 08:27:57.336: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 9.190229ms)
Feb 28 08:27:57.336: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 9.455701ms)
Feb 28 08:27:57.336: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 9.211095ms)
Feb 28 08:27:57.349: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 20.839197ms)
Feb 28 08:27:57.349: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 21.600284ms)
Feb 28 08:27:57.349: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 21.876089ms)
Feb 28 08:27:57.349: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 21.40674ms)
Feb 28 08:27:57.356: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 7.058116ms)
Feb 28 08:27:57.356: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 7.219797ms)
Feb 28 08:27:57.356: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 6.530339ms)
Feb 28 08:27:57.356: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 6.497098ms)
Feb 28 08:27:57.357: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.081805ms)
Feb 28 08:27:57.357: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 6.766484ms)
Feb 28 08:27:57.357: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.198ms)
Feb 28 08:27:57.357: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 7.225841ms)
Feb 28 08:27:57.357: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 6.814537ms)
Feb 28 08:27:57.357: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 8.183548ms)
Feb 28 08:27:57.361: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 11.4704ms)
Feb 28 08:27:57.361: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 10.701958ms)
Feb 28 08:27:57.361: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 10.651271ms)
Feb 28 08:27:57.361: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 11.193219ms)
Feb 28 08:27:57.361: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 11.664168ms)
Feb 28 08:27:57.361: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 11.957975ms)
Feb 28 08:27:57.371: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 9.073957ms)
Feb 28 08:27:57.371: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 9.230335ms)
Feb 28 08:27:57.371: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.571823ms)
Feb 28 08:27:57.371: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.734514ms)
Feb 28 08:27:57.371: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 9.17438ms)
Feb 28 08:27:57.372: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 9.925056ms)
Feb 28 08:27:57.372: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 10.55492ms)
Feb 28 08:27:57.372: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 10.730457ms)
Feb 28 08:27:57.372: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 11.51214ms)
Feb 28 08:27:57.373: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 10.778356ms)
Feb 28 08:27:57.373: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 10.863174ms)
Feb 28 08:27:57.373: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 11.923457ms)
Feb 28 08:27:57.373: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 11.500865ms)
Feb 28 08:27:57.373: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 12.345107ms)
Feb 28 08:27:57.373: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 12.038457ms)
Feb 28 08:27:57.374: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 12.015109ms)
Feb 28 08:27:57.382: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 7.015029ms)
Feb 28 08:27:57.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 7.992211ms)
Feb 28 08:27:57.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 8.646171ms)
Feb 28 08:27:57.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 7.945644ms)
Feb 28 08:27:57.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 8.441181ms)
Feb 28 08:27:57.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 8.624511ms)
Feb 28 08:27:57.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 8.90757ms)
Feb 28 08:27:57.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 8.520799ms)
Feb 28 08:27:57.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 9.007526ms)
Feb 28 08:27:57.383: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 8.439636ms)
Feb 28 08:27:57.400: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 25.131977ms)
Feb 28 08:27:57.400: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 24.881089ms)
Feb 28 08:27:57.400: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 25.492463ms)
Feb 28 08:27:57.400: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 25.581949ms)
Feb 28 08:27:57.400: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 25.189474ms)
Feb 28 08:27:57.400: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 25.434641ms)
Feb 28 08:27:57.407: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 6.765086ms)
Feb 28 08:27:57.407: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:443/proxy/... (200; 7.610771ms)
Feb 28 08:27:57.408: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw/proxy/rewriteme"... (200; 8.356676ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:1080/proxy/rewri... (200; 10.066135ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:162/proxy/: bar (200; 10.540418ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.872089ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:160/proxy/: foo (200; 9.812019ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-ggr87/pods/http:proxy-service-4qg8n-mbwlw:1080/proxy/... (200; 10.661588ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:460/proxy/: tls baz (200; 10.533422ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname1/proxy/: tls baz (200; 10.836007ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/pods/https:proxy-service-4qg8n-mbwlw:462/proxy/: tls qux (200; 10.380736ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/https:proxy-service-4qg8n:tlsportname2/proxy/: tls qux (200; 11.095001ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname1/proxy/: foo (200; 11.192436ms)
Feb 28 08:27:57.411: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/proxy-service-4qg8n:portname2/proxy/: bar (200; 11.012313ms)
Feb 28 08:27:57.412: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname2/proxy/: bar (200; 10.743893ms)
Feb 28 08:27:57.412: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-ggr87/services/http:proxy-service-4qg8n:portname1/proxy/: foo (200; 10.833944ms)
STEP: deleting { ReplicationController} proxy-service-4qg8n in namespace e2e-tests-proxy-ggr87, will wait for the garbage collector to delete the pods
Feb 28 08:27:57.471: INFO: Deleting { ReplicationController} proxy-service-4qg8n took: 6.035668ms
Feb 28 08:27:57.572: INFO: Terminating { ReplicationController} proxy-service-4qg8n pods took: 100.269861ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:27:59.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-ggr87" for this suite.
Feb 28 08:28:05.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:28:05.104: INFO: namespace: e2e-tests-proxy-ggr87, resource: bindings, ignored listing per whitelist
Feb 28 08:28:05.257: INFO: namespace e2e-tests-proxy-ggr87 deletion completed in 6.17956284s

• [SLOW TEST:18.770 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:28:05.257: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-wwqrs
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-c53df50f-3b32-11e9-b1ab-fe0431d33f49
STEP: Creating configMap with name cm-test-opt-upd-c53df558-3b32-11e9-b1ab-fe0431d33f49
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c53df50f-3b32-11e9-b1ab-fe0431d33f49
STEP: Updating configmap cm-test-opt-upd-c53df558-3b32-11e9-b1ab-fe0431d33f49
STEP: Creating configMap with name cm-test-opt-create-c53df573-3b32-11e9-b1ab-fe0431d33f49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:28:09.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-wwqrs" for this suite.
Feb 28 08:28:31.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:28:32.148: INFO: namespace: e2e-tests-configmap-wwqrs, resource: bindings, ignored listing per whitelist
Feb 28 08:28:32.169: INFO: namespace e2e-tests-configmap-wwqrs deletion completed in 22.186159483s

• [SLOW TEST:26.913 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:28:32.170: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-wcc5g
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:28:32.409: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:28:33.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-wcc5g" for this suite.
Feb 28 08:28:39.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:28:39.102: INFO: namespace: e2e-tests-custom-resource-definition-wcc5g, resource: bindings, ignored listing per whitelist
Feb 28 08:28:39.201: INFO: namespace e2e-tests-custom-resource-definition-wcc5g deletion completed in 6.188864846s

• [SLOW TEST:7.031 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:28:39.201: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-xxkk4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 28 08:28:39.503: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:28:43.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-xxkk4" for this suite.
Feb 28 08:28:49.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:28:49.577: INFO: namespace: e2e-tests-init-container-xxkk4, resource: bindings, ignored listing per whitelist
Feb 28 08:28:49.613: INFO: namespace e2e-tests-init-container-xxkk4 deletion completed in 6.189093093s

• [SLOW TEST:10.412 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:28:49.613: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sfb4z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:28:49.915: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dfb4cc8b-3b32-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-sfb4z" to be "success or failure"
Feb 28 08:28:49.921: INFO: Pod "downwardapi-volume-dfb4cc8b-3b32-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.213122ms
Feb 28 08:28:51.926: INFO: Pod "downwardapi-volume-dfb4cc8b-3b32-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011295581s
STEP: Saw pod success
Feb 28 08:28:51.926: INFO: Pod "downwardapi-volume-dfb4cc8b-3b32-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:28:51.931: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-dfb4cc8b-3b32-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:28:51.948: INFO: Waiting for pod downwardapi-volume-dfb4cc8b-3b32-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:28:51.952: INFO: Pod downwardapi-volume-dfb4cc8b-3b32-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:28:51.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sfb4z" for this suite.
Feb 28 08:28:57.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:28:58.218: INFO: namespace: e2e-tests-projected-sfb4z, resource: bindings, ignored listing per whitelist
Feb 28 08:28:58.321: INFO: namespace e2e-tests-projected-sfb4z deletion completed in 6.362612786s

• [SLOW TEST:8.708 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:28:58.321: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-njvtz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:28:59.118: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 28 08:28:59.127: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 28 08:29:04.132: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 08:29:04.132: INFO: Creating deployment "test-rolling-update-deployment"
Feb 28 08:29:04.137: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 28 08:29:04.145: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 28 08:29:06.156: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 28 08:29:06.164: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:29:06.175: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-njvtz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-njvtz/deployments/test-rolling-update-deployment,UID:e82f8a0c-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:14552,Generation:1,CreationTimestamp:2019-02-28 08:29:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 08:29:04 +0000 UTC 2019-02-28 08:29:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 08:29:05 +0000 UTC 2019-02-28 08:29:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 08:29:06.180: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-njvtz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-njvtz/replicasets/test-rolling-update-deployment-65b7695dcf,UID:e831d58f-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:14545,Generation:1,CreationTimestamp:2019-02-28 08:29:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e82f8a0c-3b32-11e9-a4f3-1a76b5824175 0xc0021ea5a7 0xc0021ea5a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 08:29:06.180: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 28 08:29:06.180: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-njvtz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-njvtz/replicasets/test-rolling-update-controller,UID:e5326460-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:14551,Generation:2,CreationTimestamp:2019-02-28 08:28:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e82f8a0c-3b32-11e9-a4f3-1a76b5824175 0xc0021ea447 0xc0021ea448}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:29:06.185: INFO: Pod "test-rolling-update-deployment-65b7695dcf-qbng7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-qbng7,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-njvtz,SelfLink:/api/v1/namespaces/e2e-tests-deployment-njvtz/pods/test-rolling-update-deployment-65b7695dcf-qbng7,UID:e83242e0-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:14544,Generation:0,CreationTimestamp:2019-02-28 08:29:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.183/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf e831d58f-3b32-11e9-a4f3-1a76b5824175 0xc002448097 0xc002448098}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fdr2v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fdr2v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-fdr2v true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002448100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002448120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:04 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.1.183,StartTime:2019-02-28 08:29:04 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://11b09c2754346e08021f98921711443db6667f1939c5811c7ba9d0bc6e932fdb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:29:06.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-njvtz" for this suite.
Feb 28 08:29:12.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:12.360: INFO: namespace: e2e-tests-deployment-njvtz, resource: bindings, ignored listing per whitelist
Feb 28 08:29:12.372: INFO: namespace e2e-tests-deployment-njvtz deletion completed in 6.180014981s

• [SLOW TEST:14.051 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:29:12.372: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-xdfzw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:29:12.717: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 28 08:29:17.722: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 08:29:17.722: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 28 08:29:19.725: INFO: Creating deployment "test-rollover-deployment"
Feb 28 08:29:19.733: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 28 08:29:21.741: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 28 08:29:21.749: INFO: Ensure that both replica sets have 1 created replica
Feb 28 08:29:21.756: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 28 08:29:21.769: INFO: Updating deployment test-rollover-deployment
Feb 28 08:29:21.769: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 28 08:29:23.778: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 28 08:29:23.784: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 28 08:29:23.792: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:29:23.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939362, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:29:25.802: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:29:25.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939362, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:29:27.800: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:29:27.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939362, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:29:29.805: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:29:29.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939362, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:29:31.803: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 08:29:31.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939362, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686939359, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 08:29:33.803: INFO: 
Feb 28 08:29:33.803: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:29:33.817: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-xdfzw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xdfzw/deployments/test-rollover-deployment,UID:f17ae001-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:14674,Generation:2,CreationTimestamp:2019-02-28 08:29:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-28 08:29:19 +0000 UTC 2019-02-28 08:29:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-28 08:29:32 +0000 UTC 2019-02-28 08:29:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 08:29:33.823: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-xdfzw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xdfzw/replicasets/test-rollover-deployment-5b76ff8c4,UID:f2b26f97-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:14667,Generation:2,CreationTimestamp:2019-02-28 08:29:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment f17ae001-3b32-11e9-a4f3-1a76b5824175 0xc002abead0 0xc002abead1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 08:29:33.823: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 28 08:29:33.823: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-xdfzw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xdfzw/replicasets/test-rollover-controller,UID:ed4c4d63-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:14673,Generation:2,CreationTimestamp:2019-02-28 08:29:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment f17ae001-3b32-11e9-a4f3-1a76b5824175 0xc002abea17 0xc002abea18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:29:33.823: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-xdfzw,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xdfzw/replicasets/test-rollover-deployment-6975f4fb87,UID:f17caa09-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:14634,Generation:2,CreationTimestamp:2019-02-28 08:29:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment f17ae001-3b32-11e9-a4f3-1a76b5824175 0xc002abeb97 0xc002abeb98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:29:33.827: INFO: Pod "test-rollover-deployment-5b76ff8c4-q9gtp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-q9gtp,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-xdfzw,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xdfzw/pods/test-rollover-deployment-5b76ff8c4-q9gtp,UID:f2b53c74-3b32-11e9-a4f3-1a76b5824175,ResourceVersion:14645,Generation:0,CreationTimestamp:2019-02-28 08:29:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.186/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 f2b26f97-3b32-11e9-a4f3-1a76b5824175 0xc002abf6d0 0xc002abf6d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zrpbn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zrpbn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-zrpbn true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002abf730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002abf750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:29:21 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.1.186,StartTime:2019-02-28 08:29:21 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-28 08:29:22 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://4bc023739c43fc68278df3dc6d007322040f3d6c222ff9f08da91fc5f90bc0fa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:29:33.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xdfzw" for this suite.
Feb 28 08:29:39.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:39.909: INFO: namespace: e2e-tests-deployment-xdfzw, resource: bindings, ignored listing per whitelist
Feb 28 08:29:40.037: INFO: namespace e2e-tests-deployment-xdfzw deletion completed in 6.203143954s

• [SLOW TEST:27.665 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:29:40.037: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-stlrd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:29:40.314: INFO: Waiting up to 5m0s for pod "downward-api-fdbf1125-3b32-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-stlrd" to be "success or failure"
Feb 28 08:29:40.318: INFO: Pod "downward-api-fdbf1125-3b32-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.454445ms
Feb 28 08:29:42.322: INFO: Pod "downward-api-fdbf1125-3b32-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008098034s
STEP: Saw pod success
Feb 28 08:29:42.322: INFO: Pod "downward-api-fdbf1125-3b32-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:29:42.326: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downward-api-fdbf1125-3b32-11e9-b1ab-fe0431d33f49 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:29:42.345: INFO: Waiting for pod downward-api-fdbf1125-3b32-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:29:42.348: INFO: Pod downward-api-fdbf1125-3b32-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:29:42.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-stlrd" for this suite.
Feb 28 08:29:48.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:48.589: INFO: namespace: e2e-tests-downward-api-stlrd, resource: bindings, ignored listing per whitelist
Feb 28 08:29:48.649: INFO: namespace e2e-tests-downward-api-stlrd deletion completed in 6.297906008s

• [SLOW TEST:8.612 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:29:48.650: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-pfhpw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-02df1fd3-3b33-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 08:29:48.925: INFO: Waiting up to 5m0s for pod "pod-secrets-02dff55b-3b33-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-secrets-pfhpw" to be "success or failure"
Feb 28 08:29:48.930: INFO: Pod "pod-secrets-02dff55b-3b33-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.380692ms
Feb 28 08:29:50.934: INFO: Pod "pod-secrets-02dff55b-3b33-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009009692s
STEP: Saw pod success
Feb 28 08:29:50.934: INFO: Pod "pod-secrets-02dff55b-3b33-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:29:50.938: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-secrets-02dff55b-3b33-11e9-b1ab-fe0431d33f49 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:29:50.958: INFO: Waiting for pod pod-secrets-02dff55b-3b33-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:29:50.963: INFO: Pod pod-secrets-02dff55b-3b33-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:29:50.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pfhpw" for this suite.
Feb 28 08:29:56.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:29:57.021: INFO: namespace: e2e-tests-secrets-pfhpw, resource: bindings, ignored listing per whitelist
Feb 28 08:29:57.126: INFO: namespace e2e-tests-secrets-pfhpw deletion completed in 6.159429508s

• [SLOW TEST:8.476 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:29:57.126: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-fbfp2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 08:29:57.475: INFO: Number of nodes with available pods: 0
Feb 28 08:29:57.475: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 08:29:58.490: INFO: Number of nodes with available pods: 0
Feb 28 08:29:58.490: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 08:29:59.483: INFO: Number of nodes with available pods: 2
Feb 28 08:29:59.483: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 28 08:29:59.507: INFO: Number of nodes with available pods: 1
Feb 28 08:29:59.507: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:00.517: INFO: Number of nodes with available pods: 1
Feb 28 08:30:00.517: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:01.521: INFO: Number of nodes with available pods: 1
Feb 28 08:30:01.521: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:02.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:02.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:03.515: INFO: Number of nodes with available pods: 1
Feb 28 08:30:03.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:04.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:04.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:05.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:05.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:06.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:06.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:07.525: INFO: Number of nodes with available pods: 1
Feb 28 08:30:07.525: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:08.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:08.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:09.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:09.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:10.515: INFO: Number of nodes with available pods: 1
Feb 28 08:30:10.515: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:11.517: INFO: Number of nodes with available pods: 1
Feb 28 08:30:11.517: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:12.517: INFO: Number of nodes with available pods: 1
Feb 28 08:30:12.518: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:13.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:13.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:14.517: INFO: Number of nodes with available pods: 1
Feb 28 08:30:14.517: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:15.515: INFO: Number of nodes with available pods: 1
Feb 28 08:30:15.515: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:16.518: INFO: Number of nodes with available pods: 1
Feb 28 08:30:16.518: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:17.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:17.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:18.515: INFO: Number of nodes with available pods: 1
Feb 28 08:30:18.515: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:19.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:19.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:20.521: INFO: Number of nodes with available pods: 1
Feb 28 08:30:20.521: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:21.518: INFO: Number of nodes with available pods: 1
Feb 28 08:30:21.518: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:22.519: INFO: Number of nodes with available pods: 1
Feb 28 08:30:22.519: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:23.517: INFO: Number of nodes with available pods: 1
Feb 28 08:30:23.517: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:24.517: INFO: Number of nodes with available pods: 1
Feb 28 08:30:24.517: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:25.515: INFO: Number of nodes with available pods: 1
Feb 28 08:30:25.515: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:26.518: INFO: Number of nodes with available pods: 1
Feb 28 08:30:26.518: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:27.517: INFO: Number of nodes with available pods: 1
Feb 28 08:30:27.517: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:28.519: INFO: Number of nodes with available pods: 1
Feb 28 08:30:28.519: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:29.519: INFO: Number of nodes with available pods: 1
Feb 28 08:30:29.519: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:30.517: INFO: Number of nodes with available pods: 1
Feb 28 08:30:30.517: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:31.516: INFO: Number of nodes with available pods: 1
Feb 28 08:30:31.516: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:32.518: INFO: Number of nodes with available pods: 1
Feb 28 08:30:32.518: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:33.515: INFO: Number of nodes with available pods: 1
Feb 28 08:30:33.515: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr is running more than one daemon pod
Feb 28 08:30:34.515: INFO: Number of nodes with available pods: 2
Feb 28 08:30:34.515: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-fbfp2, will wait for the garbage collector to delete the pods
Feb 28 08:30:34.579: INFO: Deleting {extensions DaemonSet} daemon-set took: 7.049805ms
Feb 28 08:30:34.679: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.163217ms
Feb 28 08:31:14.684: INFO: Number of nodes with available pods: 0
Feb 28 08:31:14.684: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:31:14.688: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-fbfp2/daemonsets","resourceVersion":"14968"},"items":null}

Feb 28 08:31:14.691: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-fbfp2/pods","resourceVersion":"14968"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:31:14.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-fbfp2" for this suite.
Feb 28 08:31:20.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:31:20.746: INFO: namespace: e2e-tests-daemonsets-fbfp2, resource: bindings, ignored listing per whitelist
Feb 28 08:31:20.969: INFO: namespace e2e-tests-daemonsets-fbfp2 deletion completed in 6.262167899s

• [SLOW TEST:83.843 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:31:20.969: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-gp2xw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-gp2xw
Feb 28 08:31:23.344: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-gp2xw
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:31:23.351: INFO: Initial restart count of pod liveness-exec is 0
Feb 28 08:32:09.490: INFO: Restart count of pod e2e-tests-container-probe-gp2xw/liveness-exec is now 1 (46.139101029s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:32:09.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-gp2xw" for this suite.
Feb 28 08:32:15.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:32:15.596: INFO: namespace: e2e-tests-container-probe-gp2xw, resource: bindings, ignored listing per whitelist
Feb 28 08:32:15.654: INFO: namespace e2e-tests-container-probe-gp2xw deletion completed in 6.150696928s

• [SLOW TEST:54.685 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:32:15.654: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5pmpm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:32:15.910: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a7d35f2-3b33-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-5pmpm" to be "success or failure"
Feb 28 08:32:15.914: INFO: Pod "downwardapi-volume-5a7d35f2-3b33-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.202543ms
Feb 28 08:32:17.919: INFO: Pod "downwardapi-volume-5a7d35f2-3b33-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00852811s
STEP: Saw pod success
Feb 28 08:32:17.919: INFO: Pod "downwardapi-volume-5a7d35f2-3b33-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:32:17.923: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-5a7d35f2-3b33-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:32:17.975: INFO: Waiting for pod downwardapi-volume-5a7d35f2-3b33-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:32:17.978: INFO: Pod downwardapi-volume-5a7d35f2-3b33-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:32:17.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5pmpm" for this suite.
Feb 28 08:32:23.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:32:24.094: INFO: namespace: e2e-tests-projected-5pmpm, resource: bindings, ignored listing per whitelist
Feb 28 08:32:24.160: INFO: namespace e2e-tests-projected-5pmpm deletion completed in 6.177704414s

• [SLOW TEST:8.506 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:32:24.160: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-8ckcl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-xmgw9
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-djxpl
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:32:30.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-8ckcl" for this suite.
Feb 28 08:32:36.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:32:37.150: INFO: namespace: e2e-tests-namespaces-8ckcl, resource: bindings, ignored listing per whitelist
Feb 28 08:32:37.183: INFO: namespace e2e-tests-namespaces-8ckcl deletion completed in 6.2631115s
STEP: Destroying namespace "e2e-tests-nsdeletetest-xmgw9" for this suite.
Feb 28 08:32:37.187: INFO: Namespace e2e-tests-nsdeletetest-xmgw9 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-djxpl" for this suite.
Feb 28 08:32:43.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:32:43.359: INFO: namespace: e2e-tests-nsdeletetest-djxpl, resource: bindings, ignored listing per whitelist
Feb 28 08:32:43.377: INFO: namespace e2e-tests-nsdeletetest-djxpl deletion completed in 6.190620605s

• [SLOW TEST:19.217 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:32:43.378: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-dcg59
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 08:32:47.753: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:32:47.757: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 08:32:49.757: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:32:49.761: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 08:32:51.757: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:32:51.762: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 08:32:53.757: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:32:53.763: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 08:32:55.757: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 08:32:55.763: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:32:55.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-dcg59" for this suite.
Feb 28 08:33:17.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:33:17.834: INFO: namespace: e2e-tests-container-lifecycle-hook-dcg59, resource: bindings, ignored listing per whitelist
Feb 28 08:33:17.968: INFO: namespace e2e-tests-container-lifecycle-hook-dcg59 deletion completed in 22.187887861s

• [SLOW TEST:34.590 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:33:17.968: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-r29hq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 08:33:18.330: INFO: Waiting up to 5m0s for pod "pod-7fb12979-3b33-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-r29hq" to be "success or failure"
Feb 28 08:33:18.337: INFO: Pod "pod-7fb12979-3b33-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 7.344806ms
Feb 28 08:33:20.345: INFO: Pod "pod-7fb12979-3b33-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015830018s
STEP: Saw pod success
Feb 28 08:33:20.345: INFO: Pod "pod-7fb12979-3b33-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:33:20.352: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-7fb12979-3b33-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:33:20.378: INFO: Waiting for pod pod-7fb12979-3b33-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:33:20.385: INFO: Pod pod-7fb12979-3b33-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:33:20.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-r29hq" for this suite.
Feb 28 08:33:26.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:33:26.893: INFO: namespace: e2e-tests-emptydir-r29hq, resource: bindings, ignored listing per whitelist
Feb 28 08:33:26.995: INFO: namespace e2e-tests-emptydir-r29hq deletion completed in 6.598683227s

• [SLOW TEST:9.027 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:33:26.995: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-5t26n
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-5t26n
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 28 08:33:27.443: INFO: Found 0 stateful pods, waiting for 3
Feb 28 08:33:37.454: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:33:37.455: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:33:37.455: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 08:33:37.514: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 28 08:33:47.575: INFO: Updating stateful set ss2
Feb 28 08:33:47.595: INFO: Waiting for Pod e2e-tests-statefulset-5t26n/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 28 08:33:57.625: INFO: Found 1 stateful pods, waiting for 3
Feb 28 08:34:07.631: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:34:07.631: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:34:07.631: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 28 08:34:07.658: INFO: Updating stateful set ss2
Feb 28 08:34:07.665: INFO: Waiting for Pod e2e-tests-statefulset-5t26n/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 28 08:34:17.699: INFO: Updating stateful set ss2
Feb 28 08:34:17.709: INFO: Waiting for StatefulSet e2e-tests-statefulset-5t26n/ss2 to complete update
Feb 28 08:34:17.709: INFO: Waiting for Pod e2e-tests-statefulset-5t26n/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:34:27.718: INFO: Deleting all statefulset in ns e2e-tests-statefulset-5t26n
Feb 28 08:34:27.723: INFO: Scaling statefulset ss2 to 0
Feb 28 08:34:57.740: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:34:57.744: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:34:57.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-5t26n" for this suite.
Feb 28 08:35:03.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:03.873: INFO: namespace: e2e-tests-statefulset-5t26n, resource: bindings, ignored listing per whitelist
Feb 28 08:35:03.933: INFO: namespace e2e-tests-statefulset-5t26n deletion completed in 6.169541517s

• [SLOW TEST:96.938 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:35:03.933: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-btn67
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 08:35:04.411: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-btn67'
Feb 28 08:35:05.820: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 28 08:35:05.820: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
Feb 28 08:35:05.828: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-btn67'
Feb 28 08:35:05.939: INFO: stderr: ""
Feb 28 08:35:05.939: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:35:05.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-btn67" for this suite.
Feb 28 08:35:27.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:35:28.094: INFO: namespace: e2e-tests-kubectl-btn67, resource: bindings, ignored listing per whitelist
Feb 28 08:35:28.120: INFO: namespace e2e-tests-kubectl-btn67 deletion completed in 22.175414605s

• [SLOW TEST:24.187 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:35:28.121: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-vt7tg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:35:28.546: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 08:35:28.560: INFO: Number of nodes with available pods: 0
Feb 28 08:35:28.560: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 08:35:29.573: INFO: Number of nodes with available pods: 0
Feb 28 08:35:29.573: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 08:35:30.577: INFO: Number of nodes with available pods: 2
Feb 28 08:35:30.577: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 28 08:35:30.618: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:30.618: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:31.632: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:31.632: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:32.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:32.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:33.629: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:33.629: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:34.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:34.630: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:35.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:35.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:36.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:36.630: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:37.633: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:37.633: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:38.640: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:38.641: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:39.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:39.630: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:40.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:40.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:41.632: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:41.632: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:42.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:42.630: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:43.633: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:43.633: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:44.632: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:44.632: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:45.634: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:45.634: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:46.636: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:46.636: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:47.634: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:47.634: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:48.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:48.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:49.637: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:49.637: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:50.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:50.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:51.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:51.630: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:52.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:52.632: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:53.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:53.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:54.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:54.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:55.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:55.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:56.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:56.630: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:57.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:57.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:58.629: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:58.629: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:59.629: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:35:59.630: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:00.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:00.631: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:01.639: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:01.639: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:02.629: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:02.629: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:03.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:03.630: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:04.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:04.630: INFO: Wrong image for pod: daemon-set-l4tbf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:04.630: INFO: Pod daemon-set-l4tbf is not available
Feb 28 08:36:05.631: INFO: Pod daemon-set-67shg is not available
Feb 28 08:36:05.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:06.632: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:07.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:08.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:09.629: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:10.633: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:11.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:12.633: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:13.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:14.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:15.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:16.636: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:17.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:18.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:19.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:20.632: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:21.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:22.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:23.632: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:24.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:25.634: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:26.629: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:27.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:28.651: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:29.629: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:30.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:31.638: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:32.636: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:33.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:34.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:35.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:36.629: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:36.629: INFO: Pod daemon-set-bghmz is not available
Feb 28 08:36:37.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:37.630: INFO: Pod daemon-set-bghmz is not available
Feb 28 08:36:38.637: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:38.637: INFO: Pod daemon-set-bghmz is not available
Feb 28 08:36:39.631: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:39.631: INFO: Pod daemon-set-bghmz is not available
Feb 28 08:36:40.630: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:40.630: INFO: Pod daemon-set-bghmz is not available
Feb 28 08:36:41.632: INFO: Wrong image for pod: daemon-set-bghmz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 28 08:36:41.632: INFO: Pod daemon-set-bghmz is not available
Feb 28 08:36:42.630: INFO: Pod daemon-set-j5pnz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 28 08:36:42.645: INFO: Number of nodes with available pods: 1
Feb 28 08:36:42.645: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 08:36:43.672: INFO: Number of nodes with available pods: 1
Feb 28 08:36:43.672: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 08:36:44.656: INFO: Number of nodes with available pods: 1
Feb 28 08:36:44.656: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 08:36:45.658: INFO: Number of nodes with available pods: 1
Feb 28 08:36:45.658: INFO: Node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc is running more than one daemon pod
Feb 28 08:36:46.661: INFO: Number of nodes with available pods: 2
Feb 28 08:36:46.661: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-vt7tg, will wait for the garbage collector to delete the pods
Feb 28 08:36:46.774: INFO: Deleting {extensions DaemonSet} daemon-set took: 20.973762ms
Feb 28 08:36:46.874: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.199533ms
Feb 28 08:36:52.491: INFO: Number of nodes with available pods: 0
Feb 28 08:36:52.491: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 08:36:52.497: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-vt7tg/daemonsets","resourceVersion":"16001"},"items":null}

Feb 28 08:36:52.503: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-vt7tg/pods","resourceVersion":"16001"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:36:52.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-vt7tg" for this suite.
Feb 28 08:36:58.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:36:58.654: INFO: namespace: e2e-tests-daemonsets-vt7tg, resource: bindings, ignored listing per whitelist
Feb 28 08:36:58.696: INFO: namespace e2e-tests-daemonsets-vt7tg deletion completed in 6.165068058s

• [SLOW TEST:90.574 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:36:58.696: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-lxjbs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-033bd25b-3b34-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume configMaps
Feb 28 08:36:59.021: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-033c7bff-3b34-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-lxjbs" to be "success or failure"
Feb 28 08:36:59.025: INFO: Pod "pod-projected-configmaps-033c7bff-3b34-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.819435ms
Feb 28 08:37:01.031: INFO: Pod "pod-projected-configmaps-033c7bff-3b34-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009537624s
STEP: Saw pod success
Feb 28 08:37:01.031: INFO: Pod "pod-projected-configmaps-033c7bff-3b34-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:37:01.035: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-configmaps-033c7bff-3b34-11e9-b1ab-fe0431d33f49 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 08:37:01.058: INFO: Waiting for pod pod-projected-configmaps-033c7bff-3b34-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:37:01.062: INFO: Pod pod-projected-configmaps-033c7bff-3b34-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:37:01.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lxjbs" for this suite.
Feb 28 08:37:07.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:37:07.166: INFO: namespace: e2e-tests-projected-lxjbs, resource: bindings, ignored listing per whitelist
Feb 28 08:37:07.324: INFO: namespace e2e-tests-projected-lxjbs deletion completed in 6.256805677s

• [SLOW TEST:8.628 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:37:07.324: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-wr5bp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 28 08:37:07.774: INFO: Waiting up to 5m0s for pod "client-containers-0873aeb9-3b34-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-containers-wr5bp" to be "success or failure"
Feb 28 08:37:07.780: INFO: Pod "client-containers-0873aeb9-3b34-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.854138ms
Feb 28 08:37:09.786: INFO: Pod "client-containers-0873aeb9-3b34-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01187928s
STEP: Saw pod success
Feb 28 08:37:09.786: INFO: Pod "client-containers-0873aeb9-3b34-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:37:09.790: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod client-containers-0873aeb9-3b34-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:37:09.808: INFO: Waiting for pod client-containers-0873aeb9-3b34-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:37:09.813: INFO: Pod client-containers-0873aeb9-3b34-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:37:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wr5bp" for this suite.
Feb 28 08:37:15.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:37:15.898: INFO: namespace: e2e-tests-containers-wr5bp, resource: bindings, ignored listing per whitelist
Feb 28 08:37:16.039: INFO: namespace e2e-tests-containers-wr5bp deletion completed in 6.215431243s

• [SLOW TEST:8.715 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:37:16.039: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-c6fz8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-c6fz8
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-c6fz8
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-c6fz8
Feb 28 08:37:16.319: INFO: Found 0 stateful pods, waiting for 1
Feb 28 08:37:26.326: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 28 08:37:26.331: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:37:27.091: INFO: stderr: ""
Feb 28 08:37:27.091: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:37:27.091: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:37:27.097: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 28 08:37:37.102: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:37:37.102: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:37:37.122: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:37:37.122: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:37:37.122: INFO: ss-1                                                          Pending         []
Feb 28 08:37:37.122: INFO: 
Feb 28 08:37:37.122: INFO: StatefulSet ss has not reached scale 3, at 2
Feb 28 08:37:38.128: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992355989s
Feb 28 08:37:39.133: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98691353s
Feb 28 08:37:40.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981681749s
Feb 28 08:37:41.143: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976675858s
Feb 28 08:37:42.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971905704s
Feb 28 08:37:43.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.934136845s
Feb 28 08:37:44.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.924429424s
Feb 28 08:37:45.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.919370991s
Feb 28 08:37:46.208: INFO: Verifying statefulset ss doesn't scale past 3 for another 914.201381ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-c6fz8
Feb 28 08:37:47.214: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:37:47.893: INFO: stderr: ""
Feb 28 08:37:47.893: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:37:47.893: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:37:47.893: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:37:48.599: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 28 08:37:48.599: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:37:48.599: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:37:48.599: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:37:49.222: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 28 08:37:49.222: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 08:37:49.222: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 08:37:49.227: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:37:49.227: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 08:37:49.227: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 28 08:37:49.231: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:37:49.801: INFO: stderr: ""
Feb 28 08:37:49.801: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:37:49.801: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:37:49.801: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:37:50.405: INFO: stderr: ""
Feb 28 08:37:50.405: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:37:50.405: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:37:50.405: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 08:37:51.108: INFO: stderr: ""
Feb 28 08:37:51.108: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 08:37:51.108: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 08:37:51.108: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:37:51.114: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 28 08:38:01.122: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:38:01.122: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:38:01.122: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 08:38:01.138: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:01.138: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:01.138: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:01.138: INFO: ss-2  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:01.138: INFO: 
Feb 28 08:38:01.138: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 08:38:02.144: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:02.144: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:02.144: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:02.144: INFO: ss-2  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:02.144: INFO: 
Feb 28 08:38:02.144: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 08:38:03.152: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:03.152: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:03.152: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:03.152: INFO: ss-2  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:03.152: INFO: 
Feb 28 08:38:03.152: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 08:38:04.157: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:04.157: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:04.157: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:04.157: INFO: 
Feb 28 08:38:04.157: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 08:38:05.163: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:05.163: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:05.163: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:05.163: INFO: 
Feb 28 08:38:05.163: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 08:38:06.169: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:06.169: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:06.169: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:06.170: INFO: 
Feb 28 08:38:06.170: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 08:38:07.176: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:07.176: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:07.176: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:07.176: INFO: 
Feb 28 08:38:07.176: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 08:38:08.181: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:08.181: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:08.181: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:08.181: INFO: 
Feb 28 08:38:08.181: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 08:38:09.187: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:09.187: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:09.187: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:09.187: INFO: 
Feb 28 08:38:09.187: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 08:38:10.191: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
Feb 28 08:38:10.191: INFO: ss-0  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:16 +0000 UTC  }]
Feb 28 08:38:10.191: INFO: ss-1  shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:37:37 +0000 UTC  }]
Feb 28 08:38:10.191: INFO: 
Feb 28 08:38:10.191: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-c6fz8
Feb 28 08:38:11.200: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:38:11.494: INFO: rc: 1
Feb 28 08:38:11.495: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002079950 exit status 1 <nil> <nil> true [0xc0000f7238 0xc0000f73e8 0xc0000f74d0] [0xc0000f7238 0xc0000f73e8 0xc0000f74d0] [0xc0000f7378 0xc0000f7460] [0x932420 0x932420] 0xc00188b560 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 28 08:38:21.495: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:38:21.700: INFO: rc: 1
Feb 28 08:38:21.700: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002464ea0 exit status 1 <nil> <nil> true [0xc00000f188 0xc00000f260 0xc00000f2b0] [0xc00000f188 0xc00000f260 0xc00000f2b0] [0xc00000f1f8 0xc00000f2a0] [0x932420 0x932420] 0xc000dac540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:38:31.700: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:38:31.853: INFO: rc: 1
Feb 28 08:38:31.853: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002079bf0 exit status 1 <nil> <nil> true [0xc0000f74f0 0xc0000f75a8 0xc0000f75c8] [0xc0000f74f0 0xc0000f75a8 0xc0000f75c8] [0xc0000f75a0 0xc0000f75c0] [0x932420 0x932420] 0xc00188bb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:38:41.854: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:38:43.011: INFO: rc: 1
Feb 28 08:38:43.011: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00296d110 exit status 1 <nil> <nil> true [0xc001176080 0xc001176098 0xc0011760d0] [0xc001176080 0xc001176098 0xc0011760d0] [0xc001176090 0xc0011760c0] [0x932420 0x932420] 0xc0025cb140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:38:53.011: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:38:53.217: INFO: rc: 1
Feb 28 08:38:53.217: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002079e90 exit status 1 <nil> <nil> true [0xc0000f75d8 0xc0000f7640 0xc0000f76f8] [0xc0000f75d8 0xc0000f7640 0xc0000f76f8] [0xc0000f7610 0xc0000f76c8] [0x932420 0x932420] 0xc002328ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:39:03.218: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:39:03.371: INFO: rc: 1
Feb 28 08:39:03.371: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d801b0 exit status 1 <nil> <nil> true [0xc0000f7718 0xc0000f7740 0xc0000f77e8] [0xc0000f7718 0xc0000f7740 0xc0000f77e8] [0xc0000f7730 0xc0000f77d0] [0x932420 0x932420] 0xc002328ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:39:13.371: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:39:13.521: INFO: rc: 1
Feb 28 08:39:13.521: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00296d380 exit status 1 <nil> <nil> true [0xc0011760d8 0xc0011760f0 0xc001176108] [0xc0011760d8 0xc0011760f0 0xc001176108] [0xc0011760e8 0xc001176100] [0x932420 0x932420] 0xc0025cb7a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:39:23.522: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:39:23.725: INFO: rc: 1
Feb 28 08:39:23.725: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d80450 exit status 1 <nil> <nil> true [0xc0000f77f0 0xc0000f7830 0xc0000f79c8] [0xc0000f77f0 0xc0000f7830 0xc0000f79c8] [0xc0000f7820 0xc0000f79b0] [0x932420 0x932420] 0xc0023291a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:39:33.725: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:39:33.953: INFO: rc: 1
Feb 28 08:39:33.953: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025918c0 exit status 1 <nil> <nil> true [0xc000856880 0xc0008568e8 0xc000856970] [0xc000856880 0xc0008568e8 0xc000856970] [0xc0008568e0 0xc000856900] [0x932420 0x932420] 0xc0012c5f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:39:43.966: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:39:44.059: INFO: rc: 1
Feb 28 08:39:44.059: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002591b60 exit status 1 <nil> <nil> true [0xc0008569d8 0xc000856a18 0xc000856a60] [0xc0008569d8 0xc000856a18 0xc000856a60] [0xc0008569f0 0xc000856a50] [0x932420 0x932420] 0xc002632720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:39:54.059: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:39:54.294: INFO: rc: 1
Feb 28 08:39:54.294: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bd4420 exit status 1 <nil> <nil> true [0xc0000f6000 0xc0000f6300 0xc0000f6518] [0xc0000f6000 0xc0000f6300 0xc0000f6518] [0xc0000f6268 0xc0000f64a8] [0x932420 0x932420] 0xc00188a3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:40:04.294: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:40:04.407: INFO: rc: 1
Feb 28 08:40:04.407: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002078330 exit status 1 <nil> <nil> true [0xc00000e088 0xc00000e278 0xc00000e448] [0xc00000e088 0xc00000e278 0xc00000e448] [0xc00000e260 0xc00000e390] [0x932420 0x932420] 0xc0029f4300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:40:14.407: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:40:14.511: INFO: rc: 1
Feb 28 08:40:14.512: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020785d0 exit status 1 <nil> <nil> true [0xc00000e480 0xc00000e498 0xc00000e5c8] [0xc00000e480 0xc00000e498 0xc00000e5c8] [0xc00000e490 0xc00000e550] [0x932420 0x932420] 0xc0029f4e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:40:24.512: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:40:24.619: INFO: rc: 1
Feb 28 08:40:24.619: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000edc270 exit status 1 <nil> <nil> true [0xc000856088 0xc0008560e8 0xc000856150] [0xc000856088 0xc0008560e8 0xc000856150] [0xc0008560b0 0xc000856130] [0x932420 0x932420] 0xc0012c4360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:40:34.620: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:40:34.728: INFO: rc: 1
Feb 28 08:40:34.728: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020788a0 exit status 1 <nil> <nil> true [0xc00000e5e0 0xc00000e718 0xc00000e878] [0xc00000e5e0 0xc00000e718 0xc00000e878] [0xc00000e6f0 0xc00000e7f0] [0x932420 0x932420] 0xc0029f5140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:40:44.728: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:40:44.917: INFO: rc: 1
Feb 28 08:40:44.917: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000edc5a0 exit status 1 <nil> <nil> true [0xc000856208 0xc000856298 0xc000856378] [0xc000856208 0xc000856298 0xc000856378] [0xc000856260 0xc000856370] [0x932420 0x932420] 0xc0012c4780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:40:54.917: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:40:55.112: INFO: rc: 1
Feb 28 08:40:55.113: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bd4810 exit status 1 <nil> <nil> true [0xc0000f65c8 0xc0000f6be0 0xc0000f6d50] [0xc0000f65c8 0xc0000f6be0 0xc0000f6d50] [0xc0000f6b68 0xc0000f6c88] [0x932420 0x932420] 0xc00188af60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:41:05.114: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:06.257: INFO: rc: 1
Feb 28 08:41:06.257: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000edc870 exit status 1 <nil> <nil> true [0xc000856390 0xc0008563e0 0xc0008564c8] [0xc000856390 0xc0008563e0 0xc0008564c8] [0xc0008563c0 0xc0008564c0] [0x932420 0x932420] 0xc0012c4a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:41:16.257: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:16.444: INFO: rc: 1
Feb 28 08:41:16.444: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d80300 exit status 1 <nil> <nil> true [0xc001176000 0xc001176018 0xc001176038] [0xc001176000 0xc001176018 0xc001176038] [0xc001176010 0xc001176030] [0x932420 0x932420] 0xc002328b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:41:26.445: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:26.562: INFO: rc: 1
Feb 28 08:41:26.562: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bd4c00 exit status 1 <nil> <nil> true [0xc0000f6dc0 0xc0000f6e68 0xc0000f6ed8] [0xc0000f6dc0 0xc0000f6e68 0xc0000f6ed8] [0xc0000f6e50 0xc0000f6eb8] [0x932420 0x932420] 0xc00188b3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:41:36.562: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:36.716: INFO: rc: 1
Feb 28 08:41:36.716: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d806f0 exit status 1 <nil> <nil> true [0xc001176040 0xc001176058 0xc001176078] [0xc001176040 0xc001176058 0xc001176078] [0xc001176050 0xc001176068] [0x932420 0x932420] 0xc002328e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:41:46.717: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:46.862: INFO: rc: 1
Feb 28 08:41:46.862: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000edcb10 exit status 1 <nil> <nil> true [0xc0008564e0 0xc000856590 0xc0008565c8] [0xc0008564e0 0xc000856590 0xc0008565c8] [0xc000856550 0xc0008565c0] [0x932420 0x932420] 0xc0012c4d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:41:56.864: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:41:57.099: INFO: rc: 1
Feb 28 08:41:57.100: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bd4450 exit status 1 <nil> <nil> true [0xc001176000 0xc001176018 0xc001176038] [0xc001176000 0xc001176018 0xc001176038] [0xc001176010 0xc001176030] [0x932420 0x932420] 0xc002328b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:42:07.101: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:07.212: INFO: rc: 1
Feb 28 08:42:07.212: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bd4840 exit status 1 <nil> <nil> true [0xc001176040 0xc001176058 0xc001176078] [0xc001176040 0xc001176058 0xc001176078] [0xc001176050 0xc001176068] [0x932420 0x932420] 0xc002328e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:42:17.212: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:17.306: INFO: rc: 1
Feb 28 08:42:17.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002078360 exit status 1 <nil> <nil> true [0xc0000f6000 0xc0000f6300 0xc0000f6518] [0xc0000f6000 0xc0000f6300 0xc0000f6518] [0xc0000f6268 0xc0000f64a8] [0x932420 0x932420] 0xc00188a3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:42:27.307: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:27.462: INFO: rc: 1
Feb 28 08:42:27.462: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d802d0 exit status 1 <nil> <nil> true [0xc00000e088 0xc00000e278 0xc00000e448] [0xc00000e088 0xc00000e278 0xc00000e448] [0xc00000e260 0xc00000e390] [0x932420 0x932420] 0xc0029f4300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:42:37.462: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:37.552: INFO: rc: 1
Feb 28 08:42:37.552: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002078630 exit status 1 <nil> <nil> true [0xc0000f65c8 0xc0000f6be0 0xc0000f6d50] [0xc0000f65c8 0xc0000f6be0 0xc0000f6d50] [0xc0000f6b68 0xc0000f6c88] [0x932420 0x932420] 0xc00188af60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:42:47.552: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:47.692: INFO: rc: 1
Feb 28 08:42:47.693: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d80630 exit status 1 <nil> <nil> true [0xc00000e480 0xc00000e498 0xc00000e5c8] [0xc00000e480 0xc00000e498 0xc00000e5c8] [0xc00000e490 0xc00000e550] [0x932420 0x932420] 0xc0029f4e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:42:57.693: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:42:57.803: INFO: rc: 1
Feb 28 08:42:57.803: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bd4c30 exit status 1 <nil> <nil> true [0xc001176080 0xc001176098 0xc0011760d0] [0xc001176080 0xc001176098 0xc0011760d0] [0xc001176090 0xc0011760c0] [0x932420 0x932420] 0xc002329140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:43:07.803: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:43:07.944: INFO: rc: 1
Feb 28 08:43:07.945: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002078930 exit status 1 <nil> <nil> true [0xc0000f6dc0 0xc0000f6e68 0xc0000f6ed8] [0xc0000f6dc0 0xc0000f6e68 0xc0000f6ed8] [0xc0000f6e50 0xc0000f6eb8] [0x932420 0x932420] 0xc00188b3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 28 08:43:17.945: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-c6fz8 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 08:43:18.054: INFO: rc: 1
Feb 28 08:43:18.054: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Feb 28 08:43:18.054: INFO: Scaling statefulset ss to 0
Feb 28 08:43:18.197: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:43:18.202: INFO: Deleting all statefulset in ns e2e-tests-statefulset-c6fz8
Feb 28 08:43:18.206: INFO: Scaling statefulset ss to 0
Feb 28 08:43:18.226: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:43:18.233: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:43:18.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-c6fz8" for this suite.
Feb 28 08:43:24.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:43:24.558: INFO: namespace: e2e-tests-statefulset-c6fz8, resource: bindings, ignored listing per whitelist
Feb 28 08:43:24.683: INFO: namespace e2e-tests-statefulset-c6fz8 deletion completed in 6.417824999s

• [SLOW TEST:368.644 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:43:24.684: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tdjll
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:43:24.924: INFO: Waiting up to 5m0s for pod "downward-api-e9406445-3b34-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-tdjll" to be "success or failure"
Feb 28 08:43:24.928: INFO: Pod "downward-api-e9406445-3b34-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.196458ms
Feb 28 08:43:26.933: INFO: Pod "downward-api-e9406445-3b34-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008480295s
STEP: Saw pod success
Feb 28 08:43:26.933: INFO: Pod "downward-api-e9406445-3b34-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:43:26.936: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downward-api-e9406445-3b34-11e9-b1ab-fe0431d33f49 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:43:26.958: INFO: Waiting for pod downward-api-e9406445-3b34-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:43:26.961: INFO: Pod downward-api-e9406445-3b34-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:43:26.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tdjll" for this suite.
Feb 28 08:43:32.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:43:33.379: INFO: namespace: e2e-tests-downward-api-tdjll, resource: bindings, ignored listing per whitelist
Feb 28 08:43:33.413: INFO: namespace e2e-tests-downward-api-tdjll deletion completed in 6.446005411s

• [SLOW TEST:8.729 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:43:33.413: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-zcmwr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 28 08:43:33.710: INFO: Waiting up to 5m0s for pod "downward-api-ee7d0ce4-3b34-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-zcmwr" to be "success or failure"
Feb 28 08:43:33.715: INFO: Pod "downward-api-ee7d0ce4-3b34-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.733772ms
Feb 28 08:43:35.724: INFO: Pod "downward-api-ee7d0ce4-3b34-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014310019s
STEP: Saw pod success
Feb 28 08:43:35.725: INFO: Pod "downward-api-ee7d0ce4-3b34-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:43:35.728: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downward-api-ee7d0ce4-3b34-11e9-b1ab-fe0431d33f49 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:43:35.750: INFO: Waiting for pod downward-api-ee7d0ce4-3b34-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:43:35.753: INFO: Pod downward-api-ee7d0ce4-3b34-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:43:35.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zcmwr" for this suite.
Feb 28 08:43:41.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:43:41.866: INFO: namespace: e2e-tests-downward-api-zcmwr, resource: bindings, ignored listing per whitelist
Feb 28 08:43:41.949: INFO: namespace e2e-tests-downward-api-zcmwr deletion completed in 6.191241597s

• [SLOW TEST:8.537 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:43:41.950: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-nprjs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:43:42.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f39d18e4-3b34-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-nprjs" to be "success or failure"
Feb 28 08:43:42.316: INFO: Pod "downwardapi-volume-f39d18e4-3b34-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.249929ms
Feb 28 08:43:44.323: INFO: Pod "downwardapi-volume-f39d18e4-3b34-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010785537s
STEP: Saw pod success
Feb 28 08:43:44.324: INFO: Pod "downwardapi-volume-f39d18e4-3b34-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:43:44.328: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-f39d18e4-3b34-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:43:44.349: INFO: Waiting for pod downwardapi-volume-f39d18e4-3b34-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:43:44.353: INFO: Pod downwardapi-volume-f39d18e4-3b34-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:43:44.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nprjs" for this suite.
Feb 28 08:43:50.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:43:50.548: INFO: namespace: e2e-tests-downward-api-nprjs, resource: bindings, ignored listing per whitelist
Feb 28 08:43:50.588: INFO: namespace e2e-tests-downward-api-nprjs deletion completed in 6.226987s

• [SLOW TEST:8.638 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:43:50.588: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-znbhs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-znbhs
Feb 28 08:43:52.946: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-znbhs
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 08:43:52.951: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:47:53.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-znbhs" for this suite.
Feb 28 08:47:59.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:47:59.858: INFO: namespace: e2e-tests-container-probe-znbhs, resource: bindings, ignored listing per whitelist
Feb 28 08:47:59.896: INFO: namespace e2e-tests-container-probe-znbhs deletion completed in 6.219243463s

• [SLOW TEST:249.308 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:47:59.896: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-2mfgz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-mktc
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:48:00.220: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mktc" in namespace "e2e-tests-subpath-2mfgz" to be "success or failure"
Feb 28 08:48:00.227: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248655ms
Feb 28 08:48:02.232: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011809858s
Feb 28 08:48:04.240: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 4.019048794s
Feb 28 08:48:06.244: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 6.023724439s
Feb 28 08:48:08.250: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 8.029187733s
Feb 28 08:48:10.255: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 10.034183755s
Feb 28 08:48:12.263: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 12.042505295s
Feb 28 08:48:14.269: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 14.048150035s
Feb 28 08:48:16.281: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 16.060268977s
Feb 28 08:48:18.286: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 18.065375203s
Feb 28 08:48:20.291: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 20.070905499s
Feb 28 08:48:22.302: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Running", Reason="", readiness=false. Elapsed: 22.081008568s
Feb 28 08:48:24.307: INFO: Pod "pod-subpath-test-secret-mktc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.086866022s
STEP: Saw pod success
Feb 28 08:48:24.307: INFO: Pod "pod-subpath-test-secret-mktc" satisfied condition "success or failure"
Feb 28 08:48:24.312: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-subpath-test-secret-mktc container test-container-subpath-secret-mktc: <nil>
STEP: delete the pod
Feb 28 08:48:24.339: INFO: Waiting for pod pod-subpath-test-secret-mktc to disappear
Feb 28 08:48:24.343: INFO: Pod pod-subpath-test-secret-mktc no longer exists
STEP: Deleting pod pod-subpath-test-secret-mktc
Feb 28 08:48:24.343: INFO: Deleting pod "pod-subpath-test-secret-mktc" in namespace "e2e-tests-subpath-2mfgz"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:48:24.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-2mfgz" for this suite.
Feb 28 08:48:30.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:30.505: INFO: namespace: e2e-tests-subpath-2mfgz, resource: bindings, ignored listing per whitelist
Feb 28 08:48:30.541: INFO: namespace e2e-tests-subpath-2mfgz deletion completed in 6.189305921s

• [SLOW TEST:30.645 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:48:30.542: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jx4bl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-9f9596b0-3b35-11e9-b1ab-fe0431d33f49
STEP: Creating configMap with name cm-test-opt-upd-9f9596f4-3b35-11e9-b1ab-fe0431d33f49
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9f9596b0-3b35-11e9-b1ab-fe0431d33f49
STEP: Updating configmap cm-test-opt-upd-9f9596f4-3b35-11e9-b1ab-fe0431d33f49
STEP: Creating configMap with name cm-test-opt-create-9f959712-3b35-11e9-b1ab-fe0431d33f49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:48:37.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jx4bl" for this suite.
Feb 28 08:48:59.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:48:59.475: INFO: namespace: e2e-tests-projected-jx4bl, resource: bindings, ignored listing per whitelist
Feb 28 08:48:59.789: INFO: namespace e2e-tests-projected-jx4bl deletion completed in 22.456684087s

• [SLOW TEST:29.248 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:48:59.790: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-zqlrm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:49:02.642: INFO: Waiting up to 5m0s for pod "client-envvars-b28ca24c-3b35-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-pods-zqlrm" to be "success or failure"
Feb 28 08:49:02.645: INFO: Pod "client-envvars-b28ca24c-3b35-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.54588ms
Feb 28 08:49:04.651: INFO: Pod "client-envvars-b28ca24c-3b35-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009628094s
STEP: Saw pod success
Feb 28 08:49:04.651: INFO: Pod "client-envvars-b28ca24c-3b35-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:49:04.655: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod client-envvars-b28ca24c-3b35-11e9-b1ab-fe0431d33f49 container env3cont: <nil>
STEP: delete the pod
Feb 28 08:49:04.674: INFO: Waiting for pod client-envvars-b28ca24c-3b35-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:49:04.685: INFO: Pod client-envvars-b28ca24c-3b35-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:49:04.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zqlrm" for this suite.
Feb 28 08:49:46.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:49:46.768: INFO: namespace: e2e-tests-pods-zqlrm, resource: bindings, ignored listing per whitelist
Feb 28 08:49:46.979: INFO: namespace e2e-tests-pods-zqlrm deletion completed in 42.289072344s

• [SLOW TEST:47.189 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:49:46.980: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-8gtxk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-smgt
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:49:47.320: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-smgt" in namespace "e2e-tests-subpath-8gtxk" to be "success or failure"
Feb 28 08:49:47.324: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.770419ms
Feb 28 08:49:49.329: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008947884s
Feb 28 08:49:51.342: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 4.021759887s
Feb 28 08:49:53.348: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 6.027952519s
Feb 28 08:49:55.356: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 8.036058195s
Feb 28 08:49:57.362: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 10.041342655s
Feb 28 08:49:59.369: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 12.048792906s
Feb 28 08:50:01.375: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 14.05429861s
Feb 28 08:50:03.380: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 16.060118018s
Feb 28 08:50:05.388: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 18.067407404s
Feb 28 08:50:07.395: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 20.074283089s
Feb 28 08:50:09.402: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Running", Reason="", readiness=false. Elapsed: 22.082021717s
Feb 28 08:50:11.409: INFO: Pod "pod-subpath-test-downwardapi-smgt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.088408679s
STEP: Saw pod success
Feb 28 08:50:11.409: INFO: Pod "pod-subpath-test-downwardapi-smgt" satisfied condition "success or failure"
Feb 28 08:50:11.415: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-subpath-test-downwardapi-smgt container test-container-subpath-downwardapi-smgt: <nil>
STEP: delete the pod
Feb 28 08:50:11.448: INFO: Waiting for pod pod-subpath-test-downwardapi-smgt to disappear
Feb 28 08:50:11.455: INFO: Pod pod-subpath-test-downwardapi-smgt no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-smgt
Feb 28 08:50:11.455: INFO: Deleting pod "pod-subpath-test-downwardapi-smgt" in namespace "e2e-tests-subpath-8gtxk"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:50:11.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-8gtxk" for this suite.
Feb 28 08:50:17.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:50:17.719: INFO: namespace: e2e-tests-subpath-8gtxk, resource: bindings, ignored listing per whitelist
Feb 28 08:50:17.839: INFO: namespace e2e-tests-subpath-8gtxk deletion completed in 6.373548991s

• [SLOW TEST:30.859 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:50:17.839: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-m5klr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 28 08:50:18.520: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-m5klr" to be "success or failure"
Feb 28 08:50:18.524: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.509112ms
Feb 28 08:50:20.533: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013278193s
STEP: Saw pod success
Feb 28 08:50:20.533: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 28 08:50:20.542: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 28 08:50:20.608: INFO: Waiting for pod pod-host-path-test to disappear
Feb 28 08:50:20.617: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:50:20.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-m5klr" for this suite.
Feb 28 08:50:26.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:50:26.870: INFO: namespace: e2e-tests-hostpath-m5klr, resource: bindings, ignored listing per whitelist
Feb 28 08:50:26.887: INFO: namespace e2e-tests-hostpath-m5klr deletion completed in 6.250201404s

• [SLOW TEST:9.048 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:50:26.887: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-27rjq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-27rjq
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-27rjq
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-27rjq
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-27rjq
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-27rjq
Feb 28 08:50:29.233: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-27rjq, name: ss-0, uid: e6213375-3b35-11e9-a4f3-1a76b5824175, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 08:50:29.279: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-27rjq, name: ss-0, uid: e6213375-3b35-11e9-a4f3-1a76b5824175, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 08:50:29.280: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-27rjq
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-27rjq
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-27rjq and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 28 08:50:31.297: INFO: Deleting all statefulset in ns e2e-tests-statefulset-27rjq
Feb 28 08:50:31.303: INFO: Scaling statefulset ss to 0
Feb 28 08:50:41.321: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 08:50:41.325: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:50:41.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-27rjq" for this suite.
Feb 28 08:50:47.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:50:47.573: INFO: namespace: e2e-tests-statefulset-27rjq, resource: bindings, ignored listing per whitelist
Feb 28 08:50:47.778: INFO: namespace e2e-tests-statefulset-27rjq deletion completed in 6.433578156s

• [SLOW TEST:20.891 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:50:47.778: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-s7b8v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 28 08:50:48.523: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 28 08:50:48.523: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:50:49.967: INFO: stderr: ""
Feb 28 08:50:49.967: INFO: stdout: "service/redis-slave created\n"
Feb 28 08:50:49.967: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 28 08:50:49.967: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:50:50.249: INFO: stderr: ""
Feb 28 08:50:50.249: INFO: stdout: "service/redis-master created\n"
Feb 28 08:50:50.249: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 28 08:50:50.250: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:50:50.522: INFO: stderr: ""
Feb 28 08:50:50.522: INFO: stdout: "service/frontend created\n"
Feb 28 08:50:50.522: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 28 08:50:50.522: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:50:50.755: INFO: stderr: ""
Feb 28 08:50:50.755: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 28 08:50:50.756: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 28 08:50:50.756: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:50:50.990: INFO: stderr: ""
Feb 28 08:50:50.990: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 28 08:50:50.990: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 28 08:50:50.990: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:50:51.227: INFO: stderr: ""
Feb 28 08:50:51.227: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 28 08:50:51.227: INFO: Waiting for all frontend pods to be Running.
Feb 28 08:51:06.280: INFO: Waiting for frontend to serve content.
Feb 28 08:51:11.353: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 28 08:51:16.443: INFO: Trying to add a new entry to the guestbook.
Feb 28 08:51:16.563: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 28 08:51:16.609: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:51:17.964: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:51:17.964: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:51:17.964: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:51:18.087: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:51:18.087: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:51:18.087: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:51:18.200: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:51:18.200: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:51:18.201: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:51:18.324: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:51:18.324: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:51:18.324: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:51:18.451: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:51:18.451: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 08:51:18.451: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-s7b8v'
Feb 28 08:51:18.576: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 08:51:18.576: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:51:18.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-s7b8v" for this suite.
Feb 28 08:51:56.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:51:56.692: INFO: namespace: e2e-tests-kubectl-s7b8v, resource: bindings, ignored listing per whitelist
Feb 28 08:51:57.058: INFO: namespace e2e-tests-kubectl-s7b8v deletion completed in 38.474193293s

• [SLOW TEST:69.279 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:51:57.058: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-zgw2q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 28 08:51:57.498: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-os-fxwlg.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml --namespace=e2e-tests-kubectl-zgw2q run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 28 08:52:00.117: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 28 08:52:00.117: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:52:02.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zgw2q" for this suite.
Feb 28 08:52:08.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:08.450: INFO: namespace: e2e-tests-kubectl-zgw2q, resource: bindings, ignored listing per whitelist
Feb 28 08:52:08.521: INFO: namespace e2e-tests-kubectl-zgw2q deletion completed in 6.384939829s

• [SLOW TEST:11.463 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:52:08.522: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-bmxqw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 28 08:52:10.902: INFO: Pod pod-hostip-2185ed05-3b36-11e9-b1ab-fe0431d33f49 has hostIP: 10.250.0.18
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:52:10.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bmxqw" for this suite.
Feb 28 08:52:32.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:33.016: INFO: namespace: e2e-tests-pods-bmxqw, resource: bindings, ignored listing per whitelist
Feb 28 08:52:33.103: INFO: namespace e2e-tests-pods-bmxqw deletion completed in 22.192223185s

• [SLOW TEST:24.582 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:52:33.104: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-7ns4q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 08:52:35.934: INFO: Successfully updated pod "pod-update-activedeadlineseconds-302c08b9-3b36-11e9-b1ab-fe0431d33f49"
Feb 28 08:52:35.934: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-302c08b9-3b36-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-pods-7ns4q" to be "terminated due to deadline exceeded"
Feb 28 08:52:35.938: INFO: Pod "pod-update-activedeadlineseconds-302c08b9-3b36-11e9-b1ab-fe0431d33f49": Phase="Running", Reason="", readiness=true. Elapsed: 3.301911ms
Feb 28 08:52:37.943: INFO: Pod "pod-update-activedeadlineseconds-302c08b9-3b36-11e9-b1ab-fe0431d33f49": Phase="Running", Reason="", readiness=true. Elapsed: 2.009035504s
Feb 28 08:52:39.949: INFO: Pod "pod-update-activedeadlineseconds-302c08b9-3b36-11e9-b1ab-fe0431d33f49": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014447141s
Feb 28 08:52:39.949: INFO: Pod "pod-update-activedeadlineseconds-302c08b9-3b36-11e9-b1ab-fe0431d33f49" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:52:39.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-7ns4q" for this suite.
Feb 28 08:52:45.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:46.077: INFO: namespace: e2e-tests-pods-7ns4q, resource: bindings, ignored listing per whitelist
Feb 28 08:52:46.158: INFO: namespace e2e-tests-pods-7ns4q deletion completed in 6.204000189s

• [SLOW TEST:13.055 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:52:46.159: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-92tlh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-37fb8461-3b36-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 08:52:46.517: INFO: Waiting up to 5m0s for pod "pod-secrets-37fce8f9-3b36-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-secrets-92tlh" to be "success or failure"
Feb 28 08:52:46.522: INFO: Pod "pod-secrets-37fce8f9-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.89919ms
Feb 28 08:52:48.530: INFO: Pod "pod-secrets-37fce8f9-3b36-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013243156s
STEP: Saw pod success
Feb 28 08:52:48.530: INFO: Pod "pod-secrets-37fce8f9-3b36-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:52:48.537: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-secrets-37fce8f9-3b36-11e9-b1ab-fe0431d33f49 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:52:48.595: INFO: Waiting for pod pod-secrets-37fce8f9-3b36-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:52:48.598: INFO: Pod pod-secrets-37fce8f9-3b36-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:52:48.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-92tlh" for this suite.
Feb 28 08:52:54.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:52:54.666: INFO: namespace: e2e-tests-secrets-92tlh, resource: bindings, ignored listing per whitelist
Feb 28 08:52:54.748: INFO: namespace e2e-tests-secrets-92tlh deletion completed in 6.145647005s

• [SLOW TEST:8.590 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:52:54.749: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bmvww
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 08:52:57.669: INFO: Successfully updated pod "annotationupdate3d1b579c-3b36-11e9-b1ab-fe0431d33f49"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:53:01.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bmvww" for this suite.
Feb 28 08:53:25.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:53:25.967: INFO: namespace: e2e-tests-projected-bmvww, resource: bindings, ignored listing per whitelist
Feb 28 08:53:25.986: INFO: namespace e2e-tests-projected-bmvww deletion completed in 24.243161296s

• [SLOW TEST:31.238 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:53:25.986: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-55sq5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-x4x5f
STEP: Creating secret with name secret-test-4fb48d57-3b36-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 08:53:26.525: INFO: Waiting up to 5m0s for pod "pod-secrets-4fd579e3-3b36-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-secrets-55sq5" to be "success or failure"
Feb 28 08:53:26.531: INFO: Pod "pod-secrets-4fd579e3-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.289698ms
Feb 28 08:53:28.536: INFO: Pod "pod-secrets-4fd579e3-3b36-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01095155s
STEP: Saw pod success
Feb 28 08:53:28.536: INFO: Pod "pod-secrets-4fd579e3-3b36-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:53:28.540: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-secrets-4fd579e3-3b36-11e9-b1ab-fe0431d33f49 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:53:28.586: INFO: Waiting for pod pod-secrets-4fd579e3-3b36-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:53:28.589: INFO: Pod pod-secrets-4fd579e3-3b36-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:53:28.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-55sq5" for this suite.
Feb 28 08:53:34.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:53:34.659: INFO: namespace: e2e-tests-secrets-55sq5, resource: bindings, ignored listing per whitelist
Feb 28 08:53:34.797: INFO: namespace e2e-tests-secrets-55sq5 deletion completed in 6.202408208s
STEP: Destroying namespace "e2e-tests-secret-namespace-x4x5f" for this suite.
Feb 28 08:53:40.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:53:40.978: INFO: namespace: e2e-tests-secret-namespace-x4x5f, resource: bindings, ignored listing per whitelist
Feb 28 08:53:41.032: INFO: namespace e2e-tests-secret-namespace-x4x5f deletion completed in 6.235184445s

• [SLOW TEST:15.046 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:53:41.043: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8bwqt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 28 08:53:43.846: INFO: Successfully updated pod "labelsupdate58a53771-3b36-11e9-b1ab-fe0431d33f49"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:53:47.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8bwqt" for this suite.
Feb 28 08:54:09.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:54:10.082: INFO: namespace: e2e-tests-projected-8bwqt, resource: bindings, ignored listing per whitelist
Feb 28 08:54:10.127: INFO: namespace e2e-tests-projected-8bwqt deletion completed in 22.241951876s

• [SLOW TEST:29.085 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:54:10.128: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-n6d92
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 28 08:54:10.511: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 08:54:10.520: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 08:54:10.540: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-gjhsc before test
Feb 28 08:54:10.561: INFO: addons-kube-lego-648f8c9f5c-r9z47 from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container kube-lego ready: true, restart count 0
Feb 28 08:54:10.561: INFO: addons-nginx-ingress-controller-8f975c795-fn6qg from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 28 08:54:10.561: INFO: blackbox-exporter-58fd9b8556-6kz6h from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 28 08:54:10.561: INFO: coredns-5f4748c5f-lk8wm from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container coredns ready: true, restart count 0
Feb 28 08:54:10.561: INFO: node-exporter-rm79f from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:54:10.561: INFO: calico-node-b4l87 from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container calico-node ready: true, restart count 0
Feb 28 08:54:10.561: INFO: addons-kubernetes-dashboard-5f64f76bd-rd55c from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 08:54:10.561: INFO: vpn-shoot-c4d998c9f-45b9w from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 28 08:54:10.561: INFO: kube-proxy-26xfb from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:54:10.561: INFO: metrics-server-85674d74c8-6gl2l from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 08:54:10.561: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-g2mjq from kube-system started at 2019-02-28 07:18:53 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.561: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 28 08:54:10.561: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr before test
Feb 28 08:54:10.603: INFO: kube-proxy-2n298 from kube-system started at 2019-02-28 07:18:56 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.603: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 08:54:10.603: INFO: node-exporter-t6qsh from kube-system started at 2019-02-28 07:18:56 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.603: INFO: 	Container node-exporter ready: true, restart count 0
Feb 28 08:54:10.603: INFO: calico-node-stfff from kube-system started at 2019-02-28 07:18:56 +0000 UTC (1 container statuses recorded)
Feb 28 08:54:10.603: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6b54357e-3b36-11e9-b1ab-fe0431d33f49 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-6b54357e-3b36-11e9-b1ab-fe0431d33f49 off the node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6b54357e-3b36-11e9-b1ab-fe0431d33f49
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:54:14.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-n6d92" for this suite.
Feb 28 08:54:28.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:54:28.840: INFO: namespace: e2e-tests-sched-pred-n6d92, resource: bindings, ignored listing per whitelist
Feb 28 08:54:28.970: INFO: namespace e2e-tests-sched-pred-n6d92 deletion completed in 14.21580117s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:18.843 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:54:28.971: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-6trjw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-x4mn
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 08:54:29.328: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-x4mn" in namespace "e2e-tests-subpath-6trjw" to be "success or failure"
Feb 28 08:54:29.334: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Pending", Reason="", readiness=false. Elapsed: 5.14535ms
Feb 28 08:54:31.339: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010398191s
Feb 28 08:54:33.346: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 4.017660303s
Feb 28 08:54:35.354: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 6.025312593s
Feb 28 08:54:37.360: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 8.031140653s
Feb 28 08:54:39.366: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 10.037967133s
Feb 28 08:54:41.374: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 12.045233223s
Feb 28 08:54:43.380: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 14.0520131s
Feb 28 08:54:45.386: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 16.058058816s
Feb 28 08:54:47.391: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 18.062896752s
Feb 28 08:54:49.397: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 20.068737908s
Feb 28 08:54:51.406: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Running", Reason="", readiness=false. Elapsed: 22.077895499s
Feb 28 08:54:53.412: INFO: Pod "pod-subpath-test-projected-x4mn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.08370941s
STEP: Saw pod success
Feb 28 08:54:53.412: INFO: Pod "pod-subpath-test-projected-x4mn" satisfied condition "success or failure"
Feb 28 08:54:53.416: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-subpath-test-projected-x4mn container test-container-subpath-projected-x4mn: <nil>
STEP: delete the pod
Feb 28 08:54:53.438: INFO: Waiting for pod pod-subpath-test-projected-x4mn to disappear
Feb 28 08:54:53.442: INFO: Pod pod-subpath-test-projected-x4mn no longer exists
STEP: Deleting pod pod-subpath-test-projected-x4mn
Feb 28 08:54:53.442: INFO: Deleting pod "pod-subpath-test-projected-x4mn" in namespace "e2e-tests-subpath-6trjw"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:54:53.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-6trjw" for this suite.
Feb 28 08:54:59.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:54:59.589: INFO: namespace: e2e-tests-subpath-6trjw, resource: bindings, ignored listing per whitelist
Feb 28 08:54:59.702: INFO: namespace e2e-tests-subpath-6trjw deletion completed in 6.250231105s

• [SLOW TEST:30.731 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:54:59.703: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-d56zx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 08:55:00.018: INFO: Waiting up to 5m0s for pod "pod-878e41df-3b36-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-emptydir-d56zx" to be "success or failure"
Feb 28 08:55:00.023: INFO: Pod "pod-878e41df-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.727225ms
Feb 28 08:55:02.029: INFO: Pod "pod-878e41df-3b36-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010610527s
STEP: Saw pod success
Feb 28 08:55:02.029: INFO: Pod "pod-878e41df-3b36-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:55:02.034: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-878e41df-3b36-11e9-b1ab-fe0431d33f49 container test-container: <nil>
STEP: delete the pod
Feb 28 08:55:02.058: INFO: Waiting for pod pod-878e41df-3b36-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:55:02.062: INFO: Pod pod-878e41df-3b36-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:55:02.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-d56zx" for this suite.
Feb 28 08:55:08.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:55:08.164: INFO: namespace: e2e-tests-emptydir-d56zx, resource: bindings, ignored listing per whitelist
Feb 28 08:55:08.214: INFO: namespace e2e-tests-emptydir-d56zx deletion completed in 6.144512899s

• [SLOW TEST:8.511 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:55:08.215: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-zskkm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-zskkm
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 08:55:08.506: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 08:55:26.597: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.234:8080/dial?request=hostName&protocol=udp&host=100.96.1.233&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-zskkm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:55:26.597: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:55:27.236: INFO: Waiting for endpoints: map[]
Feb 28 08:55:27.240: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.234:8080/dial?request=hostName&protocol=udp&host=100.96.0.67&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-zskkm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 08:55:27.240: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
Feb 28 08:55:27.707: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:55:27.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-zskkm" for this suite.
Feb 28 08:55:49.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:55:49.933: INFO: namespace: e2e-tests-pod-network-test-zskkm, resource: bindings, ignored listing per whitelist
Feb 28 08:55:49.950: INFO: namespace e2e-tests-pod-network-test-zskkm deletion completed in 22.236226302s

• [SLOW TEST:41.736 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:55:49.951: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-v5blv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 28 08:55:50.213: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 28 08:55:55.219: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 08:55:55.240: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 28 08:55:55.265: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-v5blv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-v5blv/deployments/test-cleanup-deployment,UID:a87beff4-3b36-11e9-a4f3-1a76b5824175,ResourceVersion:19086,Generation:1,CreationTimestamp:2019-02-28 08:55:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb 28 08:55:55.281: INFO: New ReplicaSet "test-cleanup-deployment-755f6b95cc" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc,GenerateName:,Namespace:e2e-tests-deployment-v5blv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-v5blv/replicasets/test-cleanup-deployment-755f6b95cc,UID:a87daef1-3b36-11e9-a4f3-1a76b5824175,ResourceVersion:19088,Generation:1,CreationTimestamp:2019-02-28 08:55:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a87beff4-3b36-11e9-a4f3-1a76b5824175 0xc001a574d7 0xc001a574d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 08:55:55.281: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 28 08:55:55.281: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-v5blv,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-v5blv/replicasets/test-cleanup-controller,UID:a57a5af5-3b36-11e9-a4f3-1a76b5824175,ResourceVersion:19087,Generation:1,CreationTimestamp:2019-02-28 08:55:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a87beff4-3b36-11e9-a4f3-1a76b5824175 0xc001a57407 0xc001a57408}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 08:55:55.295: INFO: Pod "test-cleanup-controller-trh9c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-trh9c,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-v5blv,SelfLink:/api/v1/namespaces/e2e-tests-deployment-v5blv/pods/test-cleanup-controller-trh9c,UID:a57bfb57-3b36-11e9-a4f3-1a76b5824175,ResourceVersion:19075,Generation:0,CreationTimestamp:2019-02-28 08:55:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.235/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller a57a5af5-3b36-11e9-a4f3-1a76b5824175 0xc001fba527 0xc001fba528}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-w78g7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w78g7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w78g7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001fba670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001fba690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:55:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:55:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:55:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:55:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.18,PodIP:100.96.1.235,StartTime:2019-02-28 08:55:50 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-28 08:55:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://f5088f8950aafae0e92592a8a68678f0270ea07a8d7120832592e6a611ab6972}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 08:55:55.295: INFO: Pod "test-cleanup-deployment-755f6b95cc-pdh94" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc-pdh94,GenerateName:test-cleanup-deployment-755f6b95cc-,Namespace:e2e-tests-deployment-v5blv,SelfLink:/api/v1/namespaces/e2e-tests-deployment-v5blv/pods/test-cleanup-deployment-755f6b95cc-pdh94,UID:a880147f-3b36-11e9-a4f3-1a76b5824175,ResourceVersion:19092,Generation:0,CreationTimestamp:2019-02-28 08:55:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-755f6b95cc a87daef1-3b36-11e9-a4f3-1a76b5824175 0xc001fba827 0xc001fba828}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-w78g7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w78g7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-w78g7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001fba890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001fba8b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-28 08:55:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:55:55.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-v5blv" for this suite.
Feb 28 08:56:01.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:56:01.461: INFO: namespace: e2e-tests-deployment-v5blv, resource: bindings, ignored listing per whitelist
Feb 28 08:56:01.493: INFO: namespace e2e-tests-deployment-v5blv deletion completed in 6.193372582s

• [SLOW TEST:11.543 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:56:01.494: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-xxrcd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 28 08:56:01.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-downward-api-xxrcd" to be "success or failure"
Feb 28 08:56:01.811: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796034ms
Feb 28 08:56:03.820: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012625323s
Feb 28 08:56:05.828: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020429321s
Feb 28 08:56:07.834: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026896417s
Feb 28 08:56:09.840: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032802281s
Feb 28 08:56:11.845: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 10.037159234s
Feb 28 08:56:13.850: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 12.042795453s
Feb 28 08:56:15.855: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 14.047737507s
Feb 28 08:56:17.860: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 16.052621506s
Feb 28 08:56:19.865: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 18.057180368s
Feb 28 08:56:21.872: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 20.064488414s
Feb 28 08:56:23.877: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 22.069466534s
Feb 28 08:56:25.882: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 24.074392685s
Feb 28 08:56:27.887: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 26.079716278s
Feb 28 08:56:29.894: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 28.086928807s
Feb 28 08:56:31.901: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 30.093380531s
Feb 28 08:56:33.906: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.098691814s
STEP: Saw pod success
Feb 28 08:56:33.906: INFO: Pod "downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:56:33.910: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49 container client-container: <nil>
STEP: delete the pod
Feb 28 08:56:33.931: INFO: Waiting for pod downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:56:33.935: INFO: Pod downwardapi-volume-ac637271-3b36-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:56:33.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-xxrcd" for this suite.
Feb 28 08:56:39.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:56:40.084: INFO: namespace: e2e-tests-downward-api-xxrcd, resource: bindings, ignored listing per whitelist
Feb 28 08:56:40.150: INFO: namespace e2e-tests-downward-api-xxrcd deletion completed in 6.209137789s

• [SLOW TEST:38.656 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:56:40.150: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-rztzb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 28 08:56:40.508: INFO: Waiting up to 5m0s for pod "var-expansion-c37552c2-3b36-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-var-expansion-rztzb" to be "success or failure"
Feb 28 08:56:40.513: INFO: Pod "var-expansion-c37552c2-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058568ms
Feb 28 08:56:42.519: INFO: Pod "var-expansion-c37552c2-3b36-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010437417s
STEP: Saw pod success
Feb 28 08:56:42.519: INFO: Pod "var-expansion-c37552c2-3b36-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:56:42.529: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod var-expansion-c37552c2-3b36-11e9-b1ab-fe0431d33f49 container dapi-container: <nil>
STEP: delete the pod
Feb 28 08:56:42.574: INFO: Waiting for pod var-expansion-c37552c2-3b36-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:56:42.578: INFO: Pod var-expansion-c37552c2-3b36-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:56:42.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-rztzb" for this suite.
Feb 28 08:56:48.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:56:48.690: INFO: namespace: e2e-tests-var-expansion-rztzb, resource: bindings, ignored listing per whitelist
Feb 28 08:56:48.815: INFO: namespace e2e-tests-var-expansion-rztzb deletion completed in 6.229749858s

• [SLOW TEST:8.666 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:56:48.816: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-l5vlh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-c89543fe-3b36-11e9-b1ab-fe0431d33f49
STEP: Creating a pod to test consume secrets
Feb 28 08:56:49.112: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c895e95e-3b36-11e9-b1ab-fe0431d33f49" in namespace "e2e-tests-projected-l5vlh" to be "success or failure"
Feb 28 08:56:49.116: INFO: Pod "pod-projected-secrets-c895e95e-3b36-11e9-b1ab-fe0431d33f49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.64966ms
Feb 28 08:56:51.121: INFO: Pod "pod-projected-secrets-c895e95e-3b36-11e9-b1ab-fe0431d33f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009317905s
STEP: Saw pod success
Feb 28 08:56:51.121: INFO: Pod "pod-projected-secrets-c895e95e-3b36-11e9-b1ab-fe0431d33f49" satisfied condition "success or failure"
Feb 28 08:56:51.125: INFO: Trying to get logs from node shoot--it--pub-os-fxwlg-cpu-worker-z1-5598fcc887-q26mr pod pod-projected-secrets-c895e95e-3b36-11e9-b1ab-fe0431d33f49 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 08:56:51.144: INFO: Waiting for pod pod-projected-secrets-c895e95e-3b36-11e9-b1ab-fe0431d33f49 to disappear
Feb 28 08:56:51.152: INFO: Pod pod-projected-secrets-c895e95e-3b36-11e9-b1ab-fe0431d33f49 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:56:51.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l5vlh" for this suite.
Feb 28 08:56:57.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:56:57.194: INFO: namespace: e2e-tests-projected-l5vlh, resource: bindings, ignored listing per whitelist
Feb 28 08:56:57.317: INFO: namespace e2e-tests-projected-l5vlh deletion completed in 6.158782375s

• [SLOW TEST:8.501 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 28 08:56:57.317: INFO: >>> kubeConfig: /tmp/build/0b189050/git-kubernetes_publish_conf_test_results-master_master/scripts/openstack_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-b5r9b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0228 08:57:03.653166   30646 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 28 08:57:03.653: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 28 08:57:03.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-b5r9b" for this suite.
Feb 28 08:57:09.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 08:57:09.777: INFO: namespace: e2e-tests-gc-b5r9b, resource: bindings, ignored listing per whitelist
Feb 28 08:57:09.881: INFO: namespace e2e-tests-gc-b5r9b deletion completed in 6.222731178s

• [SLOW TEST:12.564 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSFeb 28 08:57:09.881: INFO: Running AfterSuite actions on all node
Feb 28 08:57:09.891: INFO: Running AfterSuite actions on node 1
Feb 28 08:57:09.891: INFO: Skipping dumping logs from cluster

Ran 187 of 2011 Specs in 4982.505 seconds
SUCCESS! -- 187 Passed | 0 Failed | 0 Flaked | 0 Pending | 1824 Skipped PASS

Ginkgo ran 1 suite in 1h23m4.235425397s
Test Suite Passed
