Jan 29 08:48:05.931: INFO: Overriding default scale value of zero to 1
Jan 29 08:48:05.931: INFO: Overriding default milliseconds value of zero to 5000
I0129 08:48:06.591495      19 test_context.go:385] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-900852259
I0129 08:48:06.591780      19 e2e.go:304] Starting e2e run "984bf287-23a2-11e9-9015-b2c9bac28373" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1548751685 - Will randomize all specs
Will run 188 of 1814 specs

Jan 29 08:48:06.802: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:48:06.805: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 29 08:48:06.819: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 29 08:48:06.903: INFO: The status of Pod logging-elk-elasticsearch-curator-1548631800-jp4mg is Succeeded, skipping waiting
Jan 29 08:48:06.903: INFO: The status of Pod logging-elk-elasticsearch-curator-1548718200-clnkm is Succeeded, skipping waiting
Jan 29 08:48:06.903: INFO: 64 / 66 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 29 08:48:06.904: INFO: expected 28 pod replicas in namespace 'kube-system', 28 are Running and Ready.
Jan 29 08:48:06.904: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 29 08:48:06.922: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'audit-logging-fluentd-ds' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'auth-apikeys' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'auth-idp' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'auth-pap' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'auth-pdp' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'catalog-ui' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'icp-management-ingress' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kube-dns' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'logging-elk-filebeat-ds' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'monitoring-prometheus-nodeexporter' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'nginx-ingress-controller' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'platform-ui' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'service-catalog-apiserver' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'unified-router' (0 seconds elapsed)
Jan 29 08:48:06.922: INFO: e2e test version: v1.12.1
Jan 29 08:48:06.923: INFO: kube-apiserver version: v1.12.4+icp-ee
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:48:06.924: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pod-network-test
Jan 29 08:48:07.007: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jan 29 08:48:07.016: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-mxmlh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-mxmlh
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 08:48:07.123: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 29 08:48:31.772: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.1.211.186 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-mxmlh PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:48:31.773: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:48:32.959: INFO: Found all expected endpoints: [netserver-0]
Jan 29 08:48:32.961: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.1.12.146 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-mxmlh PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:48:32.961: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:48:34.112: INFO: Found all expected endpoints: [netserver-1]
Jan 29 08:48:34.116: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 10.1.148.42 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-mxmlh PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:48:34.117: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:48:35.272: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:48:35.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-mxmlh" for this suite.
Jan 29 08:48:57.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:48:57.350: INFO: namespace: e2e-tests-pod-network-test-mxmlh, resource: bindings, ignored listing per whitelist
Jan 29 08:48:57.420: INFO: namespace e2e-tests-pod-network-test-mxmlh deletion completed in 22.141957942s

â€¢ [SLOW TEST:50.496 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:48:57.420: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-b4qjg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-b7294695-23a2-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 08:48:57.636: INFO: Waiting up to 5m0s for pod "pod-configmaps-b729f621-23a2-11e9-9015-b2c9bac28373" in namespace "e2e-tests-configmap-b4qjg" to be "success or failure"
Jan 29 08:48:57.639: INFO: Pod "pod-configmaps-b729f621-23a2-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.762797ms
Jan 29 08:48:59.642: INFO: Pod "pod-configmaps-b729f621-23a2-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005652679s
Jan 29 08:49:01.645: INFO: Pod "pod-configmaps-b729f621-23a2-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008583488s
STEP: Saw pod success
Jan 29 08:49:01.645: INFO: Pod "pod-configmaps-b729f621-23a2-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:49:01.647: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-b729f621-23a2-11e9-9015-b2c9bac28373 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 08:49:01.665: INFO: Waiting for pod pod-configmaps-b729f621-23a2-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:49:01.669: INFO: Pod pod-configmaps-b729f621-23a2-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:49:01.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-b4qjg" for this suite.
Jan 29 08:49:07.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:49:07.773: INFO: namespace: e2e-tests-configmap-b4qjg, resource: bindings, ignored listing per whitelist
Jan 29 08:49:07.817: INFO: namespace e2e-tests-configmap-b4qjg deletion completed in 6.142980731s

â€¢ [SLOW TEST:10.397 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:49:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-xvttm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 29 08:49:10.064: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-bd5bf773-23a2-11e9-9015-b2c9bac28373,GenerateName:,Namespace:e2e-tests-events-xvttm,SelfLink:/api/v1/namespaces/e2e-tests-events-xvttm/pods/send-events-bd5bf773-23a2-11e9-9015-b2c9bac28373,UID:bd5f8923-23a2-11e9-a926-eeeeeeeeeeee,ResourceVersion:677792,Generation:0,CreationTimestamp:2019-01-29 08:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 10590417,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zrb2s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zrb2s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-zrb2s true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc420e4eaa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc420e4eaf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 08:49:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 08:49:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 08:49:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 08:49:08 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:10.1.211.190,StartTime:2019-01-29 08:49:08 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-01-29 08:49:09 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://15febb0943d179435b2f51a15d3dc31466e8292161b24a3eea47a459e2044b3d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jan 29 08:49:12.069: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 29 08:49:14.071: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:49:14.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-xvttm" for this suite.
Jan 29 08:49:56.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:49:56.150: INFO: namespace: e2e-tests-events-xvttm, resource: bindings, ignored listing per whitelist
Jan 29 08:49:56.227: INFO: namespace e2e-tests-events-xvttm deletion completed in 42.144860519s

â€¢ [SLOW TEST:48.409 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:49:56.227: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-8b6dz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-8b6dz
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-8b6dz
STEP: Deleting pre-stop pod
Jan 29 08:50:09.551: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:50:09.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-8b6dz" for this suite.
Jan 29 08:50:47.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:50:47.624: INFO: namespace: e2e-tests-prestop-8b6dz, resource: bindings, ignored listing per whitelist
Jan 29 08:50:47.679: INFO: namespace e2e-tests-prestop-8b6dz deletion completed in 38.11284451s

â€¢ [SLOW TEST:51.452 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:50:47.679: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8fj4k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 08:50:47.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8df796c-23a2-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-8fj4k" to be "success or failure"
Jan 29 08:50:47.942: INFO: Pod "downwardapi-volume-f8df796c-23a2-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.929389ms
Jan 29 08:50:49.945: INFO: Pod "downwardapi-volume-f8df796c-23a2-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00608666s
Jan 29 08:50:51.949: INFO: Pod "downwardapi-volume-f8df796c-23a2-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010111583s
STEP: Saw pod success
Jan 29 08:50:51.949: INFO: Pod "downwardapi-volume-f8df796c-23a2-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:50:51.952: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-f8df796c-23a2-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 08:50:51.972: INFO: Waiting for pod downwardapi-volume-f8df796c-23a2-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:50:51.977: INFO: Pod downwardapi-volume-f8df796c-23a2-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:50:51.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8fj4k" for this suite.
Jan 29 08:50:57.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:50:58.048: INFO: namespace: e2e-tests-projected-8fj4k, resource: bindings, ignored listing per whitelist
Jan 29 08:50:58.106: INFO: namespace e2e-tests-projected-8fj4k deletion completed in 6.122187978s

â€¢ [SLOW TEST:10.427 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:50:58.107: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-f5nhc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-ff181a11-23a2-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 08:50:58.331: INFO: Waiting up to 5m0s for pod "pod-secrets-ff18fb67-23a2-11e9-9015-b2c9bac28373" in namespace "e2e-tests-secrets-f5nhc" to be "success or failure"
Jan 29 08:50:58.350: INFO: Pod "pod-secrets-ff18fb67-23a2-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 18.723437ms
Jan 29 08:51:00.353: INFO: Pod "pod-secrets-ff18fb67-23a2-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02179528s
Jan 29 08:51:02.356: INFO: Pod "pod-secrets-ff18fb67-23a2-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024974036s
STEP: Saw pod success
Jan 29 08:51:02.356: INFO: Pod "pod-secrets-ff18fb67-23a2-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:51:02.359: INFO: Trying to get logs from node 9.111.254.123 pod pod-secrets-ff18fb67-23a2-11e9-9015-b2c9bac28373 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 08:51:02.381: INFO: Waiting for pod pod-secrets-ff18fb67-23a2-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:51:02.384: INFO: Pod pod-secrets-ff18fb67-23a2-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:51:02.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-f5nhc" for this suite.
Jan 29 08:51:08.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:51:08.461: INFO: namespace: e2e-tests-secrets-f5nhc, resource: bindings, ignored listing per whitelist
Jan 29 08:51:08.492: INFO: namespace e2e-tests-secrets-f5nhc deletion completed in 6.102751202s

â€¢ [SLOW TEST:10.386 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:51:08.493: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qchnq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 08:51:08.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-054d9841-23a3-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-qchnq" to be "success or failure"
Jan 29 08:51:08.754: INFO: Pod "downwardapi-volume-054d9841-23a3-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 5.485401ms
Jan 29 08:51:10.756: INFO: Pod "downwardapi-volume-054d9841-23a3-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00817372s
Jan 29 08:51:12.759: INFO: Pod "downwardapi-volume-054d9841-23a3-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011454195s
STEP: Saw pod success
Jan 29 08:51:12.759: INFO: Pod "downwardapi-volume-054d9841-23a3-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:51:12.763: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-054d9841-23a3-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 08:51:12.784: INFO: Waiting for pod downwardapi-volume-054d9841-23a3-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:51:12.787: INFO: Pod downwardapi-volume-054d9841-23a3-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:51:12.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qchnq" for this suite.
Jan 29 08:51:18.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:51:18.844: INFO: namespace: e2e-tests-downward-api-qchnq, resource: bindings, ignored listing per whitelist
Jan 29 08:51:18.904: INFO: namespace e2e-tests-downward-api-qchnq deletion completed in 6.110528556s

â€¢ [SLOW TEST:10.412 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:51:18.904: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-p5gk6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 29 08:51:19.133: INFO: Waiting up to 5m0s for pod "pod-0b7dcb54-23a3-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-p5gk6" to be "success or failure"
Jan 29 08:51:19.136: INFO: Pod "pod-0b7dcb54-23a3-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.492579ms
Jan 29 08:51:21.143: INFO: Pod "pod-0b7dcb54-23a3-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010666573s
STEP: Saw pod success
Jan 29 08:51:21.143: INFO: Pod "pod-0b7dcb54-23a3-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:51:21.146: INFO: Trying to get logs from node 9.111.254.123 pod pod-0b7dcb54-23a3-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 08:51:21.171: INFO: Waiting for pod pod-0b7dcb54-23a3-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:51:21.174: INFO: Pod pod-0b7dcb54-23a3-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:51:21.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-p5gk6" for this suite.
Jan 29 08:51:27.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:51:27.267: INFO: namespace: e2e-tests-emptydir-p5gk6, resource: bindings, ignored listing per whitelist
Jan 29 08:51:27.328: INFO: namespace e2e-tests-emptydir-p5gk6 deletion completed in 6.144704786s

â€¢ [SLOW TEST:8.424 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:51:27.329: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-xtt44
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 08:51:29.635: INFO: Waiting up to 5m0s for pod "client-envvars-11baa0ac-23a3-11e9-9015-b2c9bac28373" in namespace "e2e-tests-pods-xtt44" to be "success or failure"
Jan 29 08:51:29.641: INFO: Pod "client-envvars-11baa0ac-23a3-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 6.492926ms
Jan 29 08:51:31.644: INFO: Pod "client-envvars-11baa0ac-23a3-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009666123s
STEP: Saw pod success
Jan 29 08:51:31.644: INFO: Pod "client-envvars-11baa0ac-23a3-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:51:31.649: INFO: Trying to get logs from node 9.111.254.123 pod client-envvars-11baa0ac-23a3-11e9-9015-b2c9bac28373 container env3cont: <nil>
STEP: delete the pod
Jan 29 08:51:31.682: INFO: Waiting for pod client-envvars-11baa0ac-23a3-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:51:31.686: INFO: Pod client-envvars-11baa0ac-23a3-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:51:31.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xtt44" for this suite.
Jan 29 08:52:17.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:52:17.897: INFO: namespace: e2e-tests-pods-xtt44, resource: bindings, ignored listing per whitelist
Jan 29 08:52:17.904: INFO: namespace e2e-tests-pods-xtt44 deletion completed in 46.21051726s

â€¢ [SLOW TEST:50.576 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:52:17.905: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7gz9f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 29 08:52:18.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-7gz9f'
Jan 29 08:52:18.455: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Jan 29 08:52:18.455: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jan 29 08:52:18.469: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-9vbsn]
Jan 29 08:52:18.469: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-9vbsn" in namespace "e2e-tests-kubectl-7gz9f" to be "running and ready"
Jan 29 08:52:18.472: INFO: Pod "e2e-test-nginx-rc-9vbsn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.6017ms
Jan 29 08:52:20.476: INFO: Pod "e2e-test-nginx-rc-9vbsn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00697213s
Jan 29 08:52:22.479: INFO: Pod "e2e-test-nginx-rc-9vbsn": Phase="Running", Reason="", readiness=true. Elapsed: 4.00994614s
Jan 29 08:52:22.479: INFO: Pod "e2e-test-nginx-rc-9vbsn" satisfied condition "running and ready"
Jan 29 08:52:22.479: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-9vbsn]
Jan 29 08:52:22.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-7gz9f'
Jan 29 08:52:22.636: INFO: stderr: ""
Jan 29 08:52:22.636: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
Jan 29 08:52:22.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-7gz9f'
Jan 29 08:52:22.770: INFO: stderr: ""
Jan 29 08:52:22.771: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:52:22.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7gz9f" for this suite.
Jan 29 08:52:28.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:52:28.982: INFO: namespace: e2e-tests-kubectl-7gz9f, resource: bindings, ignored listing per whitelist
Jan 29 08:52:29.011: INFO: namespace e2e-tests-kubectl-7gz9f deletion completed in 6.229935559s

â€¢ [SLOW TEST:11.106 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:52:29.011: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-djsfp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 29 08:52:37.755: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:37.755: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:37.913: INFO: Exec stderr: ""
Jan 29 08:52:37.913: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:37.913: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:38.085: INFO: Exec stderr: ""
Jan 29 08:52:38.085: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:38.085: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:38.234: INFO: Exec stderr: ""
Jan 29 08:52:38.234: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:38.234: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:38.390: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 29 08:52:38.390: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:38.390: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:38.549: INFO: Exec stderr: ""
Jan 29 08:52:38.549: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:38.549: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:38.737: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 29 08:52:38.737: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:38.737: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:38.901: INFO: Exec stderr: ""
Jan 29 08:52:38.901: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:38.901: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:39.055: INFO: Exec stderr: ""
Jan 29 08:52:39.055: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:39.055: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:39.205: INFO: Exec stderr: ""
Jan 29 08:52:39.205: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-djsfp PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 08:52:39.205: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 08:52:39.358: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:52:39.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-djsfp" for this suite.
Jan 29 08:53:23.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:53:23.428: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-djsfp, resource: bindings, ignored listing per whitelist
Jan 29 08:53:23.463: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-djsfp deletion completed in 44.100811409s

â€¢ [SLOW TEST:54.452 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:53:23.463: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-m4j6d
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-55c3e51e-23a3-11e9-9015-b2c9bac28373
STEP: Creating secret with name s-test-opt-upd-55c3e5a4-23a3-11e9-9015-b2c9bac28373
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-55c3e51e-23a3-11e9-9015-b2c9bac28373
STEP: Updating secret s-test-opt-upd-55c3e5a4-23a3-11e9-9015-b2c9bac28373
STEP: Creating secret with name s-test-opt-create-55c3e5da-23a3-11e9-9015-b2c9bac28373
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:53:30.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-m4j6d" for this suite.
Jan 29 08:53:52.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:53:52.247: INFO: namespace: e2e-tests-secrets-m4j6d, resource: bindings, ignored listing per whitelist
Jan 29 08:53:52.395: INFO: namespace e2e-tests-secrets-m4j6d deletion completed in 22.186993515s

â€¢ [SLOW TEST:28.932 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:53:52.400: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-bvgs4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 08:53:52.604: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 29 08:53:52.640: INFO: Number of nodes with available pods: 0
Jan 29 08:53:52.640: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 29 08:53:52.658: INFO: Number of nodes with available pods: 0
Jan 29 08:53:52.658: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:53:53.661: INFO: Number of nodes with available pods: 0
Jan 29 08:53:53.661: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:53:54.661: INFO: Number of nodes with available pods: 0
Jan 29 08:53:54.661: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:53:55.662: INFO: Number of nodes with available pods: 1
Jan 29 08:53:55.662: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 29 08:53:55.679: INFO: Number of nodes with available pods: 1
Jan 29 08:53:55.679: INFO: Number of running nodes: 0, number of available pods: 1
Jan 29 08:53:56.682: INFO: Number of nodes with available pods: 0
Jan 29 08:53:56.682: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 29 08:53:56.746: INFO: Number of nodes with available pods: 0
Jan 29 08:53:56.746: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:53:57.749: INFO: Number of nodes with available pods: 0
Jan 29 08:53:57.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:53:58.750: INFO: Number of nodes with available pods: 0
Jan 29 08:53:58.750: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:53:59.749: INFO: Number of nodes with available pods: 0
Jan 29 08:53:59.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:00.753: INFO: Number of nodes with available pods: 0
Jan 29 08:54:00.753: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:01.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:01.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:02.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:02.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:03.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:03.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:04.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:04.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:05.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:05.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:06.750: INFO: Number of nodes with available pods: 0
Jan 29 08:54:06.750: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:07.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:07.750: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:08.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:08.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:09.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:09.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:10.750: INFO: Number of nodes with available pods: 0
Jan 29 08:54:10.750: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:11.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:11.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:12.750: INFO: Number of nodes with available pods: 0
Jan 29 08:54:12.750: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:13.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:13.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:14.750: INFO: Number of nodes with available pods: 0
Jan 29 08:54:14.750: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:15.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:15.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:16.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:16.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:17.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:17.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:18.750: INFO: Number of nodes with available pods: 0
Jan 29 08:54:18.750: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:19.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:19.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:20.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:20.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:21.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:21.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:22.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:22.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:23.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:23.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:24.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:24.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:25.750: INFO: Number of nodes with available pods: 0
Jan 29 08:54:25.750: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:26.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:26.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:27.751: INFO: Number of nodes with available pods: 0
Jan 29 08:54:27.751: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:28.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:28.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:29.749: INFO: Number of nodes with available pods: 0
Jan 29 08:54:29.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:30.750: INFO: Number of nodes with available pods: 0
Jan 29 08:54:30.750: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 08:54:31.749: INFO: Number of nodes with available pods: 1
Jan 29 08:54:31.749: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-bvgs4, will wait for the garbage collector to delete the pods
Jan 29 08:54:31.826: INFO: Deleting {extensions DaemonSet} daemon-set took: 6.595723ms
Jan 29 08:54:31.926: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.28962ms
Jan 29 08:55:17.031: INFO: Number of nodes with available pods: 0
Jan 29 08:55:17.031: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 08:55:17.038: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-bvgs4/daemonsets","resourceVersion":"678900"},"items":null}

Jan 29 08:55:17.040: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-bvgs4/pods","resourceVersion":"678900"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:55:17.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-bvgs4" for this suite.
Jan 29 08:55:23.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:55:23.189: INFO: namespace: e2e-tests-daemonsets-bvgs4, resource: bindings, ignored listing per whitelist
Jan 29 08:55:23.213: INFO: namespace e2e-tests-daemonsets-bvgs4 deletion completed in 6.138203215s

â€¢ [SLOW TEST:90.813 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:55:23.213: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-cmmwq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-9d1c7dc5-23a3-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 08:55:23.438: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d1d413c-23a3-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-cmmwq" to be "success or failure"
Jan 29 08:55:23.442: INFO: Pod "pod-projected-secrets-9d1d413c-23a3-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.781041ms
Jan 29 08:55:25.445: INFO: Pod "pod-projected-secrets-9d1d413c-23a3-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006679533s
STEP: Saw pod success
Jan 29 08:55:25.445: INFO: Pod "pod-projected-secrets-9d1d413c-23a3-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:55:25.447: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-secrets-9d1d413c-23a3-11e9-9015-b2c9bac28373 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 08:55:25.472: INFO: Waiting for pod pod-projected-secrets-9d1d413c-23a3-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:55:25.483: INFO: Pod pod-projected-secrets-9d1d413c-23a3-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:55:25.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cmmwq" for this suite.
Jan 29 08:55:31.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:55:31.536: INFO: namespace: e2e-tests-projected-cmmwq, resource: bindings, ignored listing per whitelist
Jan 29 08:55:31.603: INFO: namespace e2e-tests-projected-cmmwq deletion completed in 6.11315582s

â€¢ [SLOW TEST:8.390 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:55:31.606: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-lsgx2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-lsgx2
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-lsgx2
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-lsgx2
Jan 29 08:55:31.849: INFO: Found 0 stateful pods, waiting for 1
Jan 29 08:55:41.853: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 29 08:55:41.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-lsgx2 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 08:55:42.190: INFO: stderr: ""
Jan 29 08:55:42.190: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 08:55:42.190: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 29 08:55:42.193: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 29 08:55:52.197: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 08:55:52.197: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 08:55:52.241: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999093s
Jan 29 08:55:53.244: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995940487s
Jan 29 08:55:54.248: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992983064s
Jan 29 08:55:55.251: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988625672s
Jan 29 08:55:56.254: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985546825s
Jan 29 08:55:57.257: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982678029s
Jan 29 08:55:58.260: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.979649486s
Jan 29 08:55:59.263: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.97690582s
Jan 29 08:56:00.266: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.973534657s
Jan 29 08:56:01.269: INFO: Verifying statefulset ss doesn't scale past 1 for another 970.941824ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-lsgx2
Jan 29 08:56:02.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-lsgx2 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 08:56:02.580: INFO: stderr: ""
Jan 29 08:56:02.580: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 29 08:56:02.580: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 29 08:56:02.584: INFO: Found 1 stateful pods, waiting for 3
Jan 29 08:56:12.587: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 08:56:12.587: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 08:56:12.587: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 29 08:56:12.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-lsgx2 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 08:56:12.866: INFO: stderr: ""
Jan 29 08:56:12.866: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 08:56:12.866: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 29 08:56:12.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-lsgx2 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 08:56:13.178: INFO: stderr: ""
Jan 29 08:56:13.178: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 08:56:13.178: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 29 08:56:13.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-lsgx2 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 08:56:13.402: INFO: stderr: ""
Jan 29 08:56:13.402: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 08:56:13.402: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 29 08:56:13.402: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 08:56:13.406: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 29 08:56:23.412: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 08:56:23.412: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 08:56:23.412: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 08:56:23.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999996s
Jan 29 08:56:24.433: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996290868s
Jan 29 08:56:25.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992658761s
Jan 29 08:56:26.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988535256s
Jan 29 08:56:27.445: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98424737s
Jan 29 08:56:28.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980814595s
Jan 29 08:56:29.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97338652s
Jan 29 08:56:30.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96890858s
Jan 29 08:56:31.465: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965150183s
Jan 29 08:56:32.469: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.36678ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-lsgx2
Jan 29 08:56:33.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-lsgx2 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 08:56:33.735: INFO: stderr: ""
Jan 29 08:56:33.735: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 29 08:56:33.735: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 29 08:56:33.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-lsgx2 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 08:56:34.018: INFO: stderr: ""
Jan 29 08:56:34.018: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 29 08:56:34.018: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 29 08:56:34.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-lsgx2 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 08:56:34.240: INFO: stderr: ""
Jan 29 08:56:34.240: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 29 08:56:34.240: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 29 08:56:34.240: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 29 08:56:54.259: INFO: Deleting all statefulset in ns e2e-tests-statefulset-lsgx2
Jan 29 08:56:54.263: INFO: Scaling statefulset ss to 0
Jan 29 08:56:54.333: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 08:56:54.337: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:56:54.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-lsgx2" for this suite.
Jan 29 08:57:00.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:57:00.468: INFO: namespace: e2e-tests-statefulset-lsgx2, resource: bindings, ignored listing per whitelist
Jan 29 08:57:00.504: INFO: namespace e2e-tests-statefulset-lsgx2 deletion completed in 6.149487094s

â€¢ [SLOW TEST:88.899 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:57:00.505: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-25qhs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-d718473f-23a3-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 08:57:00.741: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d7190a8c-23a3-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-25qhs" to be "success or failure"
Jan 29 08:57:00.746: INFO: Pod "pod-projected-secrets-d7190a8c-23a3-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 4.520934ms
Jan 29 08:57:02.759: INFO: Pod "pod-projected-secrets-d7190a8c-23a3-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01820503s
STEP: Saw pod success
Jan 29 08:57:02.759: INFO: Pod "pod-projected-secrets-d7190a8c-23a3-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:57:02.762: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-secrets-d7190a8c-23a3-11e9-9015-b2c9bac28373 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 08:57:02.780: INFO: Waiting for pod pod-projected-secrets-d7190a8c-23a3-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:57:02.785: INFO: Pod pod-projected-secrets-d7190a8c-23a3-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:57:02.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-25qhs" for this suite.
Jan 29 08:57:08.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:57:08.845: INFO: namespace: e2e-tests-projected-25qhs, resource: bindings, ignored listing per whitelist
Jan 29 08:57:08.915: INFO: namespace e2e-tests-projected-25qhs deletion completed in 6.126321656s

â€¢ [SLOW TEST:8.410 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:57:08.915: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-v6vz9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-dc1d0f9a-23a3-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 08:57:09.139: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dc1d7565-23a3-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-v6vz9" to be "success or failure"
Jan 29 08:57:09.143: INFO: Pod "pod-projected-configmaps-dc1d7565-23a3-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.471604ms
Jan 29 08:57:11.156: INFO: Pod "pod-projected-configmaps-dc1d7565-23a3-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016814495s
STEP: Saw pod success
Jan 29 08:57:11.156: INFO: Pod "pod-projected-configmaps-dc1d7565-23a3-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:57:11.159: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-configmaps-dc1d7565-23a3-11e9-9015-b2c9bac28373 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 08:57:11.201: INFO: Waiting for pod pod-projected-configmaps-dc1d7565-23a3-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:57:11.205: INFO: Pod pod-projected-configmaps-dc1d7565-23a3-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:57:11.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v6vz9" for this suite.
Jan 29 08:57:17.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:57:17.342: INFO: namespace: e2e-tests-projected-v6vz9, resource: bindings, ignored listing per whitelist
Jan 29 08:57:17.416: INFO: namespace e2e-tests-projected-v6vz9 deletion completed in 6.204459365s

â€¢ [SLOW TEST:8.501 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:57:17.421: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-2z687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-tjs8t
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Jan 29 08:57:26.970: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-qlqmz
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:57:43.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-2z687" for this suite.
Jan 29 08:57:50.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:57:50.027: INFO: namespace: e2e-tests-namespaces-2z687, resource: bindings, ignored listing per whitelist
Jan 29 08:57:50.180: INFO: namespace e2e-tests-namespaces-2z687 deletion completed in 6.177682136s
STEP: Destroying namespace "e2e-tests-nsdeletetest-tjs8t" for this suite.
Jan 29 08:57:50.183: INFO: Namespace e2e-tests-nsdeletetest-tjs8t was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-qlqmz" for this suite.
Jan 29 08:57:56.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:57:56.267: INFO: namespace: e2e-tests-nsdeletetest-qlqmz, resource: bindings, ignored listing per whitelist
Jan 29 08:57:56.307: INFO: namespace e2e-tests-nsdeletetest-qlqmz deletion completed in 6.124155527s

â€¢ [SLOW TEST:38.886 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:57:56.307: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-znnzn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 29 08:57:59.076: INFO: Successfully updated pod "annotationupdatef85ccd06-23a3-11e9-9015-b2c9bac28373"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:58:01.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-znnzn" for this suite.
Jan 29 08:58:23.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:58:23.261: INFO: namespace: e2e-tests-downward-api-znnzn, resource: bindings, ignored listing per whitelist
Jan 29 08:58:23.317: INFO: namespace e2e-tests-downward-api-znnzn deletion completed in 22.211268755s

â€¢ [SLOW TEST:27.010 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:58:23.320: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6fdzr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jan 29 08:58:23.521: INFO: namespace e2e-tests-kubectl-6fdzr
Jan 29 08:58:23.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-6fdzr'
Jan 29 08:58:23.843: INFO: stderr: ""
Jan 29 08:58:23.843: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 29 08:58:24.847: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 08:58:24.847: INFO: Found 0 / 1
Jan 29 08:58:25.846: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 08:58:25.846: INFO: Found 1 / 1
Jan 29 08:58:25.846: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 29 08:58:25.850: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 08:58:25.850: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 08:58:25.850: INFO: wait on redis-master startup in e2e-tests-kubectl-6fdzr 
Jan 29 08:58:25.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 logs redis-master-v7b7v redis-master --namespace=e2e-tests-kubectl-6fdzr'
Jan 29 08:58:26.029: INFO: stderr: ""
Jan 29 08:58:26.030: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Jan 08:58:25.238 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Jan 08:58:25.238 # Server started, Redis version 3.2.12\n1:M 29 Jan 08:58:25.239 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Jan 08:58:25.239 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jan 29 08:58:26.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-6fdzr'
Jan 29 08:58:26.254: INFO: stderr: ""
Jan 29 08:58:26.254: INFO: stdout: "service/rm2 exposed\n"
Jan 29 08:58:26.258: INFO: Service rm2 in namespace e2e-tests-kubectl-6fdzr found.
STEP: exposing service
Jan 29 08:58:28.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-6fdzr'
Jan 29 08:58:28.427: INFO: stderr: ""
Jan 29 08:58:28.427: INFO: stdout: "service/rm3 exposed\n"
Jan 29 08:58:28.432: INFO: Service rm3 in namespace e2e-tests-kubectl-6fdzr found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:58:30.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6fdzr" for this suite.
Jan 29 08:58:52.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:58:52.508: INFO: namespace: e2e-tests-kubectl-6fdzr, resource: bindings, ignored listing per whitelist
Jan 29 08:58:52.546: INFO: namespace e2e-tests-kubectl-6fdzr deletion completed in 22.105607291s

â€¢ [SLOW TEST:29.227 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:58:52.546: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-q6k9d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 08:58:52.745: INFO: (0) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.114544ms)
Jan 29 08:58:52.750: INFO: (1) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.793357ms)
Jan 29 08:58:52.756: INFO: (2) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.681183ms)
Jan 29 08:58:52.761: INFO: (3) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.952064ms)
Jan 29 08:58:52.765: INFO: (4) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.309945ms)
Jan 29 08:58:52.770: INFO: (5) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.094409ms)
Jan 29 08:58:52.773: INFO: (6) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.287559ms)
Jan 29 08:58:52.778: INFO: (7) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.217458ms)
Jan 29 08:58:52.783: INFO: (8) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.201124ms)
Jan 29 08:58:52.788: INFO: (9) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.999315ms)
Jan 29 08:58:52.794: INFO: (10) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.852791ms)
Jan 29 08:58:52.798: INFO: (11) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.243647ms)
Jan 29 08:58:52.802: INFO: (12) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.8476ms)
Jan 29 08:58:52.808: INFO: (13) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.58711ms)
Jan 29 08:58:52.812: INFO: (14) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.072694ms)
Jan 29 08:58:52.816: INFO: (15) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.04916ms)
Jan 29 08:58:52.820: INFO: (16) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.49532ms)
Jan 29 08:58:52.824: INFO: (17) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.095492ms)
Jan 29 08:58:52.828: INFO: (18) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.991962ms)
Jan 29 08:58:52.833: INFO: (19) /api/v1/nodes/9.111.254.104:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.887287ms)
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:58:52.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-q6k9d" for this suite.
Jan 29 08:58:58.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:58:58.922: INFO: namespace: e2e-tests-proxy-q6k9d, resource: bindings, ignored listing per whitelist
Jan 29 08:58:58.947: INFO: namespace e2e-tests-proxy-q6k9d deletion completed in 6.109245414s

â€¢ [SLOW TEST:6.401 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:58:58.947: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-z2xmw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-1db41a0c-23a4-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 08:58:59.238: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1db4b08d-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-z2xmw" to be "success or failure"
Jan 29 08:58:59.244: INFO: Pod "pod-projected-configmaps-1db4b08d-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 6.417918ms
Jan 29 08:59:01.248: INFO: Pod "pod-projected-configmaps-1db4b08d-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009760402s
Jan 29 08:59:03.251: INFO: Pod "pod-projected-configmaps-1db4b08d-23a4-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013442604s
STEP: Saw pod success
Jan 29 08:59:03.251: INFO: Pod "pod-projected-configmaps-1db4b08d-23a4-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 08:59:03.255: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-configmaps-1db4b08d-23a4-11e9-9015-b2c9bac28373 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 08:59:03.280: INFO: Waiting for pod pod-projected-configmaps-1db4b08d-23a4-11e9-9015-b2c9bac28373 to disappear
Jan 29 08:59:03.284: INFO: Pod pod-projected-configmaps-1db4b08d-23a4-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:59:03.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z2xmw" for this suite.
Jan 29 08:59:09.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:59:09.348: INFO: namespace: e2e-tests-projected-z2xmw, resource: bindings, ignored listing per whitelist
Jan 29 08:59:09.403: INFO: namespace e2e-tests-projected-z2xmw deletion completed in 6.112815025s

â€¢ [SLOW TEST:10.456 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:59:09.404: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-sc9rm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Jan 29 08:59:10.130: INFO: created pod pod-service-account-defaultsa
Jan 29 08:59:10.130: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 29 08:59:10.157: INFO: created pod pod-service-account-mountsa
Jan 29 08:59:10.157: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 29 08:59:10.516: INFO: created pod pod-service-account-nomountsa
Jan 29 08:59:10.516: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 29 08:59:11.320: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 29 08:59:11.320: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 29 08:59:12.120: INFO: created pod pod-service-account-mountsa-mountspec
Jan 29 08:59:12.120: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 29 08:59:12.918: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 29 08:59:12.919: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 29 08:59:13.720: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 29 08:59:13.720: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 29 08:59:14.520: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 29 08:59:14.520: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 29 08:59:15.328: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 29 08:59:15.328: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:59:15.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-sc9rm" for this suite.
Jan 29 08:59:21.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:59:21.458: INFO: namespace: e2e-tests-svcaccounts-sc9rm, resource: bindings, ignored listing per whitelist
Jan 29 08:59:21.529: INFO: namespace e2e-tests-svcaccounts-sc9rm deletion completed in 6.178202852s

â€¢ [SLOW TEST:12.126 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:59:21.533: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-4dkqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0129 08:59:52.395067      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 08:59:52.395: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 08:59:52.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4dkqg" for this suite.
Jan 29 08:59:58.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 08:59:58.448: INFO: namespace: e2e-tests-gc-4dkqg, resource: bindings, ignored listing per whitelist
Jan 29 08:59:58.524: INFO: namespace e2e-tests-gc-4dkqg deletion completed in 6.124108827s

â€¢ [SLOW TEST:36.991 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 08:59:58.525: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-ml9zd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-ml9zd/configmap-test-41367daf-23a4-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 08:59:58.750: INFO: Waiting up to 5m0s for pod "pod-configmaps-41372ed8-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-configmap-ml9zd" to be "success or failure"
Jan 29 08:59:58.753: INFO: Pod "pod-configmaps-41372ed8-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16609ms
Jan 29 09:00:00.756: INFO: Pod "pod-configmaps-41372ed8-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006575907s
Jan 29 09:00:02.759: INFO: Pod "pod-configmaps-41372ed8-23a4-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00932856s
STEP: Saw pod success
Jan 29 09:00:02.759: INFO: Pod "pod-configmaps-41372ed8-23a4-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:00:02.762: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-41372ed8-23a4-11e9-9015-b2c9bac28373 container env-test: <nil>
STEP: delete the pod
Jan 29 09:00:02.785: INFO: Waiting for pod pod-configmaps-41372ed8-23a4-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:00:02.788: INFO: Pod pod-configmaps-41372ed8-23a4-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:00:02.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-ml9zd" for this suite.
Jan 29 09:00:08.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:00:08.838: INFO: namespace: e2e-tests-configmap-ml9zd, resource: bindings, ignored listing per whitelist
Jan 29 09:00:08.919: INFO: namespace e2e-tests-configmap-ml9zd deletion completed in 6.125562201s

â€¢ [SLOW TEST:10.395 seconds]
[sig-api-machinery] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:00:08.920: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-95jfx
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 29 09:00:09.135: INFO: Waiting up to 5m0s for pod "pod-47683ae1-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-95jfx" to be "success or failure"
Jan 29 09:00:09.140: INFO: Pod "pod-47683ae1-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 4.859034ms
Jan 29 09:00:11.143: INFO: Pod "pod-47683ae1-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007982704s
Jan 29 09:00:13.146: INFO: Pod "pod-47683ae1-23a4-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011329954s
STEP: Saw pod success
Jan 29 09:00:13.146: INFO: Pod "pod-47683ae1-23a4-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:00:13.150: INFO: Trying to get logs from node 9.111.254.123 pod pod-47683ae1-23a4-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:00:13.171: INFO: Waiting for pod pod-47683ae1-23a4-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:00:13.174: INFO: Pod pod-47683ae1-23a4-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:00:13.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-95jfx" for this suite.
Jan 29 09:00:19.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:00:19.249: INFO: namespace: e2e-tests-emptydir-95jfx, resource: bindings, ignored listing per whitelist
Jan 29 09:00:19.314: INFO: namespace e2e-tests-emptydir-95jfx deletion completed in 6.134193414s

â€¢ [SLOW TEST:10.393 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:00:19.314: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-x8cfk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 29 09:00:23.660: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:23.667: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:25.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:25.670: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:27.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:27.672: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:29.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:29.670: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:31.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:31.671: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:33.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:33.672: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:35.668: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:35.671: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:37.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:37.671: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:39.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:39.674: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:41.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:41.672: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:43.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:43.671: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:45.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:45.671: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:47.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:47.672: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:49.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:49.671: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:51.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:51.672: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:53.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:53.674: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 09:00:55.667: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 09:00:55.670: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:00:55.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-x8cfk" for this suite.
Jan 29 09:01:17.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:01:17.770: INFO: namespace: e2e-tests-container-lifecycle-hook-x8cfk, resource: bindings, ignored listing per whitelist
Jan 29 09:01:17.816: INFO: namespace e2e-tests-container-lifecycle-hook-x8cfk deletion completed in 22.133981617s

â€¢ [SLOW TEST:58.503 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:01:17.819: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2rk2r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 29 09:01:22.644: INFO: Successfully updated pod "labelsupdate7078dec2-23a4-11e9-9015-b2c9bac28373"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:01:24.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2rk2r" for this suite.
Jan 29 09:01:46.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:01:46.738: INFO: namespace: e2e-tests-downward-api-2rk2r, resource: bindings, ignored listing per whitelist
Jan 29 09:01:46.833: INFO: namespace e2e-tests-downward-api-2rk2r deletion completed in 22.167441039s

â€¢ [SLOW TEST:29.015 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:01:46.834: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-vwf95
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-81c515e6-23a4-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 09:01:47.144: INFO: Waiting up to 5m0s for pod "pod-configmaps-81c6337c-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-configmap-vwf95" to be "success or failure"
Jan 29 09:01:47.147: INFO: Pod "pod-configmaps-81c6337c-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.291469ms
Jan 29 09:01:49.154: INFO: Pod "pod-configmaps-81c6337c-23a4-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010099149s
STEP: Saw pod success
Jan 29 09:01:49.154: INFO: Pod "pod-configmaps-81c6337c-23a4-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:01:49.157: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-81c6337c-23a4-11e9-9015-b2c9bac28373 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 09:01:49.191: INFO: Waiting for pod pod-configmaps-81c6337c-23a4-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:01:49.194: INFO: Pod pod-configmaps-81c6337c-23a4-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:01:49.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vwf95" for this suite.
Jan 29 09:01:55.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:01:55.296: INFO: namespace: e2e-tests-configmap-vwf95, resource: bindings, ignored listing per whitelist
Jan 29 09:01:55.325: INFO: namespace e2e-tests-configmap-vwf95 deletion completed in 6.125327925s

â€¢ [SLOW TEST:8.491 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:01:55.326: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-6c6w6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 29 09:01:58.235: INFO: Successfully updated pod "pod-update-activedeadlineseconds-86d412e7-23a4-11e9-9015-b2c9bac28373"
Jan 29 09:01:58.235: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-86d412e7-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-pods-6c6w6" to be "terminated due to deadline exceeded"
Jan 29 09:01:58.238: INFO: Pod "pod-update-activedeadlineseconds-86d412e7-23a4-11e9-9015-b2c9bac28373": Phase="Running", Reason="", readiness=true. Elapsed: 2.653171ms
Jan 29 09:02:00.241: INFO: Pod "pod-update-activedeadlineseconds-86d412e7-23a4-11e9-9015-b2c9bac28373": Phase="Running", Reason="", readiness=true. Elapsed: 2.005664443s
Jan 29 09:02:02.244: INFO: Pod "pod-update-activedeadlineseconds-86d412e7-23a4-11e9-9015-b2c9bac28373": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00852736s
Jan 29 09:02:02.244: INFO: Pod "pod-update-activedeadlineseconds-86d412e7-23a4-11e9-9015-b2c9bac28373" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:02:02.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-6c6w6" for this suite.
Jan 29 09:02:08.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:02:08.365: INFO: namespace: e2e-tests-pods-6c6w6, resource: bindings, ignored listing per whitelist
Jan 29 09:02:08.408: INFO: namespace e2e-tests-pods-6c6w6 deletion completed in 6.15865874s

â€¢ [SLOW TEST:13.083 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:02:08.409: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ssm4l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8e9fea5e-23a4-11e9-9015-b2c9bac28373
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8e9fea5e-23a4-11e9-9015-b2c9bac28373
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:02:12.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ssm4l" for this suite.
Jan 29 09:02:34.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:02:34.738: INFO: namespace: e2e-tests-projected-ssm4l, resource: bindings, ignored listing per whitelist
Jan 29 09:02:34.787: INFO: namespace e2e-tests-projected-ssm4l deletion completed in 22.104643485s

â€¢ [SLOW TEST:26.378 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:02:34.788: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2mwp8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-9e5a5448-23a4-11e9-9015-b2c9bac28373
STEP: Creating configMap with name cm-test-opt-upd-9e5a54bf-23a4-11e9-9015-b2c9bac28373
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9e5a5448-23a4-11e9-9015-b2c9bac28373
STEP: Updating configmap cm-test-opt-upd-9e5a54bf-23a4-11e9-9015-b2c9bac28373
STEP: Creating configMap with name cm-test-opt-create-9e5a54ee-23a4-11e9-9015-b2c9bac28373
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:02:41.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2mwp8" for this suite.
Jan 29 09:03:03.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:03:03.656: INFO: namespace: e2e-tests-projected-2mwp8, resource: bindings, ignored listing per whitelist
Jan 29 09:03:03.675: INFO: namespace e2e-tests-projected-2mwp8 deletion completed in 22.161746331s

â€¢ [SLOW TEST:28.887 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:03:03.675: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-sggd5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-af8fbc33-23a4-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:03:03.947: INFO: Waiting up to 5m0s for pod "pod-secrets-af9046de-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-secrets-sggd5" to be "success or failure"
Jan 29 09:03:03.951: INFO: Pod "pod-secrets-af9046de-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.600591ms
Jan 29 09:03:05.954: INFO: Pod "pod-secrets-af9046de-23a4-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006176173s
STEP: Saw pod success
Jan 29 09:03:05.954: INFO: Pod "pod-secrets-af9046de-23a4-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:03:05.959: INFO: Trying to get logs from node 9.111.254.123 pod pod-secrets-af9046de-23a4-11e9-9015-b2c9bac28373 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 09:03:05.984: INFO: Waiting for pod pod-secrets-af9046de-23a4-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:03:05.988: INFO: Pod pod-secrets-af9046de-23a4-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:03:05.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-sggd5" for this suite.
Jan 29 09:03:12.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:03:12.074: INFO: namespace: e2e-tests-secrets-sggd5, resource: bindings, ignored listing per whitelist
Jan 29 09:03:12.109: INFO: namespace e2e-tests-secrets-sggd5 deletion completed in 6.116481966s

â€¢ [SLOW TEST:8.434 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:03:12.111: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-qnprx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 29 09:03:12.312: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-a,UID:b499a28e-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680910,Generation:0,CreationTimestamp:2019-01-29 09:03:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 09:03:12.313: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-a,UID:b499a28e-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680910,Generation:0,CreationTimestamp:2019-01-29 09:03:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 29 09:03:22.325: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-a,UID:b499a28e-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680926,Generation:0,CreationTimestamp:2019-01-29 09:03:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 29 09:03:22.325: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-a,UID:b499a28e-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680926,Generation:0,CreationTimestamp:2019-01-29 09:03:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 29 09:03:32.334: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-a,UID:b499a28e-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680944,Generation:0,CreationTimestamp:2019-01-29 09:03:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 09:03:32.334: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-a,UID:b499a28e-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680944,Generation:0,CreationTimestamp:2019-01-29 09:03:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 29 09:03:42.342: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-a,UID:b499a28e-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680961,Generation:0,CreationTimestamp:2019-01-29 09:03:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 09:03:42.342: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-a,UID:b499a28e-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680961,Generation:0,CreationTimestamp:2019-01-29 09:03:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 29 09:03:52.354: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-b,UID:cc76b107-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680978,Generation:0,CreationTimestamp:2019-01-29 09:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 09:03:52.354: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-b,UID:cc76b107-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680978,Generation:0,CreationTimestamp:2019-01-29 09:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 29 09:04:02.360: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-b,UID:cc76b107-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680996,Generation:0,CreationTimestamp:2019-01-29 09:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 09:04:02.360: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-qnprx,SelfLink:/api/v1/namespaces/e2e-tests-watch-qnprx/configmaps/e2e-watch-test-configmap-b,UID:cc76b107-23a4-11e9-a926-eeeeeeeeeeee,ResourceVersion:680996,Generation:0,CreationTimestamp:2019-01-29 09:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:04:12.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-qnprx" for this suite.
Jan 29 09:04:18.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:04:18.405: INFO: namespace: e2e-tests-watch-qnprx, resource: bindings, ignored listing per whitelist
Jan 29 09:04:18.478: INFO: namespace e2e-tests-watch-qnprx deletion completed in 6.112410755s

â€¢ [SLOW TEST:66.368 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:04:18.478: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-f5qlv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Jan 29 09:04:22.755: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-dc279e8a-23a4-11e9-9015-b2c9bac28373", GenerateName:"", Namespace:"e2e-tests-pods-f5qlv", SelfLink:"/api/v1/namespaces/e2e-tests-pods-f5qlv/pods/pod-submit-remove-dc279e8a-23a4-11e9-9015-b2c9bac28373", UID:"dc307dcb-23a4-11e9-a926-eeeeeeeeeeee", ResourceVersion:"681049", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63684349458, loc:(*time.Location)(0x6c43b60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"670357036"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-lztvx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc421f7f840), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lztvx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc420d578b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"9.111.254.123", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc42200b1a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc420d57900)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc420d57920)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc420d57928), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684349458, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684349461, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684349461, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684349458, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"9.111.254.123", PodIP:"10.1.211.139", StartTime:(*v1.Time)(0xc420d77780), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc420d777a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96", ContainerID:"docker://923fb6a68d81fb91adb19de4c31547797e72f458234638e8c7e47eb6ffa39738"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:04:34.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-f5qlv" for this suite.
Jan 29 09:04:40.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:04:40.383: INFO: namespace: e2e-tests-pods-f5qlv, resource: bindings, ignored listing per whitelist
Jan 29 09:04:40.444: INFO: namespace e2e-tests-pods-f5qlv deletion completed in 6.139389536s

â€¢ [SLOW TEST:21.966 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:04:40.446: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-c7tcn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-e93fc209-23a4-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 09:04:40.744: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e940d06a-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-c7tcn" to be "success or failure"
Jan 29 09:04:40.747: INFO: Pod "pod-projected-configmaps-e940d06a-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.342688ms
Jan 29 09:04:42.749: INFO: Pod "pod-projected-configmaps-e940d06a-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00494425s
Jan 29 09:04:44.752: INFO: Pod "pod-projected-configmaps-e940d06a-23a4-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007383042s
STEP: Saw pod success
Jan 29 09:04:44.752: INFO: Pod "pod-projected-configmaps-e940d06a-23a4-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:04:44.754: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-configmaps-e940d06a-23a4-11e9-9015-b2c9bac28373 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 09:04:44.772: INFO: Waiting for pod pod-projected-configmaps-e940d06a-23a4-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:04:44.774: INFO: Pod pod-projected-configmaps-e940d06a-23a4-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:04:44.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c7tcn" for this suite.
Jan 29 09:04:50.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:04:50.855: INFO: namespace: e2e-tests-projected-c7tcn, resource: bindings, ignored listing per whitelist
Jan 29 09:04:50.910: INFO: namespace e2e-tests-projected-c7tcn deletion completed in 6.128827508s

â€¢ [SLOW TEST:10.464 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:04:50.910: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-bg8gf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:04:51.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef7cb578-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-bg8gf" to be "success or failure"
Jan 29 09:04:51.131: INFO: Pod "downwardapi-volume-ef7cb578-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 5.505611ms
Jan 29 09:04:53.134: INFO: Pod "downwardapi-volume-ef7cb578-23a4-11e9-9015-b2c9bac28373": Phase="Running", Reason="", readiness=true. Elapsed: 2.008932221s
Jan 29 09:04:55.137: INFO: Pod "downwardapi-volume-ef7cb578-23a4-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012019717s
STEP: Saw pod success
Jan 29 09:04:55.138: INFO: Pod "downwardapi-volume-ef7cb578-23a4-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:04:55.141: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-ef7cb578-23a4-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:04:55.163: INFO: Waiting for pod downwardapi-volume-ef7cb578-23a4-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:04:55.169: INFO: Pod downwardapi-volume-ef7cb578-23a4-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:04:55.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bg8gf" for this suite.
Jan 29 09:05:01.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:05:01.285: INFO: namespace: e2e-tests-downward-api-bg8gf, resource: bindings, ignored listing per whitelist
Jan 29 09:05:01.309: INFO: namespace e2e-tests-downward-api-bg8gf deletion completed in 6.136755074s

â€¢ [SLOW TEST:10.399 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:05:01.310: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-j7n8p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 29 09:05:01.535: INFO: Waiting up to 5m0s for pod "pod-f5ae0980-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-j7n8p" to be "success or failure"
Jan 29 09:05:01.549: INFO: Pod "pod-f5ae0980-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 13.631481ms
Jan 29 09:05:03.555: INFO: Pod "pod-f5ae0980-23a4-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02011687s
STEP: Saw pod success
Jan 29 09:05:03.556: INFO: Pod "pod-f5ae0980-23a4-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:05:03.559: INFO: Trying to get logs from node 9.111.254.123 pod pod-f5ae0980-23a4-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:05:03.580: INFO: Waiting for pod pod-f5ae0980-23a4-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:05:03.583: INFO: Pod pod-f5ae0980-23a4-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:05:03.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-j7n8p" for this suite.
Jan 29 09:05:09.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:05:09.689: INFO: namespace: e2e-tests-emptydir-j7n8p, resource: bindings, ignored listing per whitelist
Jan 29 09:05:09.699: INFO: namespace e2e-tests-emptydir-j7n8p deletion completed in 6.108535703s

â€¢ [SLOW TEST:8.389 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:05:09.700: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rrt2g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:05:09.942: INFO: Waiting up to 5m0s for pod "downwardapi-volume-faae4791-23a4-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-rrt2g" to be "success or failure"
Jan 29 09:05:09.949: INFO: Pod "downwardapi-volume-faae4791-23a4-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 7.278726ms
Jan 29 09:05:11.952: INFO: Pod "downwardapi-volume-faae4791-23a4-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010748236s
STEP: Saw pod success
Jan 29 09:05:11.952: INFO: Pod "downwardapi-volume-faae4791-23a4-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:05:11.954: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-faae4791-23a4-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:05:11.975: INFO: Waiting for pod downwardapi-volume-faae4791-23a4-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:05:11.980: INFO: Pod downwardapi-volume-faae4791-23a4-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:05:11.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rrt2g" for this suite.
Jan 29 09:05:18.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:05:18.112: INFO: namespace: e2e-tests-projected-rrt2g, resource: bindings, ignored listing per whitelist
Jan 29 09:05:18.114: INFO: namespace e2e-tests-projected-rrt2g deletion completed in 6.127301756s

â€¢ [SLOW TEST:8.414 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:05:18.114: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-rc29d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 29 09:05:20.939: INFO: Successfully updated pod "pod-update-ffb46702-23a4-11e9-9015-b2c9bac28373"
STEP: verifying the updated pod is in kubernetes
Jan 29 09:05:20.944: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:05:20.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-rc29d" for this suite.
Jan 29 09:05:42.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:05:43.061: INFO: namespace: e2e-tests-pods-rc29d, resource: bindings, ignored listing per whitelist
Jan 29 09:05:43.115: INFO: namespace e2e-tests-pods-rc29d deletion completed in 22.15863732s

â€¢ [SLOW TEST:25.001 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:05:43.115: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-8p9fn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Jan 29 09:05:43.333: INFO: Waiting up to 5m0s for pod "var-expansion-0e98b8df-23a5-11e9-9015-b2c9bac28373" in namespace "e2e-tests-var-expansion-8p9fn" to be "success or failure"
Jan 29 09:05:43.337: INFO: Pod "var-expansion-0e98b8df-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.803667ms
Jan 29 09:05:45.341: INFO: Pod "var-expansion-0e98b8df-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007261309s
Jan 29 09:05:47.344: INFO: Pod "var-expansion-0e98b8df-23a5-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010407078s
STEP: Saw pod success
Jan 29 09:05:47.344: INFO: Pod "var-expansion-0e98b8df-23a5-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:05:47.348: INFO: Trying to get logs from node 9.111.254.123 pod var-expansion-0e98b8df-23a5-11e9-9015-b2c9bac28373 container dapi-container: <nil>
STEP: delete the pod
Jan 29 09:05:47.369: INFO: Waiting for pod var-expansion-0e98b8df-23a5-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:05:47.372: INFO: Pod var-expansion-0e98b8df-23a5-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:05:47.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-8p9fn" for this suite.
Jan 29 09:05:53.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:05:53.493: INFO: namespace: e2e-tests-var-expansion-8p9fn, resource: bindings, ignored listing per whitelist
Jan 29 09:05:53.507: INFO: namespace e2e-tests-var-expansion-8p9fn deletion completed in 6.130263443s

â€¢ [SLOW TEST:10.392 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:05:53.507: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ks8m8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Jan 29 09:05:53.697: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-900852259 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:05:53.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ks8m8" for this suite.
Jan 29 09:05:59.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:05:59.961: INFO: namespace: e2e-tests-kubectl-ks8m8, resource: bindings, ignored listing per whitelist
Jan 29 09:06:00.018: INFO: namespace e2e-tests-kubectl-ks8m8 deletion completed in 6.198103226s

â€¢ [SLOW TEST:6.511 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:06:00.018: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-rn4zq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-18b157c6-23a5-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:06:00.338: INFO: Waiting up to 5m0s for pod "pod-secrets-18b25549-23a5-11e9-9015-b2c9bac28373" in namespace "e2e-tests-secrets-rn4zq" to be "success or failure"
Jan 29 09:06:00.342: INFO: Pod "pod-secrets-18b25549-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008857ms
Jan 29 09:06:02.346: INFO: Pod "pod-secrets-18b25549-23a5-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007343261s
STEP: Saw pod success
Jan 29 09:06:02.346: INFO: Pod "pod-secrets-18b25549-23a5-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:06:02.348: INFO: Trying to get logs from node 9.111.254.123 pod pod-secrets-18b25549-23a5-11e9-9015-b2c9bac28373 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 09:06:02.370: INFO: Waiting for pod pod-secrets-18b25549-23a5-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:06:02.372: INFO: Pod pod-secrets-18b25549-23a5-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:06:02.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rn4zq" for this suite.
Jan 29 09:06:08.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:06:08.406: INFO: namespace: e2e-tests-secrets-rn4zq, resource: bindings, ignored listing per whitelist
Jan 29 09:06:08.481: INFO: namespace e2e-tests-secrets-rn4zq deletion completed in 6.104047449s

â€¢ [SLOW TEST:8.463 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:06:08.484: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-4j5nm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0129 09:06:18.870157      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 09:06:18.870: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:06:18.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4j5nm" for this suite.
Jan 29 09:06:24.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:06:24.933: INFO: namespace: e2e-tests-gc-4j5nm, resource: bindings, ignored listing per whitelist
Jan 29 09:06:24.991: INFO: namespace e2e-tests-gc-4j5nm deletion completed in 6.11240052s

â€¢ [SLOW TEST:16.507 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:06:24.991: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-56ns7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 29 09:06:25.242: INFO: Waiting up to 5m0s for pod "pod-278f6a2a-23a5-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-56ns7" to be "success or failure"
Jan 29 09:06:25.244: INFO: Pod "pod-278f6a2a-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.487919ms
Jan 29 09:06:27.248: INFO: Pod "pod-278f6a2a-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006378978s
Jan 29 09:06:29.251: INFO: Pod "pod-278f6a2a-23a5-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009424995s
STEP: Saw pod success
Jan 29 09:06:29.251: INFO: Pod "pod-278f6a2a-23a5-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:06:29.257: INFO: Trying to get logs from node 9.111.254.123 pod pod-278f6a2a-23a5-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:06:29.287: INFO: Waiting for pod pod-278f6a2a-23a5-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:06:29.292: INFO: Pod pod-278f6a2a-23a5-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:06:29.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-56ns7" for this suite.
Jan 29 09:06:35.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:06:35.375: INFO: namespace: e2e-tests-emptydir-56ns7, resource: bindings, ignored listing per whitelist
Jan 29 09:06:35.425: INFO: namespace e2e-tests-emptydir-56ns7 deletion completed in 6.122247807s

â€¢ [SLOW TEST:10.434 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:06:35.425: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2cddc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
Jan 29 09:06:35.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-2cddc'
Jan 29 09:06:36.026: INFO: stderr: ""
Jan 29 09:06:36.026: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Jan 29 09:06:37.029: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:06:37.029: INFO: Found 0 / 1
Jan 29 09:06:38.030: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:06:38.030: INFO: Found 1 / 1
Jan 29 09:06:38.030: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 29 09:06:38.033: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:06:38.033: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jan 29 09:06:38.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 logs redis-master-x84tl redis-master --namespace=e2e-tests-kubectl-2cddc'
Jan 29 09:06:38.189: INFO: stderr: ""
Jan 29 09:06:38.189: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Jan 09:06:37.444 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Jan 09:06:37.444 # Server started, Redis version 3.2.12\n1:M 29 Jan 09:06:37.444 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Jan 09:06:37.445 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jan 29 09:06:38.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 log redis-master-x84tl redis-master --namespace=e2e-tests-kubectl-2cddc --tail=1'
Jan 29 09:06:38.334: INFO: stderr: ""
Jan 29 09:06:38.334: INFO: stdout: "1:M 29 Jan 09:06:37.445 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jan 29 09:06:38.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 log redis-master-x84tl redis-master --namespace=e2e-tests-kubectl-2cddc --limit-bytes=1'
Jan 29 09:06:38.468: INFO: stderr: ""
Jan 29 09:06:38.468: INFO: stdout: " "
STEP: exposing timestamps
Jan 29 09:06:38.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 log redis-master-x84tl redis-master --namespace=e2e-tests-kubectl-2cddc --tail=1 --timestamps'
Jan 29 09:06:38.604: INFO: stderr: ""
Jan 29 09:06:38.605: INFO: stdout: "2019-01-29T09:06:37.445226906Z 1:M 29 Jan 09:06:37.445 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jan 29 09:06:41.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 log redis-master-x84tl redis-master --namespace=e2e-tests-kubectl-2cddc --since=1s'
Jan 29 09:06:41.254: INFO: stderr: ""
Jan 29 09:06:41.254: INFO: stdout: ""
Jan 29 09:06:41.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 log redis-master-x84tl redis-master --namespace=e2e-tests-kubectl-2cddc --since=24h'
Jan 29 09:06:41.391: INFO: stderr: ""
Jan 29 09:06:41.391: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Jan 09:06:37.444 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Jan 09:06:37.444 # Server started, Redis version 3.2.12\n1:M 29 Jan 09:06:37.444 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Jan 09:06:37.445 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
Jan 29 09:06:41.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-2cddc'
Jan 29 09:06:41.521: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:06:41.521: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jan 29 09:06:41.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-2cddc'
Jan 29 09:06:41.668: INFO: stderr: "No resources found.\n"
Jan 29 09:06:41.669: INFO: stdout: ""
Jan 29 09:06:41.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -l name=nginx --namespace=e2e-tests-kubectl-2cddc -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 09:06:41.814: INFO: stderr: ""
Jan 29 09:06:41.814: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:06:41.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2cddc" for this suite.
Jan 29 09:06:47.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:06:47.846: INFO: namespace: e2e-tests-kubectl-2cddc, resource: bindings, ignored listing per whitelist
Jan 29 09:06:47.973: INFO: namespace e2e-tests-kubectl-2cddc deletion completed in 6.154317089s

â€¢ [SLOW TEST:12.548 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:06:47.974: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-fkz4j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-fkz4j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fkz4j to expose endpoints map[]
Jan 29 09:06:48.181: INFO: Get endpoints failed (3.536581ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jan 29 09:06:49.184: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fkz4j exposes endpoints map[] (1.006498463s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-fkz4j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fkz4j to expose endpoints map[pod1:[100]]
Jan 29 09:06:51.264: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fkz4j exposes endpoints map[pod1:[100]] (2.023152826s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-fkz4j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fkz4j to expose endpoints map[pod1:[100] pod2:[101]]
Jan 29 09:06:53.357: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fkz4j exposes endpoints map[pod1:[100] pod2:[101]] (2.020106806s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-fkz4j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fkz4j to expose endpoints map[pod2:[101]]
Jan 29 09:06:54.374: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fkz4j exposes endpoints map[pod2:[101]] (1.012502562s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-fkz4j
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fkz4j to expose endpoints map[]
Jan 29 09:06:55.385: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fkz4j exposes endpoints map[] (1.005307681s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:06:55.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-fkz4j" for this suite.
Jan 29 09:07:17.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:07:17.438: INFO: namespace: e2e-tests-services-fkz4j, resource: bindings, ignored listing per whitelist
Jan 29 09:07:17.535: INFO: namespace e2e-tests-services-fkz4j deletion completed in 22.127434131s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

â€¢ [SLOW TEST:29.561 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:07:17.537: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-5jctg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:07:17.736: INFO: (0) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.430218ms)
Jan 29 09:07:17.741: INFO: (1) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.424334ms)
Jan 29 09:07:17.747: INFO: (2) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.559007ms)
Jan 29 09:07:17.752: INFO: (3) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.469755ms)
Jan 29 09:07:17.758: INFO: (4) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.64704ms)
Jan 29 09:07:17.763: INFO: (5) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.571843ms)
Jan 29 09:07:17.767: INFO: (6) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.528406ms)
Jan 29 09:07:17.770: INFO: (7) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.638499ms)
Jan 29 09:07:17.775: INFO: (8) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.888417ms)
Jan 29 09:07:17.780: INFO: (9) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.366043ms)
Jan 29 09:07:17.784: INFO: (10) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.252693ms)
Jan 29 09:07:17.788: INFO: (11) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.716133ms)
Jan 29 09:07:17.791: INFO: (12) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.525079ms)
Jan 29 09:07:17.796: INFO: (13) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.753647ms)
Jan 29 09:07:17.800: INFO: (14) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.939249ms)
Jan 29 09:07:17.804: INFO: (15) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.911472ms)
Jan 29 09:07:17.808: INFO: (16) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.169123ms)
Jan 29 09:07:17.813: INFO: (17) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.624201ms)
Jan 29 09:07:17.816: INFO: (18) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.386325ms)
Jan 29 09:07:17.820: INFO: (19) /api/v1/nodes/9.111.254.104/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.979281ms)
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:07:17.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-5jctg" for this suite.
Jan 29 09:07:23.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:07:23.914: INFO: namespace: e2e-tests-proxy-5jctg, resource: bindings, ignored listing per whitelist
Jan 29 09:07:23.933: INFO: namespace e2e-tests-proxy-5jctg deletion completed in 6.1078415s

â€¢ [SLOW TEST:6.397 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:07:23.934: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rjrs7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:07:24.139: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ab047ab-23a5-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-rjrs7" to be "success or failure"
Jan 29 09:07:24.145: INFO: Pod "downwardapi-volume-4ab047ab-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 6.305519ms
Jan 29 09:07:26.149: INFO: Pod "downwardapi-volume-4ab047ab-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009472034s
Jan 29 09:07:28.152: INFO: Pod "downwardapi-volume-4ab047ab-23a5-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012496055s
STEP: Saw pod success
Jan 29 09:07:28.152: INFO: Pod "downwardapi-volume-4ab047ab-23a5-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:07:28.154: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-4ab047ab-23a5-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:07:28.169: INFO: Waiting for pod downwardapi-volume-4ab047ab-23a5-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:07:28.171: INFO: Pod downwardapi-volume-4ab047ab-23a5-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:07:28.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rjrs7" for this suite.
Jan 29 09:07:34.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:07:34.241: INFO: namespace: e2e-tests-downward-api-rjrs7, resource: bindings, ignored listing per whitelist
Jan 29 09:07:34.294: INFO: namespace e2e-tests-downward-api-rjrs7 deletion completed in 6.114873642s

â€¢ [SLOW TEST:10.360 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:07:34.294: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rnnwc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 29 09:07:34.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-rnnwc'
Jan 29 09:07:34.761: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Jan 29 09:07:34.761: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
Jan 29 09:07:36.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-rnnwc'
Jan 29 09:07:36.927: INFO: stderr: ""
Jan 29 09:07:36.927: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:07:36.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rnnwc" for this suite.
Jan 29 09:07:42.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:07:43.083: INFO: namespace: e2e-tests-kubectl-rnnwc, resource: bindings, ignored listing per whitelist
Jan 29 09:07:43.101: INFO: namespace e2e-tests-kubectl-rnnwc deletion completed in 6.168796819s

â€¢ [SLOW TEST:8.807 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:07:43.101: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-lgcrr
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:07:43.283: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:07:44.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-lgcrr" for this suite.
Jan 29 09:07:50.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:07:50.436: INFO: namespace: e2e-tests-custom-resource-definition-lgcrr, resource: bindings, ignored listing per whitelist
Jan 29 09:07:50.473: INFO: namespace e2e-tests-custom-resource-definition-lgcrr deletion completed in 6.13914842s

â€¢ [SLOW TEST:7.372 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:07:50.474: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-4r6ph
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:07:51.187: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5a946ef9-23a5-11e9-a926-eeeeeeeeeeee", Controller:(*bool)(0xc4224792a6), BlockOwnerDeletion:(*bool)(0xc4224792a7)}}
Jan 29 09:07:51.241: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5a8df401-23a5-11e9-a926-eeeeeeeeeeee", Controller:(*bool)(0xc42200eb22), BlockOwnerDeletion:(*bool)(0xc42200eb23)}}
Jan 29 09:07:51.271: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5a90c130-23a5-11e9-a926-eeeeeeeeeeee", Controller:(*bool)(0xc4224795ca), BlockOwnerDeletion:(*bool)(0xc4224795cb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:07:56.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4r6ph" for this suite.
Jan 29 09:08:02.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:08:02.410: INFO: namespace: e2e-tests-gc-4r6ph, resource: bindings, ignored listing per whitelist
Jan 29 09:08:02.429: INFO: namespace e2e-tests-gc-4r6ph deletion completed in 6.136799724s

â€¢ [SLOW TEST:11.955 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:08:02.429: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-5qt7c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-5qt7c
Jan 29 09:08:04.743: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-5qt7c
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 09:08:04.748: INFO: Initial restart count of pod liveness-http is 0
Jan 29 09:08:26.787: INFO: Restart count of pod e2e-tests-container-probe-5qt7c/liveness-http is now 1 (22.039278112s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:08:26.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-5qt7c" for this suite.
Jan 29 09:08:32.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:08:32.943: INFO: namespace: e2e-tests-container-probe-5qt7c, resource: bindings, ignored listing per whitelist
Jan 29 09:08:32.946: INFO: namespace e2e-tests-container-probe-5qt7c deletion completed in 6.135286019s

â€¢ [SLOW TEST:30.518 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:08:32.946: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-qh49d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-73d5d642-23a5-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 09:08:33.243: INFO: Waiting up to 5m0s for pod "pod-configmaps-73d69cae-23a5-11e9-9015-b2c9bac28373" in namespace "e2e-tests-configmap-qh49d" to be "success or failure"
Jan 29 09:08:33.246: INFO: Pod "pod-configmaps-73d69cae-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638648ms
Jan 29 09:08:35.249: INFO: Pod "pod-configmaps-73d69cae-23a5-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00565378s
STEP: Saw pod success
Jan 29 09:08:35.249: INFO: Pod "pod-configmaps-73d69cae-23a5-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:08:35.251: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-73d69cae-23a5-11e9-9015-b2c9bac28373 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 09:08:35.276: INFO: Waiting for pod pod-configmaps-73d69cae-23a5-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:08:35.279: INFO: Pod pod-configmaps-73d69cae-23a5-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:08:35.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-qh49d" for this suite.
Jan 29 09:08:41.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:08:41.313: INFO: namespace: e2e-tests-configmap-qh49d, resource: bindings, ignored listing per whitelist
Jan 29 09:08:41.405: INFO: namespace e2e-tests-configmap-qh49d deletion completed in 6.12174257s

â€¢ [SLOW TEST:8.459 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:08:41.405: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-mllsv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 29 09:08:45.756: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 09:08:45.759: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 09:08:47.760: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 09:08:47.765: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 09:08:49.760: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 09:08:49.763: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 09:08:51.760: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 09:08:51.763: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 09:08:53.760: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 09:08:53.765: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 09:08:55.761: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 09:08:55.767: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:08:55.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-mllsv" for this suite.
Jan 29 09:09:17.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:09:17.876: INFO: namespace: e2e-tests-container-lifecycle-hook-mllsv, resource: bindings, ignored listing per whitelist
Jan 29 09:09:17.886: INFO: namespace e2e-tests-container-lifecycle-hook-mllsv deletion completed in 22.110468353s

â€¢ [SLOW TEST:36.480 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:09:17.886: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-v9fcm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-8e9e2fb0-23a5-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:09:18.136: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e9ee147-23a5-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-v9fcm" to be "success or failure"
Jan 29 09:09:18.141: INFO: Pod "pod-projected-secrets-8e9ee147-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92294ms
Jan 29 09:09:20.144: INFO: Pod "pod-projected-secrets-8e9ee147-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007629855s
Jan 29 09:09:22.147: INFO: Pod "pod-projected-secrets-8e9ee147-23a5-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010565802s
STEP: Saw pod success
Jan 29 09:09:22.147: INFO: Pod "pod-projected-secrets-8e9ee147-23a5-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:09:22.149: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-secrets-8e9ee147-23a5-11e9-9015-b2c9bac28373 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 09:09:22.169: INFO: Waiting for pod pod-projected-secrets-8e9ee147-23a5-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:09:22.172: INFO: Pod pod-projected-secrets-8e9ee147-23a5-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:09:22.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v9fcm" for this suite.
Jan 29 09:09:28.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:09:28.249: INFO: namespace: e2e-tests-projected-v9fcm, resource: bindings, ignored listing per whitelist
Jan 29 09:09:28.321: INFO: namespace e2e-tests-projected-v9fcm deletion completed in 6.141725434s

â€¢ [SLOW TEST:10.435 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:09:28.321: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tltbg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 29 09:09:28.645: INFO: Waiting up to 5m0s for pod "downward-api-94d83078-23a5-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-tltbg" to be "success or failure"
Jan 29 09:09:28.648: INFO: Pod "downward-api-94d83078-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.573483ms
Jan 29 09:09:30.651: INFO: Pod "downward-api-94d83078-23a5-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006436838s
Jan 29 09:09:32.655: INFO: Pod "downward-api-94d83078-23a5-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009857471s
STEP: Saw pod success
Jan 29 09:09:32.655: INFO: Pod "downward-api-94d83078-23a5-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:09:32.658: INFO: Trying to get logs from node 9.111.254.123 pod downward-api-94d83078-23a5-11e9-9015-b2c9bac28373 container dapi-container: <nil>
STEP: delete the pod
Jan 29 09:09:32.689: INFO: Waiting for pod downward-api-94d83078-23a5-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:09:32.692: INFO: Pod downward-api-94d83078-23a5-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:09:32.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tltbg" for this suite.
Jan 29 09:09:38.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:09:38.886: INFO: namespace: e2e-tests-downward-api-tltbg, resource: bindings, ignored listing per whitelist
Jan 29 09:09:38.924: INFO: namespace e2e-tests-downward-api-tltbg deletion completed in 6.228256284s

â€¢ [SLOW TEST:10.603 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:09:38.926: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ktjrj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
Jan 29 09:09:39.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-ktjrj'
Jan 29 09:09:39.647: INFO: stderr: ""
Jan 29 09:09:39.647: INFO: stdout: "pod/pause created\n"
Jan 29 09:09:39.647: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 29 09:09:39.647: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-ktjrj" to be "running and ready"
Jan 29 09:09:39.652: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.647844ms
Jan 29 09:09:41.655: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007574874s
Jan 29 09:09:41.655: INFO: Pod "pause" satisfied condition "running and ready"
Jan 29 09:09:41.655: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 29 09:09:41.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-ktjrj'
Jan 29 09:09:41.854: INFO: stderr: ""
Jan 29 09:09:41.854: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 29 09:09:41.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pod pause -L testing-label --namespace=e2e-tests-kubectl-ktjrj'
Jan 29 09:09:41.993: INFO: stderr: ""
Jan 29 09:09:41.993: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 29 09:09:41.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 label pods pause testing-label- --namespace=e2e-tests-kubectl-ktjrj'
Jan 29 09:09:42.247: INFO: stderr: ""
Jan 29 09:09:42.247: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 29 09:09:42.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pod pause -L testing-label --namespace=e2e-tests-kubectl-ktjrj'
Jan 29 09:09:42.374: INFO: stderr: ""
Jan 29 09:09:42.374: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
Jan 29 09:09:42.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-ktjrj'
Jan 29 09:09:42.518: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:09:42.518: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 29 09:09:42.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-ktjrj'
Jan 29 09:09:42.694: INFO: stderr: "No resources found.\n"
Jan 29 09:09:42.694: INFO: stdout: ""
Jan 29 09:09:42.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -l name=pause --namespace=e2e-tests-kubectl-ktjrj -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 09:09:42.833: INFO: stderr: ""
Jan 29 09:09:42.833: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:09:42.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ktjrj" for this suite.
Jan 29 09:09:48.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:09:48.952: INFO: namespace: e2e-tests-kubectl-ktjrj, resource: bindings, ignored listing per whitelist
Jan 29 09:09:48.983: INFO: namespace e2e-tests-kubectl-ktjrj deletion completed in 6.145898734s

â€¢ [SLOW TEST:10.058 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:09:48.984: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-dvwj8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 29 09:09:49.176: INFO: PodSpec: initContainers in spec.initContainers
Jan 29 09:10:32.382: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a126d8b9-23a5-11e9-9015-b2c9bac28373", GenerateName:"", Namespace:"e2e-tests-init-container-dvwj8", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-dvwj8/pods/pod-init-a126d8b9-23a5-11e9-9015-b2c9bac28373", UID:"a1288867-23a5-11e9-a926-eeeeeeeeeeee", ResourceVersion:"682843", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63684349789, loc:(*time.Location)(0x6c43b60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"176447484"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7qgwx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc4224da540), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7qgwx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7qgwx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7qgwx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc4228f36a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"9.111.254.123", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc4222e2d20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc4228f3740)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc4228f3760)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc4228f3768), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684349789, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684349789, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684349789, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684349789, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"9.111.254.123", PodIP:"10.1.211.187", StartTime:(*v1.Time)(0xc420ea07c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc421f3a000)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc421f3a070)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://6fab3fdbb5b90ee1c649a910e198070c1ead0d71da398c56fc55dcb217ee15ec"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc420ea0820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc420ea07e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:10:32.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-dvwj8" for this suite.
Jan 29 09:10:54.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:10:54.495: INFO: namespace: e2e-tests-init-container-dvwj8, resource: bindings, ignored listing per whitelist
Jan 29 09:10:54.513: INFO: namespace e2e-tests-init-container-dvwj8 deletion completed in 22.121983509s

â€¢ [SLOW TEST:65.530 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:10:54.514: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-ww6hr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-ww6hr
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Jan 29 09:10:54.758: INFO: Found 0 stateful pods, waiting for 3
Jan 29 09:11:04.762: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 09:11:04.762: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 09:11:04.762: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 09:11:04.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-ww6hr ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 09:11:05.120: INFO: stderr: ""
Jan 29 09:11:05.120: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 09:11:05.120: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 29 09:11:15.242: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 29 09:11:25.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-ww6hr ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:11:25.529: INFO: stderr: ""
Jan 29 09:11:25.530: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 29 09:11:25.530: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 29 09:11:35.563: INFO: Waiting for StatefulSet e2e-tests-statefulset-ww6hr/ss2 to complete update
Jan 29 09:11:35.563: INFO: Waiting for Pod e2e-tests-statefulset-ww6hr/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 29 09:11:35.563: INFO: Waiting for Pod e2e-tests-statefulset-ww6hr/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 29 09:11:35.563: INFO: Waiting for Pod e2e-tests-statefulset-ww6hr/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 29 09:11:45.569: INFO: Waiting for StatefulSet e2e-tests-statefulset-ww6hr/ss2 to complete update
Jan 29 09:11:45.569: INFO: Waiting for Pod e2e-tests-statefulset-ww6hr/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Jan 29 09:11:55.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-ww6hr ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 09:11:55.881: INFO: stderr: ""
Jan 29 09:11:55.881: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 09:11:55.881: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 29 09:12:05.935: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 29 09:12:15.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-ww6hr ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:12:16.246: INFO: stderr: ""
Jan 29 09:12:16.246: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 29 09:12:16.246: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 29 09:12:36.267: INFO: Deleting all statefulset in ns e2e-tests-statefulset-ww6hr
Jan 29 09:12:36.271: INFO: Scaling statefulset ss2 to 0
Jan 29 09:12:56.290: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 09:12:56.294: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:12:56.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-ww6hr" for this suite.
Jan 29 09:13:02.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:13:02.507: INFO: namespace: e2e-tests-statefulset-ww6hr, resource: bindings, ignored listing per whitelist
Jan 29 09:13:02.520: INFO: namespace e2e-tests-statefulset-ww6hr deletion completed in 6.203283391s

â€¢ [SLOW TEST:128.006 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:13:02.520: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-n7rh9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-n7rh9
I0129 09:13:02.741099      19 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-n7rh9, replica count: 1
I0129 09:13:03.791662      19 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 09:13:04.791887      19 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 09:13:04.898: INFO: Created: latency-svc-7pc4l
Jan 29 09:13:04.905: INFO: Got endpoints: latency-svc-7pc4l [13.125297ms]
Jan 29 09:13:04.918: INFO: Created: latency-svc-742fk
Jan 29 09:13:04.923: INFO: Created: latency-svc-jw7nx
Jan 29 09:13:04.930: INFO: Created: latency-svc-jz8qw
Jan 29 09:13:04.931: INFO: Got endpoints: latency-svc-jw7nx [25.114838ms]
Jan 29 09:13:04.931: INFO: Got endpoints: latency-svc-742fk [25.46185ms]
Jan 29 09:13:04.937: INFO: Got endpoints: latency-svc-jz8qw [31.820698ms]
Jan 29 09:13:04.938: INFO: Created: latency-svc-47jcw
Jan 29 09:13:04.944: INFO: Got endpoints: latency-svc-47jcw [38.81627ms]
Jan 29 09:13:04.953: INFO: Created: latency-svc-zkqsk
Jan 29 09:13:04.953: INFO: Created: latency-svc-dmrc9
Jan 29 09:13:04.956: INFO: Got endpoints: latency-svc-zkqsk [50.378903ms]
Jan 29 09:13:04.963: INFO: Got endpoints: latency-svc-dmrc9 [57.435191ms]
Jan 29 09:13:04.963: INFO: Created: latency-svc-gckt8
Jan 29 09:13:04.971: INFO: Got endpoints: latency-svc-gckt8 [65.564344ms]
Jan 29 09:13:04.980: INFO: Created: latency-svc-zhqqd
Jan 29 09:13:04.986: INFO: Created: latency-svc-kxw42
Jan 29 09:13:04.987: INFO: Got endpoints: latency-svc-zhqqd [81.258025ms]
Jan 29 09:13:04.999: INFO: Got endpoints: latency-svc-kxw42 [92.964656ms]
Jan 29 09:13:05.003: INFO: Created: latency-svc-n6nmp
Jan 29 09:13:05.010: INFO: Got endpoints: latency-svc-n6nmp [104.722294ms]
Jan 29 09:13:05.011: INFO: Created: latency-svc-lw9dm
Jan 29 09:13:05.016: INFO: Got endpoints: latency-svc-lw9dm [110.01031ms]
Jan 29 09:13:05.028: INFO: Created: latency-svc-lfjct
Jan 29 09:13:05.037: INFO: Created: latency-svc-tkb59
Jan 29 09:13:05.038: INFO: Created: latency-svc-jzxq6
Jan 29 09:13:05.041: INFO: Got endpoints: latency-svc-tkb59 [134.7675ms]
Jan 29 09:13:05.041: INFO: Got endpoints: latency-svc-lfjct [135.496524ms]
Jan 29 09:13:05.052: INFO: Got endpoints: latency-svc-jzxq6 [146.723383ms]
Jan 29 09:13:05.060: INFO: Created: latency-svc-ftfqf
Jan 29 09:13:05.064: INFO: Created: latency-svc-xvbmc
Jan 29 09:13:05.064: INFO: Got endpoints: latency-svc-ftfqf [158.634748ms]
Jan 29 09:13:05.070: INFO: Got endpoints: latency-svc-xvbmc [138.311318ms]
Jan 29 09:13:05.080: INFO: Created: latency-svc-k8bqt
Jan 29 09:13:05.087: INFO: Got endpoints: latency-svc-k8bqt [156.081562ms]
Jan 29 09:13:05.088: INFO: Created: latency-svc-w5ws2
Jan 29 09:13:05.089: INFO: Got endpoints: latency-svc-w5ws2 [151.122762ms]
Jan 29 09:13:05.101: INFO: Created: latency-svc-kq5b2
Jan 29 09:13:05.101: INFO: Created: latency-svc-7dvhz
Jan 29 09:13:05.101: INFO: Got endpoints: latency-svc-7dvhz [157.142654ms]
Jan 29 09:13:05.104: INFO: Got endpoints: latency-svc-kq5b2 [148.162849ms]
Jan 29 09:13:05.113: INFO: Created: latency-svc-zkngz
Jan 29 09:13:05.120: INFO: Got endpoints: latency-svc-zkngz [156.482881ms]
Jan 29 09:13:05.123: INFO: Created: latency-svc-qjnj6
Jan 29 09:13:05.130: INFO: Got endpoints: latency-svc-qjnj6 [158.642173ms]
Jan 29 09:13:05.135: INFO: Created: latency-svc-97jwz
Jan 29 09:13:05.143: INFO: Got endpoints: latency-svc-97jwz [156.474657ms]
Jan 29 09:13:05.145: INFO: Created: latency-svc-kdg9w
Jan 29 09:13:05.152: INFO: Created: latency-svc-g4cgr
Jan 29 09:13:05.152: INFO: Got endpoints: latency-svc-kdg9w [153.433502ms]
Jan 29 09:13:05.156: INFO: Created: latency-svc-q7xb6
Jan 29 09:13:05.158: INFO: Got endpoints: latency-svc-g4cgr [147.792835ms]
Jan 29 09:13:05.164: INFO: Got endpoints: latency-svc-q7xb6 [146.521811ms]
Jan 29 09:13:05.167: INFO: Created: latency-svc-lqw6s
Jan 29 09:13:05.174: INFO: Got endpoints: latency-svc-lqw6s [133.079537ms]
Jan 29 09:13:05.181: INFO: Created: latency-svc-cgkgp
Jan 29 09:13:05.195: INFO: Got endpoints: latency-svc-cgkgp [154.499613ms]
Jan 29 09:13:05.197: INFO: Created: latency-svc-sh746
Jan 29 09:13:05.201: INFO: Created: latency-svc-7zmvd
Jan 29 09:13:05.203: INFO: Got endpoints: latency-svc-sh746 [150.666432ms]
Jan 29 09:13:05.209: INFO: Created: latency-svc-vl5sm
Jan 29 09:13:05.209: INFO: Got endpoints: latency-svc-7zmvd [144.362052ms]
Jan 29 09:13:05.214: INFO: Got endpoints: latency-svc-vl5sm [144.395343ms]
Jan 29 09:13:05.216: INFO: Created: latency-svc-vkjhh
Jan 29 09:13:05.220: INFO: Got endpoints: latency-svc-vkjhh [133.010298ms]
Jan 29 09:13:05.223: INFO: Created: latency-svc-q2h4l
Jan 29 09:13:05.229: INFO: Created: latency-svc-7p7c6
Jan 29 09:13:05.229: INFO: Got endpoints: latency-svc-q2h4l [140.774051ms]
Jan 29 09:13:05.235: INFO: Got endpoints: latency-svc-7p7c6 [133.670356ms]
Jan 29 09:13:05.240: INFO: Created: latency-svc-l4hwc
Jan 29 09:13:05.242: INFO: Got endpoints: latency-svc-l4hwc [137.948642ms]
Jan 29 09:13:05.246: INFO: Created: latency-svc-q9q4m
Jan 29 09:13:05.249: INFO: Created: latency-svc-99g7d
Jan 29 09:13:05.253: INFO: Got endpoints: latency-svc-q9q4m [133.843697ms]
Jan 29 09:13:05.258: INFO: Created: latency-svc-p6tg9
Jan 29 09:13:05.261: INFO: Created: latency-svc-cxcpv
Jan 29 09:13:05.264: INFO: Created: latency-svc-h4wr9
Jan 29 09:13:05.270: INFO: Created: latency-svc-vffp6
Jan 29 09:13:05.276: INFO: Created: latency-svc-8xbwh
Jan 29 09:13:05.289: INFO: Created: latency-svc-d27fb
Jan 29 09:13:05.295: INFO: Created: latency-svc-jml55
Jan 29 09:13:05.302: INFO: Created: latency-svc-r9wng
Jan 29 09:13:05.307: INFO: Created: latency-svc-fxd2f
Jan 29 09:13:05.307: INFO: Got endpoints: latency-svc-99g7d [177.131856ms]
Jan 29 09:13:05.314: INFO: Created: latency-svc-ng67v
Jan 29 09:13:05.323: INFO: Created: latency-svc-6czkp
Jan 29 09:13:05.326: INFO: Created: latency-svc-tkhs4
Jan 29 09:13:05.336: INFO: Created: latency-svc-khm9p
Jan 29 09:13:05.336: INFO: Created: latency-svc-2lf29
Jan 29 09:13:05.343: INFO: Created: latency-svc-x884r
Jan 29 09:13:05.353: INFO: Got endpoints: latency-svc-p6tg9 [209.005068ms]
Jan 29 09:13:05.362: INFO: Created: latency-svc-hjtng
Jan 29 09:13:05.405: INFO: Got endpoints: latency-svc-cxcpv [252.716231ms]
Jan 29 09:13:05.414: INFO: Created: latency-svc-cfqdd
Jan 29 09:13:05.453: INFO: Got endpoints: latency-svc-h4wr9 [294.932162ms]
Jan 29 09:13:05.461: INFO: Created: latency-svc-6p5p5
Jan 29 09:13:05.506: INFO: Got endpoints: latency-svc-vffp6 [341.548528ms]
Jan 29 09:13:05.515: INFO: Created: latency-svc-kt5h6
Jan 29 09:13:05.554: INFO: Got endpoints: latency-svc-8xbwh [380.73022ms]
Jan 29 09:13:05.566: INFO: Created: latency-svc-97r99
Jan 29 09:13:05.604: INFO: Got endpoints: latency-svc-d27fb [408.278589ms]
Jan 29 09:13:05.611: INFO: Created: latency-svc-ccg6l
Jan 29 09:13:05.658: INFO: Got endpoints: latency-svc-jml55 [454.468851ms]
Jan 29 09:13:05.668: INFO: Created: latency-svc-n4l7g
Jan 29 09:13:05.704: INFO: Got endpoints: latency-svc-r9wng [495.39784ms]
Jan 29 09:13:05.716: INFO: Created: latency-svc-h5tsn
Jan 29 09:13:05.759: INFO: Got endpoints: latency-svc-fxd2f [544.580521ms]
Jan 29 09:13:05.770: INFO: Created: latency-svc-27m5z
Jan 29 09:13:05.803: INFO: Got endpoints: latency-svc-ng67v [583.007672ms]
Jan 29 09:13:05.811: INFO: Created: latency-svc-7js49
Jan 29 09:13:05.853: INFO: Got endpoints: latency-svc-6czkp [623.419933ms]
Jan 29 09:13:05.861: INFO: Created: latency-svc-hv6sp
Jan 29 09:13:05.905: INFO: Got endpoints: latency-svc-tkhs4 [669.857093ms]
Jan 29 09:13:05.917: INFO: Created: latency-svc-2mcn9
Jan 29 09:13:05.956: INFO: Got endpoints: latency-svc-khm9p [713.862755ms]
Jan 29 09:13:05.966: INFO: Created: latency-svc-5wpvw
Jan 29 09:13:06.004: INFO: Got endpoints: latency-svc-2lf29 [750.351403ms]
Jan 29 09:13:06.013: INFO: Created: latency-svc-hxr7v
Jan 29 09:13:06.054: INFO: Got endpoints: latency-svc-x884r [746.640639ms]
Jan 29 09:13:06.064: INFO: Created: latency-svc-8zzgj
Jan 29 09:13:06.106: INFO: Got endpoints: latency-svc-hjtng [753.793572ms]
Jan 29 09:13:06.116: INFO: Created: latency-svc-x87g4
Jan 29 09:13:06.154: INFO: Got endpoints: latency-svc-cfqdd [748.842207ms]
Jan 29 09:13:06.164: INFO: Created: latency-svc-ll7bh
Jan 29 09:13:06.204: INFO: Got endpoints: latency-svc-6p5p5 [750.432929ms]
Jan 29 09:13:06.213: INFO: Created: latency-svc-gzzqc
Jan 29 09:13:06.254: INFO: Got endpoints: latency-svc-kt5h6 [747.971037ms]
Jan 29 09:13:06.263: INFO: Created: latency-svc-8jzzh
Jan 29 09:13:06.315: INFO: Got endpoints: latency-svc-97r99 [760.602332ms]
Jan 29 09:13:06.323: INFO: Created: latency-svc-czw95
Jan 29 09:13:06.361: INFO: Got endpoints: latency-svc-ccg6l [757.343454ms]
Jan 29 09:13:06.386: INFO: Created: latency-svc-7ql59
Jan 29 09:13:06.404: INFO: Got endpoints: latency-svc-n4l7g [745.891785ms]
Jan 29 09:13:06.412: INFO: Created: latency-svc-hmblw
Jan 29 09:13:06.453: INFO: Got endpoints: latency-svc-h5tsn [748.41546ms]
Jan 29 09:13:06.461: INFO: Created: latency-svc-7jf5h
Jan 29 09:13:06.504: INFO: Got endpoints: latency-svc-27m5z [744.854601ms]
Jan 29 09:13:06.511: INFO: Created: latency-svc-xvphp
Jan 29 09:13:06.554: INFO: Got endpoints: latency-svc-7js49 [750.75162ms]
Jan 29 09:13:06.564: INFO: Created: latency-svc-cp7xt
Jan 29 09:13:06.603: INFO: Got endpoints: latency-svc-hv6sp [749.893678ms]
Jan 29 09:13:06.612: INFO: Created: latency-svc-9ppbh
Jan 29 09:13:06.654: INFO: Got endpoints: latency-svc-2mcn9 [748.731826ms]
Jan 29 09:13:06.668: INFO: Created: latency-svc-jn8pf
Jan 29 09:13:06.705: INFO: Got endpoints: latency-svc-5wpvw [748.288381ms]
Jan 29 09:13:06.715: INFO: Created: latency-svc-wr2w4
Jan 29 09:13:06.754: INFO: Got endpoints: latency-svc-hxr7v [750.517569ms]
Jan 29 09:13:06.765: INFO: Created: latency-svc-n6g6n
Jan 29 09:13:06.803: INFO: Got endpoints: latency-svc-8zzgj [749.323331ms]
Jan 29 09:13:06.812: INFO: Created: latency-svc-v72nh
Jan 29 09:13:06.854: INFO: Got endpoints: latency-svc-x87g4 [747.329876ms]
Jan 29 09:13:06.864: INFO: Created: latency-svc-6r77n
Jan 29 09:13:06.903: INFO: Got endpoints: latency-svc-ll7bh [749.378757ms]
Jan 29 09:13:06.911: INFO: Created: latency-svc-94pwv
Jan 29 09:13:06.953: INFO: Got endpoints: latency-svc-gzzqc [748.858367ms]
Jan 29 09:13:06.961: INFO: Created: latency-svc-rdt8j
Jan 29 09:13:07.004: INFO: Got endpoints: latency-svc-8jzzh [750.219892ms]
Jan 29 09:13:07.015: INFO: Created: latency-svc-vzfv4
Jan 29 09:13:07.055: INFO: Got endpoints: latency-svc-czw95 [739.85792ms]
Jan 29 09:13:07.065: INFO: Created: latency-svc-whj7x
Jan 29 09:13:07.103: INFO: Got endpoints: latency-svc-7ql59 [741.44492ms]
Jan 29 09:13:07.113: INFO: Created: latency-svc-n4xcp
Jan 29 09:13:07.155: INFO: Got endpoints: latency-svc-hmblw [751.237283ms]
Jan 29 09:13:07.167: INFO: Created: latency-svc-5z4km
Jan 29 09:13:07.203: INFO: Got endpoints: latency-svc-7jf5h [750.631192ms]
Jan 29 09:13:07.213: INFO: Created: latency-svc-jvswg
Jan 29 09:13:07.253: INFO: Got endpoints: latency-svc-xvphp [748.81954ms]
Jan 29 09:13:07.273: INFO: Created: latency-svc-6btrz
Jan 29 09:13:07.306: INFO: Got endpoints: latency-svc-cp7xt [752.054983ms]
Jan 29 09:13:07.318: INFO: Created: latency-svc-2fhbj
Jan 29 09:13:07.355: INFO: Got endpoints: latency-svc-9ppbh [751.514572ms]
Jan 29 09:13:07.364: INFO: Created: latency-svc-kcgjw
Jan 29 09:13:07.406: INFO: Got endpoints: latency-svc-jn8pf [751.99691ms]
Jan 29 09:13:07.416: INFO: Created: latency-svc-g47dp
Jan 29 09:13:07.453: INFO: Got endpoints: latency-svc-wr2w4 [748.562061ms]
Jan 29 09:13:07.460: INFO: Created: latency-svc-2h8lq
Jan 29 09:13:07.504: INFO: Got endpoints: latency-svc-n6g6n [749.218418ms]
Jan 29 09:13:07.511: INFO: Created: latency-svc-2zgrd
Jan 29 09:13:07.555: INFO: Got endpoints: latency-svc-v72nh [751.615328ms]
Jan 29 09:13:07.565: INFO: Created: latency-svc-4nv4c
Jan 29 09:13:07.603: INFO: Got endpoints: latency-svc-6r77n [748.979313ms]
Jan 29 09:13:07.613: INFO: Created: latency-svc-jqq5g
Jan 29 09:13:07.653: INFO: Got endpoints: latency-svc-94pwv [749.097874ms]
Jan 29 09:13:07.663: INFO: Created: latency-svc-t2zbz
Jan 29 09:13:07.704: INFO: Got endpoints: latency-svc-rdt8j [751.468047ms]
Jan 29 09:13:07.715: INFO: Created: latency-svc-rldgn
Jan 29 09:13:07.754: INFO: Got endpoints: latency-svc-vzfv4 [750.386739ms]
Jan 29 09:13:07.767: INFO: Created: latency-svc-878px
Jan 29 09:13:07.805: INFO: Got endpoints: latency-svc-whj7x [749.690065ms]
Jan 29 09:13:07.813: INFO: Created: latency-svc-lwrk7
Jan 29 09:13:07.853: INFO: Got endpoints: latency-svc-n4xcp [750.005622ms]
Jan 29 09:13:07.865: INFO: Created: latency-svc-lpkvg
Jan 29 09:13:07.903: INFO: Got endpoints: latency-svc-5z4km [747.94345ms]
Jan 29 09:13:07.912: INFO: Created: latency-svc-jz6zr
Jan 29 09:13:07.953: INFO: Got endpoints: latency-svc-jvswg [749.918141ms]
Jan 29 09:13:07.965: INFO: Created: latency-svc-9zvxq
Jan 29 09:13:08.010: INFO: Got endpoints: latency-svc-6btrz [756.609453ms]
Jan 29 09:13:08.023: INFO: Created: latency-svc-qhx7m
Jan 29 09:13:08.059: INFO: Got endpoints: latency-svc-2fhbj [752.359439ms]
Jan 29 09:13:08.069: INFO: Created: latency-svc-mzznx
Jan 29 09:13:08.103: INFO: Got endpoints: latency-svc-kcgjw [748.847165ms]
Jan 29 09:13:08.114: INFO: Created: latency-svc-6xggz
Jan 29 09:13:08.154: INFO: Got endpoints: latency-svc-g47dp [747.984395ms]
Jan 29 09:13:08.167: INFO: Created: latency-svc-xh2g9
Jan 29 09:13:08.204: INFO: Got endpoints: latency-svc-2h8lq [750.50881ms]
Jan 29 09:13:08.213: INFO: Created: latency-svc-gnmjq
Jan 29 09:13:08.254: INFO: Got endpoints: latency-svc-2zgrd [750.213584ms]
Jan 29 09:13:08.261: INFO: Created: latency-svc-qhtq7
Jan 29 09:13:08.305: INFO: Got endpoints: latency-svc-4nv4c [750.257944ms]
Jan 29 09:13:08.315: INFO: Created: latency-svc-sqzwp
Jan 29 09:13:08.353: INFO: Got endpoints: latency-svc-jqq5g [749.47635ms]
Jan 29 09:13:08.361: INFO: Created: latency-svc-96f46
Jan 29 09:13:08.405: INFO: Got endpoints: latency-svc-t2zbz [751.933549ms]
Jan 29 09:13:08.413: INFO: Created: latency-svc-v7jmx
Jan 29 09:13:08.453: INFO: Got endpoints: latency-svc-rldgn [748.616516ms]
Jan 29 09:13:08.460: INFO: Created: latency-svc-fqkv2
Jan 29 09:13:08.505: INFO: Got endpoints: latency-svc-878px [750.0882ms]
Jan 29 09:13:08.513: INFO: Created: latency-svc-hlv6n
Jan 29 09:13:08.554: INFO: Got endpoints: latency-svc-lwrk7 [749.379345ms]
Jan 29 09:13:08.565: INFO: Created: latency-svc-kgn8j
Jan 29 09:13:08.605: INFO: Got endpoints: latency-svc-lpkvg [752.265362ms]
Jan 29 09:13:08.613: INFO: Created: latency-svc-knpjg
Jan 29 09:13:08.654: INFO: Got endpoints: latency-svc-jz6zr [751.241259ms]
Jan 29 09:13:08.668: INFO: Created: latency-svc-q5zjw
Jan 29 09:13:08.704: INFO: Got endpoints: latency-svc-9zvxq [750.762664ms]
Jan 29 09:13:08.714: INFO: Created: latency-svc-49vpn
Jan 29 09:13:08.754: INFO: Got endpoints: latency-svc-qhx7m [744.226456ms]
Jan 29 09:13:08.763: INFO: Created: latency-svc-v4kk7
Jan 29 09:13:08.804: INFO: Got endpoints: latency-svc-mzznx [745.22411ms]
Jan 29 09:13:08.815: INFO: Created: latency-svc-gd7vx
Jan 29 09:13:08.854: INFO: Got endpoints: latency-svc-6xggz [750.443962ms]
Jan 29 09:13:08.862: INFO: Created: latency-svc-4tbj4
Jan 29 09:13:08.904: INFO: Got endpoints: latency-svc-xh2g9 [749.917627ms]
Jan 29 09:13:08.912: INFO: Created: latency-svc-sqp8r
Jan 29 09:13:08.954: INFO: Got endpoints: latency-svc-gnmjq [749.663588ms]
Jan 29 09:13:08.967: INFO: Created: latency-svc-kg7gl
Jan 29 09:13:09.004: INFO: Got endpoints: latency-svc-qhtq7 [749.97889ms]
Jan 29 09:13:09.012: INFO: Created: latency-svc-stslg
Jan 29 09:13:09.054: INFO: Got endpoints: latency-svc-sqzwp [748.386946ms]
Jan 29 09:13:09.064: INFO: Created: latency-svc-tg88h
Jan 29 09:13:09.104: INFO: Got endpoints: latency-svc-96f46 [750.795422ms]
Jan 29 09:13:09.113: INFO: Created: latency-svc-55h8h
Jan 29 09:13:09.154: INFO: Got endpoints: latency-svc-v7jmx [749.621917ms]
Jan 29 09:13:09.160: INFO: Created: latency-svc-jvjr7
Jan 29 09:13:09.203: INFO: Got endpoints: latency-svc-fqkv2 [750.541744ms]
Jan 29 09:13:09.214: INFO: Created: latency-svc-jhl99
Jan 29 09:13:09.253: INFO: Got endpoints: latency-svc-hlv6n [748.57266ms]
Jan 29 09:13:09.266: INFO: Created: latency-svc-ws269
Jan 29 09:13:09.304: INFO: Got endpoints: latency-svc-kgn8j [749.417089ms]
Jan 29 09:13:09.326: INFO: Created: latency-svc-xt2z9
Jan 29 09:13:09.356: INFO: Got endpoints: latency-svc-knpjg [751.201114ms]
Jan 29 09:13:09.367: INFO: Created: latency-svc-9qd47
Jan 29 09:13:09.404: INFO: Got endpoints: latency-svc-q5zjw [749.582397ms]
Jan 29 09:13:09.414: INFO: Created: latency-svc-zhqtb
Jan 29 09:13:09.453: INFO: Got endpoints: latency-svc-49vpn [748.979534ms]
Jan 29 09:13:09.463: INFO: Created: latency-svc-6jpz2
Jan 29 09:13:09.504: INFO: Got endpoints: latency-svc-v4kk7 [748.857358ms]
Jan 29 09:13:09.514: INFO: Created: latency-svc-6bnmq
Jan 29 09:13:09.554: INFO: Got endpoints: latency-svc-gd7vx [749.695643ms]
Jan 29 09:13:09.560: INFO: Created: latency-svc-cspmr
Jan 29 09:13:09.604: INFO: Got endpoints: latency-svc-4tbj4 [749.747344ms]
Jan 29 09:13:09.613: INFO: Created: latency-svc-b2m47
Jan 29 09:13:09.653: INFO: Got endpoints: latency-svc-sqp8r [749.089659ms]
Jan 29 09:13:09.661: INFO: Created: latency-svc-vvdx9
Jan 29 09:13:09.704: INFO: Got endpoints: latency-svc-kg7gl [750.243467ms]
Jan 29 09:13:09.711: INFO: Created: latency-svc-nzhsg
Jan 29 09:13:09.755: INFO: Got endpoints: latency-svc-stslg [750.425139ms]
Jan 29 09:13:09.766: INFO: Created: latency-svc-d2nvx
Jan 29 09:13:09.803: INFO: Got endpoints: latency-svc-tg88h [748.340524ms]
Jan 29 09:13:09.814: INFO: Created: latency-svc-hjw44
Jan 29 09:13:09.852: INFO: Got endpoints: latency-svc-55h8h [748.311639ms]
Jan 29 09:13:09.860: INFO: Created: latency-svc-6t7fw
Jan 29 09:13:09.903: INFO: Got endpoints: latency-svc-jvjr7 [748.306621ms]
Jan 29 09:13:09.912: INFO: Created: latency-svc-qhf7t
Jan 29 09:13:09.953: INFO: Got endpoints: latency-svc-jhl99 [749.304419ms]
Jan 29 09:13:09.963: INFO: Created: latency-svc-q57g2
Jan 29 09:13:10.009: INFO: Got endpoints: latency-svc-ws269 [755.535282ms]
Jan 29 09:13:10.018: INFO: Created: latency-svc-tcz82
Jan 29 09:13:10.057: INFO: Got endpoints: latency-svc-xt2z9 [752.968586ms]
Jan 29 09:13:10.072: INFO: Created: latency-svc-n4hmq
Jan 29 09:13:10.108: INFO: Got endpoints: latency-svc-9qd47 [751.500307ms]
Jan 29 09:13:10.121: INFO: Created: latency-svc-2s9l2
Jan 29 09:13:10.155: INFO: Got endpoints: latency-svc-zhqtb [751.133419ms]
Jan 29 09:13:10.163: INFO: Created: latency-svc-dm9d9
Jan 29 09:13:10.203: INFO: Got endpoints: latency-svc-6jpz2 [750.155169ms]
Jan 29 09:13:10.215: INFO: Created: latency-svc-pmg66
Jan 29 09:13:10.253: INFO: Got endpoints: latency-svc-6bnmq [749.007404ms]
Jan 29 09:13:10.261: INFO: Created: latency-svc-9zgq6
Jan 29 09:13:10.304: INFO: Got endpoints: latency-svc-cspmr [749.776074ms]
Jan 29 09:13:10.311: INFO: Created: latency-svc-s2lgc
Jan 29 09:13:10.354: INFO: Got endpoints: latency-svc-b2m47 [749.214976ms]
Jan 29 09:13:10.363: INFO: Created: latency-svc-vvncg
Jan 29 09:13:10.405: INFO: Got endpoints: latency-svc-vvdx9 [751.335362ms]
Jan 29 09:13:10.416: INFO: Created: latency-svc-6f8jj
Jan 29 09:13:10.455: INFO: Got endpoints: latency-svc-nzhsg [750.403321ms]
Jan 29 09:13:10.466: INFO: Created: latency-svc-824hz
Jan 29 09:13:10.505: INFO: Got endpoints: latency-svc-d2nvx [750.160118ms]
Jan 29 09:13:10.515: INFO: Created: latency-svc-tdwc2
Jan 29 09:13:10.559: INFO: Got endpoints: latency-svc-hjw44 [755.983182ms]
Jan 29 09:13:10.575: INFO: Created: latency-svc-q55pd
Jan 29 09:13:10.603: INFO: Got endpoints: latency-svc-6t7fw [750.897781ms]
Jan 29 09:13:10.611: INFO: Created: latency-svc-2fbpk
Jan 29 09:13:10.653: INFO: Got endpoints: latency-svc-qhf7t [749.48376ms]
Jan 29 09:13:10.662: INFO: Created: latency-svc-wwlxp
Jan 29 09:13:10.703: INFO: Got endpoints: latency-svc-q57g2 [750.490496ms]
Jan 29 09:13:10.711: INFO: Created: latency-svc-msfsc
Jan 29 09:13:10.753: INFO: Got endpoints: latency-svc-tcz82 [744.26411ms]
Jan 29 09:13:10.761: INFO: Created: latency-svc-dkb9d
Jan 29 09:13:10.806: INFO: Got endpoints: latency-svc-n4hmq [748.569194ms]
Jan 29 09:13:10.814: INFO: Created: latency-svc-x79d9
Jan 29 09:13:10.857: INFO: Got endpoints: latency-svc-2s9l2 [749.250055ms]
Jan 29 09:13:10.867: INFO: Created: latency-svc-8tqfq
Jan 29 09:13:10.906: INFO: Got endpoints: latency-svc-dm9d9 [750.332218ms]
Jan 29 09:13:10.915: INFO: Created: latency-svc-jlqrt
Jan 29 09:13:10.953: INFO: Got endpoints: latency-svc-pmg66 [749.467615ms]
Jan 29 09:13:10.966: INFO: Created: latency-svc-wmtmp
Jan 29 09:13:11.007: INFO: Got endpoints: latency-svc-9zgq6 [753.858989ms]
Jan 29 09:13:11.015: INFO: Created: latency-svc-rr8gj
Jan 29 09:13:11.053: INFO: Got endpoints: latency-svc-s2lgc [748.871045ms]
Jan 29 09:13:11.061: INFO: Created: latency-svc-5tr9q
Jan 29 09:13:11.107: INFO: Got endpoints: latency-svc-vvncg [753.412652ms]
Jan 29 09:13:11.117: INFO: Created: latency-svc-6wxj5
Jan 29 09:13:11.154: INFO: Got endpoints: latency-svc-6f8jj [749.126613ms]
Jan 29 09:13:11.172: INFO: Created: latency-svc-rz525
Jan 29 09:13:11.203: INFO: Got endpoints: latency-svc-824hz [748.761452ms]
Jan 29 09:13:11.210: INFO: Created: latency-svc-pc8kt
Jan 29 09:13:11.254: INFO: Got endpoints: latency-svc-tdwc2 [748.687766ms]
Jan 29 09:13:11.260: INFO: Created: latency-svc-zg9zx
Jan 29 09:13:11.303: INFO: Got endpoints: latency-svc-q55pd [743.756003ms]
Jan 29 09:13:11.312: INFO: Created: latency-svc-6n2z4
Jan 29 09:13:11.353: INFO: Got endpoints: latency-svc-2fbpk [749.87356ms]
Jan 29 09:13:11.362: INFO: Created: latency-svc-mvdq5
Jan 29 09:13:11.404: INFO: Got endpoints: latency-svc-wwlxp [750.82325ms]
Jan 29 09:13:11.419: INFO: Created: latency-svc-lgmrf
Jan 29 09:13:11.454: INFO: Got endpoints: latency-svc-msfsc [750.838018ms]
Jan 29 09:13:11.463: INFO: Created: latency-svc-xvcsk
Jan 29 09:13:11.504: INFO: Got endpoints: latency-svc-dkb9d [750.5248ms]
Jan 29 09:13:11.511: INFO: Created: latency-svc-qlmrw
Jan 29 09:13:11.559: INFO: Got endpoints: latency-svc-x79d9 [753.468388ms]
Jan 29 09:13:11.567: INFO: Created: latency-svc-m6tg4
Jan 29 09:13:11.608: INFO: Got endpoints: latency-svc-8tqfq [750.408658ms]
Jan 29 09:13:11.620: INFO: Created: latency-svc-25m8d
Jan 29 09:13:11.656: INFO: Got endpoints: latency-svc-jlqrt [750.210057ms]
Jan 29 09:13:11.670: INFO: Created: latency-svc-jdrh6
Jan 29 09:13:11.703: INFO: Got endpoints: latency-svc-wmtmp [750.323051ms]
Jan 29 09:13:11.711: INFO: Created: latency-svc-97bgs
Jan 29 09:13:11.755: INFO: Got endpoints: latency-svc-rr8gj [748.037834ms]
Jan 29 09:13:11.767: INFO: Created: latency-svc-26dg7
Jan 29 09:13:11.804: INFO: Got endpoints: latency-svc-5tr9q [751.141656ms]
Jan 29 09:13:11.813: INFO: Created: latency-svc-h7jn4
Jan 29 09:13:11.855: INFO: Got endpoints: latency-svc-6wxj5 [747.892977ms]
Jan 29 09:13:11.865: INFO: Created: latency-svc-9vcsc
Jan 29 09:13:11.903: INFO: Got endpoints: latency-svc-rz525 [748.96565ms]
Jan 29 09:13:11.913: INFO: Created: latency-svc-h2pkl
Jan 29 09:13:11.954: INFO: Got endpoints: latency-svc-pc8kt [750.9273ms]
Jan 29 09:13:11.979: INFO: Created: latency-svc-5vx8z
Jan 29 09:13:12.005: INFO: Got endpoints: latency-svc-zg9zx [751.374565ms]
Jan 29 09:13:12.013: INFO: Created: latency-svc-n8wfz
Jan 29 09:13:12.053: INFO: Got endpoints: latency-svc-6n2z4 [750.583906ms]
Jan 29 09:13:12.062: INFO: Created: latency-svc-4t78b
Jan 29 09:13:12.105: INFO: Got endpoints: latency-svc-mvdq5 [751.386512ms]
Jan 29 09:13:12.113: INFO: Created: latency-svc-x744m
Jan 29 09:13:12.153: INFO: Got endpoints: latency-svc-lgmrf [749.006277ms]
Jan 29 09:13:12.161: INFO: Created: latency-svc-s55fz
Jan 29 09:13:12.203: INFO: Got endpoints: latency-svc-xvcsk [748.825303ms]
Jan 29 09:13:12.212: INFO: Created: latency-svc-7cg4s
Jan 29 09:13:12.253: INFO: Got endpoints: latency-svc-qlmrw [748.911643ms]
Jan 29 09:13:12.261: INFO: Created: latency-svc-7shsp
Jan 29 09:13:12.305: INFO: Got endpoints: latency-svc-m6tg4 [745.896398ms]
Jan 29 09:13:12.314: INFO: Created: latency-svc-trp8j
Jan 29 09:13:12.354: INFO: Got endpoints: latency-svc-25m8d [746.03976ms]
Jan 29 09:13:12.364: INFO: Created: latency-svc-226hm
Jan 29 09:13:12.404: INFO: Got endpoints: latency-svc-jdrh6 [747.949995ms]
Jan 29 09:13:12.414: INFO: Created: latency-svc-fv8hb
Jan 29 09:13:12.453: INFO: Got endpoints: latency-svc-97bgs [749.354604ms]
Jan 29 09:13:12.461: INFO: Created: latency-svc-qx6t7
Jan 29 09:13:12.503: INFO: Got endpoints: latency-svc-26dg7 [747.442237ms]
Jan 29 09:13:12.511: INFO: Created: latency-svc-qncnt
Jan 29 09:13:12.554: INFO: Got endpoints: latency-svc-h7jn4 [750.142243ms]
Jan 29 09:13:12.567: INFO: Created: latency-svc-2d5bs
Jan 29 09:13:12.603: INFO: Got endpoints: latency-svc-9vcsc [747.664534ms]
Jan 29 09:13:12.611: INFO: Created: latency-svc-qqgjv
Jan 29 09:13:12.652: INFO: Got endpoints: latency-svc-h2pkl [749.141444ms]
Jan 29 09:13:12.666: INFO: Created: latency-svc-46r9n
Jan 29 09:13:12.705: INFO: Got endpoints: latency-svc-5vx8z [750.587114ms]
Jan 29 09:13:12.715: INFO: Created: latency-svc-mx5jv
Jan 29 09:13:12.756: INFO: Got endpoints: latency-svc-n8wfz [750.297131ms]
Jan 29 09:13:12.805: INFO: Got endpoints: latency-svc-4t78b [751.373241ms]
Jan 29 09:13:12.853: INFO: Got endpoints: latency-svc-x744m [747.93147ms]
Jan 29 09:13:12.903: INFO: Got endpoints: latency-svc-s55fz [750.168504ms]
Jan 29 09:13:12.953: INFO: Got endpoints: latency-svc-7cg4s [749.240555ms]
Jan 29 09:13:13.003: INFO: Got endpoints: latency-svc-7shsp [750.647271ms]
Jan 29 09:13:13.053: INFO: Got endpoints: latency-svc-trp8j [747.305559ms]
Jan 29 09:13:13.103: INFO: Got endpoints: latency-svc-226hm [748.984642ms]
Jan 29 09:13:13.154: INFO: Got endpoints: latency-svc-fv8hb [750.325122ms]
Jan 29 09:13:13.207: INFO: Got endpoints: latency-svc-qx6t7 [754.2035ms]
Jan 29 09:13:13.258: INFO: Got endpoints: latency-svc-qncnt [754.922708ms]
Jan 29 09:13:13.307: INFO: Got endpoints: latency-svc-2d5bs [752.391872ms]
Jan 29 09:13:13.354: INFO: Got endpoints: latency-svc-qqgjv [751.207363ms]
Jan 29 09:13:13.404: INFO: Got endpoints: latency-svc-46r9n [751.988928ms]
Jan 29 09:13:13.454: INFO: Got endpoints: latency-svc-mx5jv [748.663016ms]
Jan 29 09:13:13.454: INFO: Latencies: [25.114838ms 25.46185ms 31.820698ms 38.81627ms 50.378903ms 57.435191ms 65.564344ms 81.258025ms 92.964656ms 104.722294ms 110.01031ms 133.010298ms 133.079537ms 133.670356ms 133.843697ms 134.7675ms 135.496524ms 137.948642ms 138.311318ms 140.774051ms 144.362052ms 144.395343ms 146.521811ms 146.723383ms 147.792835ms 148.162849ms 150.666432ms 151.122762ms 153.433502ms 154.499613ms 156.081562ms 156.474657ms 156.482881ms 157.142654ms 158.634748ms 158.642173ms 177.131856ms 209.005068ms 252.716231ms 294.932162ms 341.548528ms 380.73022ms 408.278589ms 454.468851ms 495.39784ms 544.580521ms 583.007672ms 623.419933ms 669.857093ms 713.862755ms 739.85792ms 741.44492ms 743.756003ms 744.226456ms 744.26411ms 744.854601ms 745.22411ms 745.891785ms 745.896398ms 746.03976ms 746.640639ms 747.305559ms 747.329876ms 747.442237ms 747.664534ms 747.892977ms 747.93147ms 747.94345ms 747.949995ms 747.971037ms 747.984395ms 748.037834ms 748.288381ms 748.306621ms 748.311639ms 748.340524ms 748.386946ms 748.41546ms 748.562061ms 748.569194ms 748.57266ms 748.616516ms 748.663016ms 748.687766ms 748.731826ms 748.761452ms 748.81954ms 748.825303ms 748.842207ms 748.847165ms 748.857358ms 748.858367ms 748.871045ms 748.911643ms 748.96565ms 748.979313ms 748.979534ms 748.984642ms 749.006277ms 749.007404ms 749.089659ms 749.097874ms 749.126613ms 749.141444ms 749.214976ms 749.218418ms 749.240555ms 749.250055ms 749.304419ms 749.323331ms 749.354604ms 749.378757ms 749.379345ms 749.417089ms 749.467615ms 749.47635ms 749.48376ms 749.582397ms 749.621917ms 749.663588ms 749.690065ms 749.695643ms 749.747344ms 749.776074ms 749.87356ms 749.893678ms 749.917627ms 749.918141ms 749.97889ms 750.005622ms 750.0882ms 750.142243ms 750.155169ms 750.160118ms 750.168504ms 750.210057ms 750.213584ms 750.219892ms 750.243467ms 750.257944ms 750.297131ms 750.323051ms 750.325122ms 750.332218ms 750.351403ms 750.386739ms 750.403321ms 750.408658ms 750.425139ms 750.432929ms 750.443962ms 750.490496ms 750.50881ms 750.517569ms 750.5248ms 750.541744ms 750.583906ms 750.587114ms 750.631192ms 750.647271ms 750.75162ms 750.762664ms 750.795422ms 750.82325ms 750.838018ms 750.897781ms 750.9273ms 751.133419ms 751.141656ms 751.201114ms 751.207363ms 751.237283ms 751.241259ms 751.335362ms 751.373241ms 751.374565ms 751.386512ms 751.468047ms 751.500307ms 751.514572ms 751.615328ms 751.933549ms 751.988928ms 751.99691ms 752.054983ms 752.265362ms 752.359439ms 752.391872ms 752.968586ms 753.412652ms 753.468388ms 753.793572ms 753.858989ms 754.2035ms 754.922708ms 755.535282ms 755.983182ms 756.609453ms 757.343454ms 760.602332ms]
Jan 29 09:13:13.454: INFO: 50 %ile: 749.089659ms
Jan 29 09:13:13.454: INFO: 90 %ile: 751.615328ms
Jan 29 09:13:13.454: INFO: 99 %ile: 757.343454ms
Jan 29 09:13:13.454: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:13:13.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-n7rh9" for this suite.
Jan 29 09:13:25.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:13:25.570: INFO: namespace: e2e-tests-svc-latency-n7rh9, resource: bindings, ignored listing per whitelist
Jan 29 09:13:25.605: INFO: namespace e2e-tests-svc-latency-n7rh9 deletion completed in 12.145164613s

â€¢ [SLOW TEST:23.085 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:13:25.605: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-m5dzf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:13:25.805: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 29 09:13:25.838: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 29 09:13:30.845: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 29 09:13:30.845: INFO: Creating deployment "test-rolling-update-deployment"
Jan 29 09:13:30.932: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 29 09:13:30.951: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 29 09:13:32.960: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 29 09:13:32.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684350010, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684350010, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684350011, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684350010, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 09:13:34.967: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 29 09:13:34.978: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-m5dzf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-m5dzf/deployments/test-rolling-update-deployment,UID:25485576-23a6-11e9-a926-eeeeeeeeeeee,ResourceVersion:684938,Generation:1,CreationTimestamp:2019-01-29 09:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-01-29 09:13:30 +0000 UTC 2019-01-29 09:13:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-01-29 09:13:33 +0000 UTC 2019-01-29 09:13:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 09:13:34.981: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-m5dzf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-m5dzf/replicasets/test-rolling-update-deployment-65b7695dcf,UID:2559f9ac-23a6-11e9-a926-eeeeeeeeeeee,ResourceVersion:684929,Generation:1,CreationTimestamp:2019-01-29 09:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 25485576-23a6-11e9-a926-eeeeeeeeeeee 0xc42227e277 0xc42227e278}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 29 09:13:34.981: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 29 09:13:34.981: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-m5dzf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-m5dzf/replicasets/test-rolling-update-controller,UID:224899c3-23a6-11e9-a926-eeeeeeeeeeee,ResourceVersion:684937,Generation:2,CreationTimestamp:2019-01-29 09:13:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 25485576-23a6-11e9-a926-eeeeeeeeeeee 0xc42227e1ae 0xc42227e1af}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 29 09:13:34.985: INFO: Pod "test-rolling-update-deployment-65b7695dcf-96cmk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-96cmk,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-m5dzf,SelfLink:/api/v1/namespaces/e2e-tests-deployment-m5dzf/pods/test-rolling-update-deployment-65b7695dcf-96cmk,UID:255b9fe5-23a6-11e9-a926-eeeeeeeeeeee,ResourceVersion:684928,Generation:0,CreationTimestamp:2019-01-29 09:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf 2559f9ac-23a6-11e9-a926-eeeeeeeeeeee 0xc42227ead7 0xc42227ead8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bk497 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bk497,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bk497 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42227eb50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42227eb70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:13:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:13:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:13:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:13:31 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:10.1.211.191,StartTime:2019-01-29 09:13:31 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-01-29 09:13:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://d770d44940f410f2534fb054279acb85f0bf1305f6497ef8af8669311ce58241}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:13:34.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-m5dzf" for this suite.
Jan 29 09:13:41.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:13:41.201: INFO: namespace: e2e-tests-deployment-m5dzf, resource: bindings, ignored listing per whitelist
Jan 29 09:13:41.226: INFO: namespace e2e-tests-deployment-m5dzf deletion completed in 6.234401112s

â€¢ [SLOW TEST:15.621 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:13:41.226: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-m4zwd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 29 09:13:49.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:13:49.569: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:13:51.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:13:51.574: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:13:53.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:13:53.575: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:13:55.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:13:55.573: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:13:57.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:13:57.573: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:13:59.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:13:59.574: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:14:01.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:14:01.573: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:14:03.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:14:03.574: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:14:05.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:14:05.573: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:14:07.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:14:07.573: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:14:09.570: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:14:09.574: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:14:11.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:14:11.573: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:14:13.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:14:13.574: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 09:14:15.569: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 09:14:15.574: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:14:15.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-m4zwd" for this suite.
Jan 29 09:14:37.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:14:37.700: INFO: namespace: e2e-tests-container-lifecycle-hook-m4zwd, resource: bindings, ignored listing per whitelist
Jan 29 09:14:37.714: INFO: namespace e2e-tests-container-lifecycle-hook-m4zwd deletion completed in 22.134116041s

â€¢ [SLOW TEST:56.488 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:14:37.714: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-4phq5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 29 09:14:37.938: INFO: Waiting up to 5m0s for pod "pod-4d3f5aab-23a6-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-4phq5" to be "success or failure"
Jan 29 09:14:37.940: INFO: Pod "pod-4d3f5aab-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597367ms
Jan 29 09:14:39.943: INFO: Pod "pod-4d3f5aab-23a6-11e9-9015-b2c9bac28373": Phase="Running", Reason="", readiness=true. Elapsed: 2.005669482s
Jan 29 09:14:41.947: INFO: Pod "pod-4d3f5aab-23a6-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009485534s
STEP: Saw pod success
Jan 29 09:14:41.947: INFO: Pod "pod-4d3f5aab-23a6-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:14:41.950: INFO: Trying to get logs from node 9.111.254.123 pod pod-4d3f5aab-23a6-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:14:41.968: INFO: Waiting for pod pod-4d3f5aab-23a6-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:14:41.970: INFO: Pod pod-4d3f5aab-23a6-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:14:41.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-4phq5" for this suite.
Jan 29 09:14:47.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:14:48.030: INFO: namespace: e2e-tests-emptydir-4phq5, resource: bindings, ignored listing per whitelist
Jan 29 09:14:48.095: INFO: namespace e2e-tests-emptydir-4phq5 deletion completed in 6.119150643s

â€¢ [SLOW TEST:10.381 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:14:48.097: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-zf9wz
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 29 09:14:48.336: INFO: Waiting up to 5m0s for pod "pod-536f9efb-23a6-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-zf9wz" to be "success or failure"
Jan 29 09:14:48.338: INFO: Pod "pod-536f9efb-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222697ms
Jan 29 09:14:50.342: INFO: Pod "pod-536f9efb-23a6-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005634777s
STEP: Saw pod success
Jan 29 09:14:50.342: INFO: Pod "pod-536f9efb-23a6-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:14:50.344: INFO: Trying to get logs from node 9.111.254.123 pod pod-536f9efb-23a6-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:14:50.369: INFO: Waiting for pod pod-536f9efb-23a6-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:14:50.380: INFO: Pod pod-536f9efb-23a6-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:14:50.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-zf9wz" for this suite.
Jan 29 09:14:56.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:14:56.480: INFO: namespace: e2e-tests-emptydir-zf9wz, resource: bindings, ignored listing per whitelist
Jan 29 09:14:56.487: INFO: namespace e2e-tests-emptydir-zf9wz deletion completed in 6.103339407s

â€¢ [SLOW TEST:8.391 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:14:56.488: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4n8cf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-58706740-23a6-11e9-9015-b2c9bac28373
STEP: Creating secret with name s-test-opt-upd-58706d92-23a6-11e9-9015-b2c9bac28373
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-58706740-23a6-11e9-9015-b2c9bac28373
STEP: Updating secret s-test-opt-upd-58706d92-23a6-11e9-9015-b2c9bac28373
STEP: Creating secret with name s-test-opt-create-58706db5-23a6-11e9-9015-b2c9bac28373
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:16:21.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4n8cf" for this suite.
Jan 29 09:16:43.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:16:43.683: INFO: namespace: e2e-tests-projected-4n8cf, resource: bindings, ignored listing per whitelist
Jan 29 09:16:43.722: INFO: namespace e2e-tests-projected-4n8cf deletion completed in 22.121566821s

â€¢ [SLOW TEST:107.234 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:16:43.722: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-nhkm9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-kfwn
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 09:16:43.951: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kfwn" in namespace "e2e-tests-subpath-nhkm9" to be "success or failure"
Jan 29 09:16:43.955: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.683145ms
Jan 29 09:16:45.958: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006712675s
Jan 29 09:16:47.962: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 4.011208035s
Jan 29 09:16:49.965: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 6.014179584s
Jan 29 09:16:51.968: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 8.016982281s
Jan 29 09:16:53.972: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 10.020496577s
Jan 29 09:16:55.975: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 12.023450875s
Jan 29 09:16:57.978: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 14.026921656s
Jan 29 09:16:59.981: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 16.029902614s
Jan 29 09:17:01.985: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 18.03335024s
Jan 29 09:17:03.987: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 20.036129386s
Jan 29 09:17:05.991: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Running", Reason="", readiness=false. Elapsed: 22.039419508s
Jan 29 09:17:07.997: INFO: Pod "pod-subpath-test-configmap-kfwn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.045636499s
STEP: Saw pod success
Jan 29 09:17:07.997: INFO: Pod "pod-subpath-test-configmap-kfwn" satisfied condition "success or failure"
Jan 29 09:17:08.004: INFO: Trying to get logs from node 9.111.254.123 pod pod-subpath-test-configmap-kfwn container test-container-subpath-configmap-kfwn: <nil>
STEP: delete the pod
Jan 29 09:17:08.036: INFO: Waiting for pod pod-subpath-test-configmap-kfwn to disappear
Jan 29 09:17:08.039: INFO: Pod pod-subpath-test-configmap-kfwn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kfwn
Jan 29 09:17:08.039: INFO: Deleting pod "pod-subpath-test-configmap-kfwn" in namespace "e2e-tests-subpath-nhkm9"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:17:08.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-nhkm9" for this suite.
Jan 29 09:17:14.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:17:14.146: INFO: namespace: e2e-tests-subpath-nhkm9, resource: bindings, ignored listing per whitelist
Jan 29 09:17:14.215: INFO: namespace e2e-tests-subpath-nhkm9 deletion completed in 6.166707083s

â€¢ [SLOW TEST:30.492 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:17:14.215: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qrgzg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jan 29 09:17:14.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:14.951: INFO: stderr: ""
Jan 29 09:17:14.951: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 09:17:14.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:15.110: INFO: stderr: ""
Jan 29 09:17:15.110: INFO: stdout: "update-demo-nautilus-9kcq9 update-demo-nautilus-xcml5 "
Jan 29 09:17:15.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-9kcq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:15.252: INFO: stderr: ""
Jan 29 09:17:15.252: INFO: stdout: ""
Jan 29 09:17:15.252: INFO: update-demo-nautilus-9kcq9 is created but not running
Jan 29 09:17:20.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:20.414: INFO: stderr: ""
Jan 29 09:17:20.414: INFO: stdout: "update-demo-nautilus-9kcq9 update-demo-nautilus-xcml5 "
Jan 29 09:17:20.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-9kcq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:20.542: INFO: stderr: ""
Jan 29 09:17:20.542: INFO: stdout: "true"
Jan 29 09:17:20.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-9kcq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:20.668: INFO: stderr: ""
Jan 29 09:17:20.668: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:17:20.668: INFO: validating pod update-demo-nautilus-9kcq9
Jan 29 09:17:20.675: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:17:20.675: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:17:20.675: INFO: update-demo-nautilus-9kcq9 is verified up and running
Jan 29 09:17:20.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-xcml5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:20.799: INFO: stderr: ""
Jan 29 09:17:20.799: INFO: stdout: "true"
Jan 29 09:17:20.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-xcml5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:20.946: INFO: stderr: ""
Jan 29 09:17:20.946: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:17:20.946: INFO: validating pod update-demo-nautilus-xcml5
Jan 29 09:17:20.953: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:17:20.953: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:17:20.953: INFO: update-demo-nautilus-xcml5 is verified up and running
STEP: using delete to clean up resources
Jan 29 09:17:20.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:21.103: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:17:21.103: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 29 09:17:21.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-qrgzg'
Jan 29 09:17:21.279: INFO: stderr: "No resources found.\n"
Jan 29 09:17:21.279: INFO: stdout: ""
Jan 29 09:17:21.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -l name=update-demo --namespace=e2e-tests-kubectl-qrgzg -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 09:17:21.483: INFO: stderr: ""
Jan 29 09:17:21.483: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:17:21.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qrgzg" for this suite.
Jan 29 09:17:43.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:17:43.611: INFO: namespace: e2e-tests-kubectl-qrgzg, resource: bindings, ignored listing per whitelist
Jan 29 09:17:43.619: INFO: namespace e2e-tests-kubectl-qrgzg deletion completed in 22.131080125s

â€¢ [SLOW TEST:29.404 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:17:43.621: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-l4cj4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-jsl9
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 09:17:43.850: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jsl9" in namespace "e2e-tests-subpath-l4cj4" to be "success or failure"
Jan 29 09:17:43.855: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.687127ms
Jan 29 09:17:45.859: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008005682s
Jan 29 09:17:47.862: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 4.011032757s
Jan 29 09:17:49.865: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 6.014157439s
Jan 29 09:17:51.868: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 8.017203757s
Jan 29 09:17:53.871: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 10.020160317s
Jan 29 09:17:55.879: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 12.028830109s
Jan 29 09:17:57.883: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 14.032695572s
Jan 29 09:17:59.886: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 16.035781886s
Jan 29 09:18:01.890: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 18.039571477s
Jan 29 09:18:03.894: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 20.043044404s
Jan 29 09:18:05.897: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Running", Reason="", readiness=false. Elapsed: 22.045975977s
Jan 29 09:18:07.900: INFO: Pod "pod-subpath-test-secret-jsl9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.049311926s
STEP: Saw pod success
Jan 29 09:18:07.900: INFO: Pod "pod-subpath-test-secret-jsl9" satisfied condition "success or failure"
Jan 29 09:18:07.903: INFO: Trying to get logs from node 9.111.254.123 pod pod-subpath-test-secret-jsl9 container test-container-subpath-secret-jsl9: <nil>
STEP: delete the pod
Jan 29 09:18:07.925: INFO: Waiting for pod pod-subpath-test-secret-jsl9 to disappear
Jan 29 09:18:07.932: INFO: Pod pod-subpath-test-secret-jsl9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-jsl9
Jan 29 09:18:07.932: INFO: Deleting pod "pod-subpath-test-secret-jsl9" in namespace "e2e-tests-subpath-l4cj4"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:18:07.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-l4cj4" for this suite.
Jan 29 09:18:13.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:18:14.108: INFO: namespace: e2e-tests-subpath-l4cj4, resource: bindings, ignored listing per whitelist
Jan 29 09:18:14.122: INFO: namespace e2e-tests-subpath-l4cj4 deletion completed in 6.180191777s

â€¢ [SLOW TEST:30.502 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:18:14.122: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jnzn7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-ce3bb741-23a6-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:18:14.339: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ce3c7a74-23a6-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-jnzn7" to be "success or failure"
Jan 29 09:18:14.346: INFO: Pod "pod-projected-secrets-ce3c7a74-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 6.905502ms
Jan 29 09:18:16.352: INFO: Pod "pod-projected-secrets-ce3c7a74-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013338694s
Jan 29 09:18:18.358: INFO: Pod "pod-projected-secrets-ce3c7a74-23a6-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018630783s
STEP: Saw pod success
Jan 29 09:18:18.358: INFO: Pod "pod-projected-secrets-ce3c7a74-23a6-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:18:18.361: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-secrets-ce3c7a74-23a6-11e9-9015-b2c9bac28373 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 09:18:18.397: INFO: Waiting for pod pod-projected-secrets-ce3c7a74-23a6-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:18:18.402: INFO: Pod pod-projected-secrets-ce3c7a74-23a6-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:18:18.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jnzn7" for this suite.
Jan 29 09:18:24.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:18:24.457: INFO: namespace: e2e-tests-projected-jnzn7, resource: bindings, ignored listing per whitelist
Jan 29 09:18:24.534: INFO: namespace e2e-tests-projected-jnzn7 deletion completed in 6.124296559s

â€¢ [SLOW TEST:10.412 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:18:24.535: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-n47wp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Jan 29 09:18:28.848: INFO: Pod pod-hostip-d4713981-23a6-11e9-9015-b2c9bac28373 has hostIP: 9.111.254.123
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:18:28.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-n47wp" for this suite.
Jan 29 09:18:50.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:18:50.924: INFO: namespace: e2e-tests-pods-n47wp, resource: bindings, ignored listing per whitelist
Jan 29 09:18:50.968: INFO: namespace e2e-tests-pods-n47wp deletion completed in 22.114808659s

â€¢ [SLOW TEST:26.433 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:18:50.969: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-6zr9x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:18:51.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4335887-23a6-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-6zr9x" to be "success or failure"
Jan 29 09:18:51.244: INFO: Pod "downwardapi-volume-e4335887-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.867154ms
Jan 29 09:18:53.247: INFO: Pod "downwardapi-volume-e4335887-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00539792s
Jan 29 09:18:55.250: INFO: Pod "downwardapi-volume-e4335887-23a6-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008649133s
STEP: Saw pod success
Jan 29 09:18:55.250: INFO: Pod "downwardapi-volume-e4335887-23a6-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:18:55.254: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-e4335887-23a6-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:18:55.281: INFO: Waiting for pod downwardapi-volume-e4335887-23a6-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:18:55.284: INFO: Pod downwardapi-volume-e4335887-23a6-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:18:55.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6zr9x" for this suite.
Jan 29 09:19:01.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:19:01.360: INFO: namespace: e2e-tests-downward-api-6zr9x, resource: bindings, ignored listing per whitelist
Jan 29 09:19:01.397: INFO: namespace e2e-tests-downward-api-6zr9x deletion completed in 6.109009914s

â€¢ [SLOW TEST:10.428 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:19:01.397: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-x7p6d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Jan 29 09:19:01.640: INFO: Waiting up to 5m0s for pod "var-expansion-ea689332-23a6-11e9-9015-b2c9bac28373" in namespace "e2e-tests-var-expansion-x7p6d" to be "success or failure"
Jan 29 09:19:01.642: INFO: Pod "var-expansion-ea689332-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327001ms
Jan 29 09:19:03.645: INFO: Pod "var-expansion-ea689332-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005212625s
Jan 29 09:19:05.649: INFO: Pod "var-expansion-ea689332-23a6-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009619694s
STEP: Saw pod success
Jan 29 09:19:05.649: INFO: Pod "var-expansion-ea689332-23a6-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:19:05.652: INFO: Trying to get logs from node 9.111.254.123 pod var-expansion-ea689332-23a6-11e9-9015-b2c9bac28373 container dapi-container: <nil>
STEP: delete the pod
Jan 29 09:19:05.680: INFO: Waiting for pod var-expansion-ea689332-23a6-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:19:05.683: INFO: Pod var-expansion-ea689332-23a6-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:19:05.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-x7p6d" for this suite.
Jan 29 09:19:11.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:19:11.738: INFO: namespace: e2e-tests-var-expansion-x7p6d, resource: bindings, ignored listing per whitelist
Jan 29 09:19:11.829: INFO: namespace e2e-tests-var-expansion-x7p6d deletion completed in 6.128620452s

â€¢ [SLOW TEST:10.431 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:19:11.829: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-6kfxp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 29 09:19:12.039: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:19:16.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-6kfxp" for this suite.
Jan 29 09:19:22.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:19:22.783: INFO: namespace: e2e-tests-init-container-6kfxp, resource: bindings, ignored listing per whitelist
Jan 29 09:19:22.819: INFO: namespace e2e-tests-init-container-6kfxp deletion completed in 6.116123855s

â€¢ [SLOW TEST:10.990 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:19:22.820: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-ntk8m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Jan 29 09:19:23.022: INFO: Waiting up to 5m0s for pod "client-containers-f72e1b32-23a6-11e9-9015-b2c9bac28373" in namespace "e2e-tests-containers-ntk8m" to be "success or failure"
Jan 29 09:19:23.026: INFO: Pod "client-containers-f72e1b32-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479943ms
Jan 29 09:19:25.029: INFO: Pod "client-containers-f72e1b32-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00647924s
Jan 29 09:19:27.032: INFO: Pod "client-containers-f72e1b32-23a6-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009922081s
STEP: Saw pod success
Jan 29 09:19:27.032: INFO: Pod "client-containers-f72e1b32-23a6-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:19:27.035: INFO: Trying to get logs from node 9.111.254.123 pod client-containers-f72e1b32-23a6-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:19:27.054: INFO: Waiting for pod client-containers-f72e1b32-23a6-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:19:27.060: INFO: Pod client-containers-f72e1b32-23a6-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:19:27.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-ntk8m" for this suite.
Jan 29 09:19:33.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:19:33.139: INFO: namespace: e2e-tests-containers-ntk8m, resource: bindings, ignored listing per whitelist
Jan 29 09:19:33.169: INFO: namespace e2e-tests-containers-ntk8m deletion completed in 6.102918835s

â€¢ [SLOW TEST:10.350 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:19:33.169: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-xfml5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-fd5972f7-23a6-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:19:33.443: INFO: Waiting up to 5m0s for pod "pod-secrets-fd5a1234-23a6-11e9-9015-b2c9bac28373" in namespace "e2e-tests-secrets-xfml5" to be "success or failure"
Jan 29 09:19:33.449: INFO: Pod "pod-secrets-fd5a1234-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 5.262973ms
Jan 29 09:19:35.452: INFO: Pod "pod-secrets-fd5a1234-23a6-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008600385s
Jan 29 09:19:37.455: INFO: Pod "pod-secrets-fd5a1234-23a6-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011264002s
STEP: Saw pod success
Jan 29 09:19:37.455: INFO: Pod "pod-secrets-fd5a1234-23a6-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:19:37.460: INFO: Trying to get logs from node 9.111.254.123 pod pod-secrets-fd5a1234-23a6-11e9-9015-b2c9bac28373 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 09:19:37.477: INFO: Waiting for pod pod-secrets-fd5a1234-23a6-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:19:37.480: INFO: Pod pod-secrets-fd5a1234-23a6-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:19:37.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xfml5" for this suite.
Jan 29 09:19:43.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:19:43.549: INFO: namespace: e2e-tests-secrets-xfml5, resource: bindings, ignored listing per whitelist
Jan 29 09:19:43.589: INFO: namespace e2e-tests-secrets-xfml5 deletion completed in 6.10351786s

â€¢ [SLOW TEST:10.420 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:19:43.589: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-jqmpf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Jan 29 09:19:43.784: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 09:19:43.798: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 09:19:43.802: INFO: 
Logging pods the kubelet thinks is on node 9.111.254.104 before test
Jan 29 09:19:43.820: INFO: monitoring-prometheus-7b5f7db489-9jxtd from kube-system started at 2019-01-24 02:48:48 +0000 UTC (4 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container alert-rule-controller ready: true, restart count 0
Jan 29 09:19:43.820: INFO: 	Container configmap-reload-prometheus ready: true, restart count 0
Jan 29 09:19:43.820: INFO: 	Container prometheus ready: true, restart count 0
Jan 29 09:19:43.820: INFO: 	Container router ready: true, restart count 1
Jan 29 09:19:43.820: INFO: audit-logging-fluentd-ds-2xjm5 from kube-system started at 2019-01-24 02:51:19 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container fluentd ready: true, restart count 0
Jan 29 09:19:43.820: INFO: key-management-pep-6d7f685bf4-rnd82 from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container key-management-pep ready: true, restart count 0
Jan 29 09:19:43.820: INFO: key-management-crypto-f4b6444bc-zspwm from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container key-management-crypto ready: true, restart count 0
Jan 29 09:19:43.820: INFO: k8s-proxy-9.111.254.104 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:19:43.820: INFO: web-terminal-7776f9df5b-q6mlp from kube-system started at 2019-01-24 02:40:36 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container web-terminal ready: true, restart count 0
Jan 29 09:19:43.820: INFO: key-management-lifecycle-69b67d57c5-q5pvm from kube-system started at 2019-01-24 02:52:04 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:19:43.820: INFO: 	Container key-management-lifecycle ready: true, restart count 0
Jan 29 09:19:43.820: INFO: logging-elk-data-0 from kube-system started at 2019-01-24 02:41:33 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container es-data ready: true, restart count 0
Jan 29 09:19:43.820: INFO: calico-node-4z7zs from kube-system started at 2019-01-24 02:27:00 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 09:19:43.820: INFO: 	Container install-cni ready: true, restart count 0
Jan 29 09:19:43.820: INFO: logging-elk-master-c47cb5549-n5cmk from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container es-master ready: true, restart count 0
Jan 29 09:19:43.820: INFO: monitoring-prometheus-nodeexporter-hq52h from kube-system started at 2019-01-24 02:48:47 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.820: INFO: 	Container nodeexporter ready: true, restart count 0
Jan 29 09:19:43.820: INFO: 	Container router ready: true, restart count 0
Jan 29 09:19:43.821: INFO: monitoring-grafana-7d5d7f5c45-58ttx from kube-system started at 2019-01-24 02:48:48 +0000 UTC (3 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container dashboard-crd-controller ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 	Container grafana ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 	Container router ready: true, restart count 0
Jan 29 09:19:43.821: INFO: logging-elk-elasticsearch-curator-1548631800-jp4mg from kube-system started at 2019-01-27 23:30:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container curator ready: false, restart count 0
Jan 29 09:19:43.821: INFO: logging-elk-filebeat-ds-9mbdx from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container filebeat ready: true, restart count 0
Jan 29 09:19:43.821: INFO: custom-metrics-adapter-77c5d9c6ff-9lg44 from kube-system started at 2019-01-24 02:47:21 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container custom-metrics-adapter ready: true, restart count 2
Jan 29 09:19:43.821: INFO: nvidia-device-plugin-cjt74 from kube-system started at 2019-01-24 02:28:55 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
Jan 29 09:19:43.821: INFO: metrics-server-5bcdd5579-9v9qd from kube-system started at 2019-01-24 02:32:06 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container metrics-server ready: true, restart count 3
Jan 29 09:19:43.821: INFO: sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-zg67k from heptio-sonobuoy started at 2019-01-29 08:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 09:19:43.821: INFO: logging-elk-logstash-5444dd6fb4-6p7jm from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container logstash ready: true, restart count 0
Jan 29 09:19:43.821: INFO: monitoring-prometheus-kubestatemetrics-9fcddf7d8-jw5xn from kube-system started at 2019-01-24 02:48:48 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container kubestatemetrics ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 	Container router ready: true, restart count 0
Jan 29 09:19:43.821: INFO: key-management-persistence-656b774f7-f2kjc from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container key-management-persistence ready: true, restart count 0
Jan 29 09:19:43.821: INFO: logging-elk-client-559db59464-ps9rr from kube-system started at 2019-01-24 02:41:32 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container es-client ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 	Container router ready: true, restart count 0
Jan 29 09:19:43.821: INFO: monitoring-prometheus-elasticsearchexporter-786dbc957b-lrzfq from kube-system started at 2019-01-24 02:48:48 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container elasticsearchexporter ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 	Container router ready: true, restart count 0
Jan 29 09:19:43.821: INFO: monitoring-prometheus-alertmanager-86b46f4f8f-zlwzh from kube-system started at 2019-01-24 02:48:48 +0000 UTC (3 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container alertmanager ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 	Container configmap-reload ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 	Container router ready: true, restart count 0
Jan 29 09:19:43.821: INFO: key-management-api-85fb4b5f55-pckqx from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container key-management-api ready: true, restart count 0
Jan 29 09:19:43.821: INFO: logging-elk-elasticsearch-curator-1548718200-clnkm from kube-system started at 2019-01-28 23:30:08 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container curator ready: false, restart count 0
Jan 29 09:19:43.821: INFO: monitoring-prometheus-collectdexporter-698694c4bf-z4b44 from kube-system started at 2019-01-24 02:48:48 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.821: INFO: 	Container collectd-exporter ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 	Container router ready: true, restart count 0
Jan 29 09:19:43.821: INFO: 
Logging pods the kubelet thinks is on node 9.111.254.123 before test
Jan 29 09:19:43.849: INFO: helm-repo-6c4f6cc8-88q9m from kube-system started at 2019-01-24 02:43:40 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.849: INFO: 	Container helm-repo ready: true, restart count 0
Jan 29 09:19:43.849: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:19:43.849: INFO: audit-logging-fluentd-ds-74lnc from kube-system started at 2019-01-24 02:51:19 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.849: INFO: 	Container fluentd ready: true, restart count 0
Jan 29 09:19:43.849: INFO: image-manager-0 from kube-system started at 2019-01-24 02:21:39 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.849: INFO: 	Container icp-registry ready: true, restart count 0
Jan 29 09:19:43.849: INFO: 	Container image-manager ready: true, restart count 0
Jan 29 09:19:43.849: INFO: tiller-deploy-59766f75bd-hsznz from kube-system started at 2019-01-24 02:26:09 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.849: INFO: 	Container tiller ready: true, restart count 0
Jan 29 09:19:43.849: INFO: nvidia-device-plugin-db75j from kube-system started at 2019-01-24 02:28:55 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.849: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
Jan 29 09:19:43.849: INFO: service-catalog-apiserver-7lmpb from kube-system started at 2019-01-24 02:30:23 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.849: INFO: 	Container apiserver ready: true, restart count 0
Jan 29 09:19:43.849: INFO: logging-elk-filebeat-ds-cl7nq from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.849: INFO: 	Container filebeat ready: true, restart count 0
Jan 29 09:19:43.849: INFO: ibmcloud-image-enforcement-596b54f688-xptnx from kube-system started at 2019-01-24 02:43:04 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.849: INFO: 	Container ibmcloud-image-enforcement ready: true, restart count 0
Jan 29 09:19:43.849: INFO: dns-test-x from default started at 2019-01-29 03:23:19 +0000 UTC (3 container statuses recorded)
Jan 29 09:19:43.849: INFO: 	Container jessie-querier ready: true, restart count 8
Jan 29 09:19:43.849: INFO: 	Container querier ready: true, restart count 9
Jan 29 09:19:43.849: INFO: 	Container webserver ready: true, restart count 0
Jan 29 09:19:43.849: INFO: calico-node-mfg2r from kube-system started at 2019-01-24 02:27:00 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 09:19:43.850: INFO: 	Container install-cni ready: true, restart count 0
Jan 29 09:19:43.850: INFO: icp-mongodb-0 from kube-system started at 2019-01-24 02:29:56 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container icp-mongodb ready: true, restart count 0
Jan 29 09:19:43.850: INFO: platform-api-74f4558f76-bp48c from kube-system started at 2019-01-24 02:31:41 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container audit-service ready: true, restart count 0
Jan 29 09:19:43.850: INFO: 	Container platform-api ready: true, restart count 0
Jan 29 09:19:43.850: INFO: icp-management-ingress-5bnwr from kube-system started at 2019-01-24 02:34:40 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container icp-management-ingress ready: true, restart count 0
Jan 29 09:19:43.850: INFO: k8s-proxy-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:19:43.850: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-29 08:47:51 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 09:19:43.850: INFO: mgmt-repo-54f58b9b5c-55qlz from kube-system started at 2019-01-24 02:46:04 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:19:43.850: INFO: 	Container mgmt-repo ready: true, restart count 0
Jan 29 09:19:43.850: INFO: unified-router-rnf9h from kube-system started at 2019-01-24 02:49:45 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container unified-router ready: true, restart count 0
Jan 29 09:19:43.850: INFO: k8s-kmsplugin-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:19:43.850: INFO: nginx-ingress-controller-ghbl2 from kube-system started at 2019-01-24 02:30:49 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 29 09:19:43.850: INFO: monitoring-prometheus-nodeexporter-h889z from kube-system started at 2019-01-24 02:48:47 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container nodeexporter ready: true, restart count 0
Jan 29 09:19:43.850: INFO: 	Container router ready: true, restart count 0
Jan 29 09:19:43.850: INFO: heapster-6fb4fbb79b-dz5nf from kube-system started at 2019-01-24 02:42:38 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container heapster ready: true, restart count 0
Jan 29 09:19:43.850: INFO: catalog-ui-k8wf5 from kube-system started at 2019-01-24 02:43:56 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container catalog-ui ready: true, restart count 0
Jan 29 09:19:43.850: INFO: kube-dns-sbds7 from kube-system started at 2019-01-29 08:45:54 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container coredns ready: true, restart count 0
Jan 29 09:19:43.850: INFO: k8s-etcd-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:19:43.850: INFO: service-catalog-controller-manager-6976ff4bc6-rl78n from kube-system started at 2019-01-24 02:30:23 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container controller-manager ready: true, restart count 3
Jan 29 09:19:43.850: INFO: default-http-backend-7c4747d8d5-kzlz4 from kube-system started at 2019-01-24 02:30:49 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container default-http-backend ready: true, restart count 0
Jan 29 09:19:43.850: INFO: auth-apikeys-fmgd5 from kube-system started at 2019-01-24 02:33:21 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container auth-apikeys ready: true, restart count 0
Jan 29 09:19:43.850: INFO: calico-kube-controllers-58548fb4db-wqdxb from kube-system started at 2019-01-24 02:27:00 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 29 09:19:43.850: INFO: sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-2zbf2 from heptio-sonobuoy started at 2019-01-29 08:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 29 09:19:43.850: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 09:19:43.850: INFO: mariadb-0 from kube-system started at 2019-01-24 02:31:16 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container mariadb ready: true, restart count 0
Jan 29 09:19:43.850: INFO: auth-idp-2lvnk from kube-system started at 2019-01-24 02:32:54 +0000 UTC (4 container statuses recorded)
Jan 29 09:19:43.850: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:19:43.850: INFO: 	Container platform-auth-service ready: true, restart count 1
Jan 29 09:19:43.850: INFO: 	Container platform-identity-manager ready: true, restart count 0
Jan 29 09:19:43.850: INFO: 	Container platform-identity-provider ready: true, restart count 0
Jan 29 09:19:43.850: INFO: ibm-cert-manager-cert-manager-69f9b77599-5vw2w from cert-manager started at 2019-01-24 02:28:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.851: INFO: 	Container ibm-cert-manager ready: true, restart count 1
Jan 29 09:19:43.851: INFO: auth-pdp-2mhhx from kube-system started at 2019-01-24 02:34:14 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.851: INFO: 	Container auth-pdp ready: true, restart count 0
Jan 29 09:19:43.851: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:19:43.851: INFO: helm-api-66d8997947-whpb9 from kube-system started at 2019-01-24 02:45:03 +0000 UTC (3 container statuses recorded)
Jan 29 09:19:43.851: INFO: 	Container helmapi ready: true, restart count 0
Jan 29 09:19:43.851: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:19:43.851: INFO: 	Container rudder ready: true, restart count 0
Jan 29 09:19:43.851: INFO: sonobuoy-e2e-job-5dd02bc5a9524b35 from heptio-sonobuoy started at 2019-01-29 08:47:59 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.851: INFO: 	Container e2e ready: true, restart count 0
Jan 29 09:19:43.851: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 09:19:43.851: INFO: k8s-master-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:19:43.851: INFO: platform-deploy-665cd77468-m9l7c from kube-system started at 2019-01-24 02:31:41 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.851: INFO: 	Container platform-deploy ready: true, restart count 0
Jan 29 09:19:43.851: INFO: auth-pap-8pdg7 from kube-system started at 2019-01-24 02:33:47 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.851: INFO: 	Container auth-pap ready: true, restart count 0
Jan 29 09:19:43.851: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:19:43.851: INFO: platform-ui-kl4bl from kube-system started at 2019-01-24 02:46:41 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.851: INFO: 	Container platform-ui ready: true, restart count 0
Jan 29 09:19:43.851: INFO: 
Logging pods the kubelet thinks is on node 9.111.255.33 before test
Jan 29 09:19:43.865: INFO: logging-elk-filebeat-ds-9qmnw from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.865: INFO: 	Container filebeat ready: true, restart count 0
Jan 29 09:19:43.865: INFO: monitoring-prometheus-nodeexporter-58gsq from kube-system started at 2019-01-24 02:48:47 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.865: INFO: 	Container nodeexporter ready: true, restart count 0
Jan 29 09:19:43.865: INFO: 	Container router ready: true, restart count 0
Jan 29 09:19:43.865: INFO: audit-logging-fluentd-ds-g4kfl from kube-system started at 2019-01-24 02:51:19 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.865: INFO: 	Container fluentd ready: true, restart count 0
Jan 29 09:19:43.865: INFO: k8s-proxy-9.111.255.33 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:19:43.865: INFO: calico-node-rq54l from kube-system started at 2019-01-24 02:27:00 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.865: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 09:19:43.865: INFO: 	Container install-cni ready: true, restart count 0
Jan 29 09:19:43.865: INFO: nvidia-device-plugin-kg6z8 from kube-system started at 2019-01-24 02:28:55 +0000 UTC (1 container statuses recorded)
Jan 29 09:19:43.865: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
Jan 29 09:19:43.865: INFO: sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-bz5g5 from heptio-sonobuoy started at 2019-01-29 08:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 09:19:43.865: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 29 09:19:43.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.157e472dcf574516], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:19:44.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-jqmpf" for this suite.
Jan 29 09:19:50.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:19:51.043: INFO: namespace: e2e-tests-sched-pred-jqmpf, resource: bindings, ignored listing per whitelist
Jan 29 09:19:51.090: INFO: namespace e2e-tests-sched-pred-jqmpf deletion completed in 6.140646757s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

â€¢ [SLOW TEST:7.500 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:19:51.090: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-6fh29
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Jan 29 09:19:51.337: INFO: Waiting up to 5m0s for pod "var-expansion-0808ae5c-23a7-11e9-9015-b2c9bac28373" in namespace "e2e-tests-var-expansion-6fh29" to be "success or failure"
Jan 29 09:19:51.340: INFO: Pod "var-expansion-0808ae5c-23a7-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.35863ms
Jan 29 09:19:53.343: INFO: Pod "var-expansion-0808ae5c-23a7-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005865326s
Jan 29 09:19:55.347: INFO: Pod "var-expansion-0808ae5c-23a7-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009738328s
STEP: Saw pod success
Jan 29 09:19:55.347: INFO: Pod "var-expansion-0808ae5c-23a7-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:19:55.349: INFO: Trying to get logs from node 9.111.254.123 pod var-expansion-0808ae5c-23a7-11e9-9015-b2c9bac28373 container dapi-container: <nil>
STEP: delete the pod
Jan 29 09:19:55.366: INFO: Waiting for pod var-expansion-0808ae5c-23a7-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:19:55.370: INFO: Pod var-expansion-0808ae5c-23a7-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:19:55.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-6fh29" for this suite.
Jan 29 09:20:01.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:20:01.453: INFO: namespace: e2e-tests-var-expansion-6fh29, resource: bindings, ignored listing per whitelist
Jan 29 09:20:01.521: INFO: namespace e2e-tests-var-expansion-6fh29 deletion completed in 6.145128207s

â€¢ [SLOW TEST:10.431 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:20:01.521: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-9shnm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-9shnm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9shnm to expose endpoints map[]
Jan 29 09:20:01.723: INFO: Get endpoints failed (4.554468ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jan 29 09:20:02.725: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9shnm exposes endpoints map[] (1.006774422s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-9shnm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9shnm to expose endpoints map[pod1:[80]]
Jan 29 09:20:04.868: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9shnm exposes endpoints map[pod1:[80]] (2.030776626s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-9shnm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9shnm to expose endpoints map[pod1:[80] pod2:[80]]
Jan 29 09:20:07.967: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9shnm exposes endpoints map[pod1:[80] pod2:[80]] (3.033094789s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-9shnm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9shnm to expose endpoints map[pod2:[80]]
Jan 29 09:20:08.992: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9shnm exposes endpoints map[pod2:[80]] (1.018734541s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-9shnm
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-9shnm to expose endpoints map[]
Jan 29 09:20:09.004: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-9shnm exposes endpoints map[] (2.571103ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:20:09.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-9shnm" for this suite.
Jan 29 09:20:15.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:20:15.072: INFO: namespace: e2e-tests-services-9shnm, resource: bindings, ignored listing per whitelist
Jan 29 09:20:15.141: INFO: namespace e2e-tests-services-9shnm deletion completed in 6.111420052s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

â€¢ [SLOW TEST:13.620 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:20:15.142: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-wsfxq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Jan 29 09:20:15.435: INFO: Waiting up to 5m0s for pod "client-containers-165d7714-23a7-11e9-9015-b2c9bac28373" in namespace "e2e-tests-containers-wsfxq" to be "success or failure"
Jan 29 09:20:15.437: INFO: Pod "client-containers-165d7714-23a7-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580011ms
Jan 29 09:20:17.445: INFO: Pod "client-containers-165d7714-23a7-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010353607s
STEP: Saw pod success
Jan 29 09:20:17.445: INFO: Pod "client-containers-165d7714-23a7-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:20:17.448: INFO: Trying to get logs from node 9.111.254.123 pod client-containers-165d7714-23a7-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:20:17.472: INFO: Waiting for pod client-containers-165d7714-23a7-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:20:17.476: INFO: Pod client-containers-165d7714-23a7-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:20:17.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wsfxq" for this suite.
Jan 29 09:20:23.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:20:23.505: INFO: namespace: e2e-tests-containers-wsfxq, resource: bindings, ignored listing per whitelist
Jan 29 09:20:23.589: INFO: namespace e2e-tests-containers-wsfxq deletion completed in 6.108289124s

â€¢ [SLOW TEST:8.447 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:20:23.590: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-v82rn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 29 09:20:23.834: INFO: Waiting up to 5m0s for pod "pod-1b669107-23a7-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-v82rn" to be "success or failure"
Jan 29 09:20:23.837: INFO: Pod "pod-1b669107-23a7-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.032738ms
Jan 29 09:20:25.843: INFO: Pod "pod-1b669107-23a7-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009182064s
STEP: Saw pod success
Jan 29 09:20:25.843: INFO: Pod "pod-1b669107-23a7-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:20:25.849: INFO: Trying to get logs from node 9.111.254.123 pod pod-1b669107-23a7-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:20:25.884: INFO: Waiting for pod pod-1b669107-23a7-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:20:25.891: INFO: Pod pod-1b669107-23a7-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:20:25.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-v82rn" for this suite.
Jan 29 09:20:31.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:20:31.982: INFO: namespace: e2e-tests-emptydir-v82rn, resource: bindings, ignored listing per whitelist
Jan 29 09:20:32.032: INFO: namespace e2e-tests-emptydir-v82rn deletion completed in 6.132881497s

â€¢ [SLOW TEST:8.442 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:20:32.033: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8c9nf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 29 09:20:32.237: INFO: Waiting up to 5m0s for pod "downward-api-206f010c-23a7-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-8c9nf" to be "success or failure"
Jan 29 09:20:32.249: INFO: Pod "downward-api-206f010c-23a7-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 11.519902ms
Jan 29 09:20:34.256: INFO: Pod "downward-api-206f010c-23a7-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01901876s
STEP: Saw pod success
Jan 29 09:20:34.256: INFO: Pod "downward-api-206f010c-23a7-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:20:34.260: INFO: Trying to get logs from node 9.111.254.123 pod downward-api-206f010c-23a7-11e9-9015-b2c9bac28373 container dapi-container: <nil>
STEP: delete the pod
Jan 29 09:20:34.287: INFO: Waiting for pod downward-api-206f010c-23a7-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:20:34.290: INFO: Pod downward-api-206f010c-23a7-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:20:34.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8c9nf" for this suite.
Jan 29 09:20:40.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:20:40.368: INFO: namespace: e2e-tests-downward-api-8c9nf, resource: bindings, ignored listing per whitelist
Jan 29 09:20:40.459: INFO: namespace e2e-tests-downward-api-8c9nf deletion completed in 6.165287714s

â€¢ [SLOW TEST:8.426 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:20:40.460: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-7wwh9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 29 09:20:44.849: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 09:20:44.851: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 09:20:46.852: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 09:20:46.858: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 09:20:48.852: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 09:20:48.856: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 09:20:50.852: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 09:20:50.855: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:20:50.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-7wwh9" for this suite.
Jan 29 09:21:12.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:21:13.006: INFO: namespace: e2e-tests-container-lifecycle-hook-7wwh9, resource: bindings, ignored listing per whitelist
Jan 29 09:21:13.025: INFO: namespace e2e-tests-container-lifecycle-hook-7wwh9 deletion completed in 22.157987302s

â€¢ [SLOW TEST:32.566 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:21:13.025: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-8p49t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-8p49t
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-8p49t
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-8p49t
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-8p49t
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-8p49t
Jan 29 09:21:17.354: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-8p49t, name: ss-0, uid: 3ad1fa5b-23a7-11e9-a926-eeeeeeeeeeee, status phase: Pending. Waiting for statefulset controller to delete.
Jan 29 09:21:17.475: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-8p49t, name: ss-0, uid: 3ad1fa5b-23a7-11e9-a926-eeeeeeeeeeee, status phase: Failed. Waiting for statefulset controller to delete.
Jan 29 09:21:17.484: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-8p49t, name: ss-0, uid: 3ad1fa5b-23a7-11e9-a926-eeeeeeeeeeee, status phase: Failed. Waiting for statefulset controller to delete.
Jan 29 09:21:17.489: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-8p49t
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-8p49t
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-8p49t and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 29 09:21:23.514: INFO: Deleting all statefulset in ns e2e-tests-statefulset-8p49t
Jan 29 09:21:23.518: INFO: Scaling statefulset ss to 0
Jan 29 09:21:33.535: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 09:21:33.537: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:21:33.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-8p49t" for this suite.
Jan 29 09:21:39.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:21:39.664: INFO: namespace: e2e-tests-statefulset-8p49t, resource: bindings, ignored listing per whitelist
Jan 29 09:21:39.721: INFO: namespace e2e-tests-statefulset-8p49t deletion completed in 6.165576196s

â€¢ [SLOW TEST:26.696 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:21:39.722: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-db9zj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Jan 29 09:21:40.826: INFO: Waiting up to 5m0s for pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-km4sn" in namespace "e2e-tests-svcaccounts-db9zj" to be "success or failure"
Jan 29 09:21:40.830: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-km4sn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.904348ms
Jan 29 09:21:42.834: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-km4sn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007879895s
Jan 29 09:21:44.838: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-km4sn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011683325s
STEP: Saw pod success
Jan 29 09:21:44.838: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-km4sn" satisfied condition "success or failure"
Jan 29 09:21:44.842: INFO: Trying to get logs from node 9.111.254.123 pod pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-km4sn container token-test: <nil>
STEP: delete the pod
Jan 29 09:21:44.861: INFO: Waiting for pod pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-km4sn to disappear
Jan 29 09:21:44.864: INFO: Pod pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-km4sn no longer exists
STEP: Creating a pod to test consume service account root CA
Jan 29 09:21:45.336: INFO: Waiting up to 5m0s for pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-hjvk6" in namespace "e2e-tests-svcaccounts-db9zj" to be "success or failure"
Jan 29 09:21:45.339: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-hjvk6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.559717ms
Jan 29 09:21:47.342: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-hjvk6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006759365s
Jan 29 09:21:49.345: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-hjvk6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009648991s
STEP: Saw pod success
Jan 29 09:21:49.345: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-hjvk6" satisfied condition "success or failure"
Jan 29 09:21:49.347: INFO: Trying to get logs from node 9.111.254.123 pod pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-hjvk6 container root-ca-test: <nil>
STEP: delete the pod
Jan 29 09:21:49.366: INFO: Waiting for pod pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-hjvk6 to disappear
Jan 29 09:21:49.369: INFO: Pod pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-hjvk6 no longer exists
STEP: Creating a pod to test consume service account namespace
Jan 29 09:21:49.777: INFO: Waiting up to 5m0s for pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-rrhwd" in namespace "e2e-tests-svcaccounts-db9zj" to be "success or failure"
Jan 29 09:21:49.780: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-rrhwd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895854ms
Jan 29 09:21:51.783: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-rrhwd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006039601s
Jan 29 09:21:53.786: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-rrhwd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009176507s
STEP: Saw pod success
Jan 29 09:21:53.786: INFO: Pod "pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-rrhwd" satisfied condition "success or failure"
Jan 29 09:21:53.790: INFO: Trying to get logs from node 9.111.254.123 pod pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-rrhwd container namespace-test: <nil>
STEP: delete the pod
Jan 29 09:21:53.807: INFO: Waiting for pod pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-rrhwd to disappear
Jan 29 09:21:53.810: INFO: Pod pod-service-account-4914fdfc-23a7-11e9-9015-b2c9bac28373-rrhwd no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:21:53.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-db9zj" for this suite.
Jan 29 09:21:59.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:21:59.950: INFO: namespace: e2e-tests-svcaccounts-db9zj, resource: bindings, ignored listing per whitelist
Jan 29 09:22:00.024: INFO: namespace e2e-tests-svcaccounts-db9zj deletion completed in 6.208102584s

â€¢ [SLOW TEST:20.302 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:22:00.024: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-m9vkj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0129 09:22:06.369339      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 09:22:06.369: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:22:06.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-m9vkj" for this suite.
Jan 29 09:22:12.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:22:12.497: INFO: namespace: e2e-tests-gc-m9vkj, resource: bindings, ignored listing per whitelist
Jan 29 09:22:12.514: INFO: namespace e2e-tests-gc-m9vkj deletion completed in 6.140736856s

â€¢ [SLOW TEST:12.490 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:22:12.514: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-8vj6c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 29 09:22:12.741: INFO: Waiting up to 5m0s for pod "downward-api-5c558e85-23a7-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-8vj6c" to be "success or failure"
Jan 29 09:22:12.745: INFO: Pod "downward-api-5c558e85-23a7-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188491ms
Jan 29 09:22:14.757: INFO: Pod "downward-api-5c558e85-23a7-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015645248s
STEP: Saw pod success
Jan 29 09:22:14.757: INFO: Pod "downward-api-5c558e85-23a7-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:22:14.760: INFO: Trying to get logs from node 9.111.254.123 pod downward-api-5c558e85-23a7-11e9-9015-b2c9bac28373 container dapi-container: <nil>
STEP: delete the pod
Jan 29 09:22:14.791: INFO: Waiting for pod downward-api-5c558e85-23a7-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:22:14.798: INFO: Pod downward-api-5c558e85-23a7-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:22:14.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-8vj6c" for this suite.
Jan 29 09:22:20.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:22:20.934: INFO: namespace: e2e-tests-downward-api-8vj6c, resource: bindings, ignored listing per whitelist
Jan 29 09:22:20.986: INFO: namespace e2e-tests-downward-api-8vj6c deletion completed in 6.180863728s

â€¢ [SLOW TEST:8.473 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:22:20.987: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-8w8pf
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-616448dc-23a7-11e9-9015-b2c9bac28373
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-616448dc-23a7-11e9-9015-b2c9bac28373
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:22:25.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8w8pf" for this suite.
Jan 29 09:22:47.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:22:47.389: INFO: namespace: e2e-tests-configmap-8w8pf, resource: bindings, ignored listing per whitelist
Jan 29 09:22:47.416: INFO: namespace e2e-tests-configmap-8w8pf deletion completed in 22.135209761s

â€¢ [SLOW TEST:26.429 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:22:47.416: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-fgc55
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 29 09:22:47.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-fgc55'
Jan 29 09:22:47.857: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Jan 29 09:22:47.857: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
Jan 29 09:22:47.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-fgc55'
Jan 29 09:22:48.006: INFO: stderr: ""
Jan 29 09:22:48.006: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:22:48.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fgc55" for this suite.
Jan 29 09:23:10.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:23:10.160: INFO: namespace: e2e-tests-kubectl-fgc55, resource: bindings, ignored listing per whitelist
Jan 29 09:23:10.163: INFO: namespace e2e-tests-kubectl-fgc55 deletion completed in 22.15392902s

â€¢ [SLOW TEST:22.747 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:23:10.164: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-j2ws8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-7eafc7b9-23a7-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:23:10.436: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7eb09ab9-23a7-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-j2ws8" to be "success or failure"
Jan 29 09:23:10.439: INFO: Pod "pod-projected-secrets-7eb09ab9-23a7-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.270002ms
Jan 29 09:23:12.442: INFO: Pod "pod-projected-secrets-7eb09ab9-23a7-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00585025s
STEP: Saw pod success
Jan 29 09:23:12.442: INFO: Pod "pod-projected-secrets-7eb09ab9-23a7-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:23:12.445: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-secrets-7eb09ab9-23a7-11e9-9015-b2c9bac28373 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 09:23:12.466: INFO: Waiting for pod pod-projected-secrets-7eb09ab9-23a7-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:23:12.470: INFO: Pod pod-projected-secrets-7eb09ab9-23a7-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:23:12.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j2ws8" for this suite.
Jan 29 09:23:18.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:23:18.542: INFO: namespace: e2e-tests-projected-j2ws8, resource: bindings, ignored listing per whitelist
Jan 29 09:23:18.591: INFO: namespace e2e-tests-projected-j2ws8 deletion completed in 6.113955437s

â€¢ [SLOW TEST:8.428 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:23:18.592: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-dknq8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-5t4bg in namespace e2e-tests-proxy-dknq8
I0129 09:23:18.935329      19 runners.go:180] Created replication controller with name: proxy-service-5t4bg, namespace: e2e-tests-proxy-dknq8, replica count: 1
I0129 09:23:19.988739      19 runners.go:180] proxy-service-5t4bg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 09:23:20.989076      19 runners.go:180] proxy-service-5t4bg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 09:23:21.989331      19 runners.go:180] proxy-service-5t4bg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 09:23:22.989530      19 runners.go:180] proxy-service-5t4bg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 09:23:23.989706      19 runners.go:180] proxy-service-5t4bg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 09:23:24.989970      19 runners.go:180] proxy-service-5t4bg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 09:23:24.995: INFO: setup took 6.180344019s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 29 09:23:25.003: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 7.535562ms)
Jan 29 09:23:25.003: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 6.052117ms)
Jan 29 09:23:25.005: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 9.893064ms)
Jan 29 09:23:25.008: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 11.218206ms)
Jan 29 09:23:25.009: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 12.655155ms)
Jan 29 09:23:25.009: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 13.051944ms)
Jan 29 09:23:25.010: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 15.192595ms)
Jan 29 09:23:25.018: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 22.497223ms)
Jan 29 09:23:25.020: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 23.503864ms)
Jan 29 09:23:25.023: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 28.021054ms)
Jan 29 09:23:25.023: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 26.36536ms)
Jan 29 09:23:25.023: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 26.753157ms)
Jan 29 09:23:25.023: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 27.284023ms)
Jan 29 09:23:25.024: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 26.90413ms)
Jan 29 09:23:25.025: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 28.985086ms)
Jan 29 09:23:25.033: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 37.692705ms)
Jan 29 09:23:25.039: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 5.211241ms)
Jan 29 09:23:25.048: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 11.636146ms)
Jan 29 09:23:25.048: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 14.697049ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 15.285716ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 14.741382ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 15.25051ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 15.124422ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 15.676204ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 15.262158ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 12.096964ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 12.021423ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 14.902261ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 11.978986ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 15.122107ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 14.925173ms)
Jan 29 09:23:25.049: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 12.05342ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 13.436828ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 12.750861ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 13.56243ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 14.417484ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 13.677712ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 14.354674ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 14.120952ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 14.639491ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 13.826195ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 13.00665ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 14.012781ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 14.097391ms)
Jan 29 09:23:25.064: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 14.554898ms)
Jan 29 09:23:25.067: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 16.912062ms)
Jan 29 09:23:25.067: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 17.272207ms)
Jan 29 09:23:25.068: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 16.948534ms)
Jan 29 09:23:25.080: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 11.356538ms)
Jan 29 09:23:25.082: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 13.273089ms)
Jan 29 09:23:25.084: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 15.217796ms)
Jan 29 09:23:25.084: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 15.416208ms)
Jan 29 09:23:25.085: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 15.53612ms)
Jan 29 09:23:25.085: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 16.691254ms)
Jan 29 09:23:25.086: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 16.698652ms)
Jan 29 09:23:25.086: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 16.741893ms)
Jan 29 09:23:25.086: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 16.715243ms)
Jan 29 09:23:25.086: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 16.817034ms)
Jan 29 09:23:25.088: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 19.344842ms)
Jan 29 09:23:25.089: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 20.335635ms)
Jan 29 09:23:25.089: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 20.317926ms)
Jan 29 09:23:25.089: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 20.414634ms)
Jan 29 09:23:25.089: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 20.809888ms)
Jan 29 09:23:25.090: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 21.324139ms)
Jan 29 09:23:25.103: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 10.362756ms)
Jan 29 09:23:25.104: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 11.365767ms)
Jan 29 09:23:25.104: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 11.428304ms)
Jan 29 09:23:25.104: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 11.712319ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 14.412539ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 12.889429ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 13.065245ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 13.297086ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 13.144408ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 13.688033ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 13.612203ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 13.587754ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 13.801339ms)
Jan 29 09:23:25.106: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 13.888445ms)
Jan 29 09:23:25.107: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 14.40168ms)
Jan 29 09:23:25.108: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 14.968158ms)
Jan 29 09:23:25.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 11.838747ms)
Jan 29 09:23:25.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 11.767617ms)
Jan 29 09:23:25.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 12.04258ms)
Jan 29 09:23:25.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 11.93569ms)
Jan 29 09:23:25.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 11.818947ms)
Jan 29 09:23:25.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 11.809814ms)
Jan 29 09:23:25.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 12.195344ms)
Jan 29 09:23:25.120: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 12.430982ms)
Jan 29 09:23:25.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 14.475998ms)
Jan 29 09:23:25.122: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 14.321781ms)
Jan 29 09:23:25.132: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 24.520251ms)
Jan 29 09:23:25.132: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 24.252932ms)
Jan 29 09:23:25.132: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 24.05937ms)
Jan 29 09:23:25.132: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 24.109393ms)
Jan 29 09:23:25.132: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 24.41901ms)
Jan 29 09:23:25.132: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 24.184578ms)
Jan 29 09:23:25.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 4.905146ms)
Jan 29 09:23:25.138: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 5.566628ms)
Jan 29 09:23:25.139: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 5.408728ms)
Jan 29 09:23:25.140: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 6.311914ms)
Jan 29 09:23:25.143: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 9.669504ms)
Jan 29 09:23:25.143: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 9.78969ms)
Jan 29 09:23:25.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 11.761997ms)
Jan 29 09:23:25.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 11.656615ms)
Jan 29 09:23:25.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 12.011306ms)
Jan 29 09:23:25.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 12.494602ms)
Jan 29 09:23:25.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 11.639527ms)
Jan 29 09:23:25.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 12.183889ms)
Jan 29 09:23:25.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 11.787415ms)
Jan 29 09:23:25.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 11.942754ms)
Jan 29 09:23:25.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 12.254528ms)
Jan 29 09:23:25.146: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 12.503294ms)
Jan 29 09:23:25.153: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 6.968735ms)
Jan 29 09:23:25.153: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 6.270917ms)
Jan 29 09:23:25.156: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 9.528026ms)
Jan 29 09:23:25.156: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 9.7776ms)
Jan 29 09:23:25.157: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 10.796046ms)
Jan 29 09:23:25.159: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 12.596617ms)
Jan 29 09:23:25.161: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 13.747906ms)
Jan 29 09:23:25.161: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 13.985866ms)
Jan 29 09:23:25.161: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 13.731361ms)
Jan 29 09:23:25.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 15.700993ms)
Jan 29 09:23:25.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 16.520299ms)
Jan 29 09:23:25.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 15.822975ms)
Jan 29 09:23:25.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 16.982754ms)
Jan 29 09:23:25.163: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 16.241006ms)
Jan 29 09:23:25.164: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 16.479267ms)
Jan 29 09:23:25.164: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 18.248801ms)
Jan 29 09:23:25.169: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 4.645198ms)
Jan 29 09:23:25.170: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 5.353618ms)
Jan 29 09:23:25.173: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 7.738606ms)
Jan 29 09:23:25.173: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 7.683071ms)
Jan 29 09:23:25.173: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 7.567603ms)
Jan 29 09:23:25.173: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 7.932195ms)
Jan 29 09:23:25.173: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 7.378413ms)
Jan 29 09:23:25.173: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 7.66316ms)
Jan 29 09:23:25.173: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 8.363814ms)
Jan 29 09:23:25.173: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 7.58332ms)
Jan 29 09:23:25.173: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 8.122401ms)
Jan 29 09:23:25.174: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 9.684639ms)
Jan 29 09:23:25.176: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 10.415097ms)
Jan 29 09:23:25.176: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 10.535004ms)
Jan 29 09:23:25.176: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 11.264116ms)
Jan 29 09:23:25.178: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 12.950199ms)
Jan 29 09:23:25.183: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 5.641776ms)
Jan 29 09:23:25.187: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 8.435047ms)
Jan 29 09:23:25.187: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 8.899653ms)
Jan 29 09:23:25.188: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 9.509729ms)
Jan 29 09:23:25.189: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 10.205402ms)
Jan 29 09:23:25.189: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 10.66811ms)
Jan 29 09:23:25.189: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 11.030253ms)
Jan 29 09:23:25.189: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 11.290459ms)
Jan 29 09:23:25.189: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 11.135613ms)
Jan 29 09:23:25.189: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 11.312426ms)
Jan 29 09:23:25.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 10.62316ms)
Jan 29 09:23:25.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 11.60264ms)
Jan 29 09:23:25.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 10.645705ms)
Jan 29 09:23:25.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 11.434889ms)
Jan 29 09:23:25.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 11.559239ms)
Jan 29 09:23:25.190: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 11.540741ms)
Jan 29 09:23:25.195: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 4.759143ms)
Jan 29 09:23:25.197: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 6.869052ms)
Jan 29 09:23:25.198: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 8.020737ms)
Jan 29 09:23:25.199: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 8.366951ms)
Jan 29 09:23:25.200: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 8.332072ms)
Jan 29 09:23:25.200: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 8.682187ms)
Jan 29 09:23:25.200: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 9.618749ms)
Jan 29 09:23:25.200: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 9.5882ms)
Jan 29 09:23:25.200: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 9.778813ms)
Jan 29 09:23:25.200: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 10.173568ms)
Jan 29 09:23:25.201: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 10.303173ms)
Jan 29 09:23:25.202: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 11.481241ms)
Jan 29 09:23:25.204: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 12.681093ms)
Jan 29 09:23:25.204: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 13.367041ms)
Jan 29 09:23:25.204: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 13.398635ms)
Jan 29 09:23:25.204: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 13.166806ms)
Jan 29 09:23:25.207: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 2.966802ms)
Jan 29 09:23:25.209: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 5.092758ms)
Jan 29 09:23:25.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 7.09639ms)
Jan 29 09:23:25.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 6.599892ms)
Jan 29 09:23:25.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 6.982593ms)
Jan 29 09:23:25.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 7.804139ms)
Jan 29 09:23:25.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 6.643475ms)
Jan 29 09:23:25.212: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 7.65425ms)
Jan 29 09:23:25.216: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 10.805557ms)
Jan 29 09:23:25.216: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 11.027727ms)
Jan 29 09:23:25.216: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 11.26976ms)
Jan 29 09:23:25.218: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 12.409487ms)
Jan 29 09:23:25.218: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 12.868862ms)
Jan 29 09:23:25.218: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 13.099895ms)
Jan 29 09:23:25.218: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 13.062449ms)
Jan 29 09:23:25.218: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 12.910429ms)
Jan 29 09:23:25.223: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 4.970165ms)
Jan 29 09:23:25.228: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 9.331819ms)
Jan 29 09:23:25.228: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 9.468278ms)
Jan 29 09:23:25.229: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 10.173928ms)
Jan 29 09:23:25.230: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 11.757405ms)
Jan 29 09:23:25.230: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 12.099033ms)
Jan 29 09:23:25.230: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 11.916192ms)
Jan 29 09:23:25.231: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 12.240482ms)
Jan 29 09:23:25.231: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 12.364092ms)
Jan 29 09:23:25.231: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 12.26814ms)
Jan 29 09:23:25.231: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 12.332022ms)
Jan 29 09:23:25.231: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 12.377438ms)
Jan 29 09:23:25.231: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 12.364798ms)
Jan 29 09:23:25.233: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 14.49996ms)
Jan 29 09:23:25.233: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 14.508172ms)
Jan 29 09:23:25.233: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 14.63333ms)
Jan 29 09:23:25.243: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 9.560533ms)
Jan 29 09:23:25.243: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 9.407279ms)
Jan 29 09:23:25.243: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 9.400269ms)
Jan 29 09:23:25.243: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 9.469184ms)
Jan 29 09:23:25.243: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 9.531013ms)
Jan 29 09:23:25.243: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 9.661663ms)
Jan 29 09:23:25.243: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 9.785401ms)
Jan 29 09:23:25.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 11.14377ms)
Jan 29 09:23:25.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 11.261095ms)
Jan 29 09:23:25.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 11.335512ms)
Jan 29 09:23:25.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 11.17569ms)
Jan 29 09:23:25.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 11.300803ms)
Jan 29 09:23:25.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 11.486001ms)
Jan 29 09:23:25.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 11.508545ms)
Jan 29 09:23:25.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 11.996899ms)
Jan 29 09:23:25.246: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 12.29275ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 13.230211ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 13.340576ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 13.278982ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 13.379658ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 13.363517ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 8.549095ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 13.345994ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 13.304948ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 13.387483ms)
Jan 29 09:23:25.259: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 13.630165ms)
Jan 29 09:23:25.260: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 13.406089ms)
Jan 29 09:23:25.260: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 13.476237ms)
Jan 29 09:23:25.260: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 13.545649ms)
Jan 29 09:23:25.260: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 13.785849ms)
Jan 29 09:23:25.263: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 16.899915ms)
Jan 29 09:23:25.263: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 16.946195ms)
Jan 29 09:23:25.272: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 9.292354ms)
Jan 29 09:23:25.272: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 9.085547ms)
Jan 29 09:23:25.273: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 9.277316ms)
Jan 29 09:23:25.274: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 11.158563ms)
Jan 29 09:23:25.274: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 10.883671ms)
Jan 29 09:23:25.274: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 10.967013ms)
Jan 29 09:23:25.274: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 10.781448ms)
Jan 29 09:23:25.275: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 11.52447ms)
Jan 29 09:23:25.275: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 11.550038ms)
Jan 29 09:23:25.275: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 11.313267ms)
Jan 29 09:23:25.276: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 13.023655ms)
Jan 29 09:23:25.276: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 12.881322ms)
Jan 29 09:23:25.276: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 13.113702ms)
Jan 29 09:23:25.277: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 13.74831ms)
Jan 29 09:23:25.277: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 13.871897ms)
Jan 29 09:23:25.278: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 14.1929ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 8.48975ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 8.806172ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 8.810816ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 9.226831ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 9.06571ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 8.627484ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 9.028846ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 8.855559ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 8.785023ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 8.961554ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 9.164061ms)
Jan 29 09:23:25.287: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 9.334699ms)
Jan 29 09:23:25.288: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 9.347439ms)
Jan 29 09:23:25.288: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 10.237743ms)
Jan 29 09:23:25.288: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 9.757757ms)
Jan 29 09:23:25.291: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 12.778195ms)
Jan 29 09:23:25.297: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 5.369099ms)
Jan 29 09:23:25.300: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 7.342686ms)
Jan 29 09:23:25.300: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 7.802499ms)
Jan 29 09:23:25.300: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 7.964328ms)
Jan 29 09:23:25.301: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 9.958604ms)
Jan 29 09:23:25.301: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 9.065417ms)
Jan 29 09:23:25.301: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 8.247735ms)
Jan 29 09:23:25.301: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 9.016264ms)
Jan 29 09:23:25.302: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 8.961595ms)
Jan 29 09:23:25.302: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 9.166561ms)
Jan 29 09:23:25.302: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 8.443689ms)
Jan 29 09:23:25.302: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 8.718799ms)
Jan 29 09:23:25.304: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 11.410788ms)
Jan 29 09:23:25.305: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 11.797319ms)
Jan 29 09:23:25.305: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 11.833221ms)
Jan 29 09:23:25.307: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 13.682498ms)
Jan 29 09:23:25.319: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 12.134338ms)
Jan 29 09:23:25.319: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 11.847107ms)
Jan 29 09:23:25.319: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 11.730829ms)
Jan 29 09:23:25.319: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 11.74685ms)
Jan 29 09:23:25.320: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 11.792992ms)
Jan 29 09:23:25.320: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 12.878307ms)
Jan 29 09:23:25.320: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 13.355629ms)
Jan 29 09:23:25.320: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 12.464142ms)
Jan 29 09:23:25.327: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 8.261494ms)
Jan 29 09:23:25.327: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 8.258871ms)
Jan 29 09:23:25.327: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 8.419071ms)
Jan 29 09:23:25.329: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 21.358221ms)
Jan 29 09:23:25.336: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 17.403082ms)
Jan 29 09:23:25.336: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 17.417655ms)
Jan 29 09:23:25.337: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 17.958177ms)
Jan 29 09:23:25.344: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 25.132694ms)
Jan 29 09:23:25.353: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname2/proxy/: tls qux (200; 8.849594ms)
Jan 29 09:23:25.353: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:462/proxy/: tls qux (200; 7.772138ms)
Jan 29 09:23:25.353: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:460/proxy/: tls baz (200; 9.525104ms)
Jan 29 09:23:25.353: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:1080/proxy/... (200; 8.178954ms)
Jan 29 09:23:25.354: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb/proxy/rewriteme"... (200; 8.848751ms)
Jan 29 09:23:25.355: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 9.210481ms)
Jan 29 09:23:25.355: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/https:proxy-service-5t4bg-b6rrb:443/proxy/... (200; 11.052106ms)
Jan 29 09:23:25.355: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:1080/proxy/rewri... (200; 10.285938ms)
Jan 29 09:23:25.355: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/http:proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 9.823725ms)
Jan 29 09:23:25.355: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:160/proxy/: foo (200; 10.265006ms)
Jan 29 09:23:25.355: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname1/proxy/: foo (200; 11.103844ms)
Jan 29 09:23:25.355: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/pods/proxy-service-5t4bg-b6rrb:162/proxy/: bar (200; 9.457421ms)
Jan 29 09:23:25.359: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/https:proxy-service-5t4bg:tlsportname1/proxy/: tls baz (200; 14.321246ms)
Jan 29 09:23:25.359: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/http:proxy-service-5t4bg:portname2/proxy/: bar (200; 14.739132ms)
Jan 29 09:23:25.359: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname2/proxy/: bar (200; 14.002536ms)
Jan 29 09:23:25.363: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-dknq8/services/proxy-service-5t4bg:portname1/proxy/: foo (200; 17.280264ms)
STEP: deleting { ReplicationController} proxy-service-5t4bg in namespace e2e-tests-proxy-dknq8, will wait for the garbage collector to delete the pods
Jan 29 09:23:25.423: INFO: Deleting { ReplicationController} proxy-service-5t4bg took: 5.594984ms
Jan 29 09:23:25.523: INFO: Terminating { ReplicationController} proxy-service-5t4bg pods took: 100.59331ms
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:23:34.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-dknq8" for this suite.
Jan 29 09:23:40.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:23:40.468: INFO: namespace: e2e-tests-proxy-dknq8, resource: bindings, ignored listing per whitelist
Jan 29 09:23:40.531: INFO: namespace e2e-tests-proxy-dknq8 deletion completed in 6.195036375s

â€¢ [SLOW TEST:21.939 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:23:40.531: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-n82r9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jan 29 09:23:40.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-n82r9'
Jan 29 09:23:41.155: INFO: stderr: ""
Jan 29 09:23:41.155: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 29 09:23:42.161: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:23:42.161: INFO: Found 0 / 1
Jan 29 09:23:43.158: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:23:43.158: INFO: Found 1 / 1
Jan 29 09:23:43.158: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 29 09:23:43.161: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:23:43.161: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 09:23:43.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 patch pod redis-master-h7th8 --namespace=e2e-tests-kubectl-n82r9 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 29 09:23:43.336: INFO: stderr: ""
Jan 29 09:23:43.336: INFO: stdout: "pod/redis-master-h7th8 patched\n"
STEP: checking annotations
Jan 29 09:23:43.342: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:23:43.342: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:23:43.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-n82r9" for this suite.
Jan 29 09:24:05.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:24:05.457: INFO: namespace: e2e-tests-kubectl-n82r9, resource: bindings, ignored listing per whitelist
Jan 29 09:24:05.465: INFO: namespace e2e-tests-kubectl-n82r9 deletion completed in 22.114963125s

â€¢ [SLOW TEST:24.934 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:24:05.466: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-qtqnb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-9fa780ea-23a7-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:24:05.743: INFO: Waiting up to 5m0s for pod "pod-secrets-9fa80ce3-23a7-11e9-9015-b2c9bac28373" in namespace "e2e-tests-secrets-qtqnb" to be "success or failure"
Jan 29 09:24:05.758: INFO: Pod "pod-secrets-9fa80ce3-23a7-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 15.711565ms
Jan 29 09:24:07.762: INFO: Pod "pod-secrets-9fa80ce3-23a7-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018912271s
Jan 29 09:24:09.765: INFO: Pod "pod-secrets-9fa80ce3-23a7-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021850446s
STEP: Saw pod success
Jan 29 09:24:09.765: INFO: Pod "pod-secrets-9fa80ce3-23a7-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:24:09.767: INFO: Trying to get logs from node 9.111.254.123 pod pod-secrets-9fa80ce3-23a7-11e9-9015-b2c9bac28373 container secret-env-test: <nil>
STEP: delete the pod
Jan 29 09:24:09.786: INFO: Waiting for pod pod-secrets-9fa80ce3-23a7-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:24:09.789: INFO: Pod pod-secrets-9fa80ce3-23a7-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:24:09.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qtqnb" for this suite.
Jan 29 09:24:15.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:24:15.867: INFO: namespace: e2e-tests-secrets-qtqnb, resource: bindings, ignored listing per whitelist
Jan 29 09:24:15.923: INFO: namespace e2e-tests-secrets-qtqnb deletion completed in 6.128179854s

â€¢ [SLOW TEST:10.458 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:24:15.925: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-v84cb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 29 09:24:16.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-v84cb'
Jan 29 09:24:16.372: INFO: stderr: ""
Jan 29 09:24:16.372: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
Jan 29 09:24:16.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-v84cb'
Jan 29 09:24:18.792: INFO: stderr: ""
Jan 29 09:24:18.792: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:24:18.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-v84cb" for this suite.
Jan 29 09:24:24.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:24:24.885: INFO: namespace: e2e-tests-kubectl-v84cb, resource: bindings, ignored listing per whitelist
Jan 29 09:24:24.910: INFO: namespace e2e-tests-kubectl-v84cb deletion completed in 6.110299853s

â€¢ [SLOW TEST:8.984 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:24:24.910: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-85w9s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-85w9s
Jan 29 09:24:29.147: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-85w9s
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 09:24:29.150: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:28:29.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-85w9s" for this suite.
Jan 29 09:28:35.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:28:35.638: INFO: namespace: e2e-tests-container-probe-85w9s, resource: bindings, ignored listing per whitelist
Jan 29 09:28:35.725: INFO: namespace e2e-tests-container-probe-85w9s deletion completed in 6.16821836s

â€¢ [SLOW TEST:250.816 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:28:35.726: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jbnlp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:28:35.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 version --client'
Jan 29 09:28:36.010: INFO: stderr: ""
Jan 29 09:28:36.010: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.1\", GitCommit:\"4ed3216f3ec431b140b1d899130a69fc671678f4\", GitTreeState:\"clean\", BuildDate:\"2018-10-05T16:46:06Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jan 29 09:28:36.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-jbnlp'
Jan 29 09:28:36.446: INFO: stderr: ""
Jan 29 09:28:36.446: INFO: stdout: "replicationcontroller/redis-master created\n"
Jan 29 09:28:36.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-jbnlp'
Jan 29 09:28:36.715: INFO: stderr: ""
Jan 29 09:28:36.715: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 29 09:28:37.721: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:28:37.721: INFO: Found 0 / 1
Jan 29 09:28:38.718: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:28:38.718: INFO: Found 1 / 1
Jan 29 09:28:38.718: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 29 09:28:38.722: INFO: Selector matched 1 pods for map[app:redis]
Jan 29 09:28:38.723: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 09:28:38.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 describe pod redis-master-5prws --namespace=e2e-tests-kubectl-jbnlp'
Jan 29 09:28:38.901: INFO: stderr: ""
Jan 29 09:28:38.901: INFO: stdout: "Name:               redis-master-5prws\nNamespace:          e2e-tests-kubectl-jbnlp\nPriority:           0\nPriorityClassName:  <none>\nNode:               9.111.254.123/9.111.254.123\nStart Time:         Tue, 29 Jan 2019 09:28:36 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 10.1.211.168\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://baba4ec43a8418188d44858dc1d986bde20222afc41bd6c8404d3aa747061c76\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 29 Jan 2019 09:28:37 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8cqhr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-8cqhr:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-8cqhr\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                    Message\n  ----    ------     ----  ----                    -------\n  Normal  Scheduled  2s    default-scheduler       Successfully assigned e2e-tests-kubectl-jbnlp/redis-master-5prws to 9.111.254.123\n  Normal  Pulled     1s    kubelet, 9.111.254.123  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, 9.111.254.123  Created container\n  Normal  Started    1s    kubelet, 9.111.254.123  Started container\n"
Jan 29 09:28:38.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 describe rc redis-master --namespace=e2e-tests-kubectl-jbnlp'
Jan 29 09:28:39.052: INFO: stderr: ""
Jan 29 09:28:39.052: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-jbnlp\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-5prws\n"
Jan 29 09:28:39.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 describe service redis-master --namespace=e2e-tests-kubectl-jbnlp'
Jan 29 09:28:39.195: INFO: stderr: ""
Jan 29 09:28:39.195: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-jbnlp\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.199.146\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.1.211.168:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 29 09:28:39.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 describe node 9.111.254.104'
Jan 29 09:28:39.376: INFO: stderr: ""
Jan 29 09:28:39.376: INFO: stdout: "Name:               9.111.254.104\nRoles:              management\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=9.111.254.104\n                    management=true\n                    node-role.kubernetes.io/management=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 24 Jan 2019 02:25:57 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  OutOfDisk        False   Tue, 29 Jan 2019 09:28:34 +0000   Thu, 24 Jan 2019 02:25:57 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure   False   Tue, 29 Jan 2019 09:28:34 +0000   Thu, 24 Jan 2019 02:25:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 29 Jan 2019 09:28:34 +0000   Thu, 24 Jan 2019 02:25:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 29 Jan 2019 09:28:34 +0000   Thu, 24 Jan 2019 02:25:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 29 Jan 2019 09:28:34 +0000   Thu, 24 Jan 2019 02:27:07 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  9.111.254.104\n  Hostname:    9.111.254.104\nCapacity:\n cpu:                8\n ephemeral-storage:  254060400Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16424672Ki\n pods:               80\nAllocatable:\n cpu:                7600m\n ephemeral-storage:  251860848Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             15273696Ki\n pods:               80\nSystem Info:\n Machine ID:                 8ba3a9b9133e4c898d7801a0f1eb1a3e\n System UUID:                24D3E92A-1F1A-427C-A920-A8BC13961934\n Boot ID:                    5ca35445-9e1e-4d0f-8072-c84226ca115c\n Kernel Version:             4.15.0-42-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.0\n Kubelet Version:            v1.12.4+icp-ee\n Kube-Proxy Version:         v1.12.4+icp-ee\nNon-terminated Pods:         (25 in total)\n  Namespace                  Name                                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                            ------------  ----------  ---------------  -------------\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-zg67k         0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                audit-logging-fluentd-ds-2xjm5                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                calico-node-4z7zs                                               300m (3%)     0 (0%)      150Mi (1%)       0 (0%)\n  kube-system                custom-metrics-adapter-77c5d9c6ff-9lg44                         256m (3%)     0 (0%)      256Mi (1%)       0 (0%)\n  kube-system                k8s-proxy-9.111.254.104                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                key-management-api-85fb4b5f55-pckqx                             0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                key-management-crypto-f4b6444bc-zspwm                           0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                key-management-lifecycle-69b67d57c5-q5pvm                       0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                key-management-pep-6d7f685bf4-rnd82                             0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                key-management-persistence-656b774f7-f2kjc                      0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                logging-elk-client-559db59464-ps9rr                             0 (0%)        0 (0%)      1536Mi (10%)     1536Mi (10%)\n  kube-system                logging-elk-data-0                                              0 (0%)        0 (0%)      3072M (19%)      3072M (19%)\n  kube-system                logging-elk-filebeat-ds-9mbdx                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                logging-elk-logstash-5444dd6fb4-6p7jm                           0 (0%)        0 (0%)      1Gi (6%)         1Gi (6%)\n  kube-system                logging-elk-master-c47cb5549-n5cmk                              0 (0%)        0 (0%)      1536Mi (10%)     1536Mi (10%)\n  kube-system                metrics-server-5bcdd5579-9v9qd                                  20m (0%)      0 (0%)      64Mi (0%)        0 (0%)\n  kube-system                monitoring-grafana-7d5d7f5c45-58ttx                             100m (1%)     500m (6%)   128Mi (0%)       512Mi (3%)\n  kube-system                monitoring-prometheus-7b5f7db489-9jxtd                          100m (1%)     500m (6%)   128Mi (0%)       2Gi (13%)\n  kube-system                monitoring-prometheus-alertmanager-86b46f4f8f-zlwzh             10m (0%)      200m (2%)   64Mi (0%)        256Mi (1%)\n  kube-system                monitoring-prometheus-collectdexporter-698694c4bf-z4b44         0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                monitoring-prometheus-elasticsearchexporter-786dbc957b-lrzfq    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                monitoring-prometheus-kubestatemetrics-9fcddf7d8-jw5xn          0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                monitoring-prometheus-nodeexporter-hq52h                        0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                nvidia-device-plugin-cjt74                                      150m (1%)     0 (0%)      0 (0%)           0 (0%)\n  kube-system                web-terminal-7776f9df5b-q6mlp                                   10m (0%)      100m (1%)   64Mi (0%)        512Mi (3%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource  Requests         Limits\n  --------  --------         ------\n  cpu       946m (12%)       1300m (17%)\n  memory    8068800Ki (52%)  10602176Ki (69%)\nEvents:     <none>\n"
Jan 29 09:28:39.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 describe namespace e2e-tests-kubectl-jbnlp'
Jan 29 09:28:39.525: INFO: stderr: ""
Jan 29 09:28:39.525: INFO: stdout: "Name:         e2e-tests-kubectl-jbnlp\nLabels:       e2e-framework=kubectl\n              e2e-run=984bf287-23a2-11e9-9015-b2c9bac28373\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:28:39.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jbnlp" for this suite.
Jan 29 09:29:01.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:29:01.645: INFO: namespace: e2e-tests-kubectl-jbnlp, resource: bindings, ignored listing per whitelist
Jan 29 09:29:01.743: INFO: namespace e2e-tests-kubectl-jbnlp deletion completed in 22.210528676s

â€¢ [SLOW TEST:26.017 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:29:01.743: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-hbwfs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-hbwfs/secret-test-503f4151-23a8-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:29:02.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-503fc0ea-23a8-11e9-9015-b2c9bac28373" in namespace "e2e-tests-secrets-hbwfs" to be "success or failure"
Jan 29 09:29:02.041: INFO: Pod "pod-configmaps-503fc0ea-23a8-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.897861ms
Jan 29 09:29:04.044: INFO: Pod "pod-configmaps-503fc0ea-23a8-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005815644s
STEP: Saw pod success
Jan 29 09:29:04.044: INFO: Pod "pod-configmaps-503fc0ea-23a8-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:29:04.046: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-503fc0ea-23a8-11e9-9015-b2c9bac28373 container env-test: <nil>
STEP: delete the pod
Jan 29 09:29:04.078: INFO: Waiting for pod pod-configmaps-503fc0ea-23a8-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:29:04.083: INFO: Pod pod-configmaps-503fc0ea-23a8-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:29:04.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hbwfs" for this suite.
Jan 29 09:29:10.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:29:10.211: INFO: namespace: e2e-tests-secrets-hbwfs, resource: bindings, ignored listing per whitelist
Jan 29 09:29:10.219: INFO: namespace e2e-tests-secrets-hbwfs deletion completed in 6.132111553s

â€¢ [SLOW TEST:8.477 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:29:10.220: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-wtkwk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-wtkwk.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-wtkwk.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-wtkwk.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-wtkwk.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-wtkwk.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-wtkwk.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 09:29:24.909: INFO: DNS probes using e2e-tests-dns-wtkwk/dns-test-554f0c1e-23a8-11e9-9015-b2c9bac28373 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:29:24.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-wtkwk" for this suite.
Jan 29 09:29:30.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:29:31.049: INFO: namespace: e2e-tests-dns-wtkwk, resource: bindings, ignored listing per whitelist
Jan 29 09:29:31.126: INFO: namespace e2e-tests-dns-wtkwk deletion completed in 6.195462708s

â€¢ [SLOW TEST:20.907 seconds]
[sig-network] DNS
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:29:31.127: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-25rt5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 29 09:29:31.343: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25rt5,SelfLink:/api/v1/namespaces/e2e-tests-watch-25rt5/configmaps/e2e-watch-test-label-changed,UID:61c3c383-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688489,Generation:0,CreationTimestamp:2019-01-29 09:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 09:29:31.344: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25rt5,SelfLink:/api/v1/namespaces/e2e-tests-watch-25rt5/configmaps/e2e-watch-test-label-changed,UID:61c3c383-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688490,Generation:0,CreationTimestamp:2019-01-29 09:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 29 09:29:31.344: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25rt5,SelfLink:/api/v1/namespaces/e2e-tests-watch-25rt5/configmaps/e2e-watch-test-label-changed,UID:61c3c383-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688491,Generation:0,CreationTimestamp:2019-01-29 09:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 29 09:29:41.371: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25rt5,SelfLink:/api/v1/namespaces/e2e-tests-watch-25rt5/configmaps/e2e-watch-test-label-changed,UID:61c3c383-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688510,Generation:0,CreationTimestamp:2019-01-29 09:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 09:29:41.371: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25rt5,SelfLink:/api/v1/namespaces/e2e-tests-watch-25rt5/configmaps/e2e-watch-test-label-changed,UID:61c3c383-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688511,Generation:0,CreationTimestamp:2019-01-29 09:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 29 09:29:41.372: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-25rt5,SelfLink:/api/v1/namespaces/e2e-tests-watch-25rt5/configmaps/e2e-watch-test-label-changed,UID:61c3c383-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688512,Generation:0,CreationTimestamp:2019-01-29 09:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:29:41.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-25rt5" for this suite.
Jan 29 09:29:47.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:29:47.433: INFO: namespace: e2e-tests-watch-25rt5, resource: bindings, ignored listing per whitelist
Jan 29 09:29:47.528: INFO: namespace e2e-tests-watch-25rt5 deletion completed in 6.15091618s

â€¢ [SLOW TEST:16.401 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:29:47.528: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2jxpd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Jan 29 09:29:47.706: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-900852259 proxy --unix-socket=/tmp/kubectl-proxy-unix612698918/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:29:47.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2jxpd" for this suite.
Jan 29 09:29:53.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:29:53.865: INFO: namespace: e2e-tests-kubectl-2jxpd, resource: bindings, ignored listing per whitelist
Jan 29 09:29:53.926: INFO: namespace e2e-tests-kubectl-2jxpd deletion completed in 6.106413124s

â€¢ [SLOW TEST:6.398 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:29:53.927: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-4ftmr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 29 09:29:54.109: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:29:57.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-4ftmr" for this suite.
Jan 29 09:30:03.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:30:03.649: INFO: namespace: e2e-tests-init-container-4ftmr, resource: bindings, ignored listing per whitelist
Jan 29 09:30:03.709: INFO: namespace e2e-tests-init-container-4ftmr deletion completed in 6.186648798s

â€¢ [SLOW TEST:9.782 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:30:03.711: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-2jqlh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 29 09:30:03.907: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2jqlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-2jqlh/configmaps/e2e-watch-test-watch-closed,UID:752ec42c-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688617,Generation:0,CreationTimestamp:2019-01-29 09:30:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 09:30:03.907: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2jqlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-2jqlh/configmaps/e2e-watch-test-watch-closed,UID:752ec42c-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688618,Generation:0,CreationTimestamp:2019-01-29 09:30:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 29 09:30:03.923: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2jqlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-2jqlh/configmaps/e2e-watch-test-watch-closed,UID:752ec42c-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688619,Generation:0,CreationTimestamp:2019-01-29 09:30:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 09:30:03.923: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2jqlh,SelfLink:/api/v1/namespaces/e2e-tests-watch-2jqlh/configmaps/e2e-watch-test-watch-closed,UID:752ec42c-23a8-11e9-a926-eeeeeeeeeeee,ResourceVersion:688620,Generation:0,CreationTimestamp:2019-01-29 09:30:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:30:03.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-2jqlh" for this suite.
Jan 29 09:30:09.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:30:10.063: INFO: namespace: e2e-tests-watch-2jqlh, resource: bindings, ignored listing per whitelist
Jan 29 09:30:10.163: INFO: namespace e2e-tests-watch-2jqlh deletion completed in 6.235536764s

â€¢ [SLOW TEST:6.452 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:30:10.163: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-gnwfp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0129 09:30:55.468906      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 09:30:55.468: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:30:55.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-gnwfp" for this suite.
Jan 29 09:31:01.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:31:01.591: INFO: namespace: e2e-tests-gc-gnwfp, resource: bindings, ignored listing per whitelist
Jan 29 09:31:01.647: INFO: namespace e2e-tests-gc-gnwfp deletion completed in 6.172766315s

â€¢ [SLOW TEST:51.484 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:31:01.647: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-j58q2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-bjgbc
STEP: Creating secret with name secret-test-97bc15fe-23a8-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:31:02.042: INFO: Waiting up to 5m0s for pod "pod-secrets-97d220a5-23a8-11e9-9015-b2c9bac28373" in namespace "e2e-tests-secrets-j58q2" to be "success or failure"
Jan 29 09:31:02.047: INFO: Pod "pod-secrets-97d220a5-23a8-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 4.706838ms
Jan 29 09:31:04.051: INFO: Pod "pod-secrets-97d220a5-23a8-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008505409s
Jan 29 09:31:06.055: INFO: Pod "pod-secrets-97d220a5-23a8-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012689772s
STEP: Saw pod success
Jan 29 09:31:06.055: INFO: Pod "pod-secrets-97d220a5-23a8-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:31:06.057: INFO: Trying to get logs from node 9.111.254.123 pod pod-secrets-97d220a5-23a8-11e9-9015-b2c9bac28373 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 09:31:06.074: INFO: Waiting for pod pod-secrets-97d220a5-23a8-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:31:06.078: INFO: Pod pod-secrets-97d220a5-23a8-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:31:06.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-j58q2" for this suite.
Jan 29 09:31:12.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:31:12.199: INFO: namespace: e2e-tests-secrets-j58q2, resource: bindings, ignored listing per whitelist
Jan 29 09:31:12.216: INFO: namespace e2e-tests-secrets-j58q2 deletion completed in 6.133664985s
STEP: Destroying namespace "e2e-tests-secret-namespace-bjgbc" for this suite.
Jan 29 09:31:18.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:31:18.239: INFO: namespace: e2e-tests-secret-namespace-bjgbc, resource: bindings, ignored listing per whitelist
Jan 29 09:31:18.375: INFO: namespace e2e-tests-secret-namespace-bjgbc deletion completed in 6.159222289s

â€¢ [SLOW TEST:16.728 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:31:18.376: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-4ll2d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Jan 29 09:31:18.631: INFO: Waiting up to 5m0s for pod "client-containers-a1b06a09-23a8-11e9-9015-b2c9bac28373" in namespace "e2e-tests-containers-4ll2d" to be "success or failure"
Jan 29 09:31:18.637: INFO: Pod "client-containers-a1b06a09-23a8-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 6.208124ms
Jan 29 09:31:20.645: INFO: Pod "client-containers-a1b06a09-23a8-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013570075s
STEP: Saw pod success
Jan 29 09:31:20.645: INFO: Pod "client-containers-a1b06a09-23a8-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:31:20.647: INFO: Trying to get logs from node 9.111.254.123 pod client-containers-a1b06a09-23a8-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:31:20.687: INFO: Waiting for pod client-containers-a1b06a09-23a8-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:31:20.691: INFO: Pod client-containers-a1b06a09-23a8-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:31:20.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-4ll2d" for this suite.
Jan 29 09:31:26.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:31:26.742: INFO: namespace: e2e-tests-containers-4ll2d, resource: bindings, ignored listing per whitelist
Jan 29 09:31:26.831: INFO: namespace e2e-tests-containers-4ll2d deletion completed in 6.130146849s

â€¢ [SLOW TEST:8.455 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:31:26.832: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-6tgv9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-6tgv9 A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-6tgv9 A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-6tgv9.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-6tgv9.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-6tgv9.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-6tgv9.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-6tgv9.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-6tgv9.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 137.6.0.10.in-addr.arpa. PTR)" && echo OK > /results/10.0.6.137_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 137.6.0.10.in-addr.arpa. PTR)" && echo OK > /results/10.0.6.137_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-6tgv9 A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-6tgv9;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-6tgv9 A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-6tgv9.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-6tgv9.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-6tgv9.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-6tgv9.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-6tgv9.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-6tgv9.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-6tgv9.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 137.6.0.10.in-addr.arpa. PTR)" && echo OK > /results/10.0.6.137_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 137.6.0.10.in-addr.arpa. PTR)" && echo OK > /results/10.0.6.137_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 09:31:42.272: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.276: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.279: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9 from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.281: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9 from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.287: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.289: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.292: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.295: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.318: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.323: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.325: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-6tgv9 from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.327: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9 from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.330: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.333: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.336: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.342: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:42.362: INFO: Lookups using e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9 wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9 wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9.svc wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-6tgv9 jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9 jessie_udp@dns-test-service.e2e-tests-dns-6tgv9.svc jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc]

Jan 29 09:31:52.272: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.277: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.280: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9 from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.283: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9 from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.285: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.288: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.290: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.293: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.311: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.314: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.317: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-6tgv9 from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.319: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9 from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.321: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.324: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.326: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.329: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc from pod e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373: the server could not find the requested resource (get pods dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373)
Jan 29 09:31:52.347: INFO: Lookups using e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9 wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9 wheezy_udp@dns-test-service.e2e-tests-dns-6tgv9.svc wheezy_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-6tgv9 jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9 jessie_udp@dns-test-service.e2e-tests-dns-6tgv9.svc jessie_tcp@dns-test-service.e2e-tests-dns-6tgv9.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-6tgv9.svc]

Jan 29 09:32:02.364: INFO: DNS probes using e2e-tests-dns-6tgv9/dns-test-a6be0670-23a8-11e9-9015-b2c9bac28373 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:32:02.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-6tgv9" for this suite.
Jan 29 09:32:08.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:32:08.605: INFO: namespace: e2e-tests-dns-6tgv9, resource: bindings, ignored listing per whitelist
Jan 29 09:32:08.611: INFO: namespace e2e-tests-dns-6tgv9 deletion completed in 6.178316706s

â€¢ [SLOW TEST:41.779 seconds]
[sig-network] DNS
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:32:08.611: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-8fz72
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 29 09:32:08.844: INFO: Waiting up to 5m0s for pod "pod-bfa033fa-23a8-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-8fz72" to be "success or failure"
Jan 29 09:32:08.847: INFO: Pod "pod-bfa033fa-23a8-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.94828ms
Jan 29 09:32:10.851: INFO: Pod "pod-bfa033fa-23a8-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006752473s
STEP: Saw pod success
Jan 29 09:32:10.851: INFO: Pod "pod-bfa033fa-23a8-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:32:10.853: INFO: Trying to get logs from node 9.111.254.123 pod pod-bfa033fa-23a8-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:32:10.929: INFO: Waiting for pod pod-bfa033fa-23a8-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:32:10.939: INFO: Pod pod-bfa033fa-23a8-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:32:10.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8fz72" for this suite.
Jan 29 09:32:16.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:32:17.088: INFO: namespace: e2e-tests-emptydir-8fz72, resource: bindings, ignored listing per whitelist
Jan 29 09:32:17.095: INFO: namespace e2e-tests-emptydir-8fz72 deletion completed in 6.151497932s

â€¢ [SLOW TEST:8.485 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:32:17.096: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xvb7t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 29 09:32:19.932: INFO: Successfully updated pod "annotationupdatec4b18135-23a8-11e9-9015-b2c9bac28373"
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:32:23.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xvb7t" for this suite.
Jan 29 09:32:45.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:32:46.089: INFO: namespace: e2e-tests-projected-xvb7t, resource: bindings, ignored listing per whitelist
Jan 29 09:32:46.095: INFO: namespace e2e-tests-projected-xvb7t deletion completed in 22.131606653s

â€¢ [SLOW TEST:28.999 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:32:46.098: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-54zd7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Jan 29 09:32:46.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 api-versions'
Jan 29 09:32:46.411: INFO: stderr: ""
Jan 29 09:32:46.411: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\ncoordination.k8s.io/v1beta1\ncustom.metrics.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nicp.ibm.com/v1\nmetrics.k8s.io/v1beta1\nmonitoringcontroller.cloud.ibm.com/v1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nsecurityenforcement.admission.cloud.ibm.com/v1beta1\nservicecatalog.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:32:46.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-54zd7" for this suite.
Jan 29 09:32:52.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:32:52.464: INFO: namespace: e2e-tests-kubectl-54zd7, resource: bindings, ignored listing per whitelist
Jan 29 09:32:52.560: INFO: namespace e2e-tests-kubectl-54zd7 deletion completed in 6.141571182s

â€¢ [SLOW TEST:6.463 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:32:52.562: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-xvhc6
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-d9d26e63-23a8-11e9-9015-b2c9bac28373
STEP: Creating configMap with name cm-test-opt-upd-d9d26eb1-23a8-11e9-9015-b2c9bac28373
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d9d26e63-23a8-11e9-9015-b2c9bac28373
STEP: Updating configmap cm-test-opt-upd-d9d26eb1-23a8-11e9-9015-b2c9bac28373
STEP: Creating configMap with name cm-test-opt-create-d9d26eca-23a8-11e9-9015-b2c9bac28373
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:34:07.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-xvhc6" for this suite.
Jan 29 09:34:29.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:34:29.714: INFO: namespace: e2e-tests-configmap-xvhc6, resource: bindings, ignored listing per whitelist
Jan 29 09:34:29.718: INFO: namespace e2e-tests-configmap-xvhc6 deletion completed in 22.129463023s

â€¢ [SLOW TEST:97.156 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:34:29.718: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vv7m5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 29 09:34:29.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-vv7m5'
Jan 29 09:34:30.166: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Jan 29 09:34:30.166: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
Jan 29 09:34:32.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-vv7m5'
Jan 29 09:34:32.333: INFO: stderr: ""
Jan 29 09:34:32.333: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:34:32.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vv7m5" for this suite.
Jan 29 09:35:54.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:35:54.449: INFO: namespace: e2e-tests-kubectl-vv7m5, resource: bindings, ignored listing per whitelist
Jan 29 09:35:54.451: INFO: namespace e2e-tests-kubectl-vv7m5 deletion completed in 1m22.112640336s

â€¢ [SLOW TEST:84.733 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:35:54.451: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-lmd44
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-lmd44
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 09:35:54.629: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 29 09:36:21.193: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.211.186:8080/dial?request=hostName&protocol=http&host=10.1.211.160&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-lmd44 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 09:36:21.193: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 09:36:21.361: INFO: Waiting for endpoints: map[]
Jan 29 09:36:21.364: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.211.186:8080/dial?request=hostName&protocol=http&host=10.1.148.56&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-lmd44 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 09:36:21.364: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 09:36:21.538: INFO: Waiting for endpoints: map[]
Jan 29 09:36:21.542: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.211.186:8080/dial?request=hostName&protocol=http&host=10.1.12.177&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-lmd44 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 09:36:21.542: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 09:36:21.700: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:36:21.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-lmd44" for this suite.
Jan 29 09:36:43.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:36:43.784: INFO: namespace: e2e-tests-pod-network-test-lmd44, resource: bindings, ignored listing per whitelist
Jan 29 09:36:43.819: INFO: namespace e2e-tests-pod-network-test-lmd44 deletion completed in 22.11374566s

â€¢ [SLOW TEST:49.368 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:36:43.819: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-zp4vw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Jan 29 09:36:44.036: INFO: Waiting up to 5m0s for pod "client-containers-63aa3284-23a9-11e9-9015-b2c9bac28373" in namespace "e2e-tests-containers-zp4vw" to be "success or failure"
Jan 29 09:36:44.045: INFO: Pod "client-containers-63aa3284-23a9-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 8.129769ms
Jan 29 09:36:46.048: INFO: Pod "client-containers-63aa3284-23a9-11e9-9015-b2c9bac28373": Phase="Running", Reason="", readiness=true. Elapsed: 2.01100233s
Jan 29 09:36:48.051: INFO: Pod "client-containers-63aa3284-23a9-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01448848s
STEP: Saw pod success
Jan 29 09:36:48.051: INFO: Pod "client-containers-63aa3284-23a9-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:36:48.053: INFO: Trying to get logs from node 9.111.254.123 pod client-containers-63aa3284-23a9-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:36:48.072: INFO: Waiting for pod client-containers-63aa3284-23a9-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:36:48.074: INFO: Pod client-containers-63aa3284-23a9-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:36:48.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-zp4vw" for this suite.
Jan 29 09:36:54.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:36:54.135: INFO: namespace: e2e-tests-containers-zp4vw, resource: bindings, ignored listing per whitelist
Jan 29 09:36:54.204: INFO: namespace e2e-tests-containers-zp4vw deletion completed in 6.125962156s

â€¢ [SLOW TEST:10.385 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:36:54.205: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-kt59s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-69db0d76-23a9-11e9-9015-b2c9bac28373
Jan 29 09:36:54.437: INFO: Pod name my-hostname-basic-69db0d76-23a9-11e9-9015-b2c9bac28373: Found 0 pods out of 1
Jan 29 09:36:59.440: INFO: Pod name my-hostname-basic-69db0d76-23a9-11e9-9015-b2c9bac28373: Found 1 pods out of 1
Jan 29 09:36:59.440: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-69db0d76-23a9-11e9-9015-b2c9bac28373" are running
Jan 29 09:36:59.442: INFO: Pod "my-hostname-basic-69db0d76-23a9-11e9-9015-b2c9bac28373-xkqx7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-29 09:36:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-29 09:36:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-29 09:36:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-29 09:36:54 +0000 UTC Reason: Message:}])
Jan 29 09:36:59.442: INFO: Trying to dial the pod
Jan 29 09:37:04.449: INFO: Controller my-hostname-basic-69db0d76-23a9-11e9-9015-b2c9bac28373: Got expected result from replica 1 [my-hostname-basic-69db0d76-23a9-11e9-9015-b2c9bac28373-xkqx7]: "my-hostname-basic-69db0d76-23a9-11e9-9015-b2c9bac28373-xkqx7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:37:04.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-kt59s" for this suite.
Jan 29 09:37:10.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:37:10.585: INFO: namespace: e2e-tests-replication-controller-kt59s, resource: bindings, ignored listing per whitelist
Jan 29 09:37:10.605: INFO: namespace e2e-tests-replication-controller-kt59s deletion completed in 6.151986453s

â€¢ [SLOW TEST:16.400 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:37:10.605: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-sdz6w
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-73a04863-23a9-11e9-9015-b2c9bac28373
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:37:12.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sdz6w" for this suite.
Jan 29 09:37:34.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:37:34.965: INFO: namespace: e2e-tests-configmap-sdz6w, resource: bindings, ignored listing per whitelist
Jan 29 09:37:34.990: INFO: namespace e2e-tests-configmap-sdz6w deletion completed in 22.103161289s

â€¢ [SLOW TEST:24.384 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:37:34.990: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5shm8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 29 09:37:35.237: INFO: Waiting up to 5m0s for pod "pod-822c1550-23a9-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-5shm8" to be "success or failure"
Jan 29 09:37:35.240: INFO: Pod "pod-822c1550-23a9-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638446ms
Jan 29 09:37:37.247: INFO: Pod "pod-822c1550-23a9-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009675372s
STEP: Saw pod success
Jan 29 09:37:37.247: INFO: Pod "pod-822c1550-23a9-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:37:37.253: INFO: Trying to get logs from node 9.111.254.123 pod pod-822c1550-23a9-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:37:37.278: INFO: Waiting for pod pod-822c1550-23a9-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:37:37.283: INFO: Pod pod-822c1550-23a9-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:37:37.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5shm8" for this suite.
Jan 29 09:37:43.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:37:43.397: INFO: namespace: e2e-tests-emptydir-5shm8, resource: bindings, ignored listing per whitelist
Jan 29 09:37:43.411: INFO: namespace e2e-tests-emptydir-5shm8 deletion completed in 6.123686735s

â€¢ [SLOW TEST:8.421 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:37:43.411: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-xshfk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:37:43.601: INFO: Creating deployment "nginx-deployment"
Jan 29 09:37:43.642: INFO: Waiting for observed generation 1
Jan 29 09:37:45.650: INFO: Waiting for all required pods to come up
Jan 29 09:37:45.655: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 29 09:37:47.675: INFO: Waiting for deployment "nginx-deployment" to complete
Jan 29 09:37:47.679: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jan 29 09:37:47.743: INFO: Updating deployment nginx-deployment
Jan 29 09:37:47.743: INFO: Waiting for observed generation 2
Jan 29 09:37:49.749: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 29 09:37:49.751: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 29 09:37:49.753: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 29 09:37:49.758: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 29 09:37:49.758: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 29 09:37:49.760: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 29 09:37:49.764: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jan 29 09:37:49.764: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jan 29 09:37:49.839: INFO: Updating deployment nginx-deployment
Jan 29 09:37:49.839: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jan 29 09:37:49.844: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 29 09:37:49.847: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 29 09:37:51.996: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-xshfk,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xshfk/deployments/nginx-deployment,UID:87345566-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690582,Generation:3,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-01-29 09:37:49 +0000 UTC 2019-01-29 09:37:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-01-29 09:37:50 +0000 UTC 2019-01-29 09:37:43 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jan 29 09:37:52.014: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-xshfk,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xshfk/replicasets/nginx-deployment-7dc8f79789,UID:89a849ed-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690580,Generation:3,CreationTimestamp:2019-01-29 09:37:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 87345566-23a9-11e9-a926-eeeeeeeeeeee 0xc420d9eb47 0xc420d9eb48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 29 09:37:52.014: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jan 29 09:37:52.014: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-xshfk,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xshfk/replicasets/nginx-deployment-7f9675fb8b,UID:87370ff1-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690532,Generation:3,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 87345566-23a9-11e9-a926-eeeeeeeeeeee 0xc420d9ec47 0xc420d9ec48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jan 29 09:37:52.040: INFO: Pod "nginx-deployment-7dc8f79789-6twzh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-6twzh,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-6twzh,UID:89b53d93-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690444,Generation:0,CreationTimestamp:2019-01-29 09:37:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc422392d77 0xc422392d78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422392ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422392f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:,StartTime:2019-01-29 09:37:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.041: INFO: Pod "nginx-deployment-7dc8f79789-7hpwm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-7hpwm,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-7hpwm,UID:89a92c08-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690420,Generation:0,CreationTimestamp:2019-01-29 09:37:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc422392fc0 0xc422392fc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422393330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422393360}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:,StartTime:2019-01-29 09:37:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.042: INFO: Pod "nginx-deployment-7dc8f79789-94d7g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-94d7g,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-94d7g,UID:89b33b68-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690445,Generation:0,CreationTimestamp:2019-01-29 09:37:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc422393480 0xc422393481}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422393580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223935a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:,StartTime:2019-01-29 09:37:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.042: INFO: Pod "nginx-deployment-7dc8f79789-b5vdw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-b5vdw,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-b5vdw,UID:8b110579-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690587,Generation:0,CreationTimestamp:2019-01-29 09:37:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc422393710 0xc422393711}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422393970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422393990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:,StartTime:2019-01-29 09:37:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.043: INFO: Pod "nginx-deployment-7dc8f79789-cbp6h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-cbp6h,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-cbp6h,UID:89ab6b5c-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690424,Generation:0,CreationTimestamp:2019-01-29 09:37:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b8090 0xc4221b8091}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b8110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b81a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:,StartTime:2019-01-29 09:37:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.043: INFO: Pod "nginx-deployment-7dc8f79789-fkqk7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-fkqk7,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-fkqk7,UID:89aa29bd-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690422,Generation:0,CreationTimestamp:2019-01-29 09:37:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b8510 0xc4221b8511}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b8590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b85b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:47 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:,StartTime:2019-01-29 09:37:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.044: INFO: Pod "nginx-deployment-7dc8f79789-fqfrw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-fqfrw,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-fqfrw,UID:8aea0616-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690513,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b88c0 0xc4221b88c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b8940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b8960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.044: INFO: Pod "nginx-deployment-7dc8f79789-jc46z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-jc46z,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-jc46z,UID:8afaf69b-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690578,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b8b70 0xc4221b8b71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b8d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b8d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:,StartTime:2019-01-29 09:37:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.045: INFO: Pod "nginx-deployment-7dc8f79789-kz427" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-kz427,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-kz427,UID:8afabb59-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690586,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b8f10 0xc4221b8f11}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b8f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b8fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:,StartTime:2019-01-29 09:37:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.045: INFO: Pod "nginx-deployment-7dc8f79789-v7pnh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-v7pnh,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-v7pnh,UID:8aee7f4d-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690541,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b9180 0xc4221b9181}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b9300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b9510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.045: INFO: Pod "nginx-deployment-7dc8f79789-wjj24" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-wjj24,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-wjj24,UID:8aedc745-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690530,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b9650 0xc4221b9651}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b96d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b96f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.046: INFO: Pod "nginx-deployment-7dc8f79789-z78rv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-z78rv,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-z78rv,UID:8afa2c28-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690577,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b9800 0xc4221b9801}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b9880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b98a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:,StartTime:2019-01-29 09:37:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.046: INFO: Pod "nginx-deployment-7dc8f79789-zmm65" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-zmm65,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7dc8f79789-zmm65,UID:8afb3683-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690579,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 89a849ed-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b9960 0xc4221b9961}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b99e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b9a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:,StartTime:2019-01-29 09:37:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.046: INFO: Pod "nginx-deployment-7f9675fb8b-77pd6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-77pd6,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-77pd6,UID:873c96f8-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690392,Generation:0,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4221b9bf0 0xc4221b9bf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4221b9c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4221b9c80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:10.1.148.59,StartTime:2019-01-29 09:37:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-29 09:37:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://22d8a8cc3889090dd0268e3112de63358e956220534455533e6cfd7dbe7564ac}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.047: INFO: Pod "nginx-deployment-7f9675fb8b-787dq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-787dq,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-787dq,UID:873c893e-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690367,Generation:0,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc42219a050 0xc42219a051}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42219a0c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42219a0e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:10.1.12.184,StartTime:2019-01-29 09:37:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-29 09:37:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://b02e2c771dd7cec50047047bbbd1b0c4cca8b6466c05c51fd4b444f1d75eb58b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.047: INFO: Pod "nginx-deployment-7f9675fb8b-79t97" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-79t97,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-79t97,UID:873a504a-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690371,Generation:0,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc42219a2f0 0xc42219a2f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42219a360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42219a440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:10.1.12.179,StartTime:2019-01-29 09:37:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-29 09:37:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://8d156adc3aa7954ce50a8880179d19f24202c64a0d0476f1c671b082b0e4deba}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.047: INFO: Pod "nginx-deployment-7f9675fb8b-7x5vr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-7x5vr,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-7x5vr,UID:8744618e-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690364,Generation:0,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc421f8a020 0xc421f8a021}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421f8a090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421f8a170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:10.1.12.183,StartTime:2019-01-29 09:37:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-29 09:37:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://567af1f332b1cd98212e064e7a7c17c5dc19f1957ce84c9feefd063422ecc683}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.048: INFO: Pod "nginx-deployment-7f9675fb8b-8n89p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-8n89p,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-8n89p,UID:8aee5661-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690537,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc421f8a6f0 0xc421f8a6f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421f8a800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421f8a820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.048: INFO: Pod "nginx-deployment-7f9675fb8b-dz2w8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-dz2w8,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-dz2w8,UID:873d01f9-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690374,Generation:0,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc421f8a9e7 0xc421f8a9e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421f8ab50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421f8ab70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:10.1.211.183,StartTime:2019-01-29 09:37:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-29 09:37:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://23de0424667afb6dcda8a1b04d35daeb006ca2bb411e2ba6d34d2e7f36b19569}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.049: INFO: Pod "nginx-deployment-7f9675fb8b-g7d27" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-g7d27,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-g7d27,UID:8ae98037-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690504,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d0097 0xc4222d0098}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d0180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d01a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.050: INFO: Pod "nginx-deployment-7f9675fb8b-gm25c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-gm25c,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-gm25c,UID:8aee149f-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690533,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d0257 0xc4222d0258}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d0410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d0430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.050: INFO: Pod "nginx-deployment-7f9675fb8b-jn9fz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-jn9fz,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-jn9fz,UID:8af7a17e-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690548,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d04e7 0xc4222d04e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d05e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d0600}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.050: INFO: Pod "nginx-deployment-7f9675fb8b-js7f5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-js7f5,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-js7f5,UID:8aeae09b-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690514,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d06b7 0xc4222d06b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d0730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d0750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.050: INFO: Pod "nginx-deployment-7f9675fb8b-kfkmk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-kfkmk,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-kfkmk,UID:8af7f21b-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690556,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d0807 0xc4222d0808}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d0880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d08a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:,StartTime:2019-01-29 09:37:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.051: INFO: Pod "nginx-deployment-7f9675fb8b-lndvz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-lndvz,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-lndvz,UID:8af77013-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690546,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d0957 0xc4222d0958}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d09d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d09f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:,StartTime:2019-01-29 09:37:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.051: INFO: Pod "nginx-deployment-7f9675fb8b-lr92p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-lr92p,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-lr92p,UID:8af628ac-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690555,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d0aa7 0xc4222d0aa8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d0b20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d0b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:,StartTime:2019-01-29 09:37:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.051: INFO: Pod "nginx-deployment-7f9675fb8b-pkmm2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-pkmm2,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-pkmm2,UID:8aeb1eeb-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690509,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d0bf7 0xc4222d0bf8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d0c70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d0c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.104,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.052: INFO: Pod "nginx-deployment-7f9675fb8b-r6p7t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-r6p7t,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-r6p7t,UID:8aee054c-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690542,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d0d47 0xc4222d0d48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d0dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d0de0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.053: INFO: Pod "nginx-deployment-7f9675fb8b-s4z8f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-s4z8f,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-s4z8f,UID:873bf655-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690376,Generation:0,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d0e97 0xc4222d0e98}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d0f10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d0f30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:10.1.211.169,StartTime:2019-01-29 09:37:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-29 09:37:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://b2a3f70ffcc7ec862ece37116636b0bcba0de333d04d044fa0335e50ea73942a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.053: INFO: Pod "nginx-deployment-7f9675fb8b-vk7jp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-vk7jp,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-vk7jp,UID:87460728-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690389,Generation:0,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d1017 0xc4222d1018}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d10a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d10c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:10.1.148.60,StartTime:2019-01-29 09:37:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-29 09:37:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://54a853c5149147c63c7390e755f216915f8172de97c6d86bff6f0dd69ee026fa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.054: INFO: Pod "nginx-deployment-7f9675fb8b-x6nf4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-x6nf4,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-x6nf4,UID:8745152e-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690380,Generation:0,CreationTimestamp:2019-01-29 09:37:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d1190 0xc4222d1191}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d1200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d1220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:43 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:10.1.211.188,StartTime:2019-01-29 09:37:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-29 09:37:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://47acb69ae9d5c58fa857e6db2c34f327ddf74c1f1ff01ecc4da3e834626d060d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.054: INFO: Pod "nginx-deployment-7f9675fb8b-z8l9w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-z8l9w,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-z8l9w,UID:8aee9556-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690543,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d1ad7 0xc4222d1ad8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.255.33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d1b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d1ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.255.33,PodIP:,StartTime:2019-01-29 09:37:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 29 09:37:52.054: INFO: Pod "nginx-deployment-7f9675fb8b-z99f7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-z99f7,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-xshfk,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xshfk/pods/nginx-deployment-7f9675fb8b-z99f7,UID:8af7cb38-23a9-11e9-a926-eeeeeeeeeeee,ResourceVersion:690568,Generation:0,CreationTimestamp:2019-01-29 09:37:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 87370ff1-23a9-11e9-a926-eeeeeeeeeeee 0xc4222d1ce7 0xc4222d1ce8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cpgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cpgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cpgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4222d1d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4222d1d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:37:49 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:,StartTime:2019-01-29 09:37:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:37:52.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xshfk" for this suite.
Jan 29 09:38:00.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:38:00.147: INFO: namespace: e2e-tests-deployment-xshfk, resource: bindings, ignored listing per whitelist
Jan 29 09:38:00.258: INFO: namespace e2e-tests-deployment-xshfk deletion completed in 8.197736504s

â€¢ [SLOW TEST:16.847 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:38:00.258: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-k8nqn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jan 29 09:38:00.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:00.950: INFO: stderr: ""
Jan 29 09:38:00.950: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 09:38:00.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:01.153: INFO: stderr: ""
Jan 29 09:38:01.153: INFO: stdout: "update-demo-nautilus-6lvsj update-demo-nautilus-bl2xq "
Jan 29 09:38:01.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-6lvsj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:01.309: INFO: stderr: ""
Jan 29 09:38:01.309: INFO: stdout: ""
Jan 29 09:38:01.309: INFO: update-demo-nautilus-6lvsj is created but not running
Jan 29 09:38:06.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:06.456: INFO: stderr: ""
Jan 29 09:38:06.456: INFO: stdout: "update-demo-nautilus-6lvsj update-demo-nautilus-bl2xq "
Jan 29 09:38:06.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-6lvsj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:06.576: INFO: stderr: ""
Jan 29 09:38:06.576: INFO: stdout: "true"
Jan 29 09:38:06.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-6lvsj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:06.743: INFO: stderr: ""
Jan 29 09:38:06.743: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:38:06.743: INFO: validating pod update-demo-nautilus-6lvsj
Jan 29 09:38:06.747: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:38:06.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:38:06.747: INFO: update-demo-nautilus-6lvsj is verified up and running
Jan 29 09:38:06.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-bl2xq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:06.868: INFO: stderr: ""
Jan 29 09:38:06.868: INFO: stdout: "true"
Jan 29 09:38:06.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-bl2xq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:07.006: INFO: stderr: ""
Jan 29 09:38:07.006: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:38:07.006: INFO: validating pod update-demo-nautilus-bl2xq
Jan 29 09:38:07.009: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:38:07.009: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:38:07.009: INFO: update-demo-nautilus-bl2xq is verified up and running
STEP: scaling down the replication controller
Jan 29 09:38:07.012: INFO: scanned /root for discovery docs: <nil>
Jan 29 09:38:07.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:08.181: INFO: stderr: ""
Jan 29 09:38:08.181: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 09:38:08.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:08.317: INFO: stderr: ""
Jan 29 09:38:08.317: INFO: stdout: "update-demo-nautilus-6lvsj update-demo-nautilus-bl2xq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 29 09:38:13.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:13.441: INFO: stderr: ""
Jan 29 09:38:13.441: INFO: stdout: "update-demo-nautilus-bl2xq "
Jan 29 09:38:13.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-bl2xq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:13.568: INFO: stderr: ""
Jan 29 09:38:13.568: INFO: stdout: "true"
Jan 29 09:38:13.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-bl2xq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:13.720: INFO: stderr: ""
Jan 29 09:38:13.720: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:38:13.720: INFO: validating pod update-demo-nautilus-bl2xq
Jan 29 09:38:13.724: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:38:13.724: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:38:13.724: INFO: update-demo-nautilus-bl2xq is verified up and running
STEP: scaling up the replication controller
Jan 29 09:38:13.727: INFO: scanned /root for discovery docs: <nil>
Jan 29 09:38:13.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:14.886: INFO: stderr: ""
Jan 29 09:38:14.886: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 09:38:14.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:15.028: INFO: stderr: ""
Jan 29 09:38:15.028: INFO: stdout: "update-demo-nautilus-bl2xq update-demo-nautilus-wd7bf "
Jan 29 09:38:15.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-bl2xq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:15.174: INFO: stderr: ""
Jan 29 09:38:15.174: INFO: stdout: "true"
Jan 29 09:38:15.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-bl2xq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:15.302: INFO: stderr: ""
Jan 29 09:38:15.302: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:38:15.302: INFO: validating pod update-demo-nautilus-bl2xq
Jan 29 09:38:15.306: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:38:15.306: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:38:15.306: INFO: update-demo-nautilus-bl2xq is verified up and running
Jan 29 09:38:15.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-wd7bf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:15.434: INFO: stderr: ""
Jan 29 09:38:15.434: INFO: stdout: ""
Jan 29 09:38:15.434: INFO: update-demo-nautilus-wd7bf is created but not running
Jan 29 09:38:20.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:20.557: INFO: stderr: ""
Jan 29 09:38:20.557: INFO: stdout: "update-demo-nautilus-bl2xq update-demo-nautilus-wd7bf "
Jan 29 09:38:20.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-bl2xq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:20.733: INFO: stderr: ""
Jan 29 09:38:20.733: INFO: stdout: "true"
Jan 29 09:38:20.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-bl2xq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:20.877: INFO: stderr: ""
Jan 29 09:38:20.877: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:38:20.877: INFO: validating pod update-demo-nautilus-bl2xq
Jan 29 09:38:20.879: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:38:20.879: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:38:20.879: INFO: update-demo-nautilus-bl2xq is verified up and running
Jan 29 09:38:20.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-wd7bf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:21.057: INFO: stderr: ""
Jan 29 09:38:21.057: INFO: stdout: "true"
Jan 29 09:38:21.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-wd7bf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:21.193: INFO: stderr: ""
Jan 29 09:38:21.193: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:38:21.193: INFO: validating pod update-demo-nautilus-wd7bf
Jan 29 09:38:21.198: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:38:21.198: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:38:21.198: INFO: update-demo-nautilus-wd7bf is verified up and running
STEP: using delete to clean up resources
Jan 29 09:38:21.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:21.337: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:38:21.337: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 29 09:38:21.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-k8nqn'
Jan 29 09:38:21.512: INFO: stderr: "No resources found.\n"
Jan 29 09:38:21.512: INFO: stdout: ""
Jan 29 09:38:21.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -l name=update-demo --namespace=e2e-tests-kubectl-k8nqn -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 09:38:21.670: INFO: stderr: ""
Jan 29 09:38:21.670: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:38:21.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-k8nqn" for this suite.
Jan 29 09:38:43.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:38:43.766: INFO: namespace: e2e-tests-kubectl-k8nqn, resource: bindings, ignored listing per whitelist
Jan 29 09:38:43.816: INFO: namespace e2e-tests-kubectl-k8nqn deletion completed in 22.141783956s

â€¢ [SLOW TEST:43.558 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:38:43.818: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-hh8tl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-hh8tl
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-hh8tl
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-hh8tl
Jan 29 09:38:44.033: INFO: Found 0 stateful pods, waiting for 1
Jan 29 09:38:54.037: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 29 09:38:54.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 09:38:54.324: INFO: stderr: ""
Jan 29 09:38:54.324: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 09:38:54.324: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 29 09:38:54.327: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 29 09:39:04.330: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 09:39:04.330: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 09:39:04.437: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:04.437: INFO: ss-0  9.111.254.123  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:44 +0000 UTC  }]
Jan 29 09:39:04.437: INFO: 
Jan 29 09:39:04.437: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 29 09:39:05.445: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992998821s
Jan 29 09:39:06.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989237541s
Jan 29 09:39:07.452: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985347602s
Jan 29 09:39:08.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981746145s
Jan 29 09:39:09.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97641465s
Jan 29 09:39:10.465: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972294986s
Jan 29 09:39:11.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968721222s
Jan 29 09:39:12.474: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964101609s
Jan 29 09:39:13.477: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.337246ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-hh8tl
Jan 29 09:39:14.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:39:14.781: INFO: stderr: ""
Jan 29 09:39:14.781: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 29 09:39:14.781: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 29 09:39:14.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:39:15.072: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jan 29 09:39:15.072: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 29 09:39:15.072: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 29 09:39:15.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:39:15.330: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jan 29 09:39:15.330: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 29 09:39:15.330: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 29 09:39:15.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 29 09:39:25.338: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 09:39:25.338: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 09:39:25.338: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 29 09:39:25.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 09:39:25.617: INFO: stderr: ""
Jan 29 09:39:25.617: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 09:39:25.617: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 29 09:39:25.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 09:39:25.908: INFO: stderr: ""
Jan 29 09:39:25.908: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 09:39:25.909: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 29 09:39:25.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 29 09:39:26.195: INFO: stderr: ""
Jan 29 09:39:26.195: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 29 09:39:26.195: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 29 09:39:26.195: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 09:39:26.198: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 29 09:39:36.206: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 09:39:36.206: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 09:39:36.206: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 09:39:36.221: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:36.221: INFO: ss-0  9.111.254.123  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:44 +0000 UTC  }]
Jan 29 09:39:36.221: INFO: ss-1  9.111.254.104  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:36.221: INFO: ss-2  9.111.255.33   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:36.221: INFO: 
Jan 29 09:39:36.221: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 09:39:37.224: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:37.224: INFO: ss-0  9.111.254.123  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:44 +0000 UTC  }]
Jan 29 09:39:37.224: INFO: ss-1  9.111.254.104  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:37.224: INFO: ss-2  9.111.255.33   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:37.225: INFO: 
Jan 29 09:39:37.225: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 09:39:38.228: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:38.228: INFO: ss-0  9.111.254.123  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:38:44 +0000 UTC  }]
Jan 29 09:39:38.228: INFO: ss-1  9.111.254.104  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:38.228: INFO: ss-2  9.111.255.33   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:38.228: INFO: 
Jan 29 09:39:38.228: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 09:39:39.231: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:39.231: INFO: ss-1  9.111.254.104  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:39.231: INFO: 
Jan 29 09:39:39.231: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 09:39:40.236: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:40.236: INFO: ss-1  9.111.254.104  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:40.236: INFO: 
Jan 29 09:39:40.236: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 09:39:41.239: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:41.239: INFO: ss-1  9.111.254.104  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:41.239: INFO: 
Jan 29 09:39:41.239: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 09:39:42.243: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:42.243: INFO: ss-1  9.111.254.104  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:42.243: INFO: 
Jan 29 09:39:42.243: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 09:39:43.247: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:43.247: INFO: ss-1  9.111.254.104  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:43.247: INFO: 
Jan 29 09:39:43.247: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 09:39:44.253: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:44.253: INFO: ss-1  9.111.254.104  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:44.254: INFO: 
Jan 29 09:39:44.254: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 09:39:45.257: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 29 09:39:45.257: INFO: ss-1  9.111.254.104  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:39:04 +0000 UTC  }]
Jan 29 09:39:45.257: INFO: 
Jan 29 09:39:45.257: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-hh8tl
Jan 29 09:39:46.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:39:46.444: INFO: rc: 1
Jan 29 09:39:46.444: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc4230d9cb0 exit status 1 <nil> <nil> true [0xc4201240c8 0xc420124208 0xc4201242d8] [0xc4201240c8 0xc420124208 0xc4201242d8] [0xc4201241b8 0xc420124258] [0x8fd520 0x8fd520] 0xc421b88120 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Jan 29 09:39:56.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:39:56.555: INFO: rc: 1
Jan 29 09:39:56.556: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42278e780 exit status 1 <nil> <nil> true [0xc421926158 0xc4219261b0 0xc4219261f8] [0xc421926158 0xc4219261b0 0xc4219261f8] [0xc421926180 0xc4219261e0] [0x8fd520 0x8fd520] 0xc42253a180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:40:06.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:40:06.683: INFO: rc: 1
Jan 29 09:40:06.683: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc422d06120 exit status 1 <nil> <nil> true [0xc420125b50 0xc420125b78 0xc420125c60] [0xc420125b50 0xc420125b78 0xc420125c60] [0xc420125b60 0xc420125bc8] [0x8fd520 0x8fd520] 0xc421b88240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:40:16.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:40:16.793: INFO: rc: 1
Jan 29 09:40:16.793: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42278ee70 exit status 1 <nil> <nil> true [0xc421926208 0xc421926258 0xc421926290] [0xc421926208 0xc421926258 0xc421926290] [0xc421926230 0xc421926280] [0x8fd520 0x8fd520] 0xc42253a3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:40:26.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:40:26.900: INFO: rc: 1
Jan 29 09:40:26.900: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc422d06870 exit status 1 <nil> <nil> true [0xc420125c88 0xc420125cb8 0xc420125d00] [0xc420125c88 0xc420125cb8 0xc420125d00] [0xc420125ca0 0xc420125cf8] [0x8fd520 0x8fd520] 0xc421b88360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:40:36.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:40:37.006: INFO: rc: 1
Jan 29 09:40:37.006: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42132c3f0 exit status 1 <nil> <nil> true [0xc42000eb00 0xc42000f878 0xc42000f8f0] [0xc42000eb00 0xc42000f878 0xc42000f8f0] [0xc42000f858 0xc42000f8c8] [0x8fd520 0x8fd520] 0xc421d6a060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:40:47.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:40:47.117: INFO: rc: 1
Jan 29 09:40:47.117: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42132c840 exit status 1 <nil> <nil> true [0xc42000f918 0xc42000fb70 0xc42000fbe8] [0xc42000f918 0xc42000fb70 0xc42000fbe8] [0xc42000fa00 0xc42000fbe0] [0x8fd520 0x8fd520] 0xc421d6a180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:40:57.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:40:57.219: INFO: rc: 1
Jan 29 09:40:57.219: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc422d06c60 exit status 1 <nil> <nil> true [0xc420125d08 0xc420125d58 0xc420125d78] [0xc420125d08 0xc420125d58 0xc420125d78] [0xc420125d18 0xc420125d70] [0x8fd520 0x8fd520] 0xc421b884e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:41:07.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:41:07.327: INFO: rc: 1
Jan 29 09:41:07.327: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42132cde0 exit status 1 <nil> <nil> true [0xc42000fc10 0xc42000fc48 0xc42000fc78] [0xc42000fc10 0xc42000fc48 0xc42000fc78] [0xc42000fc30 0xc42000fc70] [0x8fd520 0x8fd520] 0xc421d6a2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:41:17.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:41:17.432: INFO: rc: 1
Jan 29 09:41:17.432: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42132d230 exit status 1 <nil> <nil> true [0xc42000fc88 0xc42000fce0 0xc42000fda0] [0xc42000fc88 0xc42000fce0 0xc42000fda0] [0xc42000fcb8 0xc42000fd60] [0x8fd520 0x8fd520] 0xc421d6bd40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:41:27.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:41:27.542: INFO: rc: 1
Jan 29 09:41:27.542: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42278f350 exit status 1 <nil> <nil> true [0xc421926298 0xc4219262e0 0xc421926318] [0xc421926298 0xc4219262e0 0xc421926318] [0xc4219262c0 0xc421926310] [0x8fd520 0x8fd520] 0xc42253a4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:41:37.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:41:37.667: INFO: rc: 1
Jan 29 09:41:37.667: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc422d07110 exit status 1 <nil> <nil> true [0xc420125d80 0xc420125da8 0xc420125de0] [0xc420125d80 0xc420125da8 0xc420125de0] [0xc420125d98 0xc420125dc8] [0x8fd520 0x8fd520] 0xc421b88600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:41:47.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:41:47.793: INFO: rc: 1
Jan 29 09:41:47.793: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc4230d9f80 exit status 1 <nil> <nil> true [0xc4200e0098 0xc4200e0510 0xc4200e05c8] [0xc4200e0098 0xc4200e0510 0xc4200e05c8] [0xc4200e04d0 0xc4200e0588] [0x8fd520 0x8fd520] 0xc4222e2240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:41:57.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:41:57.909: INFO: rc: 1
Jan 29 09:41:57.909: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc423413f50 exit status 1 <nil> <nil> true [0xc421926058 0xc4219260c8 0xc421926168] [0xc421926058 0xc4219260c8 0xc421926168] [0xc4219260b8 0xc421926158] [0x8fd520 0x8fd520] 0xc42253a060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:42:07.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:42:08.039: INFO: rc: 1
Jan 29 09:42:08.039: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42278e0c0 exit status 1 <nil> <nil> true [0xc42000eb00 0xc42000f878 0xc42000f8f0] [0xc42000eb00 0xc42000f878 0xc42000f8f0] [0xc42000f858 0xc42000f8c8] [0x8fd520 0x8fd520] 0xc421d6a060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:42:18.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:42:18.154: INFO: rc: 1
Jan 29 09:42:18.154: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42132c360 exit status 1 <nil> <nil> true [0xc421926180 0xc4219261e0 0xc421926228] [0xc421926180 0xc4219261e0 0xc421926228] [0xc4219261d8 0xc421926208] [0x8fd520 0x8fd520] 0xc42253a180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:42:28.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:42:28.257: INFO: rc: 1
Jan 29 09:42:28.257: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42132c780 exit status 1 <nil> <nil> true [0xc421926230 0xc421926280 0xc4219262a8] [0xc421926230 0xc421926280 0xc4219262a8] [0xc421926260 0xc421926298] [0x8fd520 0x8fd520] 0xc42253a3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:42:38.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:42:38.388: INFO: rc: 1
Jan 29 09:42:38.388: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42278e4b0 exit status 1 <nil> <nil> true [0xc42000f918 0xc42000fb70 0xc42000fbe8] [0xc42000f918 0xc42000fb70 0xc42000fbe8] [0xc42000fa00 0xc42000fbe0] [0x8fd520 0x8fd520] 0xc421d6a180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:42:48.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:42:48.495: INFO: rc: 1
Jan 29 09:42:48.495: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc4227d03c0 exit status 1 <nil> <nil> true [0xc4200e05f0 0xc4200e0710 0xc4200e0798] [0xc4200e05f0 0xc4200e0710 0xc4200e0798] [0xc4200e06a8 0xc4200e0788] [0x8fd520 0x8fd520] 0xc4222e2480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:42:58.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:42:58.605: INFO: rc: 1
Jan 29 09:42:58.605: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42132cd80 exit status 1 <nil> <nil> true [0xc4219262c0 0xc421926310 0xc421926340] [0xc4219262c0 0xc421926310 0xc421926340] [0xc421926300 0xc421926338] [0x8fd520 0x8fd520] 0xc42253a4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:43:08.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:43:08.713: INFO: rc: 1
Jan 29 09:43:08.713: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc422d06780 exit status 1 <nil> <nil> true [0xc4201240c8 0xc420124208 0xc4201242d8] [0xc4201240c8 0xc420124208 0xc4201242d8] [0xc4201241b8 0xc420124258] [0x8fd520 0x8fd520] 0xc421b880c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:43:18.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:43:18.847: INFO: rc: 1
Jan 29 09:43:18.848: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42132d1a0 exit status 1 <nil> <nil> true [0xc421926358 0xc4219263e0 0xc421926450] [0xc421926358 0xc4219263e0 0xc421926450] [0xc421926398 0xc421926408] [0x8fd520 0x8fd520] 0xc42253a600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:43:28.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:43:28.967: INFO: rc: 1
Jan 29 09:43:28.967: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc4228c80c0 exit status 1 <nil> <nil> true [0xc4200e07e0 0xc4200e0860 0xc4200e08e0] [0xc4200e07e0 0xc4200e0860 0xc4200e08e0] [0xc4200e0828 0xc4200e08d0] [0x8fd520 0x8fd520] 0xc4222e25a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:43:38.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:43:39.081: INFO: rc: 1
Jan 29 09:43:39.081: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc4228c8510 exit status 1 <nil> <nil> true [0xc4200e0900 0xc4200e09a8 0xc4200e0a18] [0xc4200e0900 0xc4200e09a8 0xc4200e0a18] [0xc4200e0960 0xc4200e09f8] [0x8fd520 0x8fd520] 0xc4222e27e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:43:49.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:43:49.185: INFO: rc: 1
Jan 29 09:43:49.185: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42278e4e0 exit status 1 <nil> <nil> true [0xc421926458 0xc421926488 0xc4219264c8] [0xc421926458 0xc421926488 0xc4219264c8] [0xc421926470 0xc4219264b0] [0x8fd520 0x8fd520] 0xc421d6a240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:43:59.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:43:59.297: INFO: rc: 1
Jan 29 09:43:59.298: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc4230d9b60 exit status 1 <nil> <nil> true [0xc42000eb00 0xc42000f878 0xc42000f8f0] [0xc42000eb00 0xc42000f878 0xc42000f8f0] [0xc42000f858 0xc42000f8c8] [0x8fd520 0x8fd520] 0xc4222e2240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:44:09.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:44:09.413: INFO: rc: 1
Jan 29 09:44:09.413: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc4227d0450 exit status 1 <nil> <nil> true [0xc421926058 0xc4219260c8 0xc421926168] [0xc421926058 0xc4219260c8 0xc421926168] [0xc4219260b8 0xc421926158] [0x8fd520 0x8fd520] 0xc421d6a060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:44:19.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:44:19.526: INFO: rc: 1
Jan 29 09:44:19.526: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc4228c8090 exit status 1 <nil> <nil> true [0xc42000f918 0xc42000fb70 0xc42000fbe8] [0xc42000f918 0xc42000fb70 0xc42000fbe8] [0xc42000fa00 0xc42000fbe0] [0x8fd520 0x8fd520] 0xc4222e2480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:44:29.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:44:29.633: INFO: rc: 1
Jan 29 09:44:29.633: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc42278e150 exit status 1 <nil> <nil> true [0xc421926180 0xc4219261e0 0xc421926228] [0xc421926180 0xc4219261e0 0xc421926228] [0xc4219261d8 0xc421926208] [0x8fd520 0x8fd520] 0xc421d6a180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:44:39.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:44:39.750: INFO: rc: 1
Jan 29 09:44:39.750: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc4228c8540 exit status 1 <nil> <nil> true [0xc42000fc10 0xc42000fc48 0xc42000fc78] [0xc42000fc10 0xc42000fc48 0xc42000fc78] [0xc42000fc30 0xc42000fc70] [0x8fd520 0x8fd520] 0xc4222e25a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Jan 29 09:44:49.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 exec --namespace=e2e-tests-statefulset-hh8tl ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 29 09:44:49.858: INFO: rc: 1
Jan 29 09:44:49.858: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Jan 29 09:44:49.858: INFO: Scaling statefulset ss to 0
Jan 29 09:44:49.868: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 29 09:44:49.871: INFO: Deleting all statefulset in ns e2e-tests-statefulset-hh8tl
Jan 29 09:44:49.872: INFO: Scaling statefulset ss to 0
Jan 29 09:44:49.933: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 09:44:49.936: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:44:49.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-hh8tl" for this suite.
Jan 29 09:44:55.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:44:55.992: INFO: namespace: e2e-tests-statefulset-hh8tl, resource: bindings, ignored listing per whitelist
Jan 29 09:44:56.063: INFO: namespace e2e-tests-statefulset-hh8tl deletion completed in 6.114173319s

â€¢ [SLOW TEST:372.244 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:44:56.063: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-mzzmz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:44:56.244: INFO: Creating ReplicaSet my-hostname-basic-890fdfa5-23aa-11e9-9015-b2c9bac28373
Jan 29 09:44:56.336: INFO: Pod name my-hostname-basic-890fdfa5-23aa-11e9-9015-b2c9bac28373: Found 0 pods out of 1
Jan 29 09:45:01.340: INFO: Pod name my-hostname-basic-890fdfa5-23aa-11e9-9015-b2c9bac28373: Found 1 pods out of 1
Jan 29 09:45:01.340: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-890fdfa5-23aa-11e9-9015-b2c9bac28373" is running
Jan 29 09:45:01.342: INFO: Pod "my-hostname-basic-890fdfa5-23aa-11e9-9015-b2c9bac28373-4hrr8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-29 09:44:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-29 09:44:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-29 09:44:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-29 09:44:56 +0000 UTC Reason: Message:}])
Jan 29 09:45:01.342: INFO: Trying to dial the pod
Jan 29 09:45:06.352: INFO: Controller my-hostname-basic-890fdfa5-23aa-11e9-9015-b2c9bac28373: Got expected result from replica 1 [my-hostname-basic-890fdfa5-23aa-11e9-9015-b2c9bac28373-4hrr8]: "my-hostname-basic-890fdfa5-23aa-11e9-9015-b2c9bac28373-4hrr8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:45:06.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-mzzmz" for this suite.
Jan 29 09:45:12.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:45:12.410: INFO: namespace: e2e-tests-replicaset-mzzmz, resource: bindings, ignored listing per whitelist
Jan 29 09:45:12.472: INFO: namespace e2e-tests-replicaset-mzzmz deletion completed in 6.114484357s

â€¢ [SLOW TEST:16.409 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:45:12.472: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-pdx55
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Jan 29 09:45:12.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:13.147: INFO: stderr: ""
Jan 29 09:45:13.148: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 09:45:13.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:13.282: INFO: stderr: ""
Jan 29 09:45:13.282: INFO: stdout: "update-demo-nautilus-8vvp2 update-demo-nautilus-rw8b8 "
Jan 29 09:45:13.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-8vvp2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:13.414: INFO: stderr: ""
Jan 29 09:45:13.415: INFO: stdout: ""
Jan 29 09:45:13.415: INFO: update-demo-nautilus-8vvp2 is created but not running
Jan 29 09:45:18.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:18.557: INFO: stderr: ""
Jan 29 09:45:18.557: INFO: stdout: "update-demo-nautilus-8vvp2 update-demo-nautilus-rw8b8 "
Jan 29 09:45:18.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-8vvp2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:18.677: INFO: stderr: ""
Jan 29 09:45:18.677: INFO: stdout: "true"
Jan 29 09:45:18.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-8vvp2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:18.797: INFO: stderr: ""
Jan 29 09:45:18.797: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:45:18.797: INFO: validating pod update-demo-nautilus-8vvp2
Jan 29 09:45:18.800: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:45:18.800: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:45:18.800: INFO: update-demo-nautilus-8vvp2 is verified up and running
Jan 29 09:45:18.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-rw8b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:18.939: INFO: stderr: ""
Jan 29 09:45:18.940: INFO: stdout: "true"
Jan 29 09:45:18.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-nautilus-rw8b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:19.068: INFO: stderr: ""
Jan 29 09:45:19.068: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 09:45:19.068: INFO: validating pod update-demo-nautilus-rw8b8
Jan 29 09:45:19.074: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 09:45:19.074: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 09:45:19.074: INFO: update-demo-nautilus-rw8b8 is verified up and running
STEP: rolling-update to new replication controller
Jan 29 09:45:19.077: INFO: scanned /root for discovery docs: <nil>
Jan 29 09:45:19.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:41.649: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 29 09:45:41.649: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 09:45:41.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:41.790: INFO: stderr: ""
Jan 29 09:45:41.790: INFO: stdout: "update-demo-kitten-b58wb update-demo-kitten-gnlfq "
Jan 29 09:45:41.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-kitten-b58wb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:41.963: INFO: stderr: ""
Jan 29 09:45:41.963: INFO: stdout: "true"
Jan 29 09:45:41.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-kitten-b58wb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:42.099: INFO: stderr: ""
Jan 29 09:45:42.099: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 29 09:45:42.099: INFO: validating pod update-demo-kitten-b58wb
Jan 29 09:45:42.115: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 29 09:45:42.115: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 29 09:45:42.115: INFO: update-demo-kitten-b58wb is verified up and running
Jan 29 09:45:42.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-kitten-gnlfq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:42.263: INFO: stderr: ""
Jan 29 09:45:42.263: INFO: stdout: "true"
Jan 29 09:45:42.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods update-demo-kitten-gnlfq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-pdx55'
Jan 29 09:45:42.394: INFO: stderr: ""
Jan 29 09:45:42.394: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 29 09:45:42.394: INFO: validating pod update-demo-kitten-gnlfq
Jan 29 09:45:42.416: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 29 09:45:42.417: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 29 09:45:42.417: INFO: update-demo-kitten-gnlfq is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:45:42.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pdx55" for this suite.
Jan 29 09:46:04.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:46:04.546: INFO: namespace: e2e-tests-kubectl-pdx55, resource: bindings, ignored listing per whitelist
Jan 29 09:46:04.592: INFO: namespace e2e-tests-kubectl-pdx55 deletion completed in 22.171919302s

â€¢ [SLOW TEST:52.120 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:46:04.593: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rhzgq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-b1e8db68-23aa-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 09:46:04.842: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b1e95df7-23aa-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-rhzgq" to be "success or failure"
Jan 29 09:46:04.845: INFO: Pod "pod-projected-configmaps-b1e95df7-23aa-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.817922ms
Jan 29 09:46:06.849: INFO: Pod "pod-projected-configmaps-b1e95df7-23aa-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006499758s
STEP: Saw pod success
Jan 29 09:46:06.849: INFO: Pod "pod-projected-configmaps-b1e95df7-23aa-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:46:06.851: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-configmaps-b1e95df7-23aa-11e9-9015-b2c9bac28373 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 09:46:06.866: INFO: Waiting for pod pod-projected-configmaps-b1e95df7-23aa-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:46:06.869: INFO: Pod pod-projected-configmaps-b1e95df7-23aa-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:46:06.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rhzgq" for this suite.
Jan 29 09:46:12.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:46:12.924: INFO: namespace: e2e-tests-projected-rhzgq, resource: bindings, ignored listing per whitelist
Jan 29 09:46:12.976: INFO: namespace e2e-tests-projected-rhzgq deletion completed in 6.103839517s

â€¢ [SLOW TEST:8.383 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:46:12.978: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-hkb6c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 29 09:46:13.236: INFO: Waiting up to 5m0s for pod "pod-b6e771ae-23aa-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-hkb6c" to be "success or failure"
Jan 29 09:46:13.238: INFO: Pod "pod-b6e771ae-23aa-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315796ms
Jan 29 09:46:15.244: INFO: Pod "pod-b6e771ae-23aa-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008236561s
STEP: Saw pod success
Jan 29 09:46:15.244: INFO: Pod "pod-b6e771ae-23aa-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:46:15.247: INFO: Trying to get logs from node 9.111.254.123 pod pod-b6e771ae-23aa-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:46:15.265: INFO: Waiting for pod pod-b6e771ae-23aa-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:46:15.267: INFO: Pod pod-b6e771ae-23aa-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:46:15.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-hkb6c" for this suite.
Jan 29 09:46:21.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:46:21.314: INFO: namespace: e2e-tests-emptydir-hkb6c, resource: bindings, ignored listing per whitelist
Jan 29 09:46:21.394: INFO: namespace e2e-tests-emptydir-hkb6c deletion completed in 6.123183669s

â€¢ [SLOW TEST:8.416 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:46:21.396: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rgfbd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:46:21.643: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bbec48b5-23aa-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-rgfbd" to be "success or failure"
Jan 29 09:46:21.645: INFO: Pod "downwardapi-volume-bbec48b5-23aa-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137365ms
Jan 29 09:46:23.649: INFO: Pod "downwardapi-volume-bbec48b5-23aa-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005878568s
STEP: Saw pod success
Jan 29 09:46:23.649: INFO: Pod "downwardapi-volume-bbec48b5-23aa-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:46:23.651: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-bbec48b5-23aa-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:46:23.692: INFO: Waiting for pod downwardapi-volume-bbec48b5-23aa-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:46:23.697: INFO: Pod downwardapi-volume-bbec48b5-23aa-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:46:23.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rgfbd" for this suite.
Jan 29 09:46:29.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:46:29.749: INFO: namespace: e2e-tests-projected-rgfbd, resource: bindings, ignored listing per whitelist
Jan 29 09:46:29.920: INFO: namespace e2e-tests-projected-rgfbd deletion completed in 6.21575931s

â€¢ [SLOW TEST:8.523 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:46:29.920: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rh2c5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:46:30.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1016bdb-23aa-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-rh2c5" to be "success or failure"
Jan 29 09:46:30.155: INFO: Pod "downwardapi-volume-c1016bdb-23aa-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.66052ms
Jan 29 09:46:32.158: INFO: Pod "downwardapi-volume-c1016bdb-23aa-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005460653s
STEP: Saw pod success
Jan 29 09:46:32.158: INFO: Pod "downwardapi-volume-c1016bdb-23aa-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:46:32.160: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-c1016bdb-23aa-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:46:32.184: INFO: Waiting for pod downwardapi-volume-c1016bdb-23aa-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:46:32.187: INFO: Pod downwardapi-volume-c1016bdb-23aa-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:46:32.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rh2c5" for this suite.
Jan 29 09:46:38.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:46:38.311: INFO: namespace: e2e-tests-downward-api-rh2c5, resource: bindings, ignored listing per whitelist
Jan 29 09:46:38.314: INFO: namespace e2e-tests-downward-api-rh2c5 deletion completed in 6.121852244s

â€¢ [SLOW TEST:8.394 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:46:38.314: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-r7k6c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:46:38.509: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
Jan 29 09:46:38.515: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-r7k6c/daemonsets","resourceVersion":"692479"},"items":null}

Jan 29 09:46:38.517: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-r7k6c/pods","resourceVersion":"692479"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:46:38.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-r7k6c" for this suite.
Jan 29 09:46:44.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:46:44.657: INFO: namespace: e2e-tests-daemonsets-r7k6c, resource: bindings, ignored listing per whitelist
Jan 29 09:46:44.667: INFO: namespace e2e-tests-daemonsets-r7k6c deletion completed in 6.132751527s

S [SKIPPING] [6.353 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Jan 29 09:46:38.510: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:46:44.667: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-frwqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 29 09:46:44.947: INFO: Waiting up to 5m0s for pod "downward-api-c9ca0cf7-23aa-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-frwqg" to be "success or failure"
Jan 29 09:46:44.949: INFO: Pod "downward-api-c9ca0cf7-23aa-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076308ms
Jan 29 09:46:46.952: INFO: Pod "downward-api-c9ca0cf7-23aa-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004760022s
STEP: Saw pod success
Jan 29 09:46:46.952: INFO: Pod "downward-api-c9ca0cf7-23aa-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:46:46.954: INFO: Trying to get logs from node 9.111.254.123 pod downward-api-c9ca0cf7-23aa-11e9-9015-b2c9bac28373 container dapi-container: <nil>
STEP: delete the pod
Jan 29 09:46:46.983: INFO: Waiting for pod downward-api-c9ca0cf7-23aa-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:46:46.989: INFO: Pod downward-api-c9ca0cf7-23aa-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:46:46.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-frwqg" for this suite.
Jan 29 09:46:53.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:46:53.109: INFO: namespace: e2e-tests-downward-api-frwqg, resource: bindings, ignored listing per whitelist
Jan 29 09:46:53.122: INFO: namespace e2e-tests-downward-api-frwqg deletion completed in 6.127498051s

â€¢ [SLOW TEST:8.455 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:46:53.122: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-ctswx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-lvcl
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 09:46:53.345: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lvcl" in namespace "e2e-tests-subpath-ctswx" to be "success or failure"
Jan 29 09:46:53.349: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.953124ms
Jan 29 09:46:55.354: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009287165s
Jan 29 09:46:57.357: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 4.012365159s
Jan 29 09:46:59.360: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 6.015193839s
Jan 29 09:47:01.363: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 8.017990762s
Jan 29 09:47:03.366: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 10.021064167s
Jan 29 09:47:05.369: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 12.024127843s
Jan 29 09:47:07.372: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 14.026854661s
Jan 29 09:47:09.375: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 16.029928065s
Jan 29 09:47:11.378: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 18.032943605s
Jan 29 09:47:13.381: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 20.035951223s
Jan 29 09:47:15.384: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Running", Reason="", readiness=false. Elapsed: 22.039371119s
Jan 29 09:47:17.387: INFO: Pod "pod-subpath-test-downwardapi-lvcl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042386519s
STEP: Saw pod success
Jan 29 09:47:17.388: INFO: Pod "pod-subpath-test-downwardapi-lvcl" satisfied condition "success or failure"
Jan 29 09:47:17.390: INFO: Trying to get logs from node 9.111.254.123 pod pod-subpath-test-downwardapi-lvcl container test-container-subpath-downwardapi-lvcl: <nil>
STEP: delete the pod
Jan 29 09:47:17.409: INFO: Waiting for pod pod-subpath-test-downwardapi-lvcl to disappear
Jan 29 09:47:17.411: INFO: Pod pod-subpath-test-downwardapi-lvcl no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-lvcl
Jan 29 09:47:17.411: INFO: Deleting pod "pod-subpath-test-downwardapi-lvcl" in namespace "e2e-tests-subpath-ctswx"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:47:17.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-ctswx" for this suite.
Jan 29 09:47:23.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:47:23.555: INFO: namespace: e2e-tests-subpath-ctswx, resource: bindings, ignored listing per whitelist
Jan 29 09:47:23.627: INFO: namespace e2e-tests-subpath-ctswx deletion completed in 6.210660878s

â€¢ [SLOW TEST:30.506 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:47:23.628: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-fkgs8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-dh4gm
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-p559f
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:47:30.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-fkgs8" for this suite.
Jan 29 09:47:36.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:47:36.194: INFO: namespace: e2e-tests-namespaces-fkgs8, resource: bindings, ignored listing per whitelist
Jan 29 09:47:36.227: INFO: namespace e2e-tests-namespaces-fkgs8 deletion completed in 6.138996163s
STEP: Destroying namespace "e2e-tests-nsdeletetest-dh4gm" for this suite.
Jan 29 09:47:36.230: INFO: Namespace e2e-tests-nsdeletetest-dh4gm was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-p559f" for this suite.
Jan 29 09:47:42.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:47:42.353: INFO: namespace: e2e-tests-nsdeletetest-p559f, resource: bindings, ignored listing per whitelist
Jan 29 09:47:42.380: INFO: namespace e2e-tests-nsdeletetest-p559f deletion completed in 6.150231933s

â€¢ [SLOW TEST:18.753 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:47:42.382: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qgh77
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:47:42.635: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec32d892-23aa-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-qgh77" to be "success or failure"
Jan 29 09:47:42.638: INFO: Pod "downwardapi-volume-ec32d892-23aa-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258427ms
Jan 29 09:47:44.644: INFO: Pod "downwardapi-volume-ec32d892-23aa-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008531001s
STEP: Saw pod success
Jan 29 09:47:44.644: INFO: Pod "downwardapi-volume-ec32d892-23aa-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:47:44.646: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-ec32d892-23aa-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:47:44.663: INFO: Waiting for pod downwardapi-volume-ec32d892-23aa-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:47:44.667: INFO: Pod downwardapi-volume-ec32d892-23aa-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:47:44.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qgh77" for this suite.
Jan 29 09:47:50.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:47:50.700: INFO: namespace: e2e-tests-projected-qgh77, resource: bindings, ignored listing per whitelist
Jan 29 09:47:50.803: INFO: namespace e2e-tests-projected-qgh77 deletion completed in 6.132402004s

â€¢ [SLOW TEST:8.421 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:47:50.804: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-pqh5q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 29 09:47:51.044: INFO: Waiting up to 5m0s for pod "pod-f13839c8-23aa-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-pqh5q" to be "success or failure"
Jan 29 09:47:51.047: INFO: Pod "pod-f13839c8-23aa-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465527ms
Jan 29 09:47:53.055: INFO: Pod "pod-f13839c8-23aa-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01070192s
STEP: Saw pod success
Jan 29 09:47:53.055: INFO: Pod "pod-f13839c8-23aa-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:47:53.057: INFO: Trying to get logs from node 9.111.254.123 pod pod-f13839c8-23aa-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:47:53.077: INFO: Waiting for pod pod-f13839c8-23aa-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:47:53.079: INFO: Pod pod-f13839c8-23aa-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:47:53.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pqh5q" for this suite.
Jan 29 09:47:59.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:47:59.208: INFO: namespace: e2e-tests-emptydir-pqh5q, resource: bindings, ignored listing per whitelist
Jan 29 09:47:59.220: INFO: namespace e2e-tests-emptydir-pqh5q deletion completed in 6.137585466s

â€¢ [SLOW TEST:8.416 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:47:59.220: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xwks2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-f63bbe5b-23aa-11e9-9015-b2c9bac28373
STEP: Creating secret with name secret-projected-all-test-volume-f63bbddd-23aa-11e9-9015-b2c9bac28373
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 29 09:47:59.443: INFO: Waiting up to 5m0s for pod "projected-volume-f63bbd84-23aa-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-xwks2" to be "success or failure"
Jan 29 09:47:59.446: INFO: Pod "projected-volume-f63bbd84-23aa-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.679733ms
Jan 29 09:48:01.451: INFO: Pod "projected-volume-f63bbd84-23aa-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008205101s
STEP: Saw pod success
Jan 29 09:48:01.451: INFO: Pod "projected-volume-f63bbd84-23aa-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:48:01.453: INFO: Trying to get logs from node 9.111.254.123 pod projected-volume-f63bbd84-23aa-11e9-9015-b2c9bac28373 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 29 09:48:01.478: INFO: Waiting for pod projected-volume-f63bbd84-23aa-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:48:01.481: INFO: Pod projected-volume-f63bbd84-23aa-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:48:01.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xwks2" for this suite.
Jan 29 09:48:07.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:48:07.517: INFO: namespace: e2e-tests-projected-xwks2, resource: bindings, ignored listing per whitelist
Jan 29 09:48:07.596: INFO: namespace e2e-tests-projected-xwks2 deletion completed in 6.109098317s

â€¢ [SLOW TEST:8.376 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:48:07.597: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-wbl2d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:48:07.782: INFO: Creating deployment "test-recreate-deployment"
Jan 29 09:48:07.837: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 29 09:48:07.842: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 29 09:48:09.847: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 29 09:48:09.849: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 29 09:48:09.939: INFO: Updating deployment test-recreate-deployment
Jan 29 09:48:09.940: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 29 09:48:10.028: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-wbl2d,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-wbl2d/deployments/test-recreate-deployment,UID:fb3b94c3-23aa-11e9-a926-eeeeeeeeeeee,ResourceVersion:692925,Generation:2,CreationTimestamp:2019-01-29 09:48:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-01-29 09:48:09 +0000 UTC 2019-01-29 09:48:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-01-29 09:48:10 +0000 UTC 2019-01-29 09:48:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jan 29 09:48:10.034: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-wbl2d,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-wbl2d/replicasets/test-recreate-deployment-7cf749666b,UID:fc886881-23aa-11e9-a926-eeeeeeeeeeee,ResourceVersion:692924,Generation:1,CreationTimestamp:2019-01-29 09:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment fb3b94c3-23aa-11e9-a926-eeeeeeeeeeee 0xc42246b267 0xc42246b268}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 29 09:48:10.034: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 29 09:48:10.034: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-wbl2d,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-wbl2d/replicasets/test-recreate-deployment-79f694ff59,UID:fb439843-23aa-11e9-a926-eeeeeeeeeeee,ResourceVersion:692913,Generation:2,CreationTimestamp:2019-01-29 09:48:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment fb3b94c3-23aa-11e9-a926-eeeeeeeeeeee 0xc42246b117 0xc42246b118}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 29 09:48:10.038: INFO: Pod "test-recreate-deployment-7cf749666b-25c8c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-25c8c,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-wbl2d,SelfLink:/api/v1/namespaces/e2e-tests-deployment-wbl2d/pods/test-recreate-deployment-7cf749666b-25c8c,UID:fc8981c2-23aa-11e9-a926-eeeeeeeeeeee,ResourceVersion:692923,Generation:0,CreationTimestamp:2019-01-29 09:48:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b fc886881-23aa-11e9-a926-eeeeeeeeeeee 0xc4223fb2d7 0xc4223fb2d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-pdhhn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pdhhn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-pdhhn true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4223fb350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223fb370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:48:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:48:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:48:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:48:09 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:,StartTime:2019-01-29 09:48:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:48:10.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-wbl2d" for this suite.
Jan 29 09:48:16.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:48:16.132: INFO: namespace: e2e-tests-deployment-wbl2d, resource: bindings, ignored listing per whitelist
Jan 29 09:48:16.189: INFO: namespace e2e-tests-deployment-wbl2d deletion completed in 6.146776293s

â€¢ [SLOW TEST:8.592 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:48:16.189: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-sx4hx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 29 09:48:16.445: INFO: Waiting up to 5m0s for pod "pod-005c11ba-23ab-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-sx4hx" to be "success or failure"
Jan 29 09:48:16.455: INFO: Pod "pod-005c11ba-23ab-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 10.333962ms
Jan 29 09:48:18.466: INFO: Pod "pod-005c11ba-23ab-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021174025s
STEP: Saw pod success
Jan 29 09:48:18.466: INFO: Pod "pod-005c11ba-23ab-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:48:18.469: INFO: Trying to get logs from node 9.111.254.123 pod pod-005c11ba-23ab-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:48:18.487: INFO: Waiting for pod pod-005c11ba-23ab-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:48:18.490: INFO: Pod pod-005c11ba-23ab-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:48:18.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sx4hx" for this suite.
Jan 29 09:48:24.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:48:24.582: INFO: namespace: e2e-tests-emptydir-sx4hx, resource: bindings, ignored listing per whitelist
Jan 29 09:48:24.619: INFO: namespace e2e-tests-emptydir-sx4hx deletion completed in 6.125120107s

â€¢ [SLOW TEST:8.430 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:48:24.620: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-6w27p
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0129 09:48:25.867652      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 09:48:25.867: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:48:25.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-6w27p" for this suite.
Jan 29 09:48:31.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:48:31.922: INFO: namespace: e2e-tests-gc-6w27p, resource: bindings, ignored listing per whitelist
Jan 29 09:48:31.981: INFO: namespace e2e-tests-gc-6w27p deletion completed in 6.109707978s

â€¢ [SLOW TEST:7.362 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:48:31.983: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-87jzb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:48:32.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-87jzb" for this suite.
Jan 29 09:48:38.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:48:38.279: INFO: namespace: e2e-tests-services-87jzb, resource: bindings, ignored listing per whitelist
Jan 29 09:48:38.329: INFO: namespace e2e-tests-services-87jzb deletion completed in 6.138596829s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

â€¢ [SLOW TEST:6.347 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:48:38.330: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-ltpnw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Jan 29 09:48:38.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 --namespace=e2e-tests-kubectl-ltpnw run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 29 09:48:41.010: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 29 09:48:41.010: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:48:43.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-ltpnw" for this suite.
Jan 29 09:48:49.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:48:49.149: INFO: namespace: e2e-tests-kubectl-ltpnw, resource: bindings, ignored listing per whitelist
Jan 29 09:48:49.171: INFO: namespace e2e-tests-kubectl-ltpnw deletion completed in 6.152361805s

â€¢ [SLOW TEST:10.841 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:48:49.172: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tvtxl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:48:49.436: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1400a897-23ab-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-tvtxl" to be "success or failure"
Jan 29 09:48:49.439: INFO: Pod "downwardapi-volume-1400a897-23ab-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.852298ms
Jan 29 09:48:51.447: INFO: Pod "downwardapi-volume-1400a897-23ab-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010611188s
STEP: Saw pod success
Jan 29 09:48:51.447: INFO: Pod "downwardapi-volume-1400a897-23ab-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:48:51.449: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-1400a897-23ab-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:48:51.469: INFO: Waiting for pod downwardapi-volume-1400a897-23ab-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:48:51.471: INFO: Pod downwardapi-volume-1400a897-23ab-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:48:51.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tvtxl" for this suite.
Jan 29 09:48:57.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:48:57.577: INFO: namespace: e2e-tests-downward-api-tvtxl, resource: bindings, ignored listing per whitelist
Jan 29 09:48:57.604: INFO: namespace e2e-tests-downward-api-tvtxl deletion completed in 6.128209373s

â€¢ [SLOW TEST:8.433 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:48:57.605: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jqrmr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:48:57.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1908a2b6-23ab-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-jqrmr" to be "success or failure"
Jan 29 09:48:57.840: INFO: Pod "downwardapi-volume-1908a2b6-23ab-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.417083ms
Jan 29 09:48:59.849: INFO: Pod "downwardapi-volume-1908a2b6-23ab-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01187775s
STEP: Saw pod success
Jan 29 09:48:59.849: INFO: Pod "downwardapi-volume-1908a2b6-23ab-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:48:59.851: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-1908a2b6-23ab-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:48:59.876: INFO: Waiting for pod downwardapi-volume-1908a2b6-23ab-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:48:59.878: INFO: Pod downwardapi-volume-1908a2b6-23ab-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:48:59.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jqrmr" for this suite.
Jan 29 09:49:05.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:49:05.963: INFO: namespace: e2e-tests-projected-jqrmr, resource: bindings, ignored listing per whitelist
Jan 29 09:49:06.000: INFO: namespace e2e-tests-projected-jqrmr deletion completed in 6.116980272s

â€¢ [SLOW TEST:8.395 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:49:06.000: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-42zq6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:50:06.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-42zq6" for this suite.
Jan 29 09:50:28.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:50:28.364: INFO: namespace: e2e-tests-container-probe-42zq6, resource: bindings, ignored listing per whitelist
Jan 29 09:50:28.435: INFO: namespace e2e-tests-container-probe-42zq6 deletion completed in 22.19297315s

â€¢ [SLOW TEST:82.435 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:50:28.435: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-rvgcj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 29 09:50:28.749: INFO: Number of nodes with available pods: 0
Jan 29 09:50:28.749: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 09:50:29.787: INFO: Number of nodes with available pods: 0
Jan 29 09:50:29.787: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 09:50:30.755: INFO: Number of nodes with available pods: 2
Jan 29 09:50:30.755: INFO: Node 9.111.254.123 is running more than one daemon pod
Jan 29 09:50:31.756: INFO: Number of nodes with available pods: 3
Jan 29 09:50:31.756: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 29 09:50:31.783: INFO: Number of nodes with available pods: 2
Jan 29 09:50:31.783: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:50:32.791: INFO: Number of nodes with available pods: 2
Jan 29 09:50:32.791: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:50:33.795: INFO: Number of nodes with available pods: 2
Jan 29 09:50:33.795: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:50:34.790: INFO: Number of nodes with available pods: 3
Jan 29 09:50:34.790: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-rvgcj, will wait for the garbage collector to delete the pods
Jan 29 09:50:34.851: INFO: Deleting {extensions DaemonSet} daemon-set took: 5.251682ms
Jan 29 09:50:34.951: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.223101ms
Jan 29 09:51:17.054: INFO: Number of nodes with available pods: 0
Jan 29 09:51:17.054: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 09:51:17.057: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-rvgcj/daemonsets","resourceVersion":"693644"},"items":null}

Jan 29 09:51:17.058: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-rvgcj/pods","resourceVersion":"693644"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:51:17.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-rvgcj" for this suite.
Jan 29 09:51:23.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:51:23.168: INFO: namespace: e2e-tests-daemonsets-rvgcj, resource: bindings, ignored listing per whitelist
Jan 29 09:51:23.186: INFO: namespace e2e-tests-daemonsets-rvgcj deletion completed in 6.112343224s

â€¢ [SLOW TEST:54.751 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:51:23.188: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7ktd4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-6fcf7661-23ab-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 09:51:23.434: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6fcfff63-23ab-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-7ktd4" to be "success or failure"
Jan 29 09:51:23.437: INFO: Pod "pod-projected-configmaps-6fcfff63-23ab-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.566406ms
Jan 29 09:51:25.444: INFO: Pod "pod-projected-configmaps-6fcfff63-23ab-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009620605s
Jan 29 09:51:27.447: INFO: Pod "pod-projected-configmaps-6fcfff63-23ab-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012518305s
STEP: Saw pod success
Jan 29 09:51:27.447: INFO: Pod "pod-projected-configmaps-6fcfff63-23ab-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:51:27.449: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-configmaps-6fcfff63-23ab-11e9-9015-b2c9bac28373 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 09:51:27.463: INFO: Waiting for pod pod-projected-configmaps-6fcfff63-23ab-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:51:27.465: INFO: Pod pod-projected-configmaps-6fcfff63-23ab-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:51:27.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7ktd4" for this suite.
Jan 29 09:51:33.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:51:33.576: INFO: namespace: e2e-tests-projected-7ktd4, resource: bindings, ignored listing per whitelist
Jan 29 09:51:33.609: INFO: namespace e2e-tests-projected-7ktd4 deletion completed in 6.139795168s

â€¢ [SLOW TEST:10.422 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:51:33.610: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-znmwg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Jan 29 09:51:33.786: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jan 29 09:51:33.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:34.091: INFO: stderr: ""
Jan 29 09:51:34.091: INFO: stdout: "service/redis-slave created\n"
Jan 29 09:51:34.091: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jan 29 09:51:34.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:34.348: INFO: stderr: ""
Jan 29 09:51:34.348: INFO: stdout: "service/redis-master created\n"
Jan 29 09:51:34.348: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 29 09:51:34.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:34.576: INFO: stderr: ""
Jan 29 09:51:34.576: INFO: stdout: "service/frontend created\n"
Jan 29 09:51:34.576: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jan 29 09:51:34.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:34.846: INFO: stderr: ""
Jan 29 09:51:34.846: INFO: stdout: "deployment.extensions/frontend created\n"
Jan 29 09:51:34.846: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 29 09:51:34.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:35.254: INFO: stderr: ""
Jan 29 09:51:35.254: INFO: stdout: "deployment.extensions/redis-master created\n"
Jan 29 09:51:35.255: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jan 29 09:51:35.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 create -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:35.649: INFO: stderr: ""
Jan 29 09:51:35.649: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Jan 29 09:51:35.649: INFO: Waiting for all frontend pods to be Running.
Jan 29 09:51:40.700: INFO: Waiting for frontend to serve content.
Jan 29 09:51:40.804: INFO: Trying to add a new entry to the guestbook.
Jan 29 09:51:40.826: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 29 09:51:40.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:41.088: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:51:41.088: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 09:51:41.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:41.252: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:51:41.252: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 09:51:41.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:41.393: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:51:41.393: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 09:51:41.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:41.530: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:51:41.530: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 09:51:41.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:41.663: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:51:41.663: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 09:51:41.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-znmwg'
Jan 29 09:51:41.823: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 09:51:41.823: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:51:41.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-znmwg" for this suite.
Jan 29 09:52:19.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:52:19.998: INFO: namespace: e2e-tests-kubectl-znmwg, resource: bindings, ignored listing per whitelist
Jan 29 09:52:20.086: INFO: namespace e2e-tests-kubectl-znmwg deletion completed in 38.222015799s

â€¢ [SLOW TEST:46.476 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:52:20.086: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-pd96t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:52:20.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91bb8312-23ab-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-pd96t" to be "success or failure"
Jan 29 09:52:20.343: INFO: Pod "downwardapi-volume-91bb8312-23ab-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.207828ms
Jan 29 09:52:22.350: INFO: Pod "downwardapi-volume-91bb8312-23ab-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010645612s
STEP: Saw pod success
Jan 29 09:52:22.350: INFO: Pod "downwardapi-volume-91bb8312-23ab-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:52:22.357: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-91bb8312-23ab-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:52:22.379: INFO: Waiting for pod downwardapi-volume-91bb8312-23ab-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:52:22.381: INFO: Pod downwardapi-volume-91bb8312-23ab-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:52:22.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pd96t" for this suite.
Jan 29 09:52:28.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:52:28.450: INFO: namespace: e2e-tests-downward-api-pd96t, resource: bindings, ignored listing per whitelist
Jan 29 09:52:28.489: INFO: namespace e2e-tests-downward-api-pd96t deletion completed in 6.104219014s

â€¢ [SLOW TEST:8.403 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:52:28.489: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-sg8dq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 09:52:28.740: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 29 09:52:33.746: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 29 09:52:33.746: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 29 09:52:35.749: INFO: Creating deployment "test-rollover-deployment"
Jan 29 09:52:35.839: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 29 09:52:37.845: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 29 09:52:37.849: INFO: Ensure that both replica sets have 1 created replica
Jan 29 09:52:37.853: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 29 09:52:37.943: INFO: Updating deployment test-rollover-deployment
Jan 29 09:52:37.943: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 29 09:52:39.949: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 29 09:52:39.953: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 29 09:52:39.958: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 09:52:39.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352358, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 09:52:41.964: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 09:52:41.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352360, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 09:52:43.963: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 09:52:43.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352360, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 09:52:45.965: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 09:52:45.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352360, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 09:52:47.965: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 09:52:47.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352360, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 09:52:49.964: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 09:52:49.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352360, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63684352355, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 09:52:51.964: INFO: 
Jan 29 09:52:51.964: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 29 09:52:51.970: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-sg8dq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sg8dq/deployments/test-rollover-deployment,UID:9af454d0-23ab-11e9-a926-eeeeeeeeeeee,ResourceVersion:694222,Generation:2,CreationTimestamp:2019-01-29 09:52:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-01-29 09:52:35 +0000 UTC 2019-01-29 09:52:35 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-01-29 09:52:50 +0000 UTC 2019-01-29 09:52:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 09:52:51.974: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-sg8dq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sg8dq/replicasets/test-rollover-deployment-5b76ff8c4,UID:9c41f598-23ab-11e9-a926-eeeeeeeeeeee,ResourceVersion:694212,Generation:2,CreationTimestamp:2019-01-29 09:52:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9af454d0-23ab-11e9-a926-eeeeeeeeeeee 0xc422f4a357 0xc422f4a358}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 29 09:52:51.974: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 29 09:52:51.975: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-sg8dq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sg8dq/replicasets/test-rollover-controller,UID:96bd5463-23ab-11e9-a926-eeeeeeeeeeee,ResourceVersion:694221,Generation:2,CreationTimestamp:2019-01-29 09:52:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9af454d0-23ab-11e9-a926-eeeeeeeeeeee 0xc422f4a1be 0xc422f4a1bf}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 29 09:52:51.975: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-sg8dq,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-sg8dq/replicasets/test-rollover-deployment-6975f4fb87,UID:9b026c65-23ab-11e9-a926-eeeeeeeeeeee,ResourceVersion:694170,Generation:2,CreationTimestamp:2019-01-29 09:52:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9af454d0-23ab-11e9-a926-eeeeeeeeeeee 0xc422f4a417 0xc422f4a418}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 29 09:52:51.979: INFO: Pod "test-rollover-deployment-5b76ff8c4-dh49n" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-dh49n,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-sg8dq,SelfLink:/api/v1/namespaces/e2e-tests-deployment-sg8dq/pods/test-rollover-deployment-5b76ff8c4-dh49n,UID:9c471265-23ab-11e9-a926-eeeeeeeeeeee,ResourceVersion:694190,Generation:0,CreationTimestamp:2019-01-29 09:52:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 9c41f598-23ab-11e9-a926-eeeeeeeeeeee 0xc423170c80 0xc423170c81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bf9jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bf9jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bf9jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:9.111.254.123,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc423170cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc423170d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:52:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:52:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:52:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-29 09:52:37 +0000 UTC  }],Message:,Reason:,HostIP:9.111.254.123,PodIP:10.1.211.129,StartTime:2019-01-29 09:52:38 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-01-29 09:52:39 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://10f9718e9e77295b5c28e5a9598fac4d4c4343b1c42acd79e05918b0b365a958}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:52:51.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-sg8dq" for this suite.
Jan 29 09:52:57.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:52:58.092: INFO: namespace: e2e-tests-deployment-sg8dq, resource: bindings, ignored listing per whitelist
Jan 29 09:52:58.098: INFO: namespace e2e-tests-deployment-sg8dq deletion completed in 6.115047148s

â€¢ [SLOW TEST:29.609 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:52:58.098: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-897qk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-bdln
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 09:52:58.347: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bdln" in namespace "e2e-tests-subpath-897qk" to be "success or failure"
Jan 29 09:52:58.350: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Pending", Reason="", readiness=false. Elapsed: 3.348635ms
Jan 29 09:53:00.353: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006681841s
Jan 29 09:53:02.356: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 4.009380802s
Jan 29 09:53:04.359: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 6.012514348s
Jan 29 09:53:06.362: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 8.015634926s
Jan 29 09:53:08.365: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 10.018687746s
Jan 29 09:53:10.370: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 12.023514549s
Jan 29 09:53:12.373: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 14.026138531s
Jan 29 09:53:14.375: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 16.028884763s
Jan 29 09:53:16.379: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 18.032171956s
Jan 29 09:53:18.381: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 20.034823883s
Jan 29 09:53:20.384: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Running", Reason="", readiness=false. Elapsed: 22.037204261s
Jan 29 09:53:22.391: INFO: Pod "pod-subpath-test-configmap-bdln": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.044607834s
STEP: Saw pod success
Jan 29 09:53:22.391: INFO: Pod "pod-subpath-test-configmap-bdln" satisfied condition "success or failure"
Jan 29 09:53:22.394: INFO: Trying to get logs from node 9.111.254.123 pod pod-subpath-test-configmap-bdln container test-container-subpath-configmap-bdln: <nil>
STEP: delete the pod
Jan 29 09:53:22.416: INFO: Waiting for pod pod-subpath-test-configmap-bdln to disappear
Jan 29 09:53:22.419: INFO: Pod pod-subpath-test-configmap-bdln no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bdln
Jan 29 09:53:22.419: INFO: Deleting pod "pod-subpath-test-configmap-bdln" in namespace "e2e-tests-subpath-897qk"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:53:22.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-897qk" for this suite.
Jan 29 09:53:28.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:53:28.503: INFO: namespace: e2e-tests-subpath-897qk, resource: bindings, ignored listing per whitelist
Jan 29 09:53:28.535: INFO: namespace e2e-tests-subpath-897qk deletion completed in 6.110070399s

â€¢ [SLOW TEST:30.437 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:53:28.536: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-jf5rr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 29 09:53:28.736: INFO: Waiting up to 5m0s for pod "pod-ba84b20a-23ab-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-jf5rr" to be "success or failure"
Jan 29 09:53:28.738: INFO: Pod "pod-ba84b20a-23ab-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596805ms
Jan 29 09:53:30.741: INFO: Pod "pod-ba84b20a-23ab-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005709781s
Jan 29 09:53:32.745: INFO: Pod "pod-ba84b20a-23ab-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008851273s
STEP: Saw pod success
Jan 29 09:53:32.745: INFO: Pod "pod-ba84b20a-23ab-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:53:32.747: INFO: Trying to get logs from node 9.111.254.123 pod pod-ba84b20a-23ab-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 09:53:32.762: INFO: Waiting for pod pod-ba84b20a-23ab-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:53:32.764: INFO: Pod pod-ba84b20a-23ab-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:53:32.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jf5rr" for this suite.
Jan 29 09:53:38.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:53:38.793: INFO: namespace: e2e-tests-emptydir-jf5rr, resource: bindings, ignored listing per whitelist
Jan 29 09:53:38.885: INFO: namespace e2e-tests-emptydir-jf5rr deletion completed in 6.116508032s

â€¢ [SLOW TEST:10.349 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:53:38.887: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-zhm85
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-zhm85
Jan 29 09:53:41.149: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-zhm85
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 09:53:41.151: INFO: Initial restart count of pod liveness-http is 0
Jan 29 09:54:01.187: INFO: Restart count of pod e2e-tests-container-probe-zhm85/liveness-http is now 1 (20.036438244s elapsed)
Jan 29 09:54:21.220: INFO: Restart count of pod e2e-tests-container-probe-zhm85/liveness-http is now 2 (40.069019101s elapsed)
Jan 29 09:54:41.253: INFO: Restart count of pod e2e-tests-container-probe-zhm85/liveness-http is now 3 (1m0.10280886s elapsed)
Jan 29 09:55:01.285: INFO: Restart count of pod e2e-tests-container-probe-zhm85/liveness-http is now 4 (1m20.133905254s elapsed)
Jan 29 09:56:15.404: INFO: Restart count of pod e2e-tests-container-probe-zhm85/liveness-http is now 5 (2m34.253105224s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:56:15.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-zhm85" for this suite.
Jan 29 09:56:21.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:56:21.533: INFO: namespace: e2e-tests-container-probe-zhm85, resource: bindings, ignored listing per whitelist
Jan 29 09:56:21.623: INFO: namespace e2e-tests-container-probe-zhm85 deletion completed in 6.205831889s

â€¢ [SLOW TEST:162.737 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:56:21.625: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-2kqrp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:56:21.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-2kqrp" for this suite.
Jan 29 09:56:43.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:56:44.084: INFO: namespace: e2e-tests-pods-2kqrp, resource: bindings, ignored listing per whitelist
Jan 29 09:56:44.093: INFO: namespace e2e-tests-pods-2kqrp deletion completed in 22.147099395s

â€¢ [SLOW TEST:22.470 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:56:44.093: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5h8bl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-2f16b45b-23ac-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 09:56:44.338: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f176e0d-23ac-11e9-9015-b2c9bac28373" in namespace "e2e-tests-configmap-5h8bl" to be "success or failure"
Jan 29 09:56:44.340: INFO: Pod "pod-configmaps-2f176e0d-23ac-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172747ms
Jan 29 09:56:46.344: INFO: Pod "pod-configmaps-2f176e0d-23ac-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006046881s
STEP: Saw pod success
Jan 29 09:56:46.344: INFO: Pod "pod-configmaps-2f176e0d-23ac-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:56:46.346: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-2f176e0d-23ac-11e9-9015-b2c9bac28373 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 09:56:46.370: INFO: Waiting for pod pod-configmaps-2f176e0d-23ac-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:56:46.375: INFO: Pod pod-configmaps-2f176e0d-23ac-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:56:46.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5h8bl" for this suite.
Jan 29 09:56:52.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:56:52.405: INFO: namespace: e2e-tests-configmap-5h8bl, resource: bindings, ignored listing per whitelist
Jan 29 09:56:52.492: INFO: namespace e2e-tests-configmap-5h8bl deletion completed in 6.113107669s

â€¢ [SLOW TEST:8.399 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:56:52.496: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-mm5lj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-34196334-23ac-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:56:52.736: INFO: Waiting up to 5m0s for pod "pod-secrets-3419ef70-23ac-11e9-9015-b2c9bac28373" in namespace "e2e-tests-secrets-mm5lj" to be "success or failure"
Jan 29 09:56:52.739: INFO: Pod "pod-secrets-3419ef70-23ac-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.218141ms
Jan 29 09:56:54.742: INFO: Pod "pod-secrets-3419ef70-23ac-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00550368s
STEP: Saw pod success
Jan 29 09:56:54.742: INFO: Pod "pod-secrets-3419ef70-23ac-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:56:54.745: INFO: Trying to get logs from node 9.111.254.123 pod pod-secrets-3419ef70-23ac-11e9-9015-b2c9bac28373 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 09:56:54.766: INFO: Waiting for pod pod-secrets-3419ef70-23ac-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:56:54.768: INFO: Pod pod-secrets-3419ef70-23ac-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:56:54.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mm5lj" for this suite.
Jan 29 09:57:00.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:57:00.894: INFO: namespace: e2e-tests-secrets-mm5lj, resource: bindings, ignored listing per whitelist
Jan 29 09:57:00.931: INFO: namespace e2e-tests-secrets-mm5lj deletion completed in 6.157809883s

â€¢ [SLOW TEST:8.435 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:57:00.931: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dhzbp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:57:01.147: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39201225-23ac-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-dhzbp" to be "success or failure"
Jan 29 09:57:01.151: INFO: Pod "downwardapi-volume-39201225-23ac-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.58403ms
Jan 29 09:57:03.154: INFO: Pod "downwardapi-volume-39201225-23ac-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00720688s
STEP: Saw pod success
Jan 29 09:57:03.154: INFO: Pod "downwardapi-volume-39201225-23ac-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:57:03.157: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-39201225-23ac-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:57:03.181: INFO: Waiting for pod downwardapi-volume-39201225-23ac-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:57:03.183: INFO: Pod downwardapi-volume-39201225-23ac-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:57:03.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dhzbp" for this suite.
Jan 29 09:57:09.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:57:09.304: INFO: namespace: e2e-tests-projected-dhzbp, resource: bindings, ignored listing per whitelist
Jan 29 09:57:09.316: INFO: namespace e2e-tests-projected-dhzbp deletion completed in 6.129140384s

â€¢ [SLOW TEST:8.385 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:57:09.316: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-gpqpf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Jan 29 09:57:09.515: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 09:57:09.525: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 09:57:09.529: INFO: 
Logging pods the kubelet thinks is on node 9.111.254.104 before test
Jan 29 09:57:09.545: INFO: monitoring-prometheus-alertmanager-86b46f4f8f-zlwzh from kube-system started at 2019-01-24 02:48:48 +0000 UTC (3 container statuses recorded)
Jan 29 09:57:09.545: INFO: 	Container alertmanager ready: true, restart count 0
Jan 29 09:57:09.545: INFO: 	Container configmap-reload ready: true, restart count 0
Jan 29 09:57:09.545: INFO: 	Container router ready: true, restart count 0
Jan 29 09:57:09.545: INFO: key-management-api-85fb4b5f55-pckqx from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.545: INFO: 	Container key-management-api ready: true, restart count 0
Jan 29 09:57:09.545: INFO: logging-elk-elasticsearch-curator-1548718200-clnkm from kube-system started at 2019-01-28 23:30:08 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.545: INFO: 	Container curator ready: false, restart count 0
Jan 29 09:57:09.545: INFO: logging-elk-client-559db59464-ps9rr from kube-system started at 2019-01-24 02:41:32 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.545: INFO: 	Container es-client ready: true, restart count 0
Jan 29 09:57:09.545: INFO: 	Container router ready: true, restart count 0
Jan 29 09:57:09.545: INFO: monitoring-prometheus-elasticsearchexporter-786dbc957b-lrzfq from kube-system started at 2019-01-24 02:48:48 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.546: INFO: 	Container elasticsearchexporter ready: true, restart count 0
Jan 29 09:57:09.546: INFO: 	Container router ready: true, restart count 0
Jan 29 09:57:09.546: INFO: monitoring-prometheus-collectdexporter-698694c4bf-z4b44 from kube-system started at 2019-01-24 02:48:48 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.546: INFO: 	Container collectd-exporter ready: true, restart count 0
Jan 29 09:57:09.546: INFO: 	Container router ready: true, restart count 0
Jan 29 09:57:09.546: INFO: audit-logging-fluentd-ds-2xjm5 from kube-system started at 2019-01-24 02:51:19 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.546: INFO: 	Container fluentd ready: true, restart count 0
Jan 29 09:57:09.546: INFO: key-management-pep-6d7f685bf4-rnd82 from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.546: INFO: 	Container key-management-pep ready: true, restart count 0
Jan 29 09:57:09.546: INFO: key-management-crypto-f4b6444bc-zspwm from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.546: INFO: 	Container key-management-crypto ready: true, restart count 0
Jan 29 09:57:09.546: INFO: monitoring-prometheus-7b5f7db489-9jxtd from kube-system started at 2019-01-24 02:48:48 +0000 UTC (4 container statuses recorded)
Jan 29 09:57:09.546: INFO: 	Container alert-rule-controller ready: true, restart count 0
Jan 29 09:57:09.547: INFO: 	Container configmap-reload-prometheus ready: true, restart count 0
Jan 29 09:57:09.547: INFO: 	Container prometheus ready: true, restart count 0
Jan 29 09:57:09.547: INFO: 	Container router ready: true, restart count 1
Jan 29 09:57:09.547: INFO: key-management-lifecycle-69b67d57c5-q5pvm from kube-system started at 2019-01-24 02:52:04 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.547: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:57:09.547: INFO: 	Container key-management-lifecycle ready: true, restart count 0
Jan 29 09:57:09.547: INFO: k8s-proxy-9.111.254.104 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:57:09.547: INFO: web-terminal-7776f9df5b-q6mlp from kube-system started at 2019-01-24 02:40:36 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.547: INFO: 	Container web-terminal ready: true, restart count 0
Jan 29 09:57:09.547: INFO: logging-elk-data-0 from kube-system started at 2019-01-24 02:41:33 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.547: INFO: 	Container es-data ready: true, restart count 0
Jan 29 09:57:09.547: INFO: monitoring-prometheus-nodeexporter-hq52h from kube-system started at 2019-01-24 02:48:47 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.547: INFO: 	Container nodeexporter ready: true, restart count 0
Jan 29 09:57:09.547: INFO: 	Container router ready: true, restart count 0
Jan 29 09:57:09.548: INFO: monitoring-grafana-7d5d7f5c45-58ttx from kube-system started at 2019-01-24 02:48:48 +0000 UTC (3 container statuses recorded)
Jan 29 09:57:09.548: INFO: 	Container dashboard-crd-controller ready: true, restart count 0
Jan 29 09:57:09.548: INFO: 	Container grafana ready: true, restart count 0
Jan 29 09:57:09.548: INFO: 	Container router ready: true, restart count 0
Jan 29 09:57:09.548: INFO: logging-elk-elasticsearch-curator-1548631800-jp4mg from kube-system started at 2019-01-27 23:30:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.548: INFO: 	Container curator ready: false, restart count 0
Jan 29 09:57:09.548: INFO: calico-node-4z7zs from kube-system started at 2019-01-24 02:27:00 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.548: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 09:57:09.548: INFO: 	Container install-cni ready: true, restart count 0
Jan 29 09:57:09.548: INFO: logging-elk-master-c47cb5549-n5cmk from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.548: INFO: 	Container es-master ready: true, restart count 0
Jan 29 09:57:09.548: INFO: custom-metrics-adapter-77c5d9c6ff-9lg44 from kube-system started at 2019-01-24 02:47:21 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.548: INFO: 	Container custom-metrics-adapter ready: true, restart count 2
Jan 29 09:57:09.548: INFO: logging-elk-filebeat-ds-9mbdx from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.548: INFO: 	Container filebeat ready: true, restart count 0
Jan 29 09:57:09.548: INFO: sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-zg67k from heptio-sonobuoy started at 2019-01-29 08:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.548: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 29 09:57:09.548: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 29 09:57:09.548: INFO: logging-elk-logstash-5444dd6fb4-6p7jm from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.548: INFO: 	Container logstash ready: true, restart count 0
Jan 29 09:57:09.548: INFO: monitoring-prometheus-kubestatemetrics-9fcddf7d8-jw5xn from kube-system started at 2019-01-24 02:48:48 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.548: INFO: 	Container kubestatemetrics ready: true, restart count 0
Jan 29 09:57:09.549: INFO: 	Container router ready: true, restart count 0
Jan 29 09:57:09.549: INFO: key-management-persistence-656b774f7-f2kjc from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.549: INFO: 	Container key-management-persistence ready: true, restart count 0
Jan 29 09:57:09.549: INFO: nvidia-device-plugin-cjt74 from kube-system started at 2019-01-24 02:28:55 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.549: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
Jan 29 09:57:09.549: INFO: metrics-server-5bcdd5579-9v9qd from kube-system started at 2019-01-24 02:32:06 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.549: INFO: 	Container metrics-server ready: true, restart count 3
Jan 29 09:57:09.549: INFO: 
Logging pods the kubelet thinks is on node 9.111.254.123 before test
Jan 29 09:57:09.570: INFO: k8s-kmsplugin-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:57:09.570: INFO: nginx-ingress-controller-ghbl2 from kube-system started at 2019-01-24 02:30:49 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.570: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 29 09:57:09.570: INFO: monitoring-prometheus-nodeexporter-h889z from kube-system started at 2019-01-24 02:48:47 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.570: INFO: 	Container nodeexporter ready: true, restart count 0
Jan 29 09:57:09.570: INFO: 	Container router ready: true, restart count 0
Jan 29 09:57:09.570: INFO: heapster-6fb4fbb79b-dz5nf from kube-system started at 2019-01-24 02:42:38 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.570: INFO: 	Container heapster ready: true, restart count 0
Jan 29 09:57:09.570: INFO: catalog-ui-k8wf5 from kube-system started at 2019-01-24 02:43:56 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.570: INFO: 	Container catalog-ui ready: true, restart count 0
Jan 29 09:57:09.570: INFO: kube-dns-sbds7 from kube-system started at 2019-01-29 08:45:54 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.570: INFO: 	Container coredns ready: true, restart count 0
Jan 29 09:57:09.570: INFO: k8s-etcd-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:57:09.570: INFO: service-catalog-controller-manager-6976ff4bc6-rl78n from kube-system started at 2019-01-24 02:30:23 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.570: INFO: 	Container controller-manager ready: true, restart count 3
Jan 29 09:57:09.570: INFO: default-http-backend-7c4747d8d5-kzlz4 from kube-system started at 2019-01-24 02:30:49 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.570: INFO: 	Container default-http-backend ready: true, restart count 0
Jan 29 09:57:09.570: INFO: auth-apikeys-fmgd5 from kube-system started at 2019-01-24 02:33:21 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.570: INFO: 	Container auth-apikeys ready: true, restart count 0
Jan 29 09:57:09.570: INFO: calico-kube-controllers-58548fb4db-wqdxb from kube-system started at 2019-01-24 02:27:00 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.570: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 29 09:57:09.570: INFO: sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-2zbf2 from heptio-sonobuoy started at 2019-01-29 08:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 29 09:57:09.571: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 29 09:57:09.571: INFO: mariadb-0 from kube-system started at 2019-01-24 02:31:16 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container mariadb ready: true, restart count 0
Jan 29 09:57:09.571: INFO: auth-idp-2lvnk from kube-system started at 2019-01-24 02:32:54 +0000 UTC (4 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:57:09.571: INFO: 	Container platform-auth-service ready: true, restart count 1
Jan 29 09:57:09.571: INFO: 	Container platform-identity-manager ready: true, restart count 0
Jan 29 09:57:09.571: INFO: 	Container platform-identity-provider ready: true, restart count 0
Jan 29 09:57:09.571: INFO: ibm-cert-manager-cert-manager-69f9b77599-5vw2w from cert-manager started at 2019-01-24 02:28:03 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container ibm-cert-manager ready: true, restart count 1
Jan 29 09:57:09.571: INFO: auth-pdp-2mhhx from kube-system started at 2019-01-24 02:34:14 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container auth-pdp ready: true, restart count 0
Jan 29 09:57:09.571: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:57:09.571: INFO: helm-api-66d8997947-whpb9 from kube-system started at 2019-01-24 02:45:03 +0000 UTC (3 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container helmapi ready: true, restart count 0
Jan 29 09:57:09.571: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:57:09.571: INFO: 	Container rudder ready: true, restart count 0
Jan 29 09:57:09.571: INFO: sonobuoy-e2e-job-5dd02bc5a9524b35 from heptio-sonobuoy started at 2019-01-29 08:47:59 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container e2e ready: true, restart count 0
Jan 29 09:57:09.571: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 09:57:09.571: INFO: k8s-master-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:57:09.571: INFO: platform-deploy-665cd77468-m9l7c from kube-system started at 2019-01-24 02:31:41 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container platform-deploy ready: true, restart count 0
Jan 29 09:57:09.571: INFO: auth-pap-8pdg7 from kube-system started at 2019-01-24 02:33:47 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container auth-pap ready: true, restart count 0
Jan 29 09:57:09.571: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:57:09.571: INFO: platform-ui-kl4bl from kube-system started at 2019-01-24 02:46:41 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container platform-ui ready: true, restart count 0
Jan 29 09:57:09.571: INFO: helm-repo-6c4f6cc8-88q9m from kube-system started at 2019-01-24 02:43:40 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container helm-repo ready: true, restart count 0
Jan 29 09:57:09.571: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:57:09.571: INFO: audit-logging-fluentd-ds-74lnc from kube-system started at 2019-01-24 02:51:19 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container fluentd ready: true, restart count 0
Jan 29 09:57:09.571: INFO: image-manager-0 from kube-system started at 2019-01-24 02:21:39 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container icp-registry ready: true, restart count 0
Jan 29 09:57:09.571: INFO: 	Container image-manager ready: true, restart count 0
Jan 29 09:57:09.571: INFO: tiller-deploy-59766f75bd-hsznz from kube-system started at 2019-01-24 02:26:09 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container tiller ready: true, restart count 0
Jan 29 09:57:09.571: INFO: nvidia-device-plugin-db75j from kube-system started at 2019-01-24 02:28:55 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
Jan 29 09:57:09.571: INFO: service-catalog-apiserver-7lmpb from kube-system started at 2019-01-24 02:30:23 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container apiserver ready: true, restart count 0
Jan 29 09:57:09.571: INFO: logging-elk-filebeat-ds-cl7nq from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container filebeat ready: true, restart count 0
Jan 29 09:57:09.571: INFO: ibmcloud-image-enforcement-596b54f688-xptnx from kube-system started at 2019-01-24 02:43:04 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container ibmcloud-image-enforcement ready: true, restart count 0
Jan 29 09:57:09.571: INFO: dns-test-x from default started at 2019-01-29 03:23:19 +0000 UTC (3 container statuses recorded)
Jan 29 09:57:09.571: INFO: 	Container jessie-querier ready: true, restart count 11
Jan 29 09:57:09.571: INFO: 	Container querier ready: true, restart count 12
Jan 29 09:57:09.572: INFO: 	Container webserver ready: true, restart count 0
Jan 29 09:57:09.572: INFO: calico-node-mfg2r from kube-system started at 2019-01-24 02:27:00 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.572: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 09:57:09.572: INFO: 	Container install-cni ready: true, restart count 0
Jan 29 09:57:09.572: INFO: icp-mongodb-0 from kube-system started at 2019-01-24 02:29:56 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.572: INFO: 	Container icp-mongodb ready: true, restart count 0
Jan 29 09:57:09.572: INFO: platform-api-74f4558f76-bp48c from kube-system started at 2019-01-24 02:31:41 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.572: INFO: 	Container audit-service ready: true, restart count 0
Jan 29 09:57:09.572: INFO: 	Container platform-api ready: true, restart count 0
Jan 29 09:57:09.572: INFO: icp-management-ingress-5bnwr from kube-system started at 2019-01-24 02:34:40 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.572: INFO: 	Container icp-management-ingress ready: true, restart count 0
Jan 29 09:57:09.572: INFO: k8s-proxy-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:57:09.572: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-29 08:47:51 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.572: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 09:57:09.572: INFO: mgmt-repo-54f58b9b5c-55qlz from kube-system started at 2019-01-24 02:46:04 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.572: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 09:57:09.572: INFO: 	Container mgmt-repo ready: true, restart count 0
Jan 29 09:57:09.572: INFO: unified-router-rnf9h from kube-system started at 2019-01-24 02:49:45 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.572: INFO: 	Container unified-router ready: true, restart count 0
Jan 29 09:57:09.572: INFO: 
Logging pods the kubelet thinks is on node 9.111.255.33 before test
Jan 29 09:57:09.583: INFO: k8s-proxy-9.111.255.33 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 09:57:09.583: INFO: calico-node-rq54l from kube-system started at 2019-01-24 02:27:00 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.583: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 09:57:09.583: INFO: 	Container install-cni ready: true, restart count 0
Jan 29 09:57:09.584: INFO: sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-bz5g5 from heptio-sonobuoy started at 2019-01-29 08:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.584: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 29 09:57:09.584: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 29 09:57:09.584: INFO: nvidia-device-plugin-kg6z8 from kube-system started at 2019-01-24 02:28:55 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.584: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
Jan 29 09:57:09.584: INFO: audit-logging-fluentd-ds-g4kfl from kube-system started at 2019-01-24 02:51:19 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.584: INFO: 	Container fluentd ready: true, restart count 0
Jan 29 09:57:09.584: INFO: logging-elk-filebeat-ds-9qmnw from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 09:57:09.584: INFO: 	Container filebeat ready: true, restart count 0
Jan 29 09:57:09.584: INFO: monitoring-prometheus-nodeexporter-58gsq from kube-system started at 2019-01-24 02:48:47 +0000 UTC (2 container statuses recorded)
Jan 29 09:57:09.584: INFO: 	Container nodeexporter ready: true, restart count 0
Jan 29 09:57:09.584: INFO: 	Container router ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node 9.111.254.104
STEP: verifying the node has the label node 9.111.254.123
STEP: verifying the node has the label node 9.111.255.33
Jan 29 09:57:09.675: INFO: Pod ibm-cert-manager-cert-manager-69f9b77599-5vw2w requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod dns-test-x requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod sonobuoy requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod sonobuoy-e2e-job-5dd02bc5a9524b35 requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-2zbf2 requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-bz5g5 requesting resource cpu=0m on Node 9.111.255.33
Jan 29 09:57:09.675: INFO: Pod sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-zg67k requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod audit-logging-fluentd-ds-2xjm5 requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod audit-logging-fluentd-ds-74lnc requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod audit-logging-fluentd-ds-g4kfl requesting resource cpu=0m on Node 9.111.255.33
Jan 29 09:57:09.675: INFO: Pod auth-apikeys-fmgd5 requesting resource cpu=200m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod auth-idp-2lvnk requesting resource cpu=300m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod auth-pap-8pdg7 requesting resource cpu=150m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod auth-pdp-2mhhx requesting resource cpu=600m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod calico-kube-controllers-58548fb4db-wqdxb requesting resource cpu=250m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod calico-node-4z7zs requesting resource cpu=300m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod calico-node-mfg2r requesting resource cpu=300m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod calico-node-rq54l requesting resource cpu=300m on Node 9.111.255.33
Jan 29 09:57:09.675: INFO: Pod catalog-ui-k8wf5 requesting resource cpu=300m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod custom-metrics-adapter-77c5d9c6ff-9lg44 requesting resource cpu=256m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod default-http-backend-7c4747d8d5-kzlz4 requesting resource cpu=20m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod heapster-6fb4fbb79b-dz5nf requesting resource cpu=20m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod helm-api-66d8997947-whpb9 requesting resource cpu=350m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod helm-repo-6c4f6cc8-88q9m requesting resource cpu=150m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod ibmcloud-image-enforcement-596b54f688-xptnx requesting resource cpu=128m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod icp-management-ingress-5bnwr requesting resource cpu=200m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod icp-mongodb-0 requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod image-manager-0 requesting resource cpu=110m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod k8s-etcd-9.111.254.123 requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod k8s-kmsplugin-9.111.254.123 requesting resource cpu=5m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod k8s-master-9.111.254.123 requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod k8s-proxy-9.111.254.104 requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod k8s-proxy-9.111.254.123 requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod k8s-proxy-9.111.255.33 requesting resource cpu=0m on Node 9.111.255.33
Jan 29 09:57:09.675: INFO: Pod key-management-api-85fb4b5f55-pckqx requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod key-management-crypto-f4b6444bc-zspwm requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod key-management-lifecycle-69b67d57c5-q5pvm requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod key-management-pep-6d7f685bf4-rnd82 requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod key-management-persistence-656b774f7-f2kjc requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod kube-dns-sbds7 requesting resource cpu=100m on Node 9.111.254.123
Jan 29 09:57:09.675: INFO: Pod logging-elk-client-559db59464-ps9rr requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod logging-elk-data-0 requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod logging-elk-filebeat-ds-9mbdx requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.675: INFO: Pod logging-elk-filebeat-ds-9qmnw requesting resource cpu=0m on Node 9.111.255.33
Jan 29 09:57:09.676: INFO: Pod logging-elk-filebeat-ds-cl7nq requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod logging-elk-logstash-5444dd6fb4-6p7jm requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod logging-elk-master-c47cb5549-n5cmk requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod mariadb-0 requesting resource cpu=500m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod metrics-server-5bcdd5579-9v9qd requesting resource cpu=20m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod mgmt-repo-54f58b9b5c-55qlz requesting resource cpu=150m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod monitoring-grafana-7d5d7f5c45-58ttx requesting resource cpu=100m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod monitoring-prometheus-7b5f7db489-9jxtd requesting resource cpu=100m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod monitoring-prometheus-alertmanager-86b46f4f8f-zlwzh requesting resource cpu=10m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod monitoring-prometheus-collectdexporter-698694c4bf-z4b44 requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod monitoring-prometheus-elasticsearchexporter-786dbc957b-lrzfq requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod monitoring-prometheus-kubestatemetrics-9fcddf7d8-jw5xn requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod monitoring-prometheus-nodeexporter-58gsq requesting resource cpu=0m on Node 9.111.255.33
Jan 29 09:57:09.676: INFO: Pod monitoring-prometheus-nodeexporter-h889z requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod monitoring-prometheus-nodeexporter-hq52h requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod nginx-ingress-controller-ghbl2 requesting resource cpu=50m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod nvidia-device-plugin-cjt74 requesting resource cpu=0m on Node 9.111.254.104
Jan 29 09:57:09.676: INFO: Pod nvidia-device-plugin-db75j requesting resource cpu=0m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod nvidia-device-plugin-kg6z8 requesting resource cpu=0m on Node 9.111.255.33
Jan 29 09:57:09.676: INFO: Pod platform-api-74f4558f76-bp48c requesting resource cpu=50m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod platform-deploy-665cd77468-m9l7c requesting resource cpu=100m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod platform-ui-kl4bl requesting resource cpu=300m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod service-catalog-apiserver-7lmpb requesting resource cpu=100m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod service-catalog-controller-manager-6976ff4bc6-rl78n requesting resource cpu=100m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod tiller-deploy-59766f75bd-hsznz requesting resource cpu=100m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod unified-router-rnf9h requesting resource cpu=20m on Node 9.111.254.123
Jan 29 09:57:09.676: INFO: Pod web-terminal-7776f9df5b-q6mlp requesting resource cpu=10m on Node 9.111.254.104
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e38cea5-23ac-11e9-9015-b2c9bac28373.157e4938b39c50ca], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-gpqpf/filler-pod-3e38cea5-23ac-11e9-9015-b2c9bac28373 to 9.111.254.104]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e38cea5-23ac-11e9-9015-b2c9bac28373.157e493902929d5b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e38cea5-23ac-11e9-9015-b2c9bac28373.157e49390bb303d6], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e38cea5-23ac-11e9-9015-b2c9bac28373.157e49391cc92a40], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e424123-23ac-11e9-9015-b2c9bac28373.157e4938b4cd6c8a], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-gpqpf/filler-pod-3e424123-23ac-11e9-9015-b2c9bac28373 to 9.111.254.123]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e424123-23ac-11e9-9015-b2c9bac28373.157e4938f753a04c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e424123-23ac-11e9-9015-b2c9bac28373.157e4938fd9c0b15], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e424123-23ac-11e9-9015-b2c9bac28373.157e493911b6ebea], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e44ce2e-23ac-11e9-9015-b2c9bac28373.157e4938c9737cd5], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-gpqpf/filler-pod-3e44ce2e-23ac-11e9-9015-b2c9bac28373 to 9.111.255.33]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e44ce2e-23ac-11e9-9015-b2c9bac28373.157e4939151b4b09], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e44ce2e-23ac-11e9-9015-b2c9bac28373.157e49391c953b80], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e44ce2e-23ac-11e9-9015-b2c9bac28373.157e493928b0001d], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.157e4939b9e956bc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 9.111.254.104
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 9.111.254.123
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 9.111.255.33
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:57:15.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-gpqpf" for this suite.
Jan 29 09:57:21.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:57:21.274: INFO: namespace: e2e-tests-sched-pred-gpqpf, resource: bindings, ignored listing per whitelist
Jan 29 09:57:21.397: INFO: namespace e2e-tests-sched-pred-gpqpf deletion completed in 6.188209768s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

â€¢ [SLOW TEST:12.081 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:57:21.397: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-kvwfq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-kvwfq
Jan 29 09:57:23.741: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-kvwfq
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 09:57:23.744: INFO: Initial restart count of pod liveness-exec is 0
Jan 29 09:58:19.843: INFO: Restart count of pod e2e-tests-container-probe-kvwfq/liveness-exec is now 1 (56.099014285s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:58:19.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-kvwfq" for this suite.
Jan 29 09:58:25.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:58:26.014: INFO: namespace: e2e-tests-container-probe-kvwfq, resource: bindings, ignored listing per whitelist
Jan 29 09:58:26.049: INFO: namespace e2e-tests-container-probe-kvwfq deletion completed in 6.180201673s

â€¢ [SLOW TEST:64.653 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:58:26.050: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-s89mk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 09:58:26.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bde53ff-23ac-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-s89mk" to be "success or failure"
Jan 29 09:58:26.349: INFO: Pod "downwardapi-volume-6bde53ff-23ac-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 6.262835ms
Jan 29 09:58:28.353: INFO: Pod "downwardapi-volume-6bde53ff-23ac-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010428595s
Jan 29 09:58:30.357: INFO: Pod "downwardapi-volume-6bde53ff-23ac-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014371647s
STEP: Saw pod success
Jan 29 09:58:30.357: INFO: Pod "downwardapi-volume-6bde53ff-23ac-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:58:30.360: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-6bde53ff-23ac-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 09:58:30.387: INFO: Waiting for pod downwardapi-volume-6bde53ff-23ac-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:58:30.395: INFO: Pod downwardapi-volume-6bde53ff-23ac-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:58:30.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-s89mk" for this suite.
Jan 29 09:58:36.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:58:36.465: INFO: namespace: e2e-tests-projected-s89mk, resource: bindings, ignored listing per whitelist
Jan 29 09:58:36.507: INFO: namespace e2e-tests-projected-s89mk deletion completed in 6.107591913s

â€¢ [SLOW TEST:10.457 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:58:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pbv7j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-721798e0-23ac-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 09:58:36.726: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-72182b0f-23ac-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-pbv7j" to be "success or failure"
Jan 29 09:58:36.736: INFO: Pod "pod-projected-secrets-72182b0f-23ac-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 10.179918ms
Jan 29 09:58:38.740: INFO: Pod "pod-projected-secrets-72182b0f-23ac-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013493562s
STEP: Saw pod success
Jan 29 09:58:38.740: INFO: Pod "pod-projected-secrets-72182b0f-23ac-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:58:38.744: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-secrets-72182b0f-23ac-11e9-9015-b2c9bac28373 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 09:58:38.768: INFO: Waiting for pod pod-projected-secrets-72182b0f-23ac-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:58:38.770: INFO: Pod pod-projected-secrets-72182b0f-23ac-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:58:38.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pbv7j" for this suite.
Jan 29 09:58:44.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:58:44.797: INFO: namespace: e2e-tests-projected-pbv7j, resource: bindings, ignored listing per whitelist
Jan 29 09:58:44.905: INFO: namespace e2e-tests-projected-pbv7j deletion completed in 6.13036447s

â€¢ [SLOW TEST:8.397 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:58:44.905: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-75bl4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0129 09:58:55.158086      19 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 09:58:55.158: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:58:55.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-75bl4" for this suite.
Jan 29 09:59:01.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:59:01.253: INFO: namespace: e2e-tests-gc-75bl4, resource: bindings, ignored listing per whitelist
Jan 29 09:59:01.269: INFO: namespace e2e-tests-gc-75bl4 deletion completed in 6.107674754s

â€¢ [SLOW TEST:16.364 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:59:01.269: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-kv5zm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-80dac9bc-23ac-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 09:59:01.530: INFO: Waiting up to 5m0s for pod "pod-configmaps-80db4fe3-23ac-11e9-9015-b2c9bac28373" in namespace "e2e-tests-configmap-kv5zm" to be "success or failure"
Jan 29 09:59:01.535: INFO: Pod "pod-configmaps-80db4fe3-23ac-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 4.824007ms
Jan 29 09:59:03.538: INFO: Pod "pod-configmaps-80db4fe3-23ac-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00775139s
STEP: Saw pod success
Jan 29 09:59:03.538: INFO: Pod "pod-configmaps-80db4fe3-23ac-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:59:03.541: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-80db4fe3-23ac-11e9-9015-b2c9bac28373 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 09:59:03.556: INFO: Waiting for pod pod-configmaps-80db4fe3-23ac-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:59:03.560: INFO: Pod pod-configmaps-80db4fe3-23ac-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:59:03.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-kv5zm" for this suite.
Jan 29 09:59:09.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:59:09.710: INFO: namespace: e2e-tests-configmap-kv5zm, resource: bindings, ignored listing per whitelist
Jan 29 09:59:09.716: INFO: namespace e2e-tests-configmap-kv5zm deletion completed in 6.148219485s

â€¢ [SLOW TEST:8.447 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:59:09.723: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-58sjx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-85e284c9-23ac-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 09:59:09.932: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-85e3030c-23ac-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-58sjx" to be "success or failure"
Jan 29 09:59:09.934: INFO: Pod "pod-projected-configmaps-85e3030c-23ac-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287296ms
Jan 29 09:59:11.937: INFO: Pod "pod-projected-configmaps-85e3030c-23ac-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005194777s
STEP: Saw pod success
Jan 29 09:59:11.937: INFO: Pod "pod-projected-configmaps-85e3030c-23ac-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 09:59:11.940: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-configmaps-85e3030c-23ac-11e9-9015-b2c9bac28373 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 09:59:11.958: INFO: Waiting for pod pod-projected-configmaps-85e3030c-23ac-11e9-9015-b2c9bac28373 to disappear
Jan 29 09:59:11.963: INFO: Pod pod-projected-configmaps-85e3030c-23ac-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 09:59:11.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-58sjx" for this suite.
Jan 29 09:59:17.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 09:59:18.040: INFO: namespace: e2e-tests-projected-58sjx, resource: bindings, ignored listing per whitelist
Jan 29 09:59:18.113: INFO: namespace e2e-tests-projected-58sjx deletion completed in 6.144325137s

â€¢ [SLOW TEST:8.390 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 09:59:18.113: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-b8bhs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 29 09:59:18.358: INFO: Number of nodes with available pods: 0
Jan 29 09:59:18.358: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 09:59:19.370: INFO: Number of nodes with available pods: 0
Jan 29 09:59:19.370: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 09:59:20.365: INFO: Number of nodes with available pods: 0
Jan 29 09:59:20.365: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 09:59:21.365: INFO: Number of nodes with available pods: 3
Jan 29 09:59:21.365: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 29 09:59:21.384: INFO: Number of nodes with available pods: 2
Jan 29 09:59:21.384: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:22.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:22.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:23.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:23.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:24.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:24.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:25.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:25.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:26.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:26.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:27.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:27.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:28.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:28.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:29.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:29.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:30.394: INFO: Number of nodes with available pods: 2
Jan 29 09:59:30.394: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:31.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:31.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:32.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:32.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:33.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:33.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:34.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:34.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:35.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:35.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:36.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:36.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:37.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:37.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:38.393: INFO: Number of nodes with available pods: 2
Jan 29 09:59:38.393: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:39.394: INFO: Number of nodes with available pods: 2
Jan 29 09:59:39.394: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:40.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:40.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:41.393: INFO: Number of nodes with available pods: 2
Jan 29 09:59:41.393: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:42.390: INFO: Number of nodes with available pods: 2
Jan 29 09:59:42.390: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:43.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:43.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:44.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:44.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:45.393: INFO: Number of nodes with available pods: 2
Jan 29 09:59:45.393: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:46.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:46.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:47.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:47.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:48.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:48.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:49.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:49.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:50.393: INFO: Number of nodes with available pods: 2
Jan 29 09:59:50.393: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:51.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:51.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:52.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:52.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:53.394: INFO: Number of nodes with available pods: 2
Jan 29 09:59:53.394: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:54.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:54.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:55.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:55.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:56.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:56.393: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:57.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:57.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:58.392: INFO: Number of nodes with available pods: 2
Jan 29 09:59:58.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 09:59:59.391: INFO: Number of nodes with available pods: 2
Jan 29 09:59:59.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:00.391: INFO: Number of nodes with available pods: 2
Jan 29 10:00:00.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:01.391: INFO: Number of nodes with available pods: 2
Jan 29 10:00:01.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:02.392: INFO: Number of nodes with available pods: 2
Jan 29 10:00:02.393: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:03.392: INFO: Number of nodes with available pods: 2
Jan 29 10:00:03.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:04.391: INFO: Number of nodes with available pods: 2
Jan 29 10:00:04.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:05.391: INFO: Number of nodes with available pods: 2
Jan 29 10:00:05.391: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:06.397: INFO: Number of nodes with available pods: 2
Jan 29 10:00:06.397: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:07.392: INFO: Number of nodes with available pods: 2
Jan 29 10:00:07.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:08.392: INFO: Number of nodes with available pods: 2
Jan 29 10:00:08.392: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:00:09.392: INFO: Number of nodes with available pods: 3
Jan 29 10:00:09.392: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-b8bhs, will wait for the garbage collector to delete the pods
Jan 29 10:00:09.453: INFO: Deleting {extensions DaemonSet} daemon-set took: 5.265167ms
Jan 29 10:00:09.753: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 300.262833ms
Jan 29 10:00:47.056: INFO: Number of nodes with available pods: 0
Jan 29 10:00:47.056: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 10:00:47.058: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-b8bhs/daemonsets","resourceVersion":"695765"},"items":null}

Jan 29 10:00:47.061: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-b8bhs/pods","resourceVersion":"695765"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:00:47.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-b8bhs" for this suite.
Jan 29 10:00:53.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:00:53.165: INFO: namespace: e2e-tests-daemonsets-b8bhs, resource: bindings, ignored listing per whitelist
Jan 29 10:00:53.203: INFO: namespace e2e-tests-daemonsets-b8bhs deletion completed in 6.120901892s

â€¢ [SLOW TEST:95.090 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:00:53.204: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-zgxw2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Jan 29 10:00:53.393: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 10:00:53.404: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 10:00:53.406: INFO: 
Logging pods the kubelet thinks is on node 9.111.254.104 before test
Jan 29 10:00:53.422: INFO: logging-elk-client-559db59464-ps9rr from kube-system started at 2019-01-24 02:41:32 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.422: INFO: 	Container es-client ready: true, restart count 0
Jan 29 10:00:53.422: INFO: 	Container router ready: true, restart count 0
Jan 29 10:00:53.422: INFO: monitoring-prometheus-elasticsearchexporter-786dbc957b-lrzfq from kube-system started at 2019-01-24 02:48:48 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.422: INFO: 	Container elasticsearchexporter ready: true, restart count 0
Jan 29 10:00:53.422: INFO: 	Container router ready: true, restart count 0
Jan 29 10:00:53.422: INFO: monitoring-prometheus-alertmanager-86b46f4f8f-zlwzh from kube-system started at 2019-01-24 02:48:48 +0000 UTC (3 container statuses recorded)
Jan 29 10:00:53.422: INFO: 	Container alertmanager ready: true, restart count 0
Jan 29 10:00:53.422: INFO: 	Container configmap-reload ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container router ready: true, restart count 0
Jan 29 10:00:53.423: INFO: key-management-api-85fb4b5f55-pckqx from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container key-management-api ready: true, restart count 0
Jan 29 10:00:53.423: INFO: logging-elk-elasticsearch-curator-1548718200-clnkm from kube-system started at 2019-01-28 23:30:08 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container curator ready: false, restart count 0
Jan 29 10:00:53.423: INFO: monitoring-prometheus-collectdexporter-698694c4bf-z4b44 from kube-system started at 2019-01-24 02:48:48 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container collectd-exporter ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container router ready: true, restart count 0
Jan 29 10:00:53.423: INFO: monitoring-prometheus-7b5f7db489-9jxtd from kube-system started at 2019-01-24 02:48:48 +0000 UTC (4 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container alert-rule-controller ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container configmap-reload-prometheus ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container prometheus ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container router ready: true, restart count 1
Jan 29 10:00:53.423: INFO: audit-logging-fluentd-ds-2xjm5 from kube-system started at 2019-01-24 02:51:19 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container fluentd ready: true, restart count 0
Jan 29 10:00:53.423: INFO: key-management-pep-6d7f685bf4-rnd82 from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container key-management-pep ready: true, restart count 0
Jan 29 10:00:53.423: INFO: key-management-crypto-f4b6444bc-zspwm from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container key-management-crypto ready: true, restart count 0
Jan 29 10:00:53.423: INFO: k8s-proxy-9.111.254.104 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 10:00:53.423: INFO: web-terminal-7776f9df5b-q6mlp from kube-system started at 2019-01-24 02:40:36 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container web-terminal ready: true, restart count 0
Jan 29 10:00:53.423: INFO: key-management-lifecycle-69b67d57c5-q5pvm from kube-system started at 2019-01-24 02:52:04 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container key-management-lifecycle ready: true, restart count 0
Jan 29 10:00:53.423: INFO: logging-elk-data-0 from kube-system started at 2019-01-24 02:41:33 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container es-data ready: true, restart count 0
Jan 29 10:00:53.423: INFO: calico-node-4z7zs from kube-system started at 2019-01-24 02:27:00 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container install-cni ready: true, restart count 0
Jan 29 10:00:53.423: INFO: logging-elk-master-c47cb5549-n5cmk from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container es-master ready: true, restart count 0
Jan 29 10:00:53.423: INFO: monitoring-prometheus-nodeexporter-hq52h from kube-system started at 2019-01-24 02:48:47 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container nodeexporter ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container router ready: true, restart count 0
Jan 29 10:00:53.423: INFO: monitoring-grafana-7d5d7f5c45-58ttx from kube-system started at 2019-01-24 02:48:48 +0000 UTC (3 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container dashboard-crd-controller ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container grafana ready: true, restart count 0
Jan 29 10:00:53.423: INFO: 	Container router ready: true, restart count 0
Jan 29 10:00:53.423: INFO: logging-elk-elasticsearch-curator-1548631800-jp4mg from kube-system started at 2019-01-27 23:30:03 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container curator ready: false, restart count 0
Jan 29 10:00:53.423: INFO: logging-elk-filebeat-ds-9mbdx from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container filebeat ready: true, restart count 0
Jan 29 10:00:53.423: INFO: custom-metrics-adapter-77c5d9c6ff-9lg44 from kube-system started at 2019-01-24 02:47:21 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container custom-metrics-adapter ready: true, restart count 2
Jan 29 10:00:53.423: INFO: key-management-persistence-656b774f7-f2kjc from kube-system started at 2019-01-24 02:52:03 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container key-management-persistence ready: true, restart count 0
Jan 29 10:00:53.423: INFO: nvidia-device-plugin-cjt74 from kube-system started at 2019-01-24 02:28:55 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
Jan 29 10:00:53.423: INFO: metrics-server-5bcdd5579-9v9qd from kube-system started at 2019-01-24 02:32:06 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container metrics-server ready: true, restart count 3
Jan 29 10:00:53.423: INFO: sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-zg67k from heptio-sonobuoy started at 2019-01-29 08:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 29 10:00:53.423: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 29 10:00:53.423: INFO: logging-elk-logstash-5444dd6fb4-6p7jm from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.423: INFO: 	Container logstash ready: true, restart count 0
Jan 29 10:00:53.424: INFO: monitoring-prometheus-kubestatemetrics-9fcddf7d8-jw5xn from kube-system started at 2019-01-24 02:48:48 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.424: INFO: 	Container kubestatemetrics ready: true, restart count 0
Jan 29 10:00:53.424: INFO: 	Container router ready: true, restart count 0
Jan 29 10:00:53.424: INFO: 
Logging pods the kubelet thinks is on node 9.111.254.123 before test
Jan 29 10:00:53.446: INFO: mgmt-repo-54f58b9b5c-55qlz from kube-system started at 2019-01-24 02:46:04 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.447: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 10:00:53.447: INFO: 	Container mgmt-repo ready: true, restart count 0
Jan 29 10:00:53.447: INFO: unified-router-rnf9h from kube-system started at 2019-01-24 02:49:45 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.447: INFO: 	Container unified-router ready: true, restart count 0
Jan 29 10:00:53.447: INFO: k8s-proxy-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 10:00:53.447: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-29 08:47:51 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.447: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 10:00:53.448: INFO: monitoring-prometheus-nodeexporter-h889z from kube-system started at 2019-01-24 02:48:47 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container nodeexporter ready: true, restart count 0
Jan 29 10:00:53.448: INFO: 	Container router ready: true, restart count 0
Jan 29 10:00:53.448: INFO: k8s-kmsplugin-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 10:00:53.448: INFO: nginx-ingress-controller-ghbl2 from kube-system started at 2019-01-24 02:30:49 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 29 10:00:53.448: INFO: default-http-backend-7c4747d8d5-kzlz4 from kube-system started at 2019-01-24 02:30:49 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container default-http-backend ready: true, restart count 0
Jan 29 10:00:53.448: INFO: auth-apikeys-fmgd5 from kube-system started at 2019-01-24 02:33:21 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container auth-apikeys ready: true, restart count 0
Jan 29 10:00:53.448: INFO: heapster-6fb4fbb79b-dz5nf from kube-system started at 2019-01-24 02:42:38 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container heapster ready: true, restart count 0
Jan 29 10:00:53.448: INFO: catalog-ui-k8wf5 from kube-system started at 2019-01-24 02:43:56 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container catalog-ui ready: true, restart count 0
Jan 29 10:00:53.448: INFO: kube-dns-sbds7 from kube-system started at 2019-01-29 08:45:54 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container coredns ready: true, restart count 0
Jan 29 10:00:53.448: INFO: k8s-etcd-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 10:00:53.448: INFO: service-catalog-controller-manager-6976ff4bc6-rl78n from kube-system started at 2019-01-24 02:30:23 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container controller-manager ready: true, restart count 3
Jan 29 10:00:53.448: INFO: mariadb-0 from kube-system started at 2019-01-24 02:31:16 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container mariadb ready: true, restart count 0
Jan 29 10:00:53.448: INFO: auth-idp-2lvnk from kube-system started at 2019-01-24 02:32:54 +0000 UTC (4 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 10:00:53.448: INFO: 	Container platform-auth-service ready: true, restart count 1
Jan 29 10:00:53.448: INFO: 	Container platform-identity-manager ready: true, restart count 0
Jan 29 10:00:53.448: INFO: 	Container platform-identity-provider ready: true, restart count 0
Jan 29 10:00:53.448: INFO: calico-kube-controllers-58548fb4db-wqdxb from kube-system started at 2019-01-24 02:27:00 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 29 10:00:53.448: INFO: sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-2zbf2 from heptio-sonobuoy started at 2019-01-29 08:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 29 10:00:53.448: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 29 10:00:53.448: INFO: helm-api-66d8997947-whpb9 from kube-system started at 2019-01-24 02:45:03 +0000 UTC (3 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container helmapi ready: true, restart count 0
Jan 29 10:00:53.448: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 10:00:53.448: INFO: 	Container rudder ready: true, restart count 0
Jan 29 10:00:53.448: INFO: sonobuoy-e2e-job-5dd02bc5a9524b35 from heptio-sonobuoy started at 2019-01-29 08:47:59 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container e2e ready: true, restart count 0
Jan 29 10:00:53.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 10:00:53.448: INFO: ibm-cert-manager-cert-manager-69f9b77599-5vw2w from cert-manager started at 2019-01-24 02:28:03 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container ibm-cert-manager ready: true, restart count 1
Jan 29 10:00:53.448: INFO: auth-pdp-2mhhx from kube-system started at 2019-01-24 02:34:14 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container auth-pdp ready: true, restart count 0
Jan 29 10:00:53.448: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 10:00:53.448: INFO: auth-pap-8pdg7 from kube-system started at 2019-01-24 02:33:47 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container auth-pap ready: true, restart count 0
Jan 29 10:00:53.448: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 10:00:53.448: INFO: platform-ui-kl4bl from kube-system started at 2019-01-24 02:46:41 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container platform-ui ready: true, restart count 0
Jan 29 10:00:53.448: INFO: k8s-master-9.111.254.123 from kube-system started at <nil> (0 container statuses recorded)
Jan 29 10:00:53.448: INFO: platform-deploy-665cd77468-m9l7c from kube-system started at 2019-01-24 02:31:41 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.448: INFO: 	Container platform-deploy ready: true, restart count 0
Jan 29 10:00:53.448: INFO: nvidia-device-plugin-db75j from kube-system started at 2019-01-24 02:28:55 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
Jan 29 10:00:53.449: INFO: service-catalog-apiserver-7lmpb from kube-system started at 2019-01-24 02:30:23 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container apiserver ready: true, restart count 0
Jan 29 10:00:53.449: INFO: helm-repo-6c4f6cc8-88q9m from kube-system started at 2019-01-24 02:43:40 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container helm-repo ready: true, restart count 0
Jan 29 10:00:53.449: INFO: 	Container icp-audit-service ready: true, restart count 0
Jan 29 10:00:53.449: INFO: audit-logging-fluentd-ds-74lnc from kube-system started at 2019-01-24 02:51:19 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container fluentd ready: true, restart count 0
Jan 29 10:00:53.449: INFO: image-manager-0 from kube-system started at 2019-01-24 02:21:39 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container icp-registry ready: true, restart count 0
Jan 29 10:00:53.449: INFO: 	Container image-manager ready: true, restart count 0
Jan 29 10:00:53.449: INFO: tiller-deploy-59766f75bd-hsznz from kube-system started at 2019-01-24 02:26:09 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container tiller ready: true, restart count 0
Jan 29 10:00:53.449: INFO: platform-api-74f4558f76-bp48c from kube-system started at 2019-01-24 02:31:41 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container audit-service ready: true, restart count 0
Jan 29 10:00:53.449: INFO: 	Container platform-api ready: true, restart count 0
Jan 29 10:00:53.449: INFO: icp-management-ingress-5bnwr from kube-system started at 2019-01-24 02:34:40 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container icp-management-ingress ready: true, restart count 0
Jan 29 10:00:53.449: INFO: logging-elk-filebeat-ds-cl7nq from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container filebeat ready: true, restart count 0
Jan 29 10:00:53.449: INFO: ibmcloud-image-enforcement-596b54f688-xptnx from kube-system started at 2019-01-24 02:43:04 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container ibmcloud-image-enforcement ready: true, restart count 0
Jan 29 10:00:53.449: INFO: dns-test-x from default started at 2019-01-29 03:23:19 +0000 UTC (3 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container jessie-querier ready: true, restart count 11
Jan 29 10:00:53.449: INFO: 	Container querier ready: true, restart count 12
Jan 29 10:00:53.449: INFO: 	Container webserver ready: true, restart count 0
Jan 29 10:00:53.449: INFO: calico-node-mfg2r from kube-system started at 2019-01-24 02:27:00 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 10:00:53.449: INFO: 	Container install-cni ready: true, restart count 0
Jan 29 10:00:53.449: INFO: icp-mongodb-0 from kube-system started at 2019-01-24 02:29:56 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.449: INFO: 	Container icp-mongodb ready: true, restart count 0
Jan 29 10:00:53.449: INFO: 
Logging pods the kubelet thinks is on node 9.111.255.33 before test
Jan 29 10:00:53.460: INFO: audit-logging-fluentd-ds-g4kfl from kube-system started at 2019-01-24 02:51:19 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.460: INFO: 	Container fluentd ready: true, restart count 0
Jan 29 10:00:53.460: INFO: nvidia-device-plugin-kg6z8 from kube-system started at 2019-01-24 02:28:55 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.460: INFO: 	Container nvidia-device-plugin ready: true, restart count 0
Jan 29 10:00:53.460: INFO: monitoring-prometheus-nodeexporter-58gsq from kube-system started at 2019-01-24 02:48:47 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.460: INFO: 	Container nodeexporter ready: true, restart count 0
Jan 29 10:00:53.460: INFO: 	Container router ready: true, restart count 0
Jan 29 10:00:53.460: INFO: logging-elk-filebeat-ds-9qmnw from kube-system started at 2019-01-24 02:41:32 +0000 UTC (1 container statuses recorded)
Jan 29 10:00:53.460: INFO: 	Container filebeat ready: true, restart count 0
Jan 29 10:00:53.460: INFO: calico-node-rq54l from kube-system started at 2019-01-24 02:27:00 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.460: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 10:00:53.460: INFO: 	Container install-cni ready: true, restart count 0
Jan 29 10:00:53.460: INFO: sonobuoy-systemd-logs-daemon-set-e5929ec5fc934723-bz5g5 from heptio-sonobuoy started at 2019-01-29 08:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 10:00:53.460: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 29 10:00:53.460: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 29 10:00:53.461: INFO: k8s-proxy-9.111.255.33 from kube-system started at <nil> (0 container statuses recorded)
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c4dc23a5-23ac-11e9-9015-b2c9bac28373 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c4dc23a5-23ac-11e9-9015-b2c9bac28373 off the node 9.111.254.123
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c4dc23a5-23ac-11e9-9015-b2c9bac28373
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:00:57.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-zgxw2" for this suite.
Jan 29 10:01:05.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:01:05.778: INFO: namespace: e2e-tests-sched-pred-zgxw2, resource: bindings, ignored listing per whitelist
Jan 29 10:01:05.819: INFO: namespace e2e-tests-sched-pred-zgxw2 deletion completed in 8.156607788s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

â€¢ [SLOW TEST:12.615 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:01:05.819: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-tn6hj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Jan 29 10:01:06.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 cluster-info'
Jan 29 10:01:06.296: INFO: stderr: ""
Jan 29 10:01:06.296: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mcatalog-ui\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/catalog-ui:catalog-ui/proxy\x1b[0m\n\x1b[0;32mHeapster\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/heapster/proxy\x1b[0m\n\x1b[0;32mimage-manager\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/image-manager:image-manager/proxy\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mmetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mplatform-ui\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/platform-ui:platform-ui/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:01:06.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tn6hj" for this suite.
Jan 29 10:01:12.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:01:12.366: INFO: namespace: e2e-tests-kubectl-tn6hj, resource: bindings, ignored listing per whitelist
Jan 29 10:01:12.420: INFO: namespace e2e-tests-kubectl-tn6hj deletion completed in 6.116237376s

â€¢ [SLOW TEST:6.600 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:01:12.421: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ngsxk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 10:01:12.642: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf04a4d3-23ac-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-ngsxk" to be "success or failure"
Jan 29 10:01:12.645: INFO: Pod "downwardapi-volume-cf04a4d3-23ac-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.887431ms
Jan 29 10:01:14.652: INFO: Pod "downwardapi-volume-cf04a4d3-23ac-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010492328s
STEP: Saw pod success
Jan 29 10:01:14.652: INFO: Pod "downwardapi-volume-cf04a4d3-23ac-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:01:14.663: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-cf04a4d3-23ac-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 10:01:14.707: INFO: Waiting for pod downwardapi-volume-cf04a4d3-23ac-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:01:14.719: INFO: Pod downwardapi-volume-cf04a4d3-23ac-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:01:14.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ngsxk" for this suite.
Jan 29 10:01:20.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:01:20.868: INFO: namespace: e2e-tests-downward-api-ngsxk, resource: bindings, ignored listing per whitelist
Jan 29 10:01:20.937: INFO: namespace e2e-tests-downward-api-ngsxk deletion completed in 6.212947765s

â€¢ [SLOW TEST:8.516 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:01:20.937: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-b9gdq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-b9gdq
Jan 29 10:01:23.240: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-b9gdq
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 10:01:23.247: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:05:23.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-b9gdq" for this suite.
Jan 29 10:05:29.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:05:29.731: INFO: namespace: e2e-tests-container-probe-b9gdq, resource: bindings, ignored listing per whitelist
Jan 29 10:05:29.789: INFO: namespace e2e-tests-container-probe-b9gdq deletion completed in 6.131938152s

â€¢ [SLOW TEST:248.852 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:05:29.789: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-rkfqp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 10:05:29.982: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 29 10:05:30.054: INFO: Number of nodes with available pods: 0
Jan 29 10:05:30.054: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 10:05:31.074: INFO: Number of nodes with available pods: 0
Jan 29 10:05:31.074: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 10:05:32.062: INFO: Number of nodes with available pods: 0
Jan 29 10:05:32.062: INFO: Node 9.111.254.104 is running more than one daemon pod
Jan 29 10:05:33.062: INFO: Number of nodes with available pods: 3
Jan 29 10:05:33.062: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 29 10:05:33.142: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:33.142: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:33.142: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:34.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:34.151: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:34.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:35.153: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:35.153: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:35.154: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:36.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:36.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:36.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:37.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:37.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:37.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:38.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:38.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:38.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:39.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:39.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:39.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:40.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:40.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:40.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:41.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:41.151: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:41.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:42.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:42.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:42.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:43.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:43.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:43.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:44.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:44.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:44.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:45.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:45.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:45.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:46.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:46.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:46.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:47.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:47.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:47.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:48.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:48.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:48.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:49.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:49.151: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:49.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:50.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:50.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:50.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:51.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:51.151: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:51.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:52.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:52.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:52.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:53.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:53.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:53.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:54.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:54.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:54.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:55.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:55.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:55.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:56.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:56.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:56.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:57.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:57.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:57.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:58.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:58.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:58.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:59.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:59.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:05:59.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:00.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:00.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:00.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:01.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:01.151: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:01.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:02.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:02.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:02.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:03.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:03.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:03.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:04.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:04.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:04.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:05.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:05.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:05.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:06.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:06.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:06.150: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:06.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:07.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:07.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:07.150: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:07.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:08.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:08.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:08.150: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:08.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:09.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:09.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:09.149: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:09.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:10.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:10.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:10.150: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:10.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:11.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:11.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:11.150: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:11.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:12.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:12.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:12.150: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:12.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:13.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:13.151: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:13.151: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:13.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:14.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:14.151: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:14.151: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:14.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:15.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:15.149: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:15.149: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:15.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:16.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:16.150: INFO: Wrong image for pod: daemon-set-lllbq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:16.150: INFO: Pod daemon-set-lllbq is not available
Jan 29 10:06:16.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:17.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:17.150: INFO: Pod daemon-set-kpq5r is not available
Jan 29 10:06:17.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:18.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:18.149: INFO: Pod daemon-set-kpq5r is not available
Jan 29 10:06:18.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:19.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:19.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:20.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:20.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:21.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:21.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:22.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:22.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:23.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:23.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:24.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:24.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:25.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:25.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:26.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:26.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:27.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:27.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:28.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:28.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:29.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:29.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:30.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:30.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:31.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:31.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:32.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:32.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:33.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:33.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:34.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:34.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:35.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:35.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:36.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:36.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:37.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:37.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:38.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:38.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:39.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:39.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:40.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:40.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:41.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:41.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:42.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:42.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:43.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:43.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:44.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:44.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:45.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:45.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:46.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:46.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:47.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:47.151: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:48.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:48.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:49.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:49.149: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:50.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:50.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:51.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:51.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:51.151: INFO: Pod daemon-set-vgg58 is not available
Jan 29 10:06:52.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:52.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:52.150: INFO: Pod daemon-set-vgg58 is not available
Jan 29 10:06:53.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:53.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:53.150: INFO: Pod daemon-set-vgg58 is not available
Jan 29 10:06:54.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:54.150: INFO: Wrong image for pod: daemon-set-vgg58. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:54.150: INFO: Pod daemon-set-vgg58 is not available
Jan 29 10:06:55.150: INFO: Pod daemon-set-65rp7 is not available
Jan 29 10:06:55.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:56.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:57.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:58.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:06:59.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:00.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:01.152: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:02.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:03.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:04.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:05.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:06.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:07.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:08.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:09.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:10.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:11.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:12.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:13.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:14.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:15.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:16.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:17.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:18.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:19.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:20.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:21.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:22.149: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:23.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:24.151: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:25.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:26.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:27.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:27.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:28.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:28.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:29.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:29.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:30.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:30.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:31.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:31.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:32.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:32.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:33.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:33.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:34.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:34.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:35.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:35.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:36.150: INFO: Wrong image for pod: daemon-set-75xf2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 29 10:07:36.150: INFO: Pod daemon-set-75xf2 is not available
Jan 29 10:07:37.150: INFO: Pod daemon-set-7hhhz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 29 10:07:37.160: INFO: Number of nodes with available pods: 2
Jan 29 10:07:37.160: INFO: Node 9.111.255.33 is running more than one daemon pod
Jan 29 10:07:38.168: INFO: Number of nodes with available pods: 3
Jan 29 10:07:38.168: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-rkfqp, will wait for the garbage collector to delete the pods
Jan 29 10:07:38.240: INFO: Deleting {extensions DaemonSet} daemon-set took: 4.683503ms
Jan 29 10:07:38.340: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.280726ms
Jan 29 10:07:46.447: INFO: Number of nodes with available pods: 0
Jan 29 10:07:46.447: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 10:07:46.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-rkfqp/daemonsets","resourceVersion":"696762"},"items":null}

Jan 29 10:07:46.452: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-rkfqp/pods","resourceVersion":"696762"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:07:46.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-rkfqp" for this suite.
Jan 29 10:07:52.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:07:52.499: INFO: namespace: e2e-tests-daemonsets-rkfqp, resource: bindings, ignored listing per whitelist
Jan 29 10:07:52.577: INFO: namespace e2e-tests-daemonsets-rkfqp deletion completed in 6.111317252s

â€¢ [SLOW TEST:142.788 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:07:52.577: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-t2dfx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 29 10:07:52.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-t2dfx'
Jan 29 10:07:52.949: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Jan 29 10:07:52.949: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jan 29 10:07:52.954: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jan 29 10:07:52.961: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan 29 10:07:52.968: INFO: scanned /root for discovery docs: <nil>
Jan 29 10:07:52.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-t2dfx'
Jan 29 10:08:11.345: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 29 10:08:11.346: INFO: stdout: "Created e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3\nScaling up e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jan 29 10:08:11.346: INFO: stdout: "Created e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3\nScaling up e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jan 29 10:08:11.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-t2dfx'
Jan 29 10:08:11.470: INFO: stderr: ""
Jan 29 10:08:11.471: INFO: stdout: "e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3-hvppt "
Jan 29 10:08:11.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3-hvppt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-t2dfx'
Jan 29 10:08:11.618: INFO: stderr: ""
Jan 29 10:08:11.618: INFO: stdout: "true"
Jan 29 10:08:11.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pods e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3-hvppt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-t2dfx'
Jan 29 10:08:11.744: INFO: stderr: ""
Jan 29 10:08:11.744: INFO: stdout: "nginx:1.14-alpine"
Jan 29 10:08:11.744: INFO: e2e-test-nginx-rc-9218c968a719405051033fa3809ce5c3-hvppt is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
Jan 29 10:08:11.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-t2dfx'
Jan 29 10:08:11.882: INFO: stderr: ""
Jan 29 10:08:11.882: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:08:11.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-t2dfx" for this suite.
Jan 29 10:08:33.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:08:33.940: INFO: namespace: e2e-tests-kubectl-t2dfx, resource: bindings, ignored listing per whitelist
Jan 29 10:08:34.020: INFO: namespace e2e-tests-kubectl-t2dfx deletion completed in 22.133175381s

â€¢ [SLOW TEST:41.443 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:08:34.021: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vvdlb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 10:08:34.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d63bbc4a-23ad-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-vvdlb" to be "success or failure"
Jan 29 10:08:34.244: INFO: Pod "downwardapi-volume-d63bbc4a-23ad-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 6.178699ms
Jan 29 10:08:36.248: INFO: Pod "downwardapi-volume-d63bbc4a-23ad-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009946999s
Jan 29 10:08:38.252: INFO: Pod "downwardapi-volume-d63bbc4a-23ad-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014310004s
STEP: Saw pod success
Jan 29 10:08:38.252: INFO: Pod "downwardapi-volume-d63bbc4a-23ad-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:08:38.255: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-d63bbc4a-23ad-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 10:08:38.274: INFO: Waiting for pod downwardapi-volume-d63bbc4a-23ad-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:08:38.276: INFO: Pod downwardapi-volume-d63bbc4a-23ad-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:08:38.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vvdlb" for this suite.
Jan 29 10:08:44.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:08:44.404: INFO: namespace: e2e-tests-projected-vvdlb, resource: bindings, ignored listing per whitelist
Jan 29 10:08:44.410: INFO: namespace e2e-tests-projected-vvdlb deletion completed in 6.128395942s

â€¢ [SLOW TEST:10.389 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:08:44.410: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-sshk8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-dc6d7ab0-23ad-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 10:08:44.640: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc6e3435-23ad-11e9-9015-b2c9bac28373" in namespace "e2e-tests-configmap-sshk8" to be "success or failure"
Jan 29 10:08:44.642: INFO: Pod "pod-configmaps-dc6e3435-23ad-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.810047ms
Jan 29 10:08:46.648: INFO: Pod "pod-configmaps-dc6e3435-23ad-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008401226s
STEP: Saw pod success
Jan 29 10:08:46.648: INFO: Pod "pod-configmaps-dc6e3435-23ad-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:08:46.652: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-dc6e3435-23ad-11e9-9015-b2c9bac28373 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 10:08:46.679: INFO: Waiting for pod pod-configmaps-dc6e3435-23ad-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:08:46.683: INFO: Pod pod-configmaps-dc6e3435-23ad-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:08:46.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sshk8" for this suite.
Jan 29 10:08:52.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:08:52.790: INFO: namespace: e2e-tests-configmap-sshk8, resource: bindings, ignored listing per whitelist
Jan 29 10:08:52.811: INFO: namespace e2e-tests-configmap-sshk8 deletion completed in 6.123424887s

â€¢ [SLOW TEST:8.401 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:08:52.811: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-s9qvb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-s9qvb
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Jan 29 10:08:53.041: INFO: Found 0 stateful pods, waiting for 3
Jan 29 10:09:03.044: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 10:09:03.044: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 10:09:03.045: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 29 10:09:03.136: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 29 10:09:13.181: INFO: Updating stateful set ss2
Jan 29 10:09:13.186: INFO: Waiting for Pod e2e-tests-statefulset-s9qvb/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 29 10:09:23.194: INFO: Waiting for Pod e2e-tests-statefulset-s9qvb/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jan 29 10:09:33.223: INFO: Found 2 stateful pods, waiting for 3
Jan 29 10:09:43.226: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 10:09:43.226: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 10:09:43.226: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 29 10:09:43.338: INFO: Updating stateful set ss2
Jan 29 10:09:43.345: INFO: Waiting for Pod e2e-tests-statefulset-s9qvb/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 29 10:09:53.350: INFO: Waiting for Pod e2e-tests-statefulset-s9qvb/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 29 10:10:03.446: INFO: Updating stateful set ss2
Jan 29 10:10:03.461: INFO: Waiting for StatefulSet e2e-tests-statefulset-s9qvb/ss2 to complete update
Jan 29 10:10:03.461: INFO: Waiting for Pod e2e-tests-statefulset-s9qvb/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 29 10:10:13.467: INFO: Deleting all statefulset in ns e2e-tests-statefulset-s9qvb
Jan 29 10:10:13.470: INFO: Scaling statefulset ss2 to 0
Jan 29 10:10:43.539: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 10:10:43.542: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:10:43.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-s9qvb" for this suite.
Jan 29 10:10:49.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:10:49.618: INFO: namespace: e2e-tests-statefulset-s9qvb, resource: bindings, ignored listing per whitelist
Jan 29 10:10:49.680: INFO: namespace e2e-tests-statefulset-s9qvb deletion completed in 6.120691142s

â€¢ [SLOW TEST:116.869 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:10:49.681: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-c2lcn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 10:10:49.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 version'
Jan 29 10:10:49.980: INFO: stderr: ""
Jan 29 10:10:49.980: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.1\", GitCommit:\"4ed3216f3ec431b140b1d899130a69fc671678f4\", GitTreeState:\"clean\", BuildDate:\"2018-10-05T16:46:06Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.4+icp-ee\", GitCommit:\"d03f6421b5463042d87aa0211f116ba4848a0d0f\", GitTreeState:\"clean\", BuildDate:\"2019-01-17T13:14:09Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:10:49.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-c2lcn" for this suite.
Jan 29 10:10:55.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:10:56.032: INFO: namespace: e2e-tests-kubectl-c2lcn, resource: bindings, ignored listing per whitelist
Jan 29 10:10:56.101: INFO: namespace e2e-tests-kubectl-c2lcn deletion completed in 6.117072379s

â€¢ [SLOW TEST:6.420 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:10:56.104: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-l7rst
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 29 10:10:56.322: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-l7rst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7rst/configmaps/e2e-watch-test-resource-version,UID:2aec5d43-23ae-11e9-a926-eeeeeeeeeeee,ResourceVersion:697617,Generation:0,CreationTimestamp:2019-01-29 10:10:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 10:10:56.322: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-l7rst,SelfLink:/api/v1/namespaces/e2e-tests-watch-l7rst/configmaps/e2e-watch-test-resource-version,UID:2aec5d43-23ae-11e9-a926-eeeeeeeeeeee,ResourceVersion:697618,Generation:0,CreationTimestamp:2019-01-29 10:10:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:10:56.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-l7rst" for this suite.
Jan 29 10:11:02.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:11:02.472: INFO: namespace: e2e-tests-watch-l7rst, resource: bindings, ignored listing per whitelist
Jan 29 10:11:02.519: INFO: namespace e2e-tests-watch-l7rst deletion completed in 6.19254831s

â€¢ [SLOW TEST:6.416 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:11:02.519: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-8d2kq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 10:11:24.755: INFO: Container started at 2019-01-29 10:11:04 +0000 UTC, pod became ready at 2019-01-29 10:11:23 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:11:24.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8d2kq" for this suite.
Jan 29 10:11:46.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:11:46.802: INFO: namespace: e2e-tests-container-probe-8d2kq, resource: bindings, ignored listing per whitelist
Jan 29 10:11:46.887: INFO: namespace e2e-tests-container-probe-8d2kq deletion completed in 22.126138442s

â€¢ [SLOW TEST:44.368 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:11:46.887: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-c6hln
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-4930fc3d-23ae-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume secrets
Jan 29 10:11:47.137: INFO: Waiting up to 5m0s for pod "pod-secrets-4931c175-23ae-11e9-9015-b2c9bac28373" in namespace "e2e-tests-secrets-c6hln" to be "success or failure"
Jan 29 10:11:47.140: INFO: Pod "pod-secrets-4931c175-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611617ms
Jan 29 10:11:49.146: INFO: Pod "pod-secrets-4931c175-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008652874s
Jan 29 10:11:51.149: INFO: Pod "pod-secrets-4931c175-23ae-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011780911s
STEP: Saw pod success
Jan 29 10:11:51.149: INFO: Pod "pod-secrets-4931c175-23ae-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:11:51.151: INFO: Trying to get logs from node 9.111.254.123 pod pod-secrets-4931c175-23ae-11e9-9015-b2c9bac28373 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 10:11:51.166: INFO: Waiting for pod pod-secrets-4931c175-23ae-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:11:51.168: INFO: Pod pod-secrets-4931c175-23ae-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:11:51.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-c6hln" for this suite.
Jan 29 10:11:57.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:11:57.237: INFO: namespace: e2e-tests-secrets-c6hln, resource: bindings, ignored listing per whitelist
Jan 29 10:11:57.312: INFO: namespace e2e-tests-secrets-c6hln deletion completed in 6.138108747s

â€¢ [SLOW TEST:10.425 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:11:57.312: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ct4l9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 29 10:11:57.538: INFO: Waiting up to 5m0s for pod "downward-api-4f6788d2-23ae-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-ct4l9" to be "success or failure"
Jan 29 10:11:57.541: INFO: Pod "downward-api-4f6788d2-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.171411ms
Jan 29 10:11:59.545: INFO: Pod "downward-api-4f6788d2-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006390172s
Jan 29 10:12:01.548: INFO: Pod "downward-api-4f6788d2-23ae-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00924636s
STEP: Saw pod success
Jan 29 10:12:01.548: INFO: Pod "downward-api-4f6788d2-23ae-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:12:01.550: INFO: Trying to get logs from node 9.111.254.123 pod downward-api-4f6788d2-23ae-11e9-9015-b2c9bac28373 container dapi-container: <nil>
STEP: delete the pod
Jan 29 10:12:01.570: INFO: Waiting for pod downward-api-4f6788d2-23ae-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:12:01.574: INFO: Pod downward-api-4f6788d2-23ae-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:12:01.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ct4l9" for this suite.
Jan 29 10:12:07.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:12:07.621: INFO: namespace: e2e-tests-downward-api-ct4l9, resource: bindings, ignored listing per whitelist
Jan 29 10:12:07.699: INFO: namespace e2e-tests-downward-api-ct4l9 deletion completed in 6.116574168s

â€¢ [SLOW TEST:10.386 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:12:07.699: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-c4dzg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Jan 29 10:12:08.100: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-c4dzg" to be "success or failure"
Jan 29 10:12:08.102: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773039ms
Jan 29 10:12:10.105: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005367433s
Jan 29 10:12:12.108: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008370957s
STEP: Saw pod success
Jan 29 10:12:12.108: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 29 10:12:12.110: INFO: Trying to get logs from node 9.111.254.123 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 29 10:12:12.133: INFO: Waiting for pod pod-host-path-test to disappear
Jan 29 10:12:12.137: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:12:12.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-c4dzg" for this suite.
Jan 29 10:12:18.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:12:18.222: INFO: namespace: e2e-tests-hostpath-c4dzg, resource: bindings, ignored listing per whitelist
Jan 29 10:12:18.263: INFO: namespace e2e-tests-hostpath-c4dzg deletion completed in 6.121298995s

â€¢ [SLOW TEST:10.564 seconds]
[sig-storage] HostPath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:12:18.269: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-l4w9z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 10:12:18.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5be885eb-23ae-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-l4w9z" to be "success or failure"
Jan 29 10:12:18.498: INFO: Pod "downwardapi-volume-5be885eb-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971496ms
Jan 29 10:12:20.503: INFO: Pod "downwardapi-volume-5be885eb-23ae-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00861929s
STEP: Saw pod success
Jan 29 10:12:20.503: INFO: Pod "downwardapi-volume-5be885eb-23ae-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:12:20.507: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-5be885eb-23ae-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 10:12:20.533: INFO: Waiting for pod downwardapi-volume-5be885eb-23ae-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:12:20.538: INFO: Pod downwardapi-volume-5be885eb-23ae-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:12:20.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l4w9z" for this suite.
Jan 29 10:12:26.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:12:26.686: INFO: namespace: e2e-tests-projected-l4w9z, resource: bindings, ignored listing per whitelist
Jan 29 10:12:26.722: INFO: namespace e2e-tests-projected-l4w9z deletion completed in 6.162220269s

â€¢ [SLOW TEST:8.453 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:12:26.722: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-cvwf8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 29 10:12:26.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60eff4c0-23ae-11e9-9015-b2c9bac28373" in namespace "e2e-tests-downward-api-cvwf8" to be "success or failure"
Jan 29 10:12:26.948: INFO: Pod "downwardapi-volume-60eff4c0-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 12.85763ms
Jan 29 10:12:28.955: INFO: Pod "downwardapi-volume-60eff4c0-23ae-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019579486s
STEP: Saw pod success
Jan 29 10:12:28.955: INFO: Pod "downwardapi-volume-60eff4c0-23ae-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:12:28.965: INFO: Trying to get logs from node 9.111.254.123 pod downwardapi-volume-60eff4c0-23ae-11e9-9015-b2c9bac28373 container client-container: <nil>
STEP: delete the pod
Jan 29 10:12:28.995: INFO: Waiting for pod downwardapi-volume-60eff4c0-23ae-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:12:28.997: INFO: Pod downwardapi-volume-60eff4c0-23ae-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:12:28.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cvwf8" for this suite.
Jan 29 10:12:35.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:12:35.043: INFO: namespace: e2e-tests-downward-api-cvwf8, resource: bindings, ignored listing per whitelist
Jan 29 10:12:35.147: INFO: namespace e2e-tests-downward-api-cvwf8 deletion completed in 6.146407277s

â€¢ [SLOW TEST:8.425 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:12:35.148: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-bztbx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 29 10:12:35.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-bztbx'
Jan 29 10:12:35.758: INFO: stderr: ""
Jan 29 10:12:35.758: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jan 29 10:12:40.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-bztbx -o json'
Jan 29 10:12:40.947: INFO: stderr: ""
Jan 29 10:12:40.947: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-01-29T10:12:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-bztbx\",\n        \"resourceVersion\": \"698027\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-bztbx/pods/e2e-test-nginx-pod\",\n        \"uid\": \"66319b1a-23ae-11e9-a926-eeeeeeeeeeee\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-rq64p\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"nodeName\": \"9.111.254.123\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-rq64p\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-rq64p\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-29T10:12:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-29T10:12:37Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-29T10:12:37Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-29T10:12:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://d2287db399876f3696ff252fa4c943034597199d39d203790a6caaa494913558\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-01-29T10:12:37Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"9.111.254.123\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.1.211.139\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-01-29T10:12:35Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 29 10:12:40.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 replace -f - --namespace=e2e-tests-kubectl-bztbx'
Jan 29 10:12:41.344: INFO: stderr: ""
Jan 29 10:12:41.345: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Jan 29 10:12:41.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-900852259 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-bztbx'
Jan 29 10:12:54.297: INFO: stderr: ""
Jan 29 10:12:54.297: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:12:54.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bztbx" for this suite.
Jan 29 10:13:00.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:13:00.380: INFO: namespace: e2e-tests-kubectl-bztbx, resource: bindings, ignored listing per whitelist
Jan 29 10:13:00.413: INFO: namespace e2e-tests-kubectl-bztbx deletion completed in 6.111314299s

â€¢ [SLOW TEST:25.265 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:13:00.413: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8vk8t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 29 10:13:03.245: INFO: Successfully updated pod "labelsupdate7503df86-23ae-11e9-9015-b2c9bac28373"
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:13:05.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8vk8t" for this suite.
Jan 29 10:13:27.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:13:27.350: INFO: namespace: e2e-tests-projected-8vk8t, resource: bindings, ignored listing per whitelist
Jan 29 10:13:27.593: INFO: namespace e2e-tests-projected-8vk8t deletion completed in 22.321938245s

â€¢ [SLOW TEST:27.180 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:13:27.594: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-xgqrn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-xgqrn
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 10:13:27.791: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 29 10:13:50.377: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.211.188:8080/dial?request=hostName&protocol=udp&host=10.1.148.23&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-xgqrn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:13:50.377: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 10:13:50.541: INFO: Waiting for endpoints: map[]
Jan 29 10:13:50.544: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.211.188:8080/dial?request=hostName&protocol=udp&host=10.1.12.155&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-xgqrn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:13:50.544: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 10:13:50.715: INFO: Waiting for endpoints: map[]
Jan 29 10:13:50.719: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.211.188:8080/dial?request=hostName&protocol=udp&host=10.1.211.175&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-xgqrn PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:13:50.719: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 10:13:50.903: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:13:50.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-xgqrn" for this suite.
Jan 29 10:14:12.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:14:13.103: INFO: namespace: e2e-tests-pod-network-test-xgqrn, resource: bindings, ignored listing per whitelist
Jan 29 10:14:13.111: INFO: namespace e2e-tests-pod-network-test-xgqrn deletion completed in 22.188042546s

â€¢ [SLOW TEST:45.517 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:14:13.111: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-w4j8j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 29 10:14:13.340: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 29 10:14:18.344: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 29 10:14:18.345: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 29 10:14:18.454: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-w4j8j,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-w4j8j/deployments/test-cleanup-deployment,UID:a368da44-23ae-11e9-a926-eeeeeeeeeeee,ResourceVersion:698387,Generation:1,CreationTimestamp:2019-01-29 10:14:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jan 29 10:14:18.457: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:14:18.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-w4j8j" for this suite.
Jan 29 10:14:24.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:14:24.603: INFO: namespace: e2e-tests-deployment-w4j8j, resource: bindings, ignored listing per whitelist
Jan 29 10:14:24.614: INFO: namespace e2e-tests-deployment-w4j8j deletion completed in 6.143575272s

â€¢ [SLOW TEST:11.503 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:14:24.614: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gmmlj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-a733c268-23ae-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 10:14:24.844: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a7346b68-23ae-11e9-9015-b2c9bac28373" in namespace "e2e-tests-projected-gmmlj" to be "success or failure"
Jan 29 10:14:24.846: INFO: Pod "pod-projected-configmaps-a7346b68-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075765ms
Jan 29 10:14:26.851: INFO: Pod "pod-projected-configmaps-a7346b68-23ae-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007395456s
STEP: Saw pod success
Jan 29 10:14:26.851: INFO: Pod "pod-projected-configmaps-a7346b68-23ae-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:14:26.856: INFO: Trying to get logs from node 9.111.254.123 pod pod-projected-configmaps-a7346b68-23ae-11e9-9015-b2c9bac28373 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 10:14:26.881: INFO: Waiting for pod pod-projected-configmaps-a7346b68-23ae-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:14:26.886: INFO: Pod pod-projected-configmaps-a7346b68-23ae-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:14:26.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gmmlj" for this suite.
Jan 29 10:14:32.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:14:32.970: INFO: namespace: e2e-tests-projected-gmmlj, resource: bindings, ignored listing per whitelist
Jan 29 10:14:33.001: INFO: namespace e2e-tests-projected-gmmlj deletion completed in 6.104446606s

â€¢ [SLOW TEST:8.387 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:14:33.001: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-csvth
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-csvth
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 10:14:33.189: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 29 10:14:55.774: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.1.12.153:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-csvth PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:14:55.774: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 10:14:55.955: INFO: Found all expected endpoints: [netserver-0]
Jan 29 10:14:55.958: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.1.148.24:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-csvth PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:14:55.958: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 10:14:56.138: INFO: Found all expected endpoints: [netserver-1]
Jan 29 10:14:56.142: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://10.1.211.168:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-csvth PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:14:56.142: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
Jan 29 10:14:56.319: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:14:56.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-csvth" for this suite.
Jan 29 10:15:18.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:15:18.434: INFO: namespace: e2e-tests-pod-network-test-csvth, resource: bindings, ignored listing per whitelist
Jan 29 10:15:18.498: INFO: namespace e2e-tests-pod-network-test-csvth deletion completed in 22.169690509s

â€¢ [SLOW TEST:45.497 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:15:18.498: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-hh2kx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-c7520825-23ae-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 10:15:18.744: INFO: Waiting up to 5m0s for pod "pod-configmaps-c752af97-23ae-11e9-9015-b2c9bac28373" in namespace "e2e-tests-configmap-hh2kx" to be "success or failure"
Jan 29 10:15:18.749: INFO: Pod "pod-configmaps-c752af97-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 5.578118ms
Jan 29 10:15:20.753: INFO: Pod "pod-configmaps-c752af97-23ae-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008679427s
STEP: Saw pod success
Jan 29 10:15:20.753: INFO: Pod "pod-configmaps-c752af97-23ae-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:15:20.755: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-c752af97-23ae-11e9-9015-b2c9bac28373 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 10:15:20.773: INFO: Waiting for pod pod-configmaps-c752af97-23ae-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:15:20.776: INFO: Pod pod-configmaps-c752af97-23ae-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:15:20.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hh2kx" for this suite.
Jan 29 10:15:26.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:15:26.810: INFO: namespace: e2e-tests-configmap-hh2kx, resource: bindings, ignored listing per whitelist
Jan 29 10:15:26.896: INFO: namespace e2e-tests-configmap-hh2kx deletion completed in 6.114176972s

â€¢ [SLOW TEST:8.398 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:15:26.897: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-tnxgw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-tnxgw/configmap-test-cc54fe1d-23ae-11e9-9015-b2c9bac28373
STEP: Creating a pod to test consume configMaps
Jan 29 10:15:27.143: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc55bcec-23ae-11e9-9015-b2c9bac28373" in namespace "e2e-tests-configmap-tnxgw" to be "success or failure"
Jan 29 10:15:27.151: INFO: Pod "pod-configmaps-cc55bcec-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 7.888752ms
Jan 29 10:15:29.155: INFO: Pod "pod-configmaps-cc55bcec-23ae-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01176949s
STEP: Saw pod success
Jan 29 10:15:29.155: INFO: Pod "pod-configmaps-cc55bcec-23ae-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:15:29.160: INFO: Trying to get logs from node 9.111.254.123 pod pod-configmaps-cc55bcec-23ae-11e9-9015-b2c9bac28373 container env-test: <nil>
STEP: delete the pod
Jan 29 10:15:29.183: INFO: Waiting for pod pod-configmaps-cc55bcec-23ae-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:15:29.189: INFO: Pod pod-configmaps-cc55bcec-23ae-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:15:29.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-tnxgw" for this suite.
Jan 29 10:15:35.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:15:35.258: INFO: namespace: e2e-tests-configmap-tnxgw, resource: bindings, ignored listing per whitelist
Jan 29 10:15:35.312: INFO: namespace e2e-tests-configmap-tnxgw deletion completed in 6.116322179s

â€¢ [SLOW TEST:8.416 seconds]
[sig-api-machinery] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:15:35.314: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-dx55b
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 29 10:15:35.630: INFO: Waiting up to 5m0s for pod "pod-d15c1b0d-23ae-11e9-9015-b2c9bac28373" in namespace "e2e-tests-emptydir-dx55b" to be "success or failure"
Jan 29 10:15:35.634: INFO: Pod "pod-d15c1b0d-23ae-11e9-9015-b2c9bac28373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513026ms
Jan 29 10:15:37.637: INFO: Pod "pod-d15c1b0d-23ae-11e9-9015-b2c9bac28373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00705336s
STEP: Saw pod success
Jan 29 10:15:37.637: INFO: Pod "pod-d15c1b0d-23ae-11e9-9015-b2c9bac28373" satisfied condition "success or failure"
Jan 29 10:15:37.640: INFO: Trying to get logs from node 9.111.254.123 pod pod-d15c1b0d-23ae-11e9-9015-b2c9bac28373 container test-container: <nil>
STEP: delete the pod
Jan 29 10:15:37.660: INFO: Waiting for pod pod-d15c1b0d-23ae-11e9-9015-b2c9bac28373 to disappear
Jan 29 10:15:37.664: INFO: Pod pod-d15c1b0d-23ae-11e9-9015-b2c9bac28373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:15:37.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dx55b" for this suite.
Jan 29 10:15:43.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:15:43.704: INFO: namespace: e2e-tests-emptydir-dx55b, resource: bindings, ignored listing per whitelist
Jan 29 10:15:43.771: INFO: namespace e2e-tests-emptydir-dx55b deletion completed in 6.102063291s

â€¢ [SLOW TEST:8.457 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:15:43.772: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-hzhx9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-bbqx
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 10:15:44.043: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bbqx" in namespace "e2e-tests-subpath-hzhx9" to be "success or failure"
Jan 29 10:15:44.048: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.649688ms
Jan 29 10:15:46.052: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00829789s
Jan 29 10:15:48.055: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 4.01173546s
Jan 29 10:15:50.059: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 6.016212306s
Jan 29 10:15:52.062: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 8.019142061s
Jan 29 10:15:54.066: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 10.022462176s
Jan 29 10:15:56.069: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 12.02541305s
Jan 29 10:15:58.072: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 14.028910014s
Jan 29 10:16:00.076: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 16.032396508s
Jan 29 10:16:02.078: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 18.035181396s
Jan 29 10:16:04.082: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 20.038793139s
Jan 29 10:16:06.085: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Running", Reason="", readiness=false. Elapsed: 22.041876098s
Jan 29 10:16:08.089: INFO: Pod "pod-subpath-test-projected-bbqx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.04575942s
STEP: Saw pod success
Jan 29 10:16:08.089: INFO: Pod "pod-subpath-test-projected-bbqx" satisfied condition "success or failure"
Jan 29 10:16:08.095: INFO: Trying to get logs from node 9.111.254.123 pod pod-subpath-test-projected-bbqx container test-container-subpath-projected-bbqx: <nil>
STEP: delete the pod
Jan 29 10:16:08.113: INFO: Waiting for pod pod-subpath-test-projected-bbqx to disappear
Jan 29 10:16:08.116: INFO: Pod pod-subpath-test-projected-bbqx no longer exists
STEP: Deleting pod pod-subpath-test-projected-bbqx
Jan 29 10:16:08.116: INFO: Deleting pod "pod-subpath-test-projected-bbqx" in namespace "e2e-tests-subpath-hzhx9"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:16:08.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-hzhx9" for this suite.
Jan 29 10:16:14.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:16:14.197: INFO: namespace: e2e-tests-subpath-hzhx9, resource: bindings, ignored listing per whitelist
Jan 29 10:16:14.251: INFO: namespace e2e-tests-subpath-hzhx9 deletion completed in 6.122731898s

â€¢ [SLOW TEST:30.479 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Jan 29 10:16:14.253: INFO: >>> kubeConfig: /tmp/kubeconfig-900852259
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-5q6zf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 29 10:16:14.444: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Jan 29 10:16:18.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-5q6zf" for this suite.
Jan 29 10:16:40.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 29 10:16:40.940: INFO: namespace: e2e-tests-init-container-5q6zf, resource: bindings, ignored listing per whitelist
Jan 29 10:16:41.008: INFO: namespace e2e-tests-init-container-5q6zf deletion completed in 22.116995021s

â€¢ [SLOW TEST:26.755 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSJan 29 10:16:41.009: INFO: Running AfterSuite actions on all node
Jan 29 10:16:41.009: INFO: Running AfterSuite actions on node 1
Jan 29 10:16:41.009: INFO: Skipping dumping logs from cluster

Ran 187 of 1814 Specs in 5314.208 seconds
SUCCESS! -- 187 Passed | 0 Failed | 0 Pending | 1627 Skipped PASS

Ginkgo ran 1 suite in 1h28m35.384120765s
Test Suite Passed
