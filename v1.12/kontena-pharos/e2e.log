Dec  8 05:00:05.469: INFO: Overriding default scale value of zero to 1
Dec  8 05:00:05.469: INFO: Overriding default milliseconds value of zero to 5000
I1208 05:00:06.027215      17 test_context.go:385] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-036194578
I1208 05:00:06.027295      17 e2e.go:304] Starting e2e run "20a32d24-faa6-11e8-b680-ca3d6f40e675" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1544245205 - Will randomize all specs
Will run 188 of 1814 specs

Dec  8 05:00:06.218: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:00:06.220: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  8 05:00:21.260: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  8 05:00:21.305: INFO: The status of Pod pharos-telemetry-1544245200-b4zhz is Succeeded, skipping waiting
Dec  8 05:00:21.305: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  8 05:00:21.305: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Dec  8 05:00:21.305: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  8 05:00:21.316: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  8 05:00:21.316: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Dec  8 05:00:21.316: INFO: e2e test version: v1.12.1
Dec  8 05:00:21.318: INFO: kube-apiserver version: v1.12.3
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:00:21.319: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename watch
Dec  8 05:00:21.447: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec  8 05:00:21.465: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-6q7sv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  8 05:00:21.606: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-6q7sv,SelfLink:/api/v1/namespaces/e2e-tests-watch-6q7sv/configmaps/e2e-watch-test-watch-closed,UID:2a4b4106-faa6-11e8-a103-96cd63cee3b8,ResourceVersion:2106,Generation:0,CreationTimestamp:2018-12-08 05:00:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  8 05:00:21.606: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-6q7sv,SelfLink:/api/v1/namespaces/e2e-tests-watch-6q7sv/configmaps/e2e-watch-test-watch-closed,UID:2a4b4106-faa6-11e8-a103-96cd63cee3b8,ResourceVersion:2107,Generation:0,CreationTimestamp:2018-12-08 05:00:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  8 05:00:21.636: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-6q7sv,SelfLink:/api/v1/namespaces/e2e-tests-watch-6q7sv/configmaps/e2e-watch-test-watch-closed,UID:2a4b4106-faa6-11e8-a103-96cd63cee3b8,ResourceVersion:2108,Generation:0,CreationTimestamp:2018-12-08 05:00:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  8 05:00:21.636: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-6q7sv,SelfLink:/api/v1/namespaces/e2e-tests-watch-6q7sv/configmaps/e2e-watch-test-watch-closed,UID:2a4b4106-faa6-11e8-a103-96cd63cee3b8,ResourceVersion:2109,Generation:0,CreationTimestamp:2018-12-08 05:00:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:00:21.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-6q7sv" for this suite.
Dec  8 05:00:27.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:00:27.702: INFO: namespace: e2e-tests-watch-6q7sv, resource: bindings, ignored listing per whitelist
Dec  8 05:00:27.840: INFO: namespace e2e-tests-watch-6q7sv deletion completed in 6.197557648s

• [SLOW TEST:6.521 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:00:27.841: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-gxksd
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:00:28.064: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:00:29.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-gxksd" for this suite.
Dec  8 05:00:35.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:00:35.225: INFO: namespace: e2e-tests-custom-resource-definition-gxksd, resource: bindings, ignored listing per whitelist
Dec  8 05:00:35.322: INFO: namespace e2e-tests-custom-resource-definition-gxksd deletion completed in 6.148550935s

• [SLOW TEST:7.481 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:00:35.322: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-thggr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:00:35.590: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"329c35c1-faa6-11e8-a103-96cd63cee3b8", Controller:(*bool)(0xc421b3ec66), BlockOwnerDeletion:(*bool)(0xc421b3ec67)}}
Dec  8 05:00:35.609: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"32988ed1-faa6-11e8-a103-96cd63cee3b8", Controller:(*bool)(0xc4210b81c6), BlockOwnerDeletion:(*bool)(0xc4210b81c7)}}
Dec  8 05:00:35.617: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3299b89c-faa6-11e8-a103-96cd63cee3b8", Controller:(*bool)(0xc420d1f3a6), BlockOwnerDeletion:(*bool)(0xc420d1f3a7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:00:40.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-thggr" for this suite.
Dec  8 05:00:46.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:00:46.718: INFO: namespace: e2e-tests-gc-thggr, resource: bindings, ignored listing per whitelist
Dec  8 05:00:46.830: INFO: namespace e2e-tests-gc-thggr deletion completed in 6.192154398s

• [SLOW TEST:11.508 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:00:46.831: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-dtr28
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-3976ec32-faa6-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 05:00:47.061: INFO: Waiting up to 5m0s for pod "pod-configmaps-397784b1-faa6-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-configmap-dtr28" to be "success or failure"
Dec  8 05:00:47.067: INFO: Pod "pod-configmaps-397784b1-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.723542ms
Dec  8 05:00:49.071: INFO: Pod "pod-configmaps-397784b1-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009728633s
Dec  8 05:00:51.075: INFO: Pod "pod-configmaps-397784b1-faa6-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013880166s
STEP: Saw pod success
Dec  8 05:00:51.075: INFO: Pod "pod-configmaps-397784b1-faa6-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:00:51.077: INFO: Trying to get logs from node phrs-liberal-perch pod pod-configmaps-397784b1-faa6-11e8-b680-ca3d6f40e675 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 05:00:51.106: INFO: Waiting for pod pod-configmaps-397784b1-faa6-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:00:51.110: INFO: Pod pod-configmaps-397784b1-faa6-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:00:51.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-dtr28" for this suite.
Dec  8 05:00:57.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:00:57.234: INFO: namespace: e2e-tests-configmap-dtr28, resource: bindings, ignored listing per whitelist
Dec  8 05:00:57.246: INFO: namespace e2e-tests-configmap-dtr28 deletion completed in 6.132211014s

• [SLOW TEST:10.416 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:00:57.247: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-dxr5v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:00:57.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dxr5v" for this suite.
Dec  8 05:01:19.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:01:19.620: INFO: namespace: e2e-tests-pods-dxr5v, resource: bindings, ignored listing per whitelist
Dec  8 05:01:19.667: INFO: namespace e2e-tests-pods-dxr5v deletion completed in 22.193890073s

• [SLOW TEST:22.420 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:01:19.668: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-t4nhg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  8 05:01:19.948: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:19.954: INFO: Number of nodes with available pods: 0
Dec  8 05:01:19.954: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:01:20.960: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:20.966: INFO: Number of nodes with available pods: 0
Dec  8 05:01:20.966: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:01:21.960: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:21.965: INFO: Number of nodes with available pods: 0
Dec  8 05:01:21.965: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:01:22.959: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:22.964: INFO: Number of nodes with available pods: 3
Dec  8 05:01:22.964: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  8 05:01:22.995: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:23.000: INFO: Number of nodes with available pods: 2
Dec  8 05:01:23.000: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:24.014: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:24.019: INFO: Number of nodes with available pods: 2
Dec  8 05:01:24.020: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:25.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:25.011: INFO: Number of nodes with available pods: 2
Dec  8 05:01:25.011: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:26.008: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:26.013: INFO: Number of nodes with available pods: 2
Dec  8 05:01:26.013: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:27.007: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:27.012: INFO: Number of nodes with available pods: 2
Dec  8 05:01:27.012: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:28.005: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:28.009: INFO: Number of nodes with available pods: 2
Dec  8 05:01:28.009: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:29.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:29.010: INFO: Number of nodes with available pods: 2
Dec  8 05:01:29.010: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:30.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:30.012: INFO: Number of nodes with available pods: 2
Dec  8 05:01:30.012: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:31.007: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:31.012: INFO: Number of nodes with available pods: 2
Dec  8 05:01:31.012: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:32.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:32.010: INFO: Number of nodes with available pods: 2
Dec  8 05:01:32.010: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:33.005: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:33.009: INFO: Number of nodes with available pods: 2
Dec  8 05:01:33.009: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:34.005: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:34.009: INFO: Number of nodes with available pods: 2
Dec  8 05:01:34.009: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:35.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:35.010: INFO: Number of nodes with available pods: 2
Dec  8 05:01:35.010: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:36.005: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:36.009: INFO: Number of nodes with available pods: 2
Dec  8 05:01:36.009: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:37.013: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:37.017: INFO: Number of nodes with available pods: 2
Dec  8 05:01:37.017: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:38.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:38.012: INFO: Number of nodes with available pods: 2
Dec  8 05:01:38.012: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:39.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:39.009: INFO: Number of nodes with available pods: 2
Dec  8 05:01:39.010: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:40.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:40.011: INFO: Number of nodes with available pods: 2
Dec  8 05:01:40.011: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:41.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:41.010: INFO: Number of nodes with available pods: 2
Dec  8 05:01:41.010: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:42.007: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:42.013: INFO: Number of nodes with available pods: 2
Dec  8 05:01:42.013: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:43.009: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:43.013: INFO: Number of nodes with available pods: 2
Dec  8 05:01:43.014: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:44.014: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:44.018: INFO: Number of nodes with available pods: 2
Dec  8 05:01:44.018: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:45.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:45.013: INFO: Number of nodes with available pods: 2
Dec  8 05:01:45.013: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:46.007: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:46.012: INFO: Number of nodes with available pods: 2
Dec  8 05:01:46.012: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:47.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:47.011: INFO: Number of nodes with available pods: 2
Dec  8 05:01:47.011: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:48.007: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:48.012: INFO: Number of nodes with available pods: 2
Dec  8 05:01:48.012: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:49.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:49.010: INFO: Number of nodes with available pods: 2
Dec  8 05:01:49.010: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:50.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:50.010: INFO: Number of nodes with available pods: 2
Dec  8 05:01:50.010: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:51.005: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:51.009: INFO: Number of nodes with available pods: 2
Dec  8 05:01:51.009: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:52.005: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:52.010: INFO: Number of nodes with available pods: 2
Dec  8 05:01:52.010: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:53.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:53.011: INFO: Number of nodes with available pods: 2
Dec  8 05:01:53.011: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:54.007: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:54.012: INFO: Number of nodes with available pods: 2
Dec  8 05:01:54.012: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:55.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:55.010: INFO: Number of nodes with available pods: 2
Dec  8 05:01:55.010: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:56.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:56.011: INFO: Number of nodes with available pods: 2
Dec  8 05:01:56.011: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:57.005: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:57.008: INFO: Number of nodes with available pods: 2
Dec  8 05:01:57.008: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:58.005: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:58.009: INFO: Number of nodes with available pods: 2
Dec  8 05:01:58.009: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:01:59.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:01:59.012: INFO: Number of nodes with available pods: 2
Dec  8 05:01:59.012: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 05:02:00.006: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:02:00.009: INFO: Number of nodes with available pods: 3
Dec  8 05:02:00.009: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-t4nhg, will wait for the garbage collector to delete the pods
Dec  8 05:02:00.074: INFO: Deleting {extensions DaemonSet} daemon-set took: 7.437402ms
Dec  8 05:02:00.174: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.24919ms
Dec  8 05:02:38.378: INFO: Number of nodes with available pods: 0
Dec  8 05:02:38.378: INFO: Number of running nodes: 0, number of available pods: 0
Dec  8 05:02:38.383: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-t4nhg/daemonsets","resourceVersion":"2525"},"items":null}

Dec  8 05:02:38.387: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-t4nhg/pods","resourceVersion":"2525"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:02:38.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-t4nhg" for this suite.
Dec  8 05:02:44.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:02:44.452: INFO: namespace: e2e-tests-daemonsets-t4nhg, resource: bindings, ignored listing per whitelist
Dec  8 05:02:44.534: INFO: namespace e2e-tests-daemonsets-t4nhg deletion completed in 6.127529861s

• [SLOW TEST:84.866 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:02:44.534: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-rjpxf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Dec  8 05:02:44.734: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  8 05:02:44.742: INFO: Waiting for terminating namespaces to be deleted...
Dec  8 05:02:44.745: INFO: 
Logging pods the kubelet thinks is on node phrs-glorious-mastodon before test
Dec  8 05:02:44.768: INFO: rook-ceph-agent-8nlgc from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container rook-ceph-agent ready: true, restart count 0
Dec  8 05:02:44.768: INFO: rook-ceph-osd-id-0-8565c4d54-mhrr2 from kontena-storage started at 2018-12-08 04:57:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Dec  8 05:02:44.768: INFO: kube-proxy-2wrhj from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  8 05:02:44.768: INFO: dashboard-5fd9996979-b8dtn from kontena-lens started at 2018-12-08 04:55:48 +0000 UTC (2 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container dashboard ready: true, restart count 0
Dec  8 05:02:44.768: INFO: 	Container terminal-gateway ready: true, restart count 0
Dec  8 05:02:44.768: INFO: sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-q6qrz from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec  8 05:02:44.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  8 05:02:44.768: INFO: user-management-6f95f557c-clh6k from kontena-lens started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container user-management ready: true, restart count 0
Dec  8 05:02:44.768: INFO: rook-discover-swkpp from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container rook-discover ready: true, restart count 0
Dec  8 05:02:44.768: INFO: metrics-server-66d95b8778-t9z2j from kube-system started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container metrics-server ready: true, restart count 0
Dec  8 05:02:44.768: INFO: rook-ceph-mon0-4rg6w from kontena-storage started at 2018-12-08 04:56:23 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Dec  8 05:02:44.768: INFO: pharos-proxy-phrs-glorious-mastodon from kube-system started at <nil> (0 container statuses recorded)
Dec  8 05:02:44.768: INFO: weave-net-p7lbn from kube-system started at 2018-12-08 04:55:28 +0000 UTC (3 container statuses recorded)
Dec  8 05:02:44.768: INFO: 	Container weave ready: true, restart count 0
Dec  8 05:02:44.768: INFO: 	Container weave-flying-shuttle ready: true, restart count 0
Dec  8 05:02:44.768: INFO: 	Container weave-npc ready: true, restart count 0
Dec  8 05:02:44.769: INFO: rook-ceph-osd-prepare-phrs-glorious-mastodon-zglms from kontena-storage started at 2018-12-08 04:57:44 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.769: INFO: 	Container rook-ceph-osd ready: false, restart count 0
Dec  8 05:02:44.769: INFO: default-http-backend-78796f9b55-xpm76 from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.769: INFO: 	Container default-http-backend ready: true, restart count 0
Dec  8 05:02:44.769: INFO: nginx-ingress-controller-br6h6 from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.769: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  8 05:02:44.769: INFO: 
Logging pods the kubelet thinks is on node phrs-liberal-perch before test
Dec  8 05:02:44.793: INFO: pharos-proxy-phrs-liberal-perch from kube-system started at <nil> (0 container statuses recorded)
Dec  8 05:02:44.793: INFO: redis-76b74cb777-kzwbw from kontena-lens started at 2018-12-08 04:58:03 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.793: INFO: 	Container redis ready: true, restart count 0
Dec  8 05:02:44.793: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-08 04:59:40 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.793: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  8 05:02:44.793: INFO: weave-net-nxwh9 from kube-system started at 2018-12-08 04:55:28 +0000 UTC (3 container statuses recorded)
Dec  8 05:02:44.793: INFO: 	Container weave ready: true, restart count 0
Dec  8 05:02:44.793: INFO: 	Container weave-flying-shuttle ready: true, restart count 0
Dec  8 05:02:44.793: INFO: 	Container weave-npc ready: true, restart count 0
Dec  8 05:02:44.793: INFO: kube-proxy-kflkl from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.793: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  8 05:02:44.793: INFO: nginx-ingress-controller-phwbj from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.793: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  8 05:02:44.793: INFO: rook-ceph-mon1-svdpg from kontena-storage started at 2018-12-08 04:57:18 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.793: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Dec  8 05:02:44.793: INFO: rook-ceph-osd-prepare-phrs-liberal-perch-kw6qh from kontena-storage started at 2018-12-08 04:57:44 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.793: INFO: 	Container rook-ceph-osd ready: false, restart count 0
Dec  8 05:02:44.793: INFO: rook-ceph-osd-id-1-576859ddb-5xgt4 from kontena-storage started at 2018-12-08 04:57:49 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.793: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Dec  8 05:02:44.793: INFO: sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-d2p6d from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 05:02:44.793: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec  8 05:02:44.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  8 05:02:44.794: INFO: rook-discover-b5h6d from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.794: INFO: 	Container rook-discover ready: true, restart count 0
Dec  8 05:02:44.794: INFO: rook-ceph-agent-76246 from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.794: INFO: 	Container rook-ceph-agent ready: true, restart count 0
Dec  8 05:02:44.794: INFO: sonobuoy-e2e-job-31fab27f508240cf from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 05:02:44.794: INFO: 	Container e2e ready: true, restart count 0
Dec  8 05:02:44.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  8 05:02:44.794: INFO: 
Logging pods the kubelet thinks is on node phrs-one-falcon before test
Dec  8 05:02:44.821: INFO: pharos-telemetry-1544245200-b4zhz from kube-system started at 2018-12-08 05:00:05 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container agent ready: false, restart count 0
Dec  8 05:02:44.822: INFO: pharos-proxy-phrs-one-falcon from kube-system started at <nil> (0 container statuses recorded)
Dec  8 05:02:44.822: INFO: coredns-5784d68866-d7qp2 from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container coredns ready: true, restart count 0
Dec  8 05:02:44.822: INFO: weave-net-ck2kg from kube-system started at 2018-12-08 04:55:28 +0000 UTC (3 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container weave ready: true, restart count 0
Dec  8 05:02:44.822: INFO: 	Container weave-flying-shuttle ready: true, restart count 0
Dec  8 05:02:44.822: INFO: 	Container weave-npc ready: true, restart count 0
Dec  8 05:02:44.822: INFO: tiller-deploy-86bbfc6cdc-p6cg7 from kube-system started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container tiller ready: true, restart count 0
Dec  8 05:02:44.822: INFO: rook-ceph-osd-id-2-6c96b77d74-4c876 from kontena-storage started at 2018-12-08 04:57:49 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Dec  8 05:02:44.822: INFO: rook-ceph-agent-jw287 from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container rook-ceph-agent ready: true, restart count 0
Dec  8 05:02:44.822: INFO: rook-discover-5v5d2 from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container rook-discover ready: true, restart count 0
Dec  8 05:02:44.822: INFO: kontena-storage-operator-6bf6b8979d-cp4sp from kontena-storage-system started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec  8 05:02:44.822: INFO: kontena-storage-tools-59d9cfbff7-qbvq6 from kontena-storage started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Dec  8 05:02:44.822: INFO: rook-ceph-mon2-7qfn7 from kontena-storage started at 2018-12-08 04:57:26 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Dec  8 05:02:44.822: INFO: sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-cjzdv from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec  8 05:02:44.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  8 05:02:44.822: INFO: kube-proxy-xmcsc from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  8 05:02:44.822: INFO: default-http-backend-78796f9b55-g5z5h from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.822: INFO: 	Container default-http-backend ready: true, restart count 0
Dec  8 05:02:44.823: INFO: nginx-ingress-controller-hmqdb from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.823: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  8 05:02:44.823: INFO: rook-ceph-mgr-a-b49c8bf4-pthbl from kontena-storage started at 2018-12-08 04:57:40 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.823: INFO: 	Container rook-ceph-mgr-a ready: true, restart count 0
Dec  8 05:02:44.823: INFO: rook-ceph-osd-prepare-phrs-one-falcon-7b6gx from kontena-storage started at 2018-12-08 04:57:44 +0000 UTC (1 container statuses recorded)
Dec  8 05:02:44.823: INFO: 	Container rook-ceph-osd ready: false, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-80e28105-faa6-11e8-b680-ca3d6f40e675 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-80e28105-faa6-11e8-b680-ca3d6f40e675 off the node phrs-one-falcon
STEP: verifying the node doesn't have the label kubernetes.io/e2e-80e28105-faa6-11e8-b680-ca3d6f40e675
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:02:48.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-rjpxf" for this suite.
Dec  8 05:02:56.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:02:57.059: INFO: namespace: e2e-tests-sched-pred-rjpxf, resource: bindings, ignored listing per whitelist
Dec  8 05:02:57.083: INFO: namespace e2e-tests-sched-pred-rjpxf deletion completed in 8.150864836s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:12.549 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:02:57.083: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-kfll4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  8 05:03:01.817: INFO: Successfully updated pod "pod-update-8716caf6-faa6-11e8-b680-ca3d6f40e675"
STEP: verifying the updated pod is in kubernetes
Dec  8 05:03:01.826: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:03:01.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-kfll4" for this suite.
Dec  8 05:03:23.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:03:23.917: INFO: namespace: e2e-tests-pods-kfll4, resource: bindings, ignored listing per whitelist
Dec  8 05:03:24.041: INFO: namespace e2e-tests-pods-kfll4 deletion completed in 22.209978028s

• [SLOW TEST:26.958 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:03:24.041: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-sg85t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  8 05:03:24.262: INFO: Waiting up to 5m0s for pod "pod-972b36ff-faa6-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-sg85t" to be "success or failure"
Dec  8 05:03:24.271: INFO: Pod "pod-972b36ff-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 9.630199ms
Dec  8 05:03:26.276: INFO: Pod "pod-972b36ff-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014364685s
Dec  8 05:03:28.281: INFO: Pod "pod-972b36ff-faa6-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018941379s
STEP: Saw pod success
Dec  8 05:03:28.281: INFO: Pod "pod-972b36ff-faa6-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:03:28.284: INFO: Trying to get logs from node phrs-liberal-perch pod pod-972b36ff-faa6-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:03:28.311: INFO: Waiting for pod pod-972b36ff-faa6-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:03:28.318: INFO: Pod pod-972b36ff-faa6-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:03:28.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-sg85t" for this suite.
Dec  8 05:03:34.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:03:34.407: INFO: namespace: e2e-tests-emptydir-sg85t, resource: bindings, ignored listing per whitelist
Dec  8 05:03:34.439: INFO: namespace e2e-tests-emptydir-sg85t deletion completed in 6.1149301s

• [SLOW TEST:10.398 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:03:34.440: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-dlnfc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  8 05:03:34.635: INFO: Waiting up to 5m0s for pod "pod-9d5aec07-faa6-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-dlnfc" to be "success or failure"
Dec  8 05:03:34.649: INFO: Pod "pod-9d5aec07-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 14.389312ms
Dec  8 05:03:36.653: INFO: Pod "pod-9d5aec07-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01807693s
Dec  8 05:03:38.657: INFO: Pod "pod-9d5aec07-faa6-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021729481s
STEP: Saw pod success
Dec  8 05:03:38.657: INFO: Pod "pod-9d5aec07-faa6-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:03:38.666: INFO: Trying to get logs from node phrs-one-falcon pod pod-9d5aec07-faa6-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:03:38.693: INFO: Waiting for pod pod-9d5aec07-faa6-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:03:38.701: INFO: Pod pod-9d5aec07-faa6-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:03:38.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dlnfc" for this suite.
Dec  8 05:03:44.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:03:44.748: INFO: namespace: e2e-tests-emptydir-dlnfc, resource: bindings, ignored listing per whitelist
Dec  8 05:03:44.849: INFO: namespace e2e-tests-emptydir-dlnfc deletion completed in 6.142262937s

• [SLOW TEST:10.409 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:03:44.849: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-qk5xr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Dec  8 05:03:45.083: INFO: Waiting up to 5m0s for pod "var-expansion-a394ca8a-faa6-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-var-expansion-qk5xr" to be "success or failure"
Dec  8 05:03:45.105: INFO: Pod "var-expansion-a394ca8a-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 22.430042ms
Dec  8 05:03:47.109: INFO: Pod "var-expansion-a394ca8a-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026098304s
Dec  8 05:03:49.112: INFO: Pod "var-expansion-a394ca8a-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029805536s
Dec  8 05:03:51.116: INFO: Pod "var-expansion-a394ca8a-faa6-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033200081s
STEP: Saw pod success
Dec  8 05:03:51.116: INFO: Pod "var-expansion-a394ca8a-faa6-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:03:51.119: INFO: Trying to get logs from node phrs-glorious-mastodon pod var-expansion-a394ca8a-faa6-11e8-b680-ca3d6f40e675 container dapi-container: <nil>
STEP: delete the pod
Dec  8 05:03:51.143: INFO: Waiting for pod var-expansion-a394ca8a-faa6-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:03:51.146: INFO: Pod var-expansion-a394ca8a-faa6-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:03:51.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-qk5xr" for this suite.
Dec  8 05:03:57.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:03:57.176: INFO: namespace: e2e-tests-var-expansion-qk5xr, resource: bindings, ignored listing per whitelist
Dec  8 05:03:57.292: INFO: namespace e2e-tests-var-expansion-qk5xr deletion completed in 6.141129464s

• [SLOW TEST:12.443 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:03:57.292: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-wg29w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:04:57.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wg29w" for this suite.
Dec  8 05:05:19.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:05:19.703: INFO: namespace: e2e-tests-container-probe-wg29w, resource: bindings, ignored listing per whitelist
Dec  8 05:05:19.757: INFO: namespace e2e-tests-container-probe-wg29w deletion completed in 22.226771375s

• [SLOW TEST:82.466 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:05:19.758: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-cksrw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:05:20.009: INFO: Creating ReplicaSet my-hostname-basic-dc2a93e6-faa6-11e8-b680-ca3d6f40e675
Dec  8 05:05:20.024: INFO: Pod name my-hostname-basic-dc2a93e6-faa6-11e8-b680-ca3d6f40e675: Found 0 pods out of 1
Dec  8 05:05:25.029: INFO: Pod name my-hostname-basic-dc2a93e6-faa6-11e8-b680-ca3d6f40e675: Found 1 pods out of 1
Dec  8 05:05:25.029: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-dc2a93e6-faa6-11e8-b680-ca3d6f40e675" is running
Dec  8 05:05:25.033: INFO: Pod "my-hostname-basic-dc2a93e6-faa6-11e8-b680-ca3d6f40e675-blqx9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-08 05:05:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-08 05:05:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-08 05:05:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-08 05:05:20 +0000 UTC Reason: Message:}])
Dec  8 05:05:25.033: INFO: Trying to dial the pod
Dec  8 05:05:30.048: INFO: Controller my-hostname-basic-dc2a93e6-faa6-11e8-b680-ca3d6f40e675: Got expected result from replica 1 [my-hostname-basic-dc2a93e6-faa6-11e8-b680-ca3d6f40e675-blqx9]: "my-hostname-basic-dc2a93e6-faa6-11e8-b680-ca3d6f40e675-blqx9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:05:30.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-cksrw" for this suite.
Dec  8 05:05:36.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:05:36.188: INFO: namespace: e2e-tests-replicaset-cksrw, resource: bindings, ignored listing per whitelist
Dec  8 05:05:36.213: INFO: namespace e2e-tests-replicaset-cksrw deletion completed in 6.160299999s

• [SLOW TEST:16.455 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:05:36.214: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-p2tpw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:05:36.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5f20390-faa6-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-p2tpw" to be "success or failure"
Dec  8 05:05:36.441: INFO: Pod "downwardapi-volume-e5f20390-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 17.962406ms
Dec  8 05:05:38.446: INFO: Pod "downwardapi-volume-e5f20390-faa6-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022522913s
Dec  8 05:05:40.449: INFO: Pod "downwardapi-volume-e5f20390-faa6-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026244794s
STEP: Saw pod success
Dec  8 05:05:40.450: INFO: Pod "downwardapi-volume-e5f20390-faa6-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:05:40.453: INFO: Trying to get logs from node phrs-glorious-mastodon pod downwardapi-volume-e5f20390-faa6-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:05:40.474: INFO: Waiting for pod downwardapi-volume-e5f20390-faa6-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:05:40.478: INFO: Pod downwardapi-volume-e5f20390-faa6-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:05:40.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-p2tpw" for this suite.
Dec  8 05:05:46.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:05:46.533: INFO: namespace: e2e-tests-downward-api-p2tpw, resource: bindings, ignored listing per whitelist
Dec  8 05:05:46.611: INFO: namespace e2e-tests-downward-api-p2tpw deletion completed in 6.130023196s

• [SLOW TEST:10.398 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:05:46.613: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-qfjk9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-qfjk9
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Dec  8 05:05:46.820: INFO: Found 0 stateful pods, waiting for 3
Dec  8 05:05:56.825: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 05:05:56.825: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 05:05:56.825: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 05:05:56.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-qfjk9 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:05:57.048: INFO: stderr: ""
Dec  8 05:05:57.048: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:05:57.048: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  8 05:06:07.084: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  8 05:06:17.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-qfjk9 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:06:17.309: INFO: stderr: ""
Dec  8 05:06:17.309: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  8 05:06:17.309: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  8 05:06:27.339: INFO: Waiting for StatefulSet e2e-tests-statefulset-qfjk9/ss2 to complete update
Dec  8 05:06:27.339: INFO: Waiting for Pod e2e-tests-statefulset-qfjk9/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  8 05:06:27.339: INFO: Waiting for Pod e2e-tests-statefulset-qfjk9/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  8 05:06:27.339: INFO: Waiting for Pod e2e-tests-statefulset-qfjk9/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  8 05:06:37.349: INFO: Waiting for StatefulSet e2e-tests-statefulset-qfjk9/ss2 to complete update
Dec  8 05:06:37.349: INFO: Waiting for Pod e2e-tests-statefulset-qfjk9/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  8 05:06:37.349: INFO: Waiting for Pod e2e-tests-statefulset-qfjk9/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  8 05:06:47.350: INFO: Waiting for StatefulSet e2e-tests-statefulset-qfjk9/ss2 to complete update
Dec  8 05:06:47.350: INFO: Waiting for Pod e2e-tests-statefulset-qfjk9/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Dec  8 05:06:57.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-qfjk9 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:06:57.542: INFO: stderr: ""
Dec  8 05:06:57.542: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:06:57.542: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  8 05:07:07.586: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  8 05:07:17.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-qfjk9 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:07:17.827: INFO: stderr: ""
Dec  8 05:07:17.827: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  8 05:07:17.827: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  8 05:07:27.853: INFO: Waiting for StatefulSet e2e-tests-statefulset-qfjk9/ss2 to complete update
Dec  8 05:07:27.853: INFO: Waiting for Pod e2e-tests-statefulset-qfjk9/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Dec  8 05:07:27.853: INFO: Waiting for Pod e2e-tests-statefulset-qfjk9/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Dec  8 05:07:27.853: INFO: Waiting for Pod e2e-tests-statefulset-qfjk9/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  8 05:07:37.864: INFO: Deleting all statefulset in ns e2e-tests-statefulset-qfjk9
Dec  8 05:07:37.868: INFO: Scaling statefulset ss2 to 0
Dec  8 05:08:17.921: INFO: Waiting for statefulset status.replicas updated to 0
Dec  8 05:08:17.936: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:08:17.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-qfjk9" for this suite.
Dec  8 05:08:24.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:08:24.204: INFO: namespace: e2e-tests-statefulset-qfjk9, resource: bindings, ignored listing per whitelist
Dec  8 05:08:24.212: INFO: namespace e2e-tests-statefulset-qfjk9 deletion completed in 6.235339912s

• [SLOW TEST:157.600 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:08:24.212: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-vds2s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-4a193369-faa7-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:08:24.468: INFO: Waiting up to 5m0s for pod "pod-secrets-4a1a03a7-faa7-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-secrets-vds2s" to be "success or failure"
Dec  8 05:08:24.477: INFO: Pod "pod-secrets-4a1a03a7-faa7-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 9.84343ms
Dec  8 05:08:26.482: INFO: Pod "pod-secrets-4a1a03a7-faa7-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014748852s
STEP: Saw pod success
Dec  8 05:08:26.482: INFO: Pod "pod-secrets-4a1a03a7-faa7-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:08:26.487: INFO: Trying to get logs from node phrs-liberal-perch pod pod-secrets-4a1a03a7-faa7-11e8-b680-ca3d6f40e675 container secret-volume-test: <nil>
STEP: delete the pod
Dec  8 05:08:26.511: INFO: Waiting for pod pod-secrets-4a1a03a7-faa7-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:08:26.519: INFO: Pod pod-secrets-4a1a03a7-faa7-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:08:26.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-vds2s" for this suite.
Dec  8 05:08:32.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:08:32.588: INFO: namespace: e2e-tests-secrets-vds2s, resource: bindings, ignored listing per whitelist
Dec  8 05:08:32.666: INFO: namespace e2e-tests-secrets-vds2s deletion completed in 6.141503559s

• [SLOW TEST:8.454 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:08:32.666: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-fqn5w
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-fqn5w
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-fqn5w
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-fqn5w
Dec  8 05:08:32.866: INFO: Found 0 stateful pods, waiting for 1
Dec  8 05:08:42.870: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  8 05:08:42.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-fqn5w ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:08:43.068: INFO: stderr: ""
Dec  8 05:08:43.068: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:08:43.068: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  8 05:08:43.072: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  8 05:08:53.076: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  8 05:08:53.076: INFO: Waiting for statefulset status.replicas updated to 0
Dec  8 05:08:53.093: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999446s
Dec  8 05:08:54.096: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995667798s
Dec  8 05:08:55.101: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991955481s
Dec  8 05:08:56.106: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987526524s
Dec  8 05:08:57.110: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982833701s
Dec  8 05:08:58.115: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97869519s
Dec  8 05:08:59.119: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973673547s
Dec  8 05:09:00.124: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.969616798s
Dec  8 05:09:01.129: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963845835s
Dec  8 05:09:02.138: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.068212ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-fqn5w
Dec  8 05:09:03.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-fqn5w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:09:03.351: INFO: stderr: ""
Dec  8 05:09:03.351: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  8 05:09:03.351: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  8 05:09:03.355: INFO: Found 1 stateful pods, waiting for 3
Dec  8 05:09:13.371: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 05:09:13.371: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 05:09:13.371: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  8 05:09:13.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-fqn5w ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:09:13.609: INFO: stderr: ""
Dec  8 05:09:13.609: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:09:13.609: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  8 05:09:13.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-fqn5w ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:09:13.832: INFO: stderr: ""
Dec  8 05:09:13.832: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:09:13.832: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  8 05:09:13.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-fqn5w ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:09:14.046: INFO: stderr: ""
Dec  8 05:09:14.046: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:09:14.046: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  8 05:09:14.046: INFO: Waiting for statefulset status.replicas updated to 0
Dec  8 05:09:14.051: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec  8 05:09:24.060: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  8 05:09:24.060: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  8 05:09:24.060: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  8 05:09:24.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999942s
Dec  8 05:09:25.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994920414s
Dec  8 05:09:26.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990346874s
Dec  8 05:09:27.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984763008s
Dec  8 05:09:28.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979528824s
Dec  8 05:09:29.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973682083s
Dec  8 05:09:30.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969557847s
Dec  8 05:09:31.119: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965833731s
Dec  8 05:09:32.124: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960950525s
Dec  8 05:09:33.128: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.632286ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-fqn5w
Dec  8 05:09:34.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-fqn5w ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:09:34.345: INFO: stderr: ""
Dec  8 05:09:34.345: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  8 05:09:34.345: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  8 05:09:34.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-fqn5w ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:09:34.547: INFO: stderr: ""
Dec  8 05:09:34.547: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  8 05:09:34.547: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  8 05:09:34.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-fqn5w ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:09:34.754: INFO: stderr: ""
Dec  8 05:09:34.754: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  8 05:09:34.754: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  8 05:09:34.754: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  8 05:09:44.779: INFO: Deleting all statefulset in ns e2e-tests-statefulset-fqn5w
Dec  8 05:09:44.782: INFO: Scaling statefulset ss to 0
Dec  8 05:09:44.793: INFO: Waiting for statefulset status.replicas updated to 0
Dec  8 05:09:44.795: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:09:44.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-fqn5w" for this suite.
Dec  8 05:09:50.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:09:50.932: INFO: namespace: e2e-tests-statefulset-fqn5w, resource: bindings, ignored listing per whitelist
Dec  8 05:09:50.991: INFO: namespace e2e-tests-statefulset-fqn5w deletion completed in 6.173160265s

• [SLOW TEST:78.325 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:09:50.992: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-wnqjv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  8 05:09:51.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-wnqjv'
Dec  8 05:09:51.582: INFO: stderr: ""
Dec  8 05:09:51.583: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
Dec  8 05:09:51.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-wnqjv'
Dec  8 05:09:58.284: INFO: stderr: ""
Dec  8 05:09:58.284: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:09:58.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wnqjv" for this suite.
Dec  8 05:10:04.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:10:04.436: INFO: namespace: e2e-tests-kubectl-wnqjv, resource: bindings, ignored listing per whitelist
Dec  8 05:10:04.489: INFO: namespace e2e-tests-kubectl-wnqjv deletion completed in 6.200773534s

• [SLOW TEST:13.497 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:10:04.489: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-v5tbc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  8 05:10:04.737: INFO: Waiting up to 5m0s for pod "pod-85de0f08-faa7-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-v5tbc" to be "success or failure"
Dec  8 05:10:04.749: INFO: Pod "pod-85de0f08-faa7-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 12.646828ms
Dec  8 05:10:06.754: INFO: Pod "pod-85de0f08-faa7-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017013397s
STEP: Saw pod success
Dec  8 05:10:06.754: INFO: Pod "pod-85de0f08-faa7-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:10:06.759: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-85de0f08-faa7-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:10:06.791: INFO: Waiting for pod pod-85de0f08-faa7-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:10:06.797: INFO: Pod pod-85de0f08-faa7-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:10:06.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-v5tbc" for this suite.
Dec  8 05:10:12.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:10:12.866: INFO: namespace: e2e-tests-emptydir-v5tbc, resource: bindings, ignored listing per whitelist
Dec  8 05:10:13.005: INFO: namespace e2e-tests-emptydir-v5tbc deletion completed in 6.201698615s

• [SLOW TEST:8.516 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:10:13.006: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-nf7xt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  8 05:10:13.217: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:10:19.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-nf7xt" for this suite.
Dec  8 05:10:33.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:10:33.761: INFO: namespace: e2e-tests-init-container-nf7xt, resource: bindings, ignored listing per whitelist
Dec  8 05:10:33.807: INFO: namespace e2e-tests-init-container-nf7xt deletion completed in 14.172046127s

• [SLOW TEST:20.801 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:10:33.808: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-s8ktr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Dec  8 05:10:34.529: INFO: created pod pod-service-account-defaultsa
Dec  8 05:10:34.529: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  8 05:10:34.548: INFO: created pod pod-service-account-mountsa
Dec  8 05:10:34.548: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  8 05:10:34.555: INFO: created pod pod-service-account-nomountsa
Dec  8 05:10:34.555: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  8 05:10:34.568: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  8 05:10:34.568: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  8 05:10:34.588: INFO: created pod pod-service-account-mountsa-mountspec
Dec  8 05:10:34.588: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  8 05:10:34.598: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  8 05:10:34.599: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  8 05:10:34.618: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  8 05:10:34.618: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  8 05:10:34.635: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  8 05:10:34.635: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  8 05:10:34.651: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  8 05:10:34.651: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:10:34.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-s8ktr" for this suite.
Dec  8 05:10:40.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:10:40.803: INFO: namespace: e2e-tests-svcaccounts-s8ktr, resource: bindings, ignored listing per whitelist
Dec  8 05:10:40.865: INFO: namespace e2e-tests-svcaccounts-s8ktr deletion completed in 6.199019982s

• [SLOW TEST:7.058 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:10:40.866: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-qpkq5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-qpkq5
Dec  8 05:10:45.097: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-qpkq5
STEP: checking the pod's current state and verifying that restartCount is present
Dec  8 05:10:45.099: INFO: Initial restart count of pod liveness-exec is 0
Dec  8 05:11:35.241: INFO: Restart count of pod e2e-tests-container-probe-qpkq5/liveness-exec is now 1 (50.141377699s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:11:35.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qpkq5" for this suite.
Dec  8 05:11:41.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:11:41.297: INFO: namespace: e2e-tests-container-probe-qpkq5, resource: bindings, ignored listing per whitelist
Dec  8 05:11:41.396: INFO: namespace e2e-tests-container-probe-qpkq5 deletion completed in 6.130803264s

• [SLOW TEST:60.531 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:11:41.397: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-tdkpg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Dec  8 05:11:43.669: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-bf9bcd9c-faa7-11e8-b680-ca3d6f40e675", GenerateName:"", Namespace:"e2e-tests-pods-tdkpg", SelfLink:"/api/v1/namespaces/e2e-tests-pods-tdkpg/pods/pod-submit-remove-bf9bcd9c-faa7-11e8-b680-ca3d6f40e675", UID:"bfa14117-faa7-11e8-a103-96cd63cee3b8", ResourceVersion:"4524", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63679842701, loc:(*time.Location)(0x6c43b60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"time":"594161916", "name":"foo"}, Annotations:map[string]string{"kubernetes.io/psp":"00-pharos-privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-lmqd7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc42129fe80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lmqd7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc4215b3958), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"phrs-glorious-mastodon", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc421184840), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc4215b39f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc4215b3a90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc4215b3a98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679842701, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679842702, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679842702, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679842701, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.135.14.74", PodIP:"172.31.0.8", StartTime:(*v1.Time)(0xc421652f60), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc421652f80), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6", ContainerID:"docker://938c431d77400670e4a41b6d901c55674093a4c06f7bd365e03487e2778ef90f"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec  8 05:11:48.702: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:11:48.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-tdkpg" for this suite.
Dec  8 05:11:54.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:11:54.756: INFO: namespace: e2e-tests-pods-tdkpg, resource: bindings, ignored listing per whitelist
Dec  8 05:11:54.870: INFO: namespace e2e-tests-pods-tdkpg deletion completed in 6.155027792s

• [SLOW TEST:13.473 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:11:54.870: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-p27kz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  8 05:11:57.622: INFO: Successfully updated pod "annotationupdatec7a3a4ea-faa7-11e8-b680-ca3d6f40e675"
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:12:01.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p27kz" for this suite.
Dec  8 05:12:23.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:12:23.697: INFO: namespace: e2e-tests-projected-p27kz, resource: bindings, ignored listing per whitelist
Dec  8 05:12:23.881: INFO: namespace e2e-tests-projected-p27kz deletion completed in 22.220748752s

• [SLOW TEST:29.011 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:12:23.882: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-975p9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-975p9
Dec  8 05:12:28.181: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-975p9
STEP: checking the pod's current state and verifying that restartCount is present
Dec  8 05:12:28.185: INFO: Initial restart count of pod liveness-http is 0
Dec  8 05:12:46.229: INFO: Restart count of pod e2e-tests-container-probe-975p9/liveness-http is now 1 (18.044100848s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:12:46.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-975p9" for this suite.
Dec  8 05:12:52.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:12:52.441: INFO: namespace: e2e-tests-container-probe-975p9, resource: bindings, ignored listing per whitelist
Dec  8 05:12:52.447: INFO: namespace e2e-tests-container-probe-975p9 deletion completed in 6.194919043s

• [SLOW TEST:28.564 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:12:52.447: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-csxsf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-csxsf
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  8 05:12:52.669: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  8 05:13:18.899: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.31.192.8 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-csxsf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:13:18.899: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:13:20.005: INFO: Found all expected endpoints: [netserver-0]
Dec  8 05:13:20.015: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.31.0.8 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-csxsf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:13:20.016: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:13:21.115: INFO: Found all expected endpoints: [netserver-1]
Dec  8 05:13:21.120: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 172.31.240.7 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-csxsf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:13:21.120: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:13:22.215: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:13:22.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-csxsf" for this suite.
Dec  8 05:13:46.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:13:46.510: INFO: namespace: e2e-tests-pod-network-test-csxsf, resource: bindings, ignored listing per whitelist
Dec  8 05:13:46.544: INFO: namespace e2e-tests-pod-network-test-csxsf deletion completed in 24.32203018s

• [SLOW TEST:54.097 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:13:46.544: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-ncc8l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Dec  8 05:13:46.778: INFO: Waiting up to 5m0s for pod "client-containers-0a37ba62-faa8-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-containers-ncc8l" to be "success or failure"
Dec  8 05:13:46.792: INFO: Pod "client-containers-0a37ba62-faa8-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 14.150183ms
Dec  8 05:13:48.797: INFO: Pod "client-containers-0a37ba62-faa8-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018969733s
Dec  8 05:13:50.810: INFO: Pod "client-containers-0a37ba62-faa8-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031654016s
Dec  8 05:13:52.813: INFO: Pod "client-containers-0a37ba62-faa8-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035096835s
STEP: Saw pod success
Dec  8 05:13:52.813: INFO: Pod "client-containers-0a37ba62-faa8-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:13:52.816: INFO: Trying to get logs from node phrs-one-falcon pod client-containers-0a37ba62-faa8-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:13:52.858: INFO: Waiting for pod client-containers-0a37ba62-faa8-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:13:52.861: INFO: Pod client-containers-0a37ba62-faa8-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:13:52.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-ncc8l" for this suite.
Dec  8 05:13:58.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:13:58.995: INFO: namespace: e2e-tests-containers-ncc8l, resource: bindings, ignored listing per whitelist
Dec  8 05:13:59.040: INFO: namespace e2e-tests-containers-ncc8l deletion completed in 6.174980505s

• [SLOW TEST:12.496 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:13:59.040: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-wjwwt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  8 05:13:59.259: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:14:02.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-wjwwt" for this suite.
Dec  8 05:14:08.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:14:08.388: INFO: namespace: e2e-tests-init-container-wjwwt, resource: bindings, ignored listing per whitelist
Dec  8 05:14:08.420: INFO: namespace e2e-tests-init-container-wjwwt deletion completed in 6.260650659s

• [SLOW TEST:9.379 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:14:08.420: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7d48l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Dec  8 05:14:08.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:08.904: INFO: stderr: ""
Dec  8 05:14:08.904: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  8 05:14:08.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:09.052: INFO: stderr: ""
Dec  8 05:14:09.052: INFO: stdout: "update-demo-nautilus-5k9z5 update-demo-nautilus-w8q7k "
Dec  8 05:14:09.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-5k9z5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:09.171: INFO: stderr: ""
Dec  8 05:14:09.171: INFO: stdout: ""
Dec  8 05:14:09.171: INFO: update-demo-nautilus-5k9z5 is created but not running
Dec  8 05:14:14.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:14.277: INFO: stderr: ""
Dec  8 05:14:14.277: INFO: stdout: "update-demo-nautilus-5k9z5 update-demo-nautilus-w8q7k "
Dec  8 05:14:14.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-5k9z5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:14.399: INFO: stderr: ""
Dec  8 05:14:14.399: INFO: stdout: "true"
Dec  8 05:14:14.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-5k9z5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:14.502: INFO: stderr: ""
Dec  8 05:14:14.502: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  8 05:14:14.502: INFO: validating pod update-demo-nautilus-5k9z5
Dec  8 05:14:14.512: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  8 05:14:14.512: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  8 05:14:14.512: INFO: update-demo-nautilus-5k9z5 is verified up and running
Dec  8 05:14:14.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-w8q7k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:14.614: INFO: stderr: ""
Dec  8 05:14:14.614: INFO: stdout: "true"
Dec  8 05:14:14.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-w8q7k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:14.727: INFO: stderr: ""
Dec  8 05:14:14.727: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  8 05:14:14.727: INFO: validating pod update-demo-nautilus-w8q7k
Dec  8 05:14:14.735: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  8 05:14:14.735: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  8 05:14:14.735: INFO: update-demo-nautilus-w8q7k is verified up and running
STEP: using delete to clean up resources
Dec  8 05:14:14.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:14.864: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 05:14:14.864: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  8 05:14:14.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-7d48l'
Dec  8 05:14:15.018: INFO: stderr: "No resources found.\n"
Dec  8 05:14:15.018: INFO: stdout: ""
Dec  8 05:14:15.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -l name=update-demo --namespace=e2e-tests-kubectl-7d48l -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  8 05:14:15.156: INFO: stderr: ""
Dec  8 05:14:15.156: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:14:15.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7d48l" for this suite.
Dec  8 05:14:21.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:14:21.336: INFO: namespace: e2e-tests-kubectl-7d48l, resource: bindings, ignored listing per whitelist
Dec  8 05:14:21.369: INFO: namespace e2e-tests-kubectl-7d48l deletion completed in 6.206169937s

• [SLOW TEST:12.950 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:14:21.370: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lcd9t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  8 05:14:21.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-lcd9t'
Dec  8 05:14:21.726: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Dec  8 05:14:21.726: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
Dec  8 05:14:21.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-lcd9t'
Dec  8 05:14:21.904: INFO: stderr: ""
Dec  8 05:14:21.904: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:14:21.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lcd9t" for this suite.
Dec  8 05:14:27.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:14:27.957: INFO: namespace: e2e-tests-kubectl-lcd9t, resource: bindings, ignored listing per whitelist
Dec  8 05:14:28.083: INFO: namespace e2e-tests-kubectl-lcd9t deletion completed in 6.173071858s

• [SLOW TEST:6.713 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:14:28.084: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-75bb8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-22f677a4-faa8-11e8-b680-ca3d6f40e675
Dec  8 05:14:28.292: INFO: Pod name my-hostname-basic-22f677a4-faa8-11e8-b680-ca3d6f40e675: Found 0 pods out of 1
Dec  8 05:14:33.297: INFO: Pod name my-hostname-basic-22f677a4-faa8-11e8-b680-ca3d6f40e675: Found 1 pods out of 1
Dec  8 05:14:33.297: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-22f677a4-faa8-11e8-b680-ca3d6f40e675" are running
Dec  8 05:14:33.299: INFO: Pod "my-hostname-basic-22f677a4-faa8-11e8-b680-ca3d6f40e675-mrjf8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-08 05:14:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-08 05:14:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-08 05:14:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-08 05:14:28 +0000 UTC Reason: Message:}])
Dec  8 05:14:33.299: INFO: Trying to dial the pod
Dec  8 05:14:38.314: INFO: Controller my-hostname-basic-22f677a4-faa8-11e8-b680-ca3d6f40e675: Got expected result from replica 1 [my-hostname-basic-22f677a4-faa8-11e8-b680-ca3d6f40e675-mrjf8]: "my-hostname-basic-22f677a4-faa8-11e8-b680-ca3d6f40e675-mrjf8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:14:38.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-75bb8" for this suite.
Dec  8 05:14:44.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:14:44.487: INFO: namespace: e2e-tests-replication-controller-75bb8, resource: bindings, ignored listing per whitelist
Dec  8 05:14:44.487: INFO: namespace e2e-tests-replication-controller-75bb8 deletion completed in 6.16865216s

• [SLOW TEST:16.403 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:14:44.487: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-xxrt7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1208 05:14:45.760055      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  8 05:14:45.760: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:14:45.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-xxrt7" for this suite.
Dec  8 05:14:51.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:14:51.838: INFO: namespace: e2e-tests-gc-xxrt7, resource: bindings, ignored listing per whitelist
Dec  8 05:14:51.915: INFO: namespace e2e-tests-gc-xxrt7 deletion completed in 6.150623361s

• [SLOW TEST:7.428 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:14:51.915: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-kkh7g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-ksnlb
STEP: Creating secret with name secret-test-312df32b-faa8-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:14:52.334: INFO: Waiting up to 5m0s for pod "pod-secrets-314a50ea-faa8-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-secrets-kkh7g" to be "success or failure"
Dec  8 05:14:52.354: INFO: Pod "pod-secrets-314a50ea-faa8-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 19.91163ms
Dec  8 05:14:54.358: INFO: Pod "pod-secrets-314a50ea-faa8-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023771429s
STEP: Saw pod success
Dec  8 05:14:54.358: INFO: Pod "pod-secrets-314a50ea-faa8-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:14:54.361: INFO: Trying to get logs from node phrs-liberal-perch pod pod-secrets-314a50ea-faa8-11e8-b680-ca3d6f40e675 container secret-volume-test: <nil>
STEP: delete the pod
Dec  8 05:14:54.389: INFO: Waiting for pod pod-secrets-314a50ea-faa8-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:14:54.391: INFO: Pod pod-secrets-314a50ea-faa8-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:14:54.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kkh7g" for this suite.
Dec  8 05:15:00.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:15:00.450: INFO: namespace: e2e-tests-secrets-kkh7g, resource: bindings, ignored listing per whitelist
Dec  8 05:15:00.562: INFO: namespace e2e-tests-secrets-kkh7g deletion completed in 6.165800498s
STEP: Destroying namespace "e2e-tests-secret-namespace-ksnlb" for this suite.
Dec  8 05:15:06.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:15:06.624: INFO: namespace: e2e-tests-secret-namespace-ksnlb, resource: bindings, ignored listing per whitelist
Dec  8 05:15:06.795: INFO: namespace e2e-tests-secret-namespace-ksnlb deletion completed in 6.23227813s

• [SLOW TEST:14.879 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:15:06.795: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-n58mt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-3a0e6745-faa8-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 05:15:07.045: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a0f3d06-faa8-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-n58mt" to be "success or failure"
Dec  8 05:15:07.053: INFO: Pod "pod-projected-configmaps-3a0f3d06-faa8-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.809409ms
Dec  8 05:15:09.058: INFO: Pod "pod-projected-configmaps-3a0f3d06-faa8-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0130263s
STEP: Saw pod success
Dec  8 05:15:09.058: INFO: Pod "pod-projected-configmaps-3a0f3d06-faa8-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:15:09.062: INFO: Trying to get logs from node phrs-one-falcon pod pod-projected-configmaps-3a0f3d06-faa8-11e8-b680-ca3d6f40e675 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 05:15:09.109: INFO: Waiting for pod pod-projected-configmaps-3a0f3d06-faa8-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:15:09.114: INFO: Pod pod-projected-configmaps-3a0f3d06-faa8-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:15:09.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-n58mt" for this suite.
Dec  8 05:15:15.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:15:15.295: INFO: namespace: e2e-tests-projected-n58mt, resource: bindings, ignored listing per whitelist
Dec  8 05:15:15.321: INFO: namespace e2e-tests-projected-n58mt deletion completed in 6.200842939s

• [SLOW TEST:8.526 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:15:15.322: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-h879r
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-3f2586fe-faa8-11e8-b680-ca3d6f40e675
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-3f2586fe-faa8-11e8-b680-ca3d6f40e675
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:15:19.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-h879r" for this suite.
Dec  8 05:15:33.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:15:33.711: INFO: namespace: e2e-tests-configmap-h879r, resource: bindings, ignored listing per whitelist
Dec  8 05:15:33.821: INFO: namespace e2e-tests-configmap-h879r deletion completed in 14.169226976s

• [SLOW TEST:18.498 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:15:33.821: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-7zpk4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:15:34.046: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a27e770-faa8-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-7zpk4" to be "success or failure"
Dec  8 05:15:34.051: INFO: Pod "downwardapi-volume-4a27e770-faa8-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 4.536852ms
Dec  8 05:15:36.054: INFO: Pod "downwardapi-volume-4a27e770-faa8-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00824432s
STEP: Saw pod success
Dec  8 05:15:36.055: INFO: Pod "downwardapi-volume-4a27e770-faa8-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:15:36.058: INFO: Trying to get logs from node phrs-liberal-perch pod downwardapi-volume-4a27e770-faa8-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:15:36.090: INFO: Waiting for pod downwardapi-volume-4a27e770-faa8-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:15:36.093: INFO: Pod downwardapi-volume-4a27e770-faa8-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:15:36.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7zpk4" for this suite.
Dec  8 05:15:42.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:15:42.156: INFO: namespace: e2e-tests-downward-api-7zpk4, resource: bindings, ignored listing per whitelist
Dec  8 05:15:42.260: INFO: namespace e2e-tests-downward-api-7zpk4 deletion completed in 6.162337272s

• [SLOW TEST:8.439 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:15:42.260: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6qk5r
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Dec  8 05:15:42.475: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-036194578 proxy --unix-socket=/tmp/kubectl-proxy-unix122089289/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:15:42.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6qk5r" for this suite.
Dec  8 05:15:48.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:15:48.666: INFO: namespace: e2e-tests-kubectl-6qk5r, resource: bindings, ignored listing per whitelist
Dec  8 05:15:48.742: INFO: namespace e2e-tests-kubectl-6qk5r deletion completed in 6.176925228s

• [SLOW TEST:6.481 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:15:48.743: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-p9ckd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
Dec  8 05:15:48.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-p9ckd'
Dec  8 05:15:49.144: INFO: stderr: ""
Dec  8 05:15:49.144: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Dec  8 05:15:50.148: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 05:15:50.148: INFO: Found 0 / 1
Dec  8 05:15:51.148: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 05:15:51.148: INFO: Found 0 / 1
Dec  8 05:15:52.148: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 05:15:52.148: INFO: Found 1 / 1
Dec  8 05:15:52.148: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  8 05:15:52.152: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 05:15:52.152: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec  8 05:15:52.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 logs redis-master-c49vv redis-master --namespace=e2e-tests-kubectl-p9ckd'
Dec  8 05:15:52.280: INFO: stderr: ""
Dec  8 05:15:52.280: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Dec 05:15:51.365 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Dec 05:15:51.365 # Server started, Redis version 3.2.12\n1:M 08 Dec 05:15:51.365 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Dec 05:15:51.365 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec  8 05:15:52.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 log redis-master-c49vv redis-master --namespace=e2e-tests-kubectl-p9ckd --tail=1'
Dec  8 05:15:52.418: INFO: stderr: ""
Dec  8 05:15:52.418: INFO: stdout: "1:M 08 Dec 05:15:51.365 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec  8 05:15:52.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 log redis-master-c49vv redis-master --namespace=e2e-tests-kubectl-p9ckd --limit-bytes=1'
Dec  8 05:15:52.579: INFO: stderr: ""
Dec  8 05:15:52.579: INFO: stdout: " "
STEP: exposing timestamps
Dec  8 05:15:52.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 log redis-master-c49vv redis-master --namespace=e2e-tests-kubectl-p9ckd --tail=1 --timestamps'
Dec  8 05:15:52.713: INFO: stderr: ""
Dec  8 05:15:52.713: INFO: stdout: "2018-12-08T05:15:51.369584052Z 1:M 08 Dec 05:15:51.365 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec  8 05:15:55.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 log redis-master-c49vv redis-master --namespace=e2e-tests-kubectl-p9ckd --since=1s'
Dec  8 05:15:55.336: INFO: stderr: ""
Dec  8 05:15:55.336: INFO: stdout: ""
Dec  8 05:15:55.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 log redis-master-c49vv redis-master --namespace=e2e-tests-kubectl-p9ckd --since=24h'
Dec  8 05:15:55.465: INFO: stderr: ""
Dec  8 05:15:55.465: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Dec 05:15:51.365 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Dec 05:15:51.365 # Server started, Redis version 3.2.12\n1:M 08 Dec 05:15:51.365 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Dec 05:15:51.365 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
Dec  8 05:15:55.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-p9ckd'
Dec  8 05:15:55.600: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 05:15:55.600: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec  8 05:15:55.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-p9ckd'
Dec  8 05:15:55.737: INFO: stderr: "No resources found.\n"
Dec  8 05:15:55.737: INFO: stdout: ""
Dec  8 05:15:55.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -l name=nginx --namespace=e2e-tests-kubectl-p9ckd -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  8 05:15:55.844: INFO: stderr: ""
Dec  8 05:15:55.845: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:15:55.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-p9ckd" for this suite.
Dec  8 05:16:17.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:16:18.063: INFO: namespace: e2e-tests-kubectl-p9ckd, resource: bindings, ignored listing per whitelist
Dec  8 05:16:18.067: INFO: namespace e2e-tests-kubectl-p9ckd deletion completed in 22.218241565s

• [SLOW TEST:29.325 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:16:18.068: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-skwnl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  8 05:16:22.374: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:22.374: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:22.471: INFO: Exec stderr: ""
Dec  8 05:16:22.471: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:22.471: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:22.559: INFO: Exec stderr: ""
Dec  8 05:16:22.559: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:22.662: INFO: Exec stderr: ""
Dec  8 05:16:22.662: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:22.662: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:22.746: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  8 05:16:22.746: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:22.746: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:22.843: INFO: Exec stderr: ""
Dec  8 05:16:22.843: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:22.843: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:22.925: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  8 05:16:22.925: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:22.925: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:23.036: INFO: Exec stderr: ""
Dec  8 05:16:23.036: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:23.036: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:23.136: INFO: Exec stderr: ""
Dec  8 05:16:23.136: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:23.136: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:23.238: INFO: Exec stderr: ""
Dec  8 05:16:23.238: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-skwnl PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:16:23.238: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:16:23.348: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:16:23.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-skwnl" for this suite.
Dec  8 05:17:09.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:17:09.445: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-skwnl, resource: bindings, ignored listing per whitelist
Dec  8 05:17:09.550: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-skwnl deletion completed in 46.195880785s

• [SLOW TEST:51.483 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:17:09.551: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-mj67c
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-mj67c
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  8 05:17:09.794: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  8 05:17:30.002: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.192.10:8080/dial?request=hostName&protocol=udp&host=172.31.240.7&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-mj67c PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:17:30.002: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:17:30.099: INFO: Waiting for endpoints: map[]
Dec  8 05:17:30.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.192.10:8080/dial?request=hostName&protocol=udp&host=172.31.0.8&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-mj67c PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:17:30.103: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:17:30.219: INFO: Waiting for endpoints: map[]
Dec  8 05:17:30.221: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.192.10:8080/dial?request=hostName&protocol=udp&host=172.31.192.8&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-mj67c PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:17:30.221: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:17:30.306: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:17:30.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-mj67c" for this suite.
Dec  8 05:17:52.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:17:52.473: INFO: namespace: e2e-tests-pod-network-test-mj67c, resource: bindings, ignored listing per whitelist
Dec  8 05:17:52.491: INFO: namespace e2e-tests-pod-network-test-mj67c deletion completed in 22.181444766s

• [SLOW TEST:42.941 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:17:52.492: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-mzbzr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-mzbzr
Dec  8 05:17:56.711: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-mzbzr
STEP: checking the pod's current state and verifying that restartCount is present
Dec  8 05:17:56.714: INFO: Initial restart count of pod liveness-http is 0
Dec  8 05:18:08.743: INFO: Restart count of pod e2e-tests-container-probe-mzbzr/liveness-http is now 1 (12.028887664s elapsed)
Dec  8 05:18:28.793: INFO: Restart count of pod e2e-tests-container-probe-mzbzr/liveness-http is now 2 (32.07889356s elapsed)
Dec  8 05:18:48.836: INFO: Restart count of pod e2e-tests-container-probe-mzbzr/liveness-http is now 3 (52.121922978s elapsed)
Dec  8 05:19:08.890: INFO: Restart count of pod e2e-tests-container-probe-mzbzr/liveness-http is now 4 (1m12.176513143s elapsed)
Dec  8 05:20:09.046: INFO: Restart count of pod e2e-tests-container-probe-mzbzr/liveness-http is now 5 (2m12.331804084s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:20:09.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-mzbzr" for this suite.
Dec  8 05:20:15.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:20:15.267: INFO: namespace: e2e-tests-container-probe-mzbzr, resource: bindings, ignored listing per whitelist
Dec  8 05:20:15.303: INFO: namespace e2e-tests-container-probe-mzbzr deletion completed in 6.213998211s

• [SLOW TEST:142.811 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:20:15.304: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gp4w7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:20:15.579: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1f58c8c-faa8-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-gp4w7" to be "success or failure"
Dec  8 05:20:15.590: INFO: Pod "downwardapi-volume-f1f58c8c-faa8-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 11.221726ms
Dec  8 05:20:17.596: INFO: Pod "downwardapi-volume-f1f58c8c-faa8-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016576265s
STEP: Saw pod success
Dec  8 05:20:17.596: INFO: Pod "downwardapi-volume-f1f58c8c-faa8-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:20:17.601: INFO: Trying to get logs from node phrs-one-falcon pod downwardapi-volume-f1f58c8c-faa8-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:20:17.637: INFO: Waiting for pod downwardapi-volume-f1f58c8c-faa8-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:20:17.643: INFO: Pod downwardapi-volume-f1f58c8c-faa8-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:20:17.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gp4w7" for this suite.
Dec  8 05:20:23.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:20:23.788: INFO: namespace: e2e-tests-projected-gp4w7, resource: bindings, ignored listing per whitelist
Dec  8 05:20:23.898: INFO: namespace e2e-tests-projected-gp4w7 deletion completed in 6.247196037s

• [SLOW TEST:8.594 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:20:23.898: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-9wtw9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:20:24.168: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
Dec  8 05:20:24.177: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-9wtw9/daemonsets","resourceVersion":"6192"},"items":null}

Dec  8 05:20:24.182: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-9wtw9/pods","resourceVersion":"6192"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:20:24.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-9wtw9" for this suite.
Dec  8 05:20:30.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:20:30.277: INFO: namespace: e2e-tests-daemonsets-9wtw9, resource: bindings, ignored listing per whitelist
Dec  8 05:20:30.405: INFO: namespace e2e-tests-daemonsets-9wtw9 deletion completed in 6.198645389s

S [SKIPPING] [6.507 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Dec  8 05:20:24.168: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:20:30.405: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-sctfv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-faef5b89-faa8-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:20:30.645: INFO: Waiting up to 5m0s for pod "pod-secrets-faf02967-faa8-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-secrets-sctfv" to be "success or failure"
Dec  8 05:20:30.655: INFO: Pod "pod-secrets-faf02967-faa8-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 9.61463ms
Dec  8 05:20:32.660: INFO: Pod "pod-secrets-faf02967-faa8-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014540737s
STEP: Saw pod success
Dec  8 05:20:32.660: INFO: Pod "pod-secrets-faf02967-faa8-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:20:32.663: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-secrets-faf02967-faa8-11e8-b680-ca3d6f40e675 container secret-volume-test: <nil>
STEP: delete the pod
Dec  8 05:20:32.698: INFO: Waiting for pod pod-secrets-faf02967-faa8-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:20:32.702: INFO: Pod pod-secrets-faf02967-faa8-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:20:32.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-sctfv" for this suite.
Dec  8 05:20:38.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:20:38.817: INFO: namespace: e2e-tests-secrets-sctfv, resource: bindings, ignored listing per whitelist
Dec  8 05:20:38.869: INFO: namespace e2e-tests-secrets-sctfv deletion completed in 6.162152872s

• [SLOW TEST:8.464 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:20:38.870: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-l56vx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  8 05:20:41.100: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-fff67f02-faa8-11e8-b680-ca3d6f40e675,GenerateName:,Namespace:e2e-tests-events-l56vx,SelfLink:/api/v1/namespaces/e2e-tests-events-l56vx/pods/send-events-fff67f02-faa8-11e8-b680-ca3d6f40e675,UID:fff71de2-faa8-11e8-a103-96cd63cee3b8,ResourceVersion:6262,Generation:0,CreationTimestamp:2018-12-08 05:20:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 59436839,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-wft6c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wft6c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-wft6c true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-liberal-perch,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422862a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422862a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:20:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:20:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:20:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:20:39 +0000 UTC  }],Message:,Reason:,HostIP:10.135.11.179,PodIP:172.31.240.7,StartTime:2018-12-08 05:20:39 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2018-12-08 05:20:39 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://5d7732241f475e08db60387726ab456914bfab3b9ac9844ef43a014ec2907716}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec  8 05:20:43.105: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  8 05:20:45.109: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:20:45.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-l56vx" for this suite.
Dec  8 05:21:25.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:21:25.181: INFO: namespace: e2e-tests-events-l56vx, resource: bindings, ignored listing per whitelist
Dec  8 05:21:25.315: INFO: namespace e2e-tests-events-l56vx deletion completed in 40.191552402s

• [SLOW TEST:46.446 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:21:25.316: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6ch22
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-1baccdcc-faa9-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:21:25.575: INFO: Waiting up to 5m0s for pod "pod-secrets-1baddbae-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-secrets-6ch22" to be "success or failure"
Dec  8 05:21:25.597: INFO: Pod "pod-secrets-1baddbae-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 20.926059ms
Dec  8 05:21:27.601: INFO: Pod "pod-secrets-1baddbae-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025733458s
STEP: Saw pod success
Dec  8 05:21:27.601: INFO: Pod "pod-secrets-1baddbae-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:21:27.606: INFO: Trying to get logs from node phrs-one-falcon pod pod-secrets-1baddbae-faa9-11e8-b680-ca3d6f40e675 container secret-volume-test: <nil>
STEP: delete the pod
Dec  8 05:21:27.645: INFO: Waiting for pod pod-secrets-1baddbae-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:21:27.649: INFO: Pod pod-secrets-1baddbae-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:21:27.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6ch22" for this suite.
Dec  8 05:21:33.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:21:33.692: INFO: namespace: e2e-tests-secrets-6ch22, resource: bindings, ignored listing per whitelist
Dec  8 05:21:33.817: INFO: namespace e2e-tests-secrets-6ch22 deletion completed in 6.160882512s

• [SLOW TEST:8.501 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:21:33.817: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-4xxsh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Dec  8 05:21:34.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 cluster-info'
Dec  8 05:21:34.303: INFO: stderr: ""
Dec  8 05:21:34.303: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.32.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.32.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.32.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:21:34.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4xxsh" for this suite.
Dec  8 05:21:40.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:21:40.426: INFO: namespace: e2e-tests-kubectl-4xxsh, resource: bindings, ignored listing per whitelist
Dec  8 05:21:40.481: INFO: namespace e2e-tests-kubectl-4xxsh deletion completed in 6.17277163s

• [SLOW TEST:6.663 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:21:40.481: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rbn5q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  8 05:21:40.677: INFO: Waiting up to 5m0s for pod "downward-api-24af7e67-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-rbn5q" to be "success or failure"
Dec  8 05:21:40.683: INFO: Pod "downward-api-24af7e67-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 6.133941ms
Dec  8 05:21:42.688: INFO: Pod "downward-api-24af7e67-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010784672s
STEP: Saw pod success
Dec  8 05:21:42.688: INFO: Pod "downward-api-24af7e67-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:21:42.692: INFO: Trying to get logs from node phrs-glorious-mastodon pod downward-api-24af7e67-faa9-11e8-b680-ca3d6f40e675 container dapi-container: <nil>
STEP: delete the pod
Dec  8 05:21:42.727: INFO: Waiting for pod downward-api-24af7e67-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:21:42.733: INFO: Pod downward-api-24af7e67-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:21:42.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rbn5q" for this suite.
Dec  8 05:21:48.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:21:48.961: INFO: namespace: e2e-tests-downward-api-rbn5q, resource: bindings, ignored listing per whitelist
Dec  8 05:21:49.041: INFO: namespace e2e-tests-downward-api-rbn5q deletion completed in 6.302985567s

• [SLOW TEST:8.560 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:21:49.041: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-lj8j6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-lj8j6/configmap-test-29d1d61e-faa9-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 05:21:49.303: INFO: Waiting up to 5m0s for pod "pod-configmaps-29d26e76-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-configmap-lj8j6" to be "success or failure"
Dec  8 05:21:49.310: INFO: Pod "pod-configmaps-29d26e76-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 6.745758ms
Dec  8 05:21:51.315: INFO: Pod "pod-configmaps-29d26e76-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012125519s
STEP: Saw pod success
Dec  8 05:21:51.315: INFO: Pod "pod-configmaps-29d26e76-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:21:51.321: INFO: Trying to get logs from node phrs-liberal-perch pod pod-configmaps-29d26e76-faa9-11e8-b680-ca3d6f40e675 container env-test: <nil>
STEP: delete the pod
Dec  8 05:21:51.361: INFO: Waiting for pod pod-configmaps-29d26e76-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:21:51.369: INFO: Pod pod-configmaps-29d26e76-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:21:51.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lj8j6" for this suite.
Dec  8 05:21:57.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:21:57.454: INFO: namespace: e2e-tests-configmap-lj8j6, resource: bindings, ignored listing per whitelist
Dec  8 05:21:57.524: INFO: namespace e2e-tests-configmap-lj8j6 deletion completed in 6.150125094s

• [SLOW TEST:8.483 seconds]
[sig-api-machinery] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:21:57.524: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-9wgv2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:21:57.773: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2edf0ed3-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-9wgv2" to be "success or failure"
Dec  8 05:21:57.778: INFO: Pod "downwardapi-volume-2edf0ed3-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.233977ms
Dec  8 05:21:59.783: INFO: Pod "downwardapi-volume-2edf0ed3-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009546024s
STEP: Saw pod success
Dec  8 05:21:59.783: INFO: Pod "downwardapi-volume-2edf0ed3-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:21:59.786: INFO: Trying to get logs from node phrs-one-falcon pod downwardapi-volume-2edf0ed3-faa9-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:21:59.812: INFO: Waiting for pod downwardapi-volume-2edf0ed3-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:21:59.820: INFO: Pod downwardapi-volume-2edf0ed3-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:21:59.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9wgv2" for this suite.
Dec  8 05:22:05.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:22:05.891: INFO: namespace: e2e-tests-projected-9wgv2, resource: bindings, ignored listing per whitelist
Dec  8 05:22:06.058: INFO: namespace e2e-tests-projected-9wgv2 deletion completed in 6.232467124s

• [SLOW TEST:8.534 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:22:06.058: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-kb9mm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Dec  8 05:22:06.293: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-036194578 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:22:06.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kb9mm" for this suite.
Dec  8 05:22:12.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:22:12.454: INFO: namespace: e2e-tests-kubectl-kb9mm, resource: bindings, ignored listing per whitelist
Dec  8 05:22:12.601: INFO: namespace e2e-tests-kubectl-kb9mm deletion completed in 6.194915929s

• [SLOW TEST:6.543 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:22:12.601: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-x7gfc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  8 05:22:12.859: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-x7gfc,SelfLink:/api/v1/namespaces/e2e-tests-watch-x7gfc/configmaps/e2e-watch-test-label-changed,UID:37db9195-faa9-11e8-a103-96cd63cee3b8,ResourceVersion:6565,Generation:0,CreationTimestamp:2018-12-08 05:22:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  8 05:22:12.859: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-x7gfc,SelfLink:/api/v1/namespaces/e2e-tests-watch-x7gfc/configmaps/e2e-watch-test-label-changed,UID:37db9195-faa9-11e8-a103-96cd63cee3b8,ResourceVersion:6566,Generation:0,CreationTimestamp:2018-12-08 05:22:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  8 05:22:12.859: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-x7gfc,SelfLink:/api/v1/namespaces/e2e-tests-watch-x7gfc/configmaps/e2e-watch-test-label-changed,UID:37db9195-faa9-11e8-a103-96cd63cee3b8,ResourceVersion:6567,Generation:0,CreationTimestamp:2018-12-08 05:22:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  8 05:22:22.901: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-x7gfc,SelfLink:/api/v1/namespaces/e2e-tests-watch-x7gfc/configmaps/e2e-watch-test-label-changed,UID:37db9195-faa9-11e8-a103-96cd63cee3b8,ResourceVersion:6585,Generation:0,CreationTimestamp:2018-12-08 05:22:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  8 05:22:22.901: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-x7gfc,SelfLink:/api/v1/namespaces/e2e-tests-watch-x7gfc/configmaps/e2e-watch-test-label-changed,UID:37db9195-faa9-11e8-a103-96cd63cee3b8,ResourceVersion:6586,Generation:0,CreationTimestamp:2018-12-08 05:22:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  8 05:22:22.901: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-x7gfc,SelfLink:/api/v1/namespaces/e2e-tests-watch-x7gfc/configmaps/e2e-watch-test-label-changed,UID:37db9195-faa9-11e8-a103-96cd63cee3b8,ResourceVersion:6587,Generation:0,CreationTimestamp:2018-12-08 05:22:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:22:22.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-x7gfc" for this suite.
Dec  8 05:22:28.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:22:28.959: INFO: namespace: e2e-tests-watch-x7gfc, resource: bindings, ignored listing per whitelist
Dec  8 05:22:29.076: INFO: namespace e2e-tests-watch-x7gfc deletion completed in 6.169426587s

• [SLOW TEST:16.475 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:22:29.076: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-x264v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1208 05:22:39.402719      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  8 05:22:39.402: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:22:39.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-x264v" for this suite.
Dec  8 05:22:45.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:22:45.438: INFO: namespace: e2e-tests-gc-x264v, resource: bindings, ignored listing per whitelist
Dec  8 05:22:45.568: INFO: namespace e2e-tests-gc-x264v deletion completed in 6.159858285s

• [SLOW TEST:16.491 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:22:45.569: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mtbr5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  8 05:22:45.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-mtbr5'
Dec  8 05:22:45.894: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Dec  8 05:22:45.895: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Dec  8 05:22:45.904: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec  8 05:22:45.910: INFO: scanned /root for discovery docs: <nil>
Dec  8 05:22:45.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-mtbr5'
Dec  8 05:23:01.801: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  8 05:23:01.801: INFO: stdout: "Created e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9\nScaling up e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec  8 05:23:01.801: INFO: stdout: "Created e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9\nScaling up e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec  8 05:23:01.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-mtbr5'
Dec  8 05:23:01.920: INFO: stderr: ""
Dec  8 05:23:01.920: INFO: stdout: "e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9-nsdbv "
Dec  8 05:23:01.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9-nsdbv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mtbr5'
Dec  8 05:23:02.026: INFO: stderr: ""
Dec  8 05:23:02.026: INFO: stdout: "true"
Dec  8 05:23:02.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9-nsdbv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-mtbr5'
Dec  8 05:23:02.148: INFO: stderr: ""
Dec  8 05:23:02.148: INFO: stdout: "nginx:1.14-alpine"
Dec  8 05:23:02.148: INFO: e2e-test-nginx-rc-2c1ed0461ebd66ca782251f726031de9-nsdbv is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
Dec  8 05:23:02.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-mtbr5'
Dec  8 05:23:02.290: INFO: stderr: ""
Dec  8 05:23:02.290: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:23:02.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mtbr5" for this suite.
Dec  8 05:23:26.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:23:26.372: INFO: namespace: e2e-tests-kubectl-mtbr5, resource: bindings, ignored listing per whitelist
Dec  8 05:23:26.510: INFO: namespace e2e-tests-kubectl-mtbr5 deletion completed in 24.207671752s

• [SLOW TEST:40.942 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:23:26.513: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-dd6xt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-dd6xt
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  8 05:23:26.759: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  8 05:23:48.953: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.192.10:8080/dial?request=hostName&protocol=http&host=172.31.0.8&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-dd6xt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:23:48.953: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:23:49.056: INFO: Waiting for endpoints: map[]
Dec  8 05:23:49.060: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.192.10:8080/dial?request=hostName&protocol=http&host=172.31.240.7&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-dd6xt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:23:49.060: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:23:49.164: INFO: Waiting for endpoints: map[]
Dec  8 05:23:49.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.192.10:8080/dial?request=hostName&protocol=http&host=172.31.192.8&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-dd6xt PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:23:49.168: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:23:49.269: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:23:49.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-dd6xt" for this suite.
Dec  8 05:24:11.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:24:11.353: INFO: namespace: e2e-tests-pod-network-test-dd6xt, resource: bindings, ignored listing per whitelist
Dec  8 05:24:11.491: INFO: namespace e2e-tests-pod-network-test-dd6xt deletion completed in 22.215872478s

• [SLOW TEST:44.978 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:24:11.493: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gxp2q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Dec  8 05:24:11.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:12.105: INFO: stderr: ""
Dec  8 05:24:12.105: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  8 05:24:12.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:12.232: INFO: stderr: ""
Dec  8 05:24:12.232: INFO: stdout: "update-demo-nautilus-rkj7v update-demo-nautilus-zctgb "
Dec  8 05:24:12.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-rkj7v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:12.342: INFO: stderr: ""
Dec  8 05:24:12.342: INFO: stdout: ""
Dec  8 05:24:12.342: INFO: update-demo-nautilus-rkj7v is created but not running
Dec  8 05:24:17.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:17.492: INFO: stderr: ""
Dec  8 05:24:17.492: INFO: stdout: "update-demo-nautilus-rkj7v update-demo-nautilus-zctgb "
Dec  8 05:24:17.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-rkj7v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:17.623: INFO: stderr: ""
Dec  8 05:24:17.623: INFO: stdout: "true"
Dec  8 05:24:17.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-rkj7v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:17.727: INFO: stderr: ""
Dec  8 05:24:17.727: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  8 05:24:17.727: INFO: validating pod update-demo-nautilus-rkj7v
Dec  8 05:24:17.739: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  8 05:24:17.739: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  8 05:24:17.739: INFO: update-demo-nautilus-rkj7v is verified up and running
Dec  8 05:24:17.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-zctgb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:17.861: INFO: stderr: ""
Dec  8 05:24:17.861: INFO: stdout: "true"
Dec  8 05:24:17.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-zctgb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:17.976: INFO: stderr: ""
Dec  8 05:24:17.976: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  8 05:24:17.976: INFO: validating pod update-demo-nautilus-zctgb
Dec  8 05:24:17.984: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  8 05:24:17.984: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  8 05:24:17.984: INFO: update-demo-nautilus-zctgb is verified up and running
STEP: rolling-update to new replication controller
Dec  8 05:24:17.986: INFO: scanned /root for discovery docs: <nil>
Dec  8 05:24:17.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:40.522: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  8 05:24:40.522: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  8 05:24:40.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:40.648: INFO: stderr: ""
Dec  8 05:24:40.648: INFO: stdout: "update-demo-kitten-k2h5n update-demo-kitten-ztxhd "
Dec  8 05:24:40.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-kitten-k2h5n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:40.779: INFO: stderr: ""
Dec  8 05:24:40.779: INFO: stdout: "true"
Dec  8 05:24:40.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-kitten-k2h5n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:40.904: INFO: stderr: ""
Dec  8 05:24:40.904: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  8 05:24:40.904: INFO: validating pod update-demo-kitten-k2h5n
Dec  8 05:24:40.913: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  8 05:24:40.913: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  8 05:24:40.913: INFO: update-demo-kitten-k2h5n is verified up and running
Dec  8 05:24:40.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-kitten-ztxhd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:41.044: INFO: stderr: ""
Dec  8 05:24:41.044: INFO: stdout: "true"
Dec  8 05:24:41.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-kitten-ztxhd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-gxp2q'
Dec  8 05:24:41.175: INFO: stderr: ""
Dec  8 05:24:41.175: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  8 05:24:41.175: INFO: validating pod update-demo-kitten-ztxhd
Dec  8 05:24:41.183: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  8 05:24:41.183: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  8 05:24:41.183: INFO: update-demo-kitten-ztxhd is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:24:41.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gxp2q" for this suite.
Dec  8 05:25:03.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:25:03.230: INFO: namespace: e2e-tests-kubectl-gxp2q, resource: bindings, ignored listing per whitelist
Dec  8 05:25:03.406: INFO: namespace e2e-tests-kubectl-gxp2q deletion completed in 22.218567952s

• [SLOW TEST:51.913 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:25:03.406: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jcqlc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-jcqlc/configmap-test-9da9f806-faa9-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 05:25:03.658: INFO: Waiting up to 5m0s for pod "pod-configmaps-9daaf85d-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-configmap-jcqlc" to be "success or failure"
Dec  8 05:25:03.668: INFO: Pod "pod-configmaps-9daaf85d-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024552ms
Dec  8 05:25:05.673: INFO: Pod "pod-configmaps-9daaf85d-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014898125s
STEP: Saw pod success
Dec  8 05:25:05.673: INFO: Pod "pod-configmaps-9daaf85d-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:25:05.677: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-configmaps-9daaf85d-faa9-11e8-b680-ca3d6f40e675 container env-test: <nil>
STEP: delete the pod
Dec  8 05:25:05.712: INFO: Waiting for pod pod-configmaps-9daaf85d-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:25:05.717: INFO: Pod pod-configmaps-9daaf85d-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:25:05.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jcqlc" for this suite.
Dec  8 05:25:11.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:25:11.837: INFO: namespace: e2e-tests-configmap-jcqlc, resource: bindings, ignored listing per whitelist
Dec  8 05:25:11.965: INFO: namespace e2e-tests-configmap-jcqlc deletion completed in 6.241060831s

• [SLOW TEST:8.559 seconds]
[sig-api-machinery] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:25:11.965: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-vd4wj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-a2c66ed7-faa9-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:25:12.234: INFO: Waiting up to 5m0s for pod "pod-secrets-a2c79e27-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-secrets-vd4wj" to be "success or failure"
Dec  8 05:25:12.241: INFO: Pod "pod-secrets-a2c79e27-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 6.621872ms
Dec  8 05:25:14.246: INFO: Pod "pod-secrets-a2c79e27-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012142722s
STEP: Saw pod success
Dec  8 05:25:14.246: INFO: Pod "pod-secrets-a2c79e27-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:25:14.251: INFO: Trying to get logs from node phrs-liberal-perch pod pod-secrets-a2c79e27-faa9-11e8-b680-ca3d6f40e675 container secret-volume-test: <nil>
STEP: delete the pod
Dec  8 05:25:14.308: INFO: Waiting for pod pod-secrets-a2c79e27-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:25:14.314: INFO: Pod pod-secrets-a2c79e27-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:25:14.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-vd4wj" for this suite.
Dec  8 05:25:20.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:25:20.531: INFO: namespace: e2e-tests-secrets-vd4wj, resource: bindings, ignored listing per whitelist
Dec  8 05:25:20.551: INFO: namespace e2e-tests-secrets-vd4wj deletion completed in 6.23134011s

• [SLOW TEST:8.586 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:25:20.551: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-x7x27
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-a7e2056c-faa9-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 05:25:20.800: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7e317c8-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-configmap-x7x27" to be "success or failure"
Dec  8 05:25:20.807: INFO: Pod "pod-configmaps-a7e317c8-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.517563ms
Dec  8 05:25:22.812: INFO: Pod "pod-configmaps-a7e317c8-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012026256s
STEP: Saw pod success
Dec  8 05:25:22.812: INFO: Pod "pod-configmaps-a7e317c8-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:25:22.818: INFO: Trying to get logs from node phrs-one-falcon pod pod-configmaps-a7e317c8-faa9-11e8-b680-ca3d6f40e675 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 05:25:22.854: INFO: Waiting for pod pod-configmaps-a7e317c8-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:25:22.859: INFO: Pod pod-configmaps-a7e317c8-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:25:22.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-x7x27" for this suite.
Dec  8 05:25:28.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:25:28.974: INFO: namespace: e2e-tests-configmap-x7x27, resource: bindings, ignored listing per whitelist
Dec  8 05:25:29.044: INFO: namespace e2e-tests-configmap-x7x27 deletion completed in 6.176876936s

• [SLOW TEST:8.493 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:25:29.045: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-txfwl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Dec  8 05:25:29.253: INFO: Waiting up to 5m0s for pod "client-containers-aced0bc5-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-containers-txfwl" to be "success or failure"
Dec  8 05:25:29.258: INFO: Pod "client-containers-aced0bc5-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 4.801367ms
Dec  8 05:25:31.262: INFO: Pod "client-containers-aced0bc5-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008861943s
Dec  8 05:25:33.265: INFO: Pod "client-containers-aced0bc5-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012314102s
STEP: Saw pod success
Dec  8 05:25:33.265: INFO: Pod "client-containers-aced0bc5-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:25:33.269: INFO: Trying to get logs from node phrs-glorious-mastodon pod client-containers-aced0bc5-faa9-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:25:33.300: INFO: Waiting for pod client-containers-aced0bc5-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:25:33.303: INFO: Pod client-containers-aced0bc5-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:25:33.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-txfwl" for this suite.
Dec  8 05:25:39.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:25:39.388: INFO: namespace: e2e-tests-containers-txfwl, resource: bindings, ignored listing per whitelist
Dec  8 05:25:39.443: INFO: namespace e2e-tests-containers-txfwl deletion completed in 6.135954205s

• [SLOW TEST:10.398 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:25:39.443: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-fjbr8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-449h9 in namespace e2e-tests-proxy-fjbr8
I1208 05:25:39.657829      17 runners.go:180] Created replication controller with name: proxy-service-449h9, namespace: e2e-tests-proxy-fjbr8, replica count: 1
I1208 05:25:40.708395      17 runners.go:180] proxy-service-449h9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1208 05:25:41.708694      17 runners.go:180] proxy-service-449h9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1208 05:25:42.709038      17 runners.go:180] proxy-service-449h9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1208 05:25:43.709345      17 runners.go:180] proxy-service-449h9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1208 05:25:44.709693      17 runners.go:180] proxy-service-449h9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1208 05:25:45.710018      17 runners.go:180] proxy-service-449h9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1208 05:25:46.710282      17 runners.go:180] proxy-service-449h9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  8 05:25:46.713: INFO: setup took 7.071549195s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  8 05:25:46.726: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 11.663637ms)
Dec  8 05:25:46.726: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 12.081049ms)
Dec  8 05:25:46.728: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 13.85968ms)
Dec  8 05:25:46.733: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 19.098365ms)
Dec  8 05:25:46.733: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 19.169942ms)
Dec  8 05:25:46.733: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 19.545651ms)
Dec  8 05:25:46.735: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 21.367332ms)
Dec  8 05:25:46.736: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 22.234413ms)
Dec  8 05:25:46.736: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 22.116965ms)
Dec  8 05:25:46.737: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 23.44269ms)
Dec  8 05:25:46.738: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 24.008634ms)
Dec  8 05:25:46.738: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 24.048946ms)
Dec  8 05:25:46.738: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 24.268881ms)
Dec  8 05:25:46.739: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 24.535053ms)
Dec  8 05:25:46.744: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 30.087291ms)
Dec  8 05:25:46.746: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 31.641199ms)
Dec  8 05:25:46.758: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 11.459372ms)
Dec  8 05:25:46.758: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 12.381523ms)
Dec  8 05:25:46.759: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 12.547918ms)
Dec  8 05:25:46.759: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 12.635302ms)
Dec  8 05:25:46.759: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 12.865084ms)
Dec  8 05:25:46.760: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 13.690729ms)
Dec  8 05:25:46.762: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 15.723727ms)
Dec  8 05:25:46.763: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 16.466053ms)
Dec  8 05:25:46.763: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 17.182911ms)
Dec  8 05:25:46.763: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 17.308272ms)
Dec  8 05:25:46.763: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 17.3771ms)
Dec  8 05:25:46.764: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 18.137426ms)
Dec  8 05:25:46.764: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 18.284087ms)
Dec  8 05:25:46.764: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 18.403182ms)
Dec  8 05:25:46.765: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 18.661525ms)
Dec  8 05:25:46.765: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 18.529191ms)
Dec  8 05:25:46.782: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 16.311347ms)
Dec  8 05:25:46.783: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 17.884201ms)
Dec  8 05:25:46.786: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 19.288605ms)
Dec  8 05:25:46.787: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 20.218226ms)
Dec  8 05:25:46.787: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 20.91599ms)
Dec  8 05:25:46.788: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 22.27222ms)
Dec  8 05:25:46.788: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 21.723743ms)
Dec  8 05:25:46.788: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 22.868607ms)
Dec  8 05:25:46.788: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 22.538278ms)
Dec  8 05:25:46.791: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 24.833993ms)
Dec  8 05:25:46.791: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 25.372401ms)
Dec  8 05:25:46.791: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 25.739476ms)
Dec  8 05:25:46.792: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 25.44135ms)
Dec  8 05:25:46.792: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 27.295341ms)
Dec  8 05:25:46.792: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 27.024381ms)
Dec  8 05:25:46.794: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 27.62569ms)
Dec  8 05:25:46.800: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 6.071949ms)
Dec  8 05:25:46.804: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 9.328066ms)
Dec  8 05:25:46.804: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 9.629677ms)
Dec  8 05:25:46.804: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 9.745589ms)
Dec  8 05:25:46.815: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 20.296242ms)
Dec  8 05:25:46.818: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 23.305724ms)
Dec  8 05:25:46.819: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 24.354794ms)
Dec  8 05:25:46.819: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 24.574431ms)
Dec  8 05:25:46.819: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 24.759907ms)
Dec  8 05:25:46.820: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 25.135808ms)
Dec  8 05:25:46.820: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 25.040757ms)
Dec  8 05:25:46.820: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 25.440655ms)
Dec  8 05:25:46.821: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 26.698044ms)
Dec  8 05:25:46.822: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 27.379334ms)
Dec  8 05:25:46.823: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 28.303289ms)
Dec  8 05:25:46.823: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 28.878577ms)
Dec  8 05:25:46.835: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 11.658715ms)
Dec  8 05:25:46.836: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 12.382504ms)
Dec  8 05:25:46.836: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 12.225601ms)
Dec  8 05:25:46.847: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 23.888654ms)
Dec  8 05:25:46.848: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 24.377438ms)
Dec  8 05:25:46.848: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 24.697596ms)
Dec  8 05:25:46.850: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 26.291131ms)
Dec  8 05:25:46.850: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 26.543589ms)
Dec  8 05:25:46.850: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 26.839476ms)
Dec  8 05:25:46.851: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 27.195366ms)
Dec  8 05:25:46.852: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 28.589234ms)
Dec  8 05:25:46.852: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 28.665592ms)
Dec  8 05:25:46.856: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 32.325185ms)
Dec  8 05:25:46.856: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 32.368253ms)
Dec  8 05:25:46.856: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 32.557483ms)
Dec  8 05:25:46.856: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 32.698051ms)
Dec  8 05:25:46.864: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 7.549204ms)
Dec  8 05:25:46.865: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 8.305366ms)
Dec  8 05:25:46.865: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 8.445909ms)
Dec  8 05:25:46.866: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 9.521395ms)
Dec  8 05:25:46.867: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 9.787864ms)
Dec  8 05:25:46.867: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 10.198791ms)
Dec  8 05:25:46.867: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 10.254774ms)
Dec  8 05:25:46.867: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 10.825947ms)
Dec  8 05:25:46.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 16.130351ms)
Dec  8 05:25:46.873: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 16.378282ms)
Dec  8 05:25:46.876: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 19.259942ms)
Dec  8 05:25:46.877: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 20.095358ms)
Dec  8 05:25:46.881: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 24.150025ms)
Dec  8 05:25:46.881: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 24.412739ms)
Dec  8 05:25:46.883: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 26.414322ms)
Dec  8 05:25:46.885: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 28.578592ms)
Dec  8 05:25:46.902: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 16.791522ms)
Dec  8 05:25:46.903: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 17.634174ms)
Dec  8 05:25:46.903: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 17.930122ms)
Dec  8 05:25:46.904: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 18.011573ms)
Dec  8 05:25:46.905: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 19.319869ms)
Dec  8 05:25:46.905: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 19.675999ms)
Dec  8 05:25:46.905: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 19.836804ms)
Dec  8 05:25:46.905: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 19.629031ms)
Dec  8 05:25:46.905: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 19.632595ms)
Dec  8 05:25:46.906: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 20.114144ms)
Dec  8 05:25:46.906: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 20.057571ms)
Dec  8 05:25:46.906: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 20.496548ms)
Dec  8 05:25:46.906: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 20.60091ms)
Dec  8 05:25:46.907: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 21.402921ms)
Dec  8 05:25:46.907: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 21.065121ms)
Dec  8 05:25:46.909: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 23.537485ms)
Dec  8 05:25:46.919: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 10.369903ms)
Dec  8 05:25:46.920: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 10.349092ms)
Dec  8 05:25:46.921: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 11.368712ms)
Dec  8 05:25:46.921: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 11.409178ms)
Dec  8 05:25:46.921: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 11.749884ms)
Dec  8 05:25:46.924: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 14.660749ms)
Dec  8 05:25:46.925: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 15.33972ms)
Dec  8 05:25:46.925: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 15.073615ms)
Dec  8 05:25:46.925: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 15.315946ms)
Dec  8 05:25:46.925: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 15.219566ms)
Dec  8 05:25:46.925: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 15.814256ms)
Dec  8 05:25:46.925: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 15.75372ms)
Dec  8 05:25:46.925: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 16.014557ms)
Dec  8 05:25:46.928: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 18.070577ms)
Dec  8 05:25:46.928: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 18.513048ms)
Dec  8 05:25:46.928: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 18.69173ms)
Dec  8 05:25:46.945: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 16.473536ms)
Dec  8 05:25:46.945: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 16.795438ms)
Dec  8 05:25:46.947: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 18.53676ms)
Dec  8 05:25:46.947: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 19.009432ms)
Dec  8 05:25:46.949: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 20.378244ms)
Dec  8 05:25:46.949: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 20.627327ms)
Dec  8 05:25:46.949: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 20.685667ms)
Dec  8 05:25:46.949: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 20.796623ms)
Dec  8 05:25:46.950: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 21.147868ms)
Dec  8 05:25:46.950: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 21.183424ms)
Dec  8 05:25:46.950: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 21.353854ms)
Dec  8 05:25:46.950: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 21.466225ms)
Dec  8 05:25:46.950: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 21.631769ms)
Dec  8 05:25:46.950: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 21.954661ms)
Dec  8 05:25:46.950: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 21.960575ms)
Dec  8 05:25:46.952: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 23.583145ms)
Dec  8 05:25:46.963: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 11.060379ms)
Dec  8 05:25:46.964: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 11.323455ms)
Dec  8 05:25:46.964: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 11.699591ms)
Dec  8 05:25:46.965: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 12.368389ms)
Dec  8 05:25:46.975: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 22.50773ms)
Dec  8 05:25:46.975: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 22.66704ms)
Dec  8 05:25:46.975: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 22.95239ms)
Dec  8 05:25:46.978: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 26.152243ms)
Dec  8 05:25:46.978: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 25.859266ms)
Dec  8 05:25:46.980: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 27.951534ms)
Dec  8 05:25:46.980: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 28.050661ms)
Dec  8 05:25:46.980: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 28.036515ms)
Dec  8 05:25:46.980: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 27.780399ms)
Dec  8 05:25:46.980: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 28.019168ms)
Dec  8 05:25:46.980: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 27.688149ms)
Dec  8 05:25:46.980: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 27.781144ms)
Dec  8 05:25:46.986: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 5.906734ms)
Dec  8 05:25:46.993: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 12.052038ms)
Dec  8 05:25:46.993: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 12.069909ms)
Dec  8 05:25:46.993: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 12.434209ms)
Dec  8 05:25:46.994: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 12.79701ms)
Dec  8 05:25:46.996: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 15.150163ms)
Dec  8 05:25:47.000: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 19.006188ms)
Dec  8 05:25:47.005: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 24.52697ms)
Dec  8 05:25:47.005: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 24.275365ms)
Dec  8 05:25:47.005: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 24.347255ms)
Dec  8 05:25:47.007: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 26.1749ms)
Dec  8 05:25:47.007: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 26.208124ms)
Dec  8 05:25:47.007: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 26.308484ms)
Dec  8 05:25:47.007: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 26.673045ms)
Dec  8 05:25:47.009: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 28.688952ms)
Dec  8 05:25:47.009: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 28.650173ms)
Dec  8 05:25:47.023: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 11.928705ms)
Dec  8 05:25:47.024: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 14.908919ms)
Dec  8 05:25:47.025: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 15.241925ms)
Dec  8 05:25:47.025: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 15.546263ms)
Dec  8 05:25:47.026: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 16.651451ms)
Dec  8 05:25:47.027: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 16.857077ms)
Dec  8 05:25:47.027: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 16.305375ms)
Dec  8 05:25:47.027: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 16.708338ms)
Dec  8 05:25:47.027: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 17.779533ms)
Dec  8 05:25:47.028: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 17.670683ms)
Dec  8 05:25:47.031: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 21.866239ms)
Dec  8 05:25:47.032: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 22.299791ms)
Dec  8 05:25:47.034: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 23.999991ms)
Dec  8 05:25:47.034: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 23.5018ms)
Dec  8 05:25:47.034: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 24.844114ms)
Dec  8 05:25:47.035: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 24.853043ms)
Dec  8 05:25:47.044: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 9.453014ms)
Dec  8 05:25:47.047: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 12.277232ms)
Dec  8 05:25:47.048: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 13.516873ms)
Dec  8 05:25:47.049: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 13.676074ms)
Dec  8 05:25:47.049: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 13.803558ms)
Dec  8 05:25:47.049: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 13.788222ms)
Dec  8 05:25:47.051: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 15.72931ms)
Dec  8 05:25:47.051: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 15.677657ms)
Dec  8 05:25:47.051: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 16.110424ms)
Dec  8 05:25:47.051: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 16.153826ms)
Dec  8 05:25:47.051: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 16.880969ms)
Dec  8 05:25:47.052: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 16.776523ms)
Dec  8 05:25:47.052: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 16.581028ms)
Dec  8 05:25:47.053: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 18.531351ms)
Dec  8 05:25:47.053: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 18.421756ms)
Dec  8 05:25:47.054: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 19.251199ms)
Dec  8 05:25:47.064: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 9.651802ms)
Dec  8 05:25:47.065: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 10.177288ms)
Dec  8 05:25:47.065: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 11.19048ms)
Dec  8 05:25:47.066: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 11.120307ms)
Dec  8 05:25:47.067: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 12.307331ms)
Dec  8 05:25:47.067: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 13.198656ms)
Dec  8 05:25:47.067: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 12.357993ms)
Dec  8 05:25:47.067: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 12.836061ms)
Dec  8 05:25:47.068: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 13.991505ms)
Dec  8 05:25:47.069: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 13.673393ms)
Dec  8 05:25:47.069: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 14.642382ms)
Dec  8 05:25:47.070: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 14.45654ms)
Dec  8 05:25:47.070: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 14.583788ms)
Dec  8 05:25:47.070: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 15.122673ms)
Dec  8 05:25:47.070: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 14.83944ms)
Dec  8 05:25:47.071: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 15.216679ms)
Dec  8 05:25:47.085: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 14.526733ms)
Dec  8 05:25:47.087: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 16.223629ms)
Dec  8 05:25:47.087: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 16.143342ms)
Dec  8 05:25:47.087: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 16.067293ms)
Dec  8 05:25:47.087: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 16.231319ms)
Dec  8 05:25:47.087: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 16.244152ms)
Dec  8 05:25:47.088: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 17.130078ms)
Dec  8 05:25:47.088: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 17.307056ms)
Dec  8 05:25:47.088: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 17.149291ms)
Dec  8 05:25:47.089: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 17.597025ms)
Dec  8 05:25:47.089: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 18.297589ms)
Dec  8 05:25:47.089: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 18.315986ms)
Dec  8 05:25:47.089: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 18.633464ms)
Dec  8 05:25:47.090: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 18.796636ms)
Dec  8 05:25:47.090: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 18.846748ms)
Dec  8 05:25:47.090: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 19.56534ms)
Dec  8 05:25:47.098: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 7.830986ms)
Dec  8 05:25:47.099: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 7.860602ms)
Dec  8 05:25:47.106: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 15.528034ms)
Dec  8 05:25:47.106: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 15.035414ms)
Dec  8 05:25:47.107: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 15.638577ms)
Dec  8 05:25:47.107: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 15.985165ms)
Dec  8 05:25:47.107: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 16.147892ms)
Dec  8 05:25:47.107: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 15.737053ms)
Dec  8 05:25:47.107: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 16.620458ms)
Dec  8 05:25:47.107: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 16.320217ms)
Dec  8 05:25:47.108: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 16.769359ms)
Dec  8 05:25:47.109: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 17.281376ms)
Dec  8 05:25:47.109: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 17.500258ms)
Dec  8 05:25:47.109: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 18.430653ms)
Dec  8 05:25:47.114: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 22.486111ms)
Dec  8 05:25:47.114: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 23.093852ms)
Dec  8 05:25:47.122: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 8.40483ms)
Dec  8 05:25:47.130: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 15.561468ms)
Dec  8 05:25:47.130: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 15.173821ms)
Dec  8 05:25:47.131: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 15.9944ms)
Dec  8 05:25:47.131: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 16.193801ms)
Dec  8 05:25:47.133: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 18.571909ms)
Dec  8 05:25:47.133: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 18.181287ms)
Dec  8 05:25:47.134: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 18.631781ms)
Dec  8 05:25:47.134: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 18.765357ms)
Dec  8 05:25:47.134: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 18.810795ms)
Dec  8 05:25:47.136: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 21.474427ms)
Dec  8 05:25:47.137: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 21.793705ms)
Dec  8 05:25:47.137: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 23.134948ms)
Dec  8 05:25:47.138: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 23.618492ms)
Dec  8 05:25:47.138: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 22.986999ms)
Dec  8 05:25:47.138: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 24.009363ms)
Dec  8 05:25:47.147: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 8.591168ms)
Dec  8 05:25:47.149: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 9.997817ms)
Dec  8 05:25:47.149: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 9.921345ms)
Dec  8 05:25:47.149: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 10.651577ms)
Dec  8 05:25:47.149: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 10.488011ms)
Dec  8 05:25:47.152: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 12.475167ms)
Dec  8 05:25:47.152: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 13.375009ms)
Dec  8 05:25:47.152: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 13.607319ms)
Dec  8 05:25:47.152: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 13.241512ms)
Dec  8 05:25:47.155: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 15.888968ms)
Dec  8 05:25:47.155: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 16.498505ms)
Dec  8 05:25:47.156: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 16.708741ms)
Dec  8 05:25:47.156: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 16.790604ms)
Dec  8 05:25:47.159: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 20.055688ms)
Dec  8 05:25:47.159: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 20.346624ms)
Dec  8 05:25:47.159: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 20.623752ms)
Dec  8 05:25:47.169: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 9.868108ms)
Dec  8 05:25:47.170: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 10.262755ms)
Dec  8 05:25:47.170: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 10.255413ms)
Dec  8 05:25:47.170: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 10.272471ms)
Dec  8 05:25:47.170: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 10.552887ms)
Dec  8 05:25:47.170: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 10.438965ms)
Dec  8 05:25:47.173: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 13.573076ms)
Dec  8 05:25:47.173: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 14.101828ms)
Dec  8 05:25:47.174: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 13.983049ms)
Dec  8 05:25:47.174: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 14.180836ms)
Dec  8 05:25:47.174: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 14.600352ms)
Dec  8 05:25:47.174: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 14.528206ms)
Dec  8 05:25:47.174: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 14.539771ms)
Dec  8 05:25:47.174: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 14.557498ms)
Dec  8 05:25:47.176: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 16.327785ms)
Dec  8 05:25:47.176: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 16.860485ms)
Dec  8 05:25:47.187: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:162/proxy/: bar (200; 10.559072ms)
Dec  8 05:25:47.189: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:1080/proxy/rewri... (200; 11.987566ms)
Dec  8 05:25:47.190: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:162/proxy/: bar (200; 13.25949ms)
Dec  8 05:25:47.192: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:443/proxy/... (200; 14.780328ms)
Dec  8 05:25:47.192: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname2/proxy/: tls qux (200; 15.159038ms)
Dec  8 05:25:47.193: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:460/proxy/: tls baz (200; 16.278937ms)
Dec  8 05:25:47.193: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname2/proxy/: bar (200; 16.306699ms)
Dec  8 05:25:47.193: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/https:proxy-service-449h9:tlsportname1/proxy/: tls baz (200; 16.513638ms)
Dec  8 05:25:47.193: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname1/proxy/: foo (200; 16.301941ms)
Dec  8 05:25:47.194: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:1080/proxy/... (200; 17.06534ms)
Dec  8 05:25:47.194: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/http:proxy-service-449h9-vxs5w:160/proxy/: foo (200; 16.824622ms)
Dec  8 05:25:47.194: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w/proxy/rewriteme"... (200; 16.861491ms)
Dec  8 05:25:47.194: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/proxy-service-449h9:portname2/proxy/: bar (200; 16.764633ms)
Dec  8 05:25:47.194: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/services/http:proxy-service-449h9:portname1/proxy/: foo (200; 17.151248ms)
Dec  8 05:25:47.194: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/proxy-service-449h9-vxs5w:160/proxy/: foo (200; 17.476911ms)
Dec  8 05:25:47.194: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-fjbr8/pods/https:proxy-service-449h9-vxs5w:462/proxy/: tls qux (200; 17.614265ms)
STEP: deleting { ReplicationController} proxy-service-449h9 in namespace e2e-tests-proxy-fjbr8, will wait for the garbage collector to delete the pods
Dec  8 05:25:47.255: INFO: Deleting { ReplicationController} proxy-service-449h9 took: 7.606546ms
Dec  8 05:25:47.356: INFO: Terminating { ReplicationController} proxy-service-449h9 pods took: 100.289801ms
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:25:58.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-fjbr8" for this suite.
Dec  8 05:26:04.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:26:04.583: INFO: namespace: e2e-tests-proxy-fjbr8, resource: bindings, ignored listing per whitelist
Dec  8 05:26:04.638: INFO: namespace e2e-tests-proxy-fjbr8 deletion completed in 6.176642534s

• [SLOW TEST:25.195 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:26:04.639: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-klknn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  8 05:26:04.874: INFO: Waiting up to 5m0s for pod "pod-c22763f3-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-klknn" to be "success or failure"
Dec  8 05:26:04.881: INFO: Pod "pod-c22763f3-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.536145ms
Dec  8 05:26:06.886: INFO: Pod "pod-c22763f3-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012282523s
STEP: Saw pod success
Dec  8 05:26:06.886: INFO: Pod "pod-c22763f3-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:26:06.890: INFO: Trying to get logs from node phrs-one-falcon pod pod-c22763f3-faa9-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:26:06.925: INFO: Waiting for pod pod-c22763f3-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:26:06.929: INFO: Pod pod-c22763f3-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:26:06.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-klknn" for this suite.
Dec  8 05:26:12.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:26:13.101: INFO: namespace: e2e-tests-emptydir-klknn, resource: bindings, ignored listing per whitelist
Dec  8 05:26:13.122: INFO: namespace e2e-tests-emptydir-klknn deletion completed in 6.187962785s

• [SLOW TEST:8.483 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:26:13.122: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-7t8pm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  8 05:26:17.408: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  8 05:26:17.414: INFO: Pod pod-with-prestop-http-hook still exists
Dec  8 05:26:19.414: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  8 05:26:19.419: INFO: Pod pod-with-prestop-http-hook still exists
Dec  8 05:26:21.414: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  8 05:26:21.420: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:26:21.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-7t8pm" for this suite.
Dec  8 05:26:43.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:26:43.565: INFO: namespace: e2e-tests-container-lifecycle-hook-7t8pm, resource: bindings, ignored listing per whitelist
Dec  8 05:26:43.604: INFO: namespace e2e-tests-container-lifecycle-hook-7t8pm deletion completed in 22.163392164s

• [SLOW TEST:30.482 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:26:43.604: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-8xn28
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  8 05:26:43.833: INFO: Waiting up to 5m0s for pod "pod-d9612d13-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-8xn28" to be "success or failure"
Dec  8 05:26:43.845: INFO: Pod "pod-d9612d13-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 11.839286ms
Dec  8 05:26:45.849: INFO: Pod "pod-d9612d13-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016213039s
STEP: Saw pod success
Dec  8 05:26:45.849: INFO: Pod "pod-d9612d13-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:26:45.853: INFO: Trying to get logs from node phrs-one-falcon pod pod-d9612d13-faa9-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:26:45.880: INFO: Waiting for pod pod-d9612d13-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:26:45.883: INFO: Pod pod-d9612d13-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:26:45.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-8xn28" for this suite.
Dec  8 05:26:51.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:26:52.001: INFO: namespace: e2e-tests-emptydir-8xn28, resource: bindings, ignored listing per whitelist
Dec  8 05:26:52.065: INFO: namespace e2e-tests-emptydir-8xn28 deletion completed in 6.176652312s

• [SLOW TEST:8.461 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:26:52.065: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qwqdp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:26:52.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de824c88-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-qwqdp" to be "success or failure"
Dec  8 05:26:52.443: INFO: Pod "downwardapi-volume-de824c88-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 3.602775ms
Dec  8 05:26:54.447: INFO: Pod "downwardapi-volume-de824c88-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007173414s
STEP: Saw pod success
Dec  8 05:26:54.447: INFO: Pod "downwardapi-volume-de824c88-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:26:54.450: INFO: Trying to get logs from node phrs-glorious-mastodon pod downwardapi-volume-de824c88-faa9-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:26:54.478: INFO: Waiting for pod downwardapi-volume-de824c88-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:26:54.482: INFO: Pod downwardapi-volume-de824c88-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:26:54.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qwqdp" for this suite.
Dec  8 05:27:00.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:27:00.648: INFO: namespace: e2e-tests-downward-api-qwqdp, resource: bindings, ignored listing per whitelist
Dec  8 05:27:00.651: INFO: namespace e2e-tests-downward-api-qwqdp deletion completed in 6.164889288s

• [SLOW TEST:8.586 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:27:00.653: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-j9zxb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-e388a9f9-faa9-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:27:00.879: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e3899351-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-j9zxb" to be "success or failure"
Dec  8 05:27:00.885: INFO: Pod "pod-projected-secrets-e3899351-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.964997ms
Dec  8 05:27:02.889: INFO: Pod "pod-projected-secrets-e3899351-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010429002s
STEP: Saw pod success
Dec  8 05:27:02.889: INFO: Pod "pod-projected-secrets-e3899351-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:27:02.893: INFO: Trying to get logs from node phrs-liberal-perch pod pod-projected-secrets-e3899351-faa9-11e8-b680-ca3d6f40e675 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  8 05:27:02.928: INFO: Waiting for pod pod-projected-secrets-e3899351-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:27:02.932: INFO: Pod pod-projected-secrets-e3899351-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:27:02.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j9zxb" for this suite.
Dec  8 05:27:08.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:27:09.029: INFO: namespace: e2e-tests-projected-j9zxb, resource: bindings, ignored listing per whitelist
Dec  8 05:27:09.125: INFO: namespace e2e-tests-projected-j9zxb deletion completed in 6.188123642s

• [SLOW TEST:8.472 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:27:09.127: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-h9ksh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Dec  8 05:27:09.861: INFO: Waiting up to 5m0s for pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-7rtdt" in namespace "e2e-tests-svcaccounts-h9ksh" to be "success or failure"
Dec  8 05:27:09.869: INFO: Pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-7rtdt": Phase="Pending", Reason="", readiness=false. Elapsed: 7.255027ms
Dec  8 05:27:11.873: INFO: Pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-7rtdt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011443301s
STEP: Saw pod success
Dec  8 05:27:11.873: INFO: Pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-7rtdt" satisfied condition "success or failure"
Dec  8 05:27:11.877: INFO: Trying to get logs from node phrs-one-falcon pod pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-7rtdt container token-test: <nil>
STEP: delete the pod
Dec  8 05:27:11.910: INFO: Waiting for pod pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-7rtdt to disappear
Dec  8 05:27:11.916: INFO: Pod pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-7rtdt no longer exists
STEP: Creating a pod to test consume service account root CA
Dec  8 05:27:11.930: INFO: Waiting up to 5m0s for pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-m4tgk" in namespace "e2e-tests-svcaccounts-h9ksh" to be "success or failure"
Dec  8 05:27:11.944: INFO: Pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-m4tgk": Phase="Pending", Reason="", readiness=false. Elapsed: 14.079891ms
Dec  8 05:27:13.949: INFO: Pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-m4tgk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018663817s
STEP: Saw pod success
Dec  8 05:27:13.949: INFO: Pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-m4tgk" satisfied condition "success or failure"
Dec  8 05:27:13.953: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-m4tgk container root-ca-test: <nil>
STEP: delete the pod
Dec  8 05:27:13.986: INFO: Waiting for pod pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-m4tgk to disappear
Dec  8 05:27:13.991: INFO: Pod pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-m4tgk no longer exists
STEP: Creating a pod to test consume service account namespace
Dec  8 05:27:14.004: INFO: Waiting up to 5m0s for pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-q9gc7" in namespace "e2e-tests-svcaccounts-h9ksh" to be "success or failure"
Dec  8 05:27:14.017: INFO: Pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-q9gc7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.406418ms
Dec  8 05:27:16.021: INFO: Pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-q9gc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017395574s
STEP: Saw pod success
Dec  8 05:27:16.021: INFO: Pod "pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-q9gc7" satisfied condition "success or failure"
Dec  8 05:27:16.025: INFO: Trying to get logs from node phrs-liberal-perch pod pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-q9gc7 container namespace-test: <nil>
STEP: delete the pod
Dec  8 05:27:16.065: INFO: Waiting for pod pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-q9gc7 to disappear
Dec  8 05:27:16.070: INFO: Pod pod-service-account-e8e4a1c2-faa9-11e8-b680-ca3d6f40e675-q9gc7 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:27:16.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-h9ksh" for this suite.
Dec  8 05:27:22.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:27:22.140: INFO: namespace: e2e-tests-svcaccounts-h9ksh, resource: bindings, ignored listing per whitelist
Dec  8 05:27:22.281: INFO: namespace e2e-tests-svcaccounts-h9ksh deletion completed in 6.205666589s

• [SLOW TEST:13.154 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:27:22.281: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-72r66
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:27:22.527: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec  8 05:27:27.531: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  8 05:27:27.531: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  8 05:27:27.553: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-72r66,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-72r66/deployments/test-cleanup-deployment,UID:f36f7ed8-faa9-11e8-a103-96cd63cee3b8,ResourceVersion:8051,Generation:1,CreationTimestamp:2018-12-08 05:27:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Dec  8 05:27:27.556: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:27:27.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-72r66" for this suite.
Dec  8 05:27:33.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:27:33.690: INFO: namespace: e2e-tests-deployment-72r66, resource: bindings, ignored listing per whitelist
Dec  8 05:27:33.747: INFO: namespace e2e-tests-deployment-72r66 deletion completed in 6.182023367s

• [SLOW TEST:11.466 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:27:33.748: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-fjztv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:27:33.989: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7433792-faa9-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-fjztv" to be "success or failure"
Dec  8 05:27:34.000: INFO: Pod "downwardapi-volume-f7433792-faa9-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 10.868243ms
Dec  8 05:27:36.003: INFO: Pod "downwardapi-volume-f7433792-faa9-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014638985s
STEP: Saw pod success
Dec  8 05:27:36.003: INFO: Pod "downwardapi-volume-f7433792-faa9-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:27:36.007: INFO: Trying to get logs from node phrs-glorious-mastodon pod downwardapi-volume-f7433792-faa9-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:27:36.032: INFO: Waiting for pod downwardapi-volume-f7433792-faa9-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:27:36.035: INFO: Pod downwardapi-volume-f7433792-faa9-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:27:36.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fjztv" for this suite.
Dec  8 05:27:42.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:27:42.086: INFO: namespace: e2e-tests-downward-api-fjztv, resource: bindings, ignored listing per whitelist
Dec  8 05:27:42.227: INFO: namespace e2e-tests-downward-api-fjztv deletion completed in 6.187213271s

• [SLOW TEST:8.479 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:27:42.227: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-xk24t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1208 05:28:12.495364      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  8 05:28:12.495: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:28:12.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-xk24t" for this suite.
Dec  8 05:28:18.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:28:18.601: INFO: namespace: e2e-tests-gc-xk24t, resource: bindings, ignored listing per whitelist
Dec  8 05:28:18.701: INFO: namespace e2e-tests-gc-xk24t deletion completed in 6.201510666s

• [SLOW TEST:36.474 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:28:18.701: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-h6tnj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:28:18.935: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  8 05:28:18.949: INFO: Number of nodes with available pods: 0
Dec  8 05:28:18.949: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  8 05:28:18.987: INFO: Number of nodes with available pods: 0
Dec  8 05:28:18.987: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:19.992: INFO: Number of nodes with available pods: 0
Dec  8 05:28:19.992: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:20.993: INFO: Number of nodes with available pods: 1
Dec  8 05:28:20.993: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  8 05:28:21.027: INFO: Number of nodes with available pods: 1
Dec  8 05:28:21.028: INFO: Number of running nodes: 0, number of available pods: 1
Dec  8 05:28:22.033: INFO: Number of nodes with available pods: 0
Dec  8 05:28:22.033: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  8 05:28:22.061: INFO: Number of nodes with available pods: 0
Dec  8 05:28:22.061: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:23.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:23.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:24.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:24.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:25.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:25.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:26.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:26.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:27.067: INFO: Number of nodes with available pods: 0
Dec  8 05:28:27.067: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:28.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:28.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:29.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:29.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:30.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:30.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:31.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:31.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:32.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:32.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:33.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:33.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:34.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:34.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:35.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:35.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:36.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:36.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:37.073: INFO: Number of nodes with available pods: 0
Dec  8 05:28:37.073: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:38.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:38.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:39.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:39.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:40.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:40.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:41.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:41.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:42.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:42.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:43.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:43.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:44.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:44.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:45.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:45.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:46.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:46.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:47.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:47.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:48.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:48.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:49.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:49.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:50.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:50.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:51.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:51.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:52.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:52.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:53.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:53.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:54.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:54.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:55.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:55.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:56.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:56.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:57.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:57.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:58.066: INFO: Number of nodes with available pods: 0
Dec  8 05:28:58.066: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:28:59.065: INFO: Number of nodes with available pods: 0
Dec  8 05:28:59.065: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:29:00.066: INFO: Number of nodes with available pods: 1
Dec  8 05:29:00.066: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-h6tnj, will wait for the garbage collector to delete the pods
Dec  8 05:29:00.140: INFO: Deleting {extensions DaemonSet} daemon-set took: 14.719826ms
Dec  8 05:29:00.241: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.366356ms
Dec  8 05:29:32.846: INFO: Number of nodes with available pods: 0
Dec  8 05:29:32.846: INFO: Number of running nodes: 0, number of available pods: 0
Dec  8 05:29:32.849: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-h6tnj/daemonsets","resourceVersion":"8485"},"items":null}

Dec  8 05:29:32.853: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-h6tnj/pods","resourceVersion":"8485"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:29:32.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-h6tnj" for this suite.
Dec  8 05:29:38.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:29:38.946: INFO: namespace: e2e-tests-daemonsets-h6tnj, resource: bindings, ignored listing per whitelist
Dec  8 05:29:39.027: INFO: namespace e2e-tests-daemonsets-h6tnj deletion completed in 6.14481747s

• [SLOW TEST:80.326 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:29:39.027: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-t9vbq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  8 05:32:21.301: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:21.310: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:23.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:23.315: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:25.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:25.315: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:27.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:27.314: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:29.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:29.314: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:31.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:31.314: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:33.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:33.314: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:35.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:35.314: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:37.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:37.315: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:39.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:39.316: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:41.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:41.315: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:43.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:43.315: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:45.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:45.315: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:47.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:47.314: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  8 05:32:49.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  8 05:32:49.314: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:32:49.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-t9vbq" for this suite.
Dec  8 05:33:11.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:33:11.411: INFO: namespace: e2e-tests-container-lifecycle-hook-t9vbq, resource: bindings, ignored listing per whitelist
Dec  8 05:33:11.512: INFO: namespace e2e-tests-container-lifecycle-hook-t9vbq deletion completed in 22.194104975s

• [SLOW TEST:212.485 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:33:11.513: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-tlh4q
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-c09b6482-faaa-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:33:11.779: INFO: Waiting up to 5m0s for pod "pod-secrets-c09c52d7-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-secrets-tlh4q" to be "success or failure"
Dec  8 05:33:11.787: INFO: Pod "pod-secrets-c09c52d7-faaa-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.888986ms
Dec  8 05:33:13.793: INFO: Pod "pod-secrets-c09c52d7-faaa-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013254754s
STEP: Saw pod success
Dec  8 05:33:13.793: INFO: Pod "pod-secrets-c09c52d7-faaa-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:33:13.797: INFO: Trying to get logs from node phrs-one-falcon pod pod-secrets-c09c52d7-faaa-11e8-b680-ca3d6f40e675 container secret-env-test: <nil>
STEP: delete the pod
Dec  8 05:33:13.848: INFO: Waiting for pod pod-secrets-c09c52d7-faaa-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:33:13.852: INFO: Pod pod-secrets-c09c52d7-faaa-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:33:13.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tlh4q" for this suite.
Dec  8 05:33:19.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:33:19.941: INFO: namespace: e2e-tests-secrets-tlh4q, resource: bindings, ignored listing per whitelist
Dec  8 05:33:20.082: INFO: namespace e2e-tests-secrets-tlh4q deletion completed in 6.215210574s

• [SLOW TEST:8.570 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:33:20.082: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bvmr4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:33:20.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5bc27e8-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-bvmr4" to be "success or failure"
Dec  8 05:33:20.390: INFO: Pod "downwardapi-volume-c5bc27e8-faaa-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 11.549467ms
Dec  8 05:33:22.395: INFO: Pod "downwardapi-volume-c5bc27e8-faaa-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016665044s
STEP: Saw pod success
Dec  8 05:33:22.395: INFO: Pod "downwardapi-volume-c5bc27e8-faaa-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:33:22.399: INFO: Trying to get logs from node phrs-glorious-mastodon pod downwardapi-volume-c5bc27e8-faaa-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:33:22.437: INFO: Waiting for pod downwardapi-volume-c5bc27e8-faaa-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:33:22.442: INFO: Pod downwardapi-volume-c5bc27e8-faaa-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:33:22.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bvmr4" for this suite.
Dec  8 05:33:28.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:33:28.565: INFO: namespace: e2e-tests-projected-bvmr4, resource: bindings, ignored listing per whitelist
Dec  8 05:33:28.588: INFO: namespace e2e-tests-projected-bvmr4 deletion completed in 6.140492407s

• [SLOW TEST:8.506 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:33:28.588: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-hq5nb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-cac3d0dc-faaa-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:33:28.821: INFO: Waiting up to 5m0s for pod "pod-secrets-cac4db0a-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-secrets-hq5nb" to be "success or failure"
Dec  8 05:33:28.826: INFO: Pod "pod-secrets-cac4db0a-faaa-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.085143ms
Dec  8 05:33:30.830: INFO: Pod "pod-secrets-cac4db0a-faaa-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009331821s
STEP: Saw pod success
Dec  8 05:33:30.831: INFO: Pod "pod-secrets-cac4db0a-faaa-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:33:30.834: INFO: Trying to get logs from node phrs-liberal-perch pod pod-secrets-cac4db0a-faaa-11e8-b680-ca3d6f40e675 container secret-volume-test: <nil>
STEP: delete the pod
Dec  8 05:33:30.875: INFO: Waiting for pod pod-secrets-cac4db0a-faaa-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:33:30.879: INFO: Pod pod-secrets-cac4db0a-faaa-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:33:30.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-hq5nb" for this suite.
Dec  8 05:33:36.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:33:36.964: INFO: namespace: e2e-tests-secrets-hq5nb, resource: bindings, ignored listing per whitelist
Dec  8 05:33:37.045: INFO: namespace e2e-tests-secrets-hq5nb deletion completed in 6.159806792s

• [SLOW TEST:8.457 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:33:37.045: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gflg2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-cfd013ca-faaa-11e8-b680-ca3d6f40e675
STEP: Creating secret with name secret-projected-all-test-volume-cfd013ac-faaa-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  8 05:33:37.294: INFO: Waiting up to 5m0s for pod "projected-volume-cfd01364-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-gflg2" to be "success or failure"
Dec  8 05:33:37.304: INFO: Pod "projected-volume-cfd01364-faaa-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008601ms
Dec  8 05:33:39.308: INFO: Pod "projected-volume-cfd01364-faaa-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01412005s
STEP: Saw pod success
Dec  8 05:33:39.308: INFO: Pod "projected-volume-cfd01364-faaa-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:33:39.312: INFO: Trying to get logs from node phrs-one-falcon pod projected-volume-cfd01364-faaa-11e8-b680-ca3d6f40e675 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  8 05:33:39.339: INFO: Waiting for pod projected-volume-cfd01364-faaa-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:33:39.343: INFO: Pod projected-volume-cfd01364-faaa-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:33:39.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gflg2" for this suite.
Dec  8 05:33:45.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:33:45.397: INFO: namespace: e2e-tests-projected-gflg2, resource: bindings, ignored listing per whitelist
Dec  8 05:33:45.504: INFO: namespace e2e-tests-projected-gflg2 deletion completed in 6.155808468s

• [SLOW TEST:8.459 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:33:45.505: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-ghxxk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  8 05:33:48.283: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d4d96132-faaa-11e8-b680-ca3d6f40e675"
Dec  8 05:33:48.283: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d4d96132-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-pods-ghxxk" to be "terminated due to deadline exceeded"
Dec  8 05:33:48.287: INFO: Pod "pod-update-activedeadlineseconds-d4d96132-faaa-11e8-b680-ca3d6f40e675": Phase="Running", Reason="", readiness=true. Elapsed: 4.133261ms
Dec  8 05:33:50.294: INFO: Pod "pod-update-activedeadlineseconds-d4d96132-faaa-11e8-b680-ca3d6f40e675": Phase="Running", Reason="", readiness=true. Elapsed: 2.011460084s
Dec  8 05:33:52.303: INFO: Pod "pod-update-activedeadlineseconds-d4d96132-faaa-11e8-b680-ca3d6f40e675": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.020686479s
Dec  8 05:33:52.303: INFO: Pod "pod-update-activedeadlineseconds-d4d96132-faaa-11e8-b680-ca3d6f40e675" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:33:52.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-ghxxk" for this suite.
Dec  8 05:33:58.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:33:58.448: INFO: namespace: e2e-tests-pods-ghxxk, resource: bindings, ignored listing per whitelist
Dec  8 05:33:58.494: INFO: namespace e2e-tests-pods-ghxxk deletion completed in 6.175158838s

• [SLOW TEST:12.989 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:33:58.494: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-pfrsw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-dc955d4d-faaa-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 05:33:58.715: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc960ea9-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-configmap-pfrsw" to be "success or failure"
Dec  8 05:33:58.735: INFO: Pod "pod-configmaps-dc960ea9-faaa-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 20.040941ms
Dec  8 05:34:00.739: INFO: Pod "pod-configmaps-dc960ea9-faaa-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023960243s
STEP: Saw pod success
Dec  8 05:34:00.739: INFO: Pod "pod-configmaps-dc960ea9-faaa-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:34:00.743: INFO: Trying to get logs from node phrs-liberal-perch pod pod-configmaps-dc960ea9-faaa-11e8-b680-ca3d6f40e675 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 05:34:00.763: INFO: Waiting for pod pod-configmaps-dc960ea9-faaa-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:34:00.768: INFO: Pod pod-configmaps-dc960ea9-faaa-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:34:00.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-pfrsw" for this suite.
Dec  8 05:34:06.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:34:06.957: INFO: namespace: e2e-tests-configmap-pfrsw, resource: bindings, ignored listing per whitelist
Dec  8 05:34:06.957: INFO: namespace e2e-tests-configmap-pfrsw deletion completed in 6.184960382s

• [SLOW TEST:8.463 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:34:06.957: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-k9swl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:34:07.210: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1a4841f-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-k9swl" to be "success or failure"
Dec  8 05:34:07.227: INFO: Pod "downwardapi-volume-e1a4841f-faaa-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 16.81178ms
Dec  8 05:34:09.232: INFO: Pod "downwardapi-volume-e1a4841f-faaa-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022019376s
STEP: Saw pod success
Dec  8 05:34:09.232: INFO: Pod "downwardapi-volume-e1a4841f-faaa-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:34:09.236: INFO: Trying to get logs from node phrs-one-falcon pod downwardapi-volume-e1a4841f-faaa-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:34:09.269: INFO: Waiting for pod downwardapi-volume-e1a4841f-faaa-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:34:09.274: INFO: Pod downwardapi-volume-e1a4841f-faaa-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:34:09.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k9swl" for this suite.
Dec  8 05:34:15.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:34:15.429: INFO: namespace: e2e-tests-projected-k9swl, resource: bindings, ignored listing per whitelist
Dec  8 05:34:15.489: INFO: namespace e2e-tests-projected-k9swl deletion completed in 6.209290282s

• [SLOW TEST:8.532 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:34:15.490: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-6cvb9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-6cvb9/secret-test-e6bb35a8-faaa-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:34:15.738: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6bbefe0-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-secrets-6cvb9" to be "success or failure"
Dec  8 05:34:15.744: INFO: Pod "pod-configmaps-e6bbefe0-faaa-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509641ms
Dec  8 05:34:17.749: INFO: Pod "pod-configmaps-e6bbefe0-faaa-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010316077s
STEP: Saw pod success
Dec  8 05:34:17.749: INFO: Pod "pod-configmaps-e6bbefe0-faaa-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:34:17.753: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-configmaps-e6bbefe0-faaa-11e8-b680-ca3d6f40e675 container env-test: <nil>
STEP: delete the pod
Dec  8 05:34:17.782: INFO: Waiting for pod pod-configmaps-e6bbefe0-faaa-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:34:17.786: INFO: Pod pod-configmaps-e6bbefe0-faaa-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:34:17.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-6cvb9" for this suite.
Dec  8 05:34:23.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:34:23.885: INFO: namespace: e2e-tests-secrets-6cvb9, resource: bindings, ignored listing per whitelist
Dec  8 05:34:24.011: INFO: namespace e2e-tests-secrets-6cvb9 deletion completed in 6.213330876s

• [SLOW TEST:8.521 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:34:24.011: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2qf7r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  8 05:34:24.257: INFO: Waiting up to 5m0s for pod "pod-ebcf1af9-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-2qf7r" to be "success or failure"
Dec  8 05:34:24.280: INFO: Pod "pod-ebcf1af9-faaa-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 22.68048ms
Dec  8 05:34:26.285: INFO: Pod "pod-ebcf1af9-faaa-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028041531s
STEP: Saw pod success
Dec  8 05:34:26.285: INFO: Pod "pod-ebcf1af9-faaa-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:34:26.290: INFO: Trying to get logs from node phrs-liberal-perch pod pod-ebcf1af9-faaa-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:34:26.326: INFO: Waiting for pod pod-ebcf1af9-faaa-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:34:26.329: INFO: Pod pod-ebcf1af9-faaa-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:34:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2qf7r" for this suite.
Dec  8 05:34:32.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:34:32.436: INFO: namespace: e2e-tests-emptydir-2qf7r, resource: bindings, ignored listing per whitelist
Dec  8 05:34:32.527: INFO: namespace e2e-tests-emptydir-2qf7r deletion completed in 6.191586278s

• [SLOW TEST:8.517 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:34:32.528: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-pxrz9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-f0dede84-faaa-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:34:32.753: INFO: Waiting up to 5m0s for pod "pod-secrets-f0dfd815-faaa-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-secrets-pxrz9" to be "success or failure"
Dec  8 05:34:32.760: INFO: Pod "pod-secrets-f0dfd815-faaa-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.465662ms
Dec  8 05:34:34.766: INFO: Pod "pod-secrets-f0dfd815-faaa-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013056362s
STEP: Saw pod success
Dec  8 05:34:34.766: INFO: Pod "pod-secrets-f0dfd815-faaa-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:34:34.769: INFO: Trying to get logs from node phrs-one-falcon pod pod-secrets-f0dfd815-faaa-11e8-b680-ca3d6f40e675 container secret-volume-test: <nil>
STEP: delete the pod
Dec  8 05:34:34.813: INFO: Waiting for pod pod-secrets-f0dfd815-faaa-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:34:34.817: INFO: Pod pod-secrets-f0dfd815-faaa-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:34:34.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pxrz9" for this suite.
Dec  8 05:34:40.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:34:40.934: INFO: namespace: e2e-tests-secrets-pxrz9, resource: bindings, ignored listing per whitelist
Dec  8 05:34:40.964: INFO: namespace e2e-tests-secrets-pxrz9 deletion completed in 6.142470907s

• [SLOW TEST:8.437 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:34:40.965: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-l6jz4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-mtgln
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Dec  8 05:34:45.429: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-khvz6
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:35:09.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-l6jz4" for this suite.
Dec  8 05:35:15.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:35:15.786: INFO: namespace: e2e-tests-namespaces-l6jz4, resource: bindings, ignored listing per whitelist
Dec  8 05:35:15.861: INFO: namespace e2e-tests-namespaces-l6jz4 deletion completed in 6.190432993s
STEP: Destroying namespace "e2e-tests-nsdeletetest-mtgln" for this suite.
Dec  8 05:35:15.867: INFO: Namespace e2e-tests-nsdeletetest-mtgln was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-khvz6" for this suite.
Dec  8 05:35:21.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:35:22.100: INFO: namespace: e2e-tests-nsdeletetest-khvz6, resource: bindings, ignored listing per whitelist
Dec  8 05:35:22.119: INFO: namespace e2e-tests-nsdeletetest-khvz6 deletion completed in 6.251904096s

• [SLOW TEST:41.154 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:35:22.119: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5mlv6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:35:22.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 version'
Dec  8 05:35:22.471: INFO: stderr: ""
Dec  8 05:35:22.471: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.1\", GitCommit:\"4ed3216f3ec431b140b1d899130a69fc671678f4\", GitTreeState:\"clean\", BuildDate:\"2018-10-05T16:46:06Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.3\", GitCommit:\"435f92c719f279a3a67808c80521ea17d5715c66\", GitTreeState:\"clean\", BuildDate:\"2018-11-26T12:46:57Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:35:22.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5mlv6" for this suite.
Dec  8 05:35:28.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:35:28.571: INFO: namespace: e2e-tests-kubectl-5mlv6, resource: bindings, ignored listing per whitelist
Dec  8 05:35:28.702: INFO: namespace e2e-tests-kubectl-5mlv6 deletion completed in 6.21156188s

• [SLOW TEST:6.583 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:35:28.702: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-cqmpp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-125b678e-faab-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 05:35:28.928: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-125c0d93-faab-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-cqmpp" to be "success or failure"
Dec  8 05:35:28.934: INFO: Pod "pod-projected-configmaps-125c0d93-faab-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.394268ms
Dec  8 05:35:30.938: INFO: Pod "pod-projected-configmaps-125c0d93-faab-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01018681s
STEP: Saw pod success
Dec  8 05:35:30.939: INFO: Pod "pod-projected-configmaps-125c0d93-faab-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:35:30.942: INFO: Trying to get logs from node phrs-one-falcon pod pod-projected-configmaps-125c0d93-faab-11e8-b680-ca3d6f40e675 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 05:35:30.972: INFO: Waiting for pod pod-projected-configmaps-125c0d93-faab-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:35:30.975: INFO: Pod pod-projected-configmaps-125c0d93-faab-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:35:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cqmpp" for this suite.
Dec  8 05:35:36.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:35:37.088: INFO: namespace: e2e-tests-projected-cqmpp, resource: bindings, ignored listing per whitelist
Dec  8 05:35:37.118: INFO: namespace e2e-tests-projected-cqmpp deletion completed in 6.138838295s

• [SLOW TEST:8.416 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:35:37.119: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6txbs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Dec  8 05:35:37.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:37.782: INFO: stderr: ""
Dec  8 05:35:37.782: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  8 05:35:37.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:37.887: INFO: stderr: ""
Dec  8 05:35:37.887: INFO: stdout: "update-demo-nautilus-8f9kc update-demo-nautilus-hrhpn "
Dec  8 05:35:37.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-8f9kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:37.983: INFO: stderr: ""
Dec  8 05:35:37.983: INFO: stdout: ""
Dec  8 05:35:37.983: INFO: update-demo-nautilus-8f9kc is created but not running
Dec  8 05:35:42.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:43.091: INFO: stderr: ""
Dec  8 05:35:43.091: INFO: stdout: "update-demo-nautilus-8f9kc update-demo-nautilus-hrhpn "
Dec  8 05:35:43.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-8f9kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:43.195: INFO: stderr: ""
Dec  8 05:35:43.195: INFO: stdout: "true"
Dec  8 05:35:43.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-8f9kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:43.319: INFO: stderr: ""
Dec  8 05:35:43.320: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  8 05:35:43.320: INFO: validating pod update-demo-nautilus-8f9kc
Dec  8 05:35:43.327: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  8 05:35:43.327: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  8 05:35:43.327: INFO: update-demo-nautilus-8f9kc is verified up and running
Dec  8 05:35:43.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-hrhpn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:43.446: INFO: stderr: ""
Dec  8 05:35:43.446: INFO: stdout: "true"
Dec  8 05:35:43.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-hrhpn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:43.579: INFO: stderr: ""
Dec  8 05:35:43.579: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  8 05:35:43.579: INFO: validating pod update-demo-nautilus-hrhpn
Dec  8 05:35:43.587: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  8 05:35:43.587: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  8 05:35:43.587: INFO: update-demo-nautilus-hrhpn is verified up and running
STEP: scaling down the replication controller
Dec  8 05:35:43.589: INFO: scanned /root for discovery docs: <nil>
Dec  8 05:35:43.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:44.768: INFO: stderr: ""
Dec  8 05:35:44.768: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  8 05:35:44.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:44.874: INFO: stderr: ""
Dec  8 05:35:44.874: INFO: stdout: "update-demo-nautilus-8f9kc update-demo-nautilus-hrhpn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  8 05:35:49.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:49.991: INFO: stderr: ""
Dec  8 05:35:49.991: INFO: stdout: "update-demo-nautilus-8f9kc "
Dec  8 05:35:49.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-8f9kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:50.096: INFO: stderr: ""
Dec  8 05:35:50.096: INFO: stdout: "true"
Dec  8 05:35:50.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-8f9kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:50.198: INFO: stderr: ""
Dec  8 05:35:50.198: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  8 05:35:50.198: INFO: validating pod update-demo-nautilus-8f9kc
Dec  8 05:35:50.204: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  8 05:35:50.204: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  8 05:35:50.204: INFO: update-demo-nautilus-8f9kc is verified up and running
STEP: scaling up the replication controller
Dec  8 05:35:50.206: INFO: scanned /root for discovery docs: <nil>
Dec  8 05:35:50.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:51.355: INFO: stderr: ""
Dec  8 05:35:51.355: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  8 05:35:51.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:51.482: INFO: stderr: ""
Dec  8 05:35:51.482: INFO: stdout: "update-demo-nautilus-8f9kc update-demo-nautilus-nnfdd "
Dec  8 05:35:51.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-8f9kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:51.599: INFO: stderr: ""
Dec  8 05:35:51.599: INFO: stdout: "true"
Dec  8 05:35:51.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-8f9kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:51.699: INFO: stderr: ""
Dec  8 05:35:51.699: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  8 05:35:51.699: INFO: validating pod update-demo-nautilus-8f9kc
Dec  8 05:35:51.704: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  8 05:35:51.704: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  8 05:35:51.704: INFO: update-demo-nautilus-8f9kc is verified up and running
Dec  8 05:35:51.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-nnfdd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:51.793: INFO: stderr: ""
Dec  8 05:35:51.793: INFO: stdout: "true"
Dec  8 05:35:51.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods update-demo-nautilus-nnfdd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:51.891: INFO: stderr: ""
Dec  8 05:35:51.891: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  8 05:35:51.891: INFO: validating pod update-demo-nautilus-nnfdd
Dec  8 05:35:51.897: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  8 05:35:51.897: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  8 05:35:51.897: INFO: update-demo-nautilus-nnfdd is verified up and running
STEP: using delete to clean up resources
Dec  8 05:35:51.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:52.013: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 05:35:52.013: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  8 05:35:52.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-6txbs'
Dec  8 05:35:52.127: INFO: stderr: "No resources found.\n"
Dec  8 05:35:52.127: INFO: stdout: ""
Dec  8 05:35:52.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -l name=update-demo --namespace=e2e-tests-kubectl-6txbs -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  8 05:35:52.241: INFO: stderr: ""
Dec  8 05:35:52.241: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:35:52.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6txbs" for this suite.
Dec  8 05:36:14.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:36:14.363: INFO: namespace: e2e-tests-kubectl-6txbs, resource: bindings, ignored listing per whitelist
Dec  8 05:36:14.443: INFO: namespace e2e-tests-kubectl-6txbs deletion completed in 22.19559814s

• [SLOW TEST:37.324 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:36:14.443: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-t49rt
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-2da296d0-faab-11e8-b680-ca3d6f40e675
STEP: Creating configMap with name cm-test-opt-upd-2da29720-faab-11e8-b680-ca3d6f40e675
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2da296d0-faab-11e8-b680-ca3d6f40e675
STEP: Updating configmap cm-test-opt-upd-2da29720-faab-11e8-b680-ca3d6f40e675
STEP: Creating configMap with name cm-test-opt-create-2da29740-faab-11e8-b680-ca3d6f40e675
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:36:20.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-t49rt" for this suite.
Dec  8 05:36:42.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:36:43.031: INFO: namespace: e2e-tests-configmap-t49rt, resource: bindings, ignored listing per whitelist
Dec  8 05:36:43.055: INFO: namespace e2e-tests-configmap-t49rt deletion completed in 22.199021891s

• [SLOW TEST:28.613 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:36:43.056: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-hq64r
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1208 05:36:49.320356      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  8 05:36:49.320: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:36:49.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-hq64r" for this suite.
Dec  8 05:36:55.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:36:55.477: INFO: namespace: e2e-tests-gc-hq64r, resource: bindings, ignored listing per whitelist
Dec  8 05:36:55.480: INFO: namespace e2e-tests-gc-hq64r deletion completed in 6.154943704s

• [SLOW TEST:12.424 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:36:55.480: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mwcz5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:36:55.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46151784-faab-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-mwcz5" to be "success or failure"
Dec  8 05:36:55.719: INFO: Pod "downwardapi-volume-46151784-faab-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 11.783177ms
Dec  8 05:36:57.723: INFO: Pod "downwardapi-volume-46151784-faab-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01561609s
STEP: Saw pod success
Dec  8 05:36:57.723: INFO: Pod "downwardapi-volume-46151784-faab-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:36:57.726: INFO: Trying to get logs from node phrs-one-falcon pod downwardapi-volume-46151784-faab-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:36:57.753: INFO: Waiting for pod downwardapi-volume-46151784-faab-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:36:57.760: INFO: Pod downwardapi-volume-46151784-faab-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:36:57.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mwcz5" for this suite.
Dec  8 05:37:03.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:37:03.845: INFO: namespace: e2e-tests-projected-mwcz5, resource: bindings, ignored listing per whitelist
Dec  8 05:37:03.988: INFO: namespace e2e-tests-projected-mwcz5 deletion completed in 6.222801364s

• [SLOW TEST:8.508 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:37:03.989: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-zntcv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  8 05:37:04.376: INFO: Waiting up to 5m0s for pod "pod-4b3ffca1-faab-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-zntcv" to be "success or failure"
Dec  8 05:37:04.381: INFO: Pod "pod-4b3ffca1-faab-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.527194ms
Dec  8 05:37:06.386: INFO: Pod "pod-4b3ffca1-faab-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010616075s
Dec  8 05:37:08.391: INFO: Pod "pod-4b3ffca1-faab-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015547866s
STEP: Saw pod success
Dec  8 05:37:08.392: INFO: Pod "pod-4b3ffca1-faab-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:37:08.396: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-4b3ffca1-faab-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:37:08.435: INFO: Waiting for pod pod-4b3ffca1-faab-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:37:08.439: INFO: Pod pod-4b3ffca1-faab-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:37:08.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-zntcv" for this suite.
Dec  8 05:37:14.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:37:14.554: INFO: namespace: e2e-tests-emptydir-zntcv, resource: bindings, ignored listing per whitelist
Dec  8 05:37:14.654: INFO: namespace e2e-tests-emptydir-zntcv deletion completed in 6.207802273s

• [SLOW TEST:10.665 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:37:14.654: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-d6njn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:37:14.885: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec  8 05:37:19.891: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  8 05:37:19.891: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  8 05:37:21.896: INFO: Creating deployment "test-rollover-deployment"
Dec  8 05:37:21.910: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  8 05:37:23.920: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  8 05:37:23.948: INFO: Ensure that both replica sets have 1 created replica
Dec  8 05:37:23.976: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  8 05:37:23.999: INFO: Updating deployment test-rollover-deployment
Dec  8 05:37:23.999: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  8 05:37:26.024: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  8 05:37:26.034: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  8 05:37:26.043: INFO: all replica sets need to contain the pod-template-hash label
Dec  8 05:37:26.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844245, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  8 05:37:28.053: INFO: all replica sets need to contain the pod-template-hash label
Dec  8 05:37:28.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844245, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  8 05:37:30.051: INFO: all replica sets need to contain the pod-template-hash label
Dec  8 05:37:30.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844245, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  8 05:37:32.053: INFO: all replica sets need to contain the pod-template-hash label
Dec  8 05:37:32.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844245, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  8 05:37:34.052: INFO: all replica sets need to contain the pod-template-hash label
Dec  8 05:37:34.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844245, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844241, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  8 05:37:36.051: INFO: 
Dec  8 05:37:36.051: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  8 05:37:36.060: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-d6njn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d6njn/deployments/test-rollover-deployment,UID:55b38051-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10222,Generation:2,CreationTimestamp:2018-12-08 05:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2018-12-08 05:37:21 +0000 UTC 2018-12-08 05:37:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2018-12-08 05:37:35 +0000 UTC 2018-12-08 05:37:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  8 05:37:36.064: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-d6njn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d6njn/replicasets/test-rollover-deployment-5b76ff8c4,UID:56f48ffd-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10213,Generation:2,CreationTimestamp:2018-12-08 05:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 55b38051-faab-11e8-a103-96cd63cee3b8 0xc42125c597 0xc42125c598}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  8 05:37:36.064: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  8 05:37:36.064: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-d6njn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d6njn/replicasets/test-rollover-controller,UID:5182dfda-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10221,Generation:2,CreationTimestamp:2018-12-08 05:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 55b38051-faab-11e8-a103-96cd63cee3b8 0xc42125c4ae 0xc42125c4af}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  8 05:37:36.064: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-d6njn,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-d6njn/replicasets/test-rollover-deployment-6975f4fb87,UID:55b9f548-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10182,Generation:2,CreationTimestamp:2018-12-08 05:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 55b38051-faab-11e8-a103-96cd63cee3b8 0xc42125c657 0xc42125c658}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  8 05:37:36.068: INFO: Pod "test-rollover-deployment-5b76ff8c4-jpj6k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-jpj6k,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-d6njn,SelfLink:/api/v1/namespaces/e2e-tests-deployment-d6njn/pods/test-rollover-deployment-5b76ff8c4-jpj6k,UID:56fe6af2-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10193,Generation:0,CreationTimestamp:2018-12-08 05:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 56f48ffd-faab-11e8-a103-96cd63cee3b8 0xc421cb4010 0xc421cb4011}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bwvnp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bwvnp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bwvnp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-one-falcon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421cb4080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421cb40a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:37:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:37:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:37:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:37:24 +0000 UTC  }],Message:,Reason:,HostIP:10.135.44.6,PodIP:172.31.192.8,StartTime:2018-12-08 05:37:24 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2018-12-08 05:37:24 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c41cd54cd7aa2d0698d2365b32e79701211b9c2bac05350c39495c4ab783ca14}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:37:36.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-d6njn" for this suite.
Dec  8 05:37:42.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:37:42.194: INFO: namespace: e2e-tests-deployment-d6njn, resource: bindings, ignored listing per whitelist
Dec  8 05:37:42.248: INFO: namespace e2e-tests-deployment-d6njn deletion completed in 6.175382307s

• [SLOW TEST:27.594 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:37:42.248: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-sd6jp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-61f4d856-faab-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 05:37:42.476: INFO: Waiting up to 5m0s for pod "pod-configmaps-61f5b3f2-faab-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-configmap-sd6jp" to be "success or failure"
Dec  8 05:37:42.490: INFO: Pod "pod-configmaps-61f5b3f2-faab-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 13.790989ms
Dec  8 05:37:44.494: INFO: Pod "pod-configmaps-61f5b3f2-faab-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017604799s
STEP: Saw pod success
Dec  8 05:37:44.494: INFO: Pod "pod-configmaps-61f5b3f2-faab-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:37:44.498: INFO: Trying to get logs from node phrs-liberal-perch pod pod-configmaps-61f5b3f2-faab-11e8-b680-ca3d6f40e675 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 05:37:44.530: INFO: Waiting for pod pod-configmaps-61f5b3f2-faab-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:37:44.533: INFO: Pod pod-configmaps-61f5b3f2-faab-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:37:44.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sd6jp" for this suite.
Dec  8 05:37:50.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:37:50.637: INFO: namespace: e2e-tests-configmap-sd6jp, resource: bindings, ignored listing per whitelist
Dec  8 05:37:50.709: INFO: namespace e2e-tests-configmap-sd6jp deletion completed in 6.171337273s

• [SLOW TEST:8.461 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:37:50.709: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-v2ml6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-v2ml6.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-v2ml6.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-v2ml6.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-v2ml6.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-v2ml6.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-v2ml6.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  8 05:38:17.126: INFO: DNS probes using e2e-tests-dns-v2ml6/dns-test-6705cb8d-faab-11e8-b680-ca3d6f40e675 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:38:17.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-v2ml6" for this suite.
Dec  8 05:38:23.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:38:23.223: INFO: namespace: e2e-tests-dns-v2ml6, resource: bindings, ignored listing per whitelist
Dec  8 05:38:23.357: INFO: namespace e2e-tests-dns-v2ml6 deletion completed in 6.195573745s

• [SLOW TEST:32.648 seconds]
[sig-network] DNS
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:38:23.357: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-qgjdb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:38:23.585: INFO: Creating deployment "nginx-deployment"
Dec  8 05:38:23.599: INFO: Waiting for observed generation 1
Dec  8 05:38:25.608: INFO: Waiting for all required pods to come up
Dec  8 05:38:25.616: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  8 05:38:27.637: INFO: Waiting for deployment "nginx-deployment" to complete
Dec  8 05:38:27.645: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec  8 05:38:27.658: INFO: Updating deployment nginx-deployment
Dec  8 05:38:27.658: INFO: Waiting for observed generation 2
Dec  8 05:38:29.667: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  8 05:38:29.669: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  8 05:38:29.671: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  8 05:38:29.679: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  8 05:38:29.679: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  8 05:38:29.682: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  8 05:38:29.687: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec  8 05:38:29.687: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec  8 05:38:29.695: INFO: Updating deployment nginx-deployment
Dec  8 05:38:29.695: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec  8 05:38:29.707: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  8 05:38:29.718: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  8 05:38:29.760: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qgjdb/deployments/nginx-deployment,UID:7a78815d-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10623,Generation:3,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2018-12-08 05:38:28 +0000 UTC 2018-12-08 05:38:23 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.} {Available False 2018-12-08 05:38:29 +0000 UTC 2018-12-08 05:38:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec  8 05:38:29.770: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qgjdb/replicasets/nginx-deployment-7dc8f79789,UID:7ce5a3bb-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10609,Generation:3,CreationTimestamp:2018-12-08 05:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 7a78815d-faab-11e8-a103-96cd63cee3b8 0xc422411f87 0xc422411f88}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  8 05:38:29.770: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec  8 05:38:29.770: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-qgjdb/replicasets/nginx-deployment-7f9675fb8b,UID:7a7b00ea-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10606,Generation:3,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 7a78815d-faab-11e8-a103-96cd63cee3b8 0xc422820207 0xc422820208}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec  8 05:38:29.855: INFO: Pod "nginx-deployment-7dc8f79789-29w9m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-29w9m,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-29w9m,UID:7e2c6a00-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10638,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc422821927 0xc422821928}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422821990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4228219b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.855: INFO: Pod "nginx-deployment-7dc8f79789-4tphl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-4tphl,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-4tphl,UID:7cf783d7-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10585,Generation:0,CreationTimestamp:2018-12-08 05:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc422821a07 0xc422821a08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-liberal-perch,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422821a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422821a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  }],Message:,Reason:,HostIP:10.135.11.179,PodIP:,StartTime:2018-12-08 05:38:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.856: INFO: Pod "nginx-deployment-7dc8f79789-5gk5h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-5gk5h,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-5gk5h,UID:7e2288b0-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10629,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc422821c20 0xc422821c21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-liberal-perch,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422821c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422821cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:29 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.856: INFO: Pod "nginx-deployment-7dc8f79789-7r7fn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-7r7fn,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-7r7fn,UID:7e2d812e-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10630,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc422821d20 0xc422821d21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422821de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422821e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.856: INFO: Pod "nginx-deployment-7dc8f79789-8fvnw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-8fvnw,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-8fvnw,UID:7ce74391-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10563,Generation:0,CreationTimestamp:2018-12-08 05:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc422821e57 0xc422821e58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-liberal-perch,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422821ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422821f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  }],Message:,Reason:,HostIP:10.135.11.179,PodIP:,StartTime:2018-12-08 05:38:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.856: INFO: Pod "nginx-deployment-7dc8f79789-hcpbp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-hcpbp,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-hcpbp,UID:7ceb4e24-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10634,Generation:0,CreationTimestamp:2018-12-08 05:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc422821fc0 0xc422821fc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-glorious-mastodon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4223ee5b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223ee5d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  }],Message:,Reason:,HostIP:10.135.14.74,PodIP:172.31.0.10,StartTime:2018-12-08 05:38:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.856: INFO: Pod "nginx-deployment-7dc8f79789-krqz5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-krqz5,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-krqz5,UID:7e1eeb57-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10614,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc4223eeac0 0xc4223eeac1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-glorious-mastodon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4223eeb30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223eed00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:29 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.856: INFO: Pod "nginx-deployment-7dc8f79789-mn4zq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-mn4zq,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-mn4zq,UID:7e229ae1-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10632,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc4223eedb0 0xc4223eedb1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-one-falcon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4223ef1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223ef320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:29 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.857: INFO: Pod "nginx-deployment-7dc8f79789-prmlz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-prmlz,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-prmlz,UID:7e2dc1d3-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10633,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc4223ef390 0xc4223ef391}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4223ef400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223ef490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.857: INFO: Pod "nginx-deployment-7dc8f79789-v76jh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-v76jh,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-v76jh,UID:7e2d7149-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10631,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc4223ef557 0xc4223ef558}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4223ef6a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223ef6c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.857: INFO: Pod "nginx-deployment-7dc8f79789-vh5fg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-vh5fg,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-vh5fg,UID:7ceb9568-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10570,Generation:0,CreationTimestamp:2018-12-08 05:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc4223ef717 0xc4223ef718}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-one-falcon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4223ef880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223ef8a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  }],Message:,Reason:,HostIP:10.135.44.6,PodIP:,StartTime:2018-12-08 05:38:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.857: INFO: Pod "nginx-deployment-7dc8f79789-z9bd8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-z9bd8,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7dc8f79789-z9bd8,UID:7cff1b66-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10592,Generation:0,CreationTimestamp:2018-12-08 05:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 7ce5a3bb-faab-11e8-a103-96cd63cee3b8 0xc4223ef960 0xc4223ef961}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-one-falcon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4223ef9e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223efa60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:27 +0000 UTC  }],Message:,Reason:,HostIP:10.135.44.6,PodIP:,StartTime:2018-12-08 05:38:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.857: INFO: Pod "nginx-deployment-7f9675fb8b-44hnp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-44hnp,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-44hnp,UID:7a86df25-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10521,Generation:0,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc4223efc70 0xc4223efc71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-glorious-mastodon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4223eff00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4223eff20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  }],Message:,Reason:,HostIP:10.135.14.74,PodIP:172.31.0.12,StartTime:2018-12-08 05:38:23 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-08 05:38:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://8186e537d91e3141aef299f13dcdcc3005f3d06394565b7bb14b0830db67d549}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.858: INFO: Pod "nginx-deployment-7f9675fb8b-4s7g8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-4s7g8,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-4s7g8,UID:7e2d90d2-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10636,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc4223effe0 0xc4223effe1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422278450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422278500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.858: INFO: Pod "nginx-deployment-7f9675fb8b-59wns" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-59wns,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-59wns,UID:7e2d3fe7-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10639,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc422278a27 0xc422278a28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422278c50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422278d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.858: INFO: Pod "nginx-deployment-7f9675fb8b-5wlsm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-5wlsm,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-5wlsm,UID:7a85e58d-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10495,Generation:0,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc422279147 0xc422279148}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-one-falcon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422279350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422279370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  }],Message:,Reason:,HostIP:10.135.44.6,PodIP:172.31.192.10,StartTime:2018-12-08 05:38:23 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-08 05:38:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://a53deba64fb0f60cb0c3c32ce0f0f738013b02d7d9de397a5183c3fa1091a439}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.858: INFO: Pod "nginx-deployment-7f9675fb8b-6lg5x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-6lg5x,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-6lg5x,UID:7e2c4873-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10637,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc422279710 0xc422279711}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422279a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422279aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.858: INFO: Pod "nginx-deployment-7f9675fb8b-6xgz4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-6xgz4,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-6xgz4,UID:7a86ec7e-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10524,Generation:0,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc422279af7 0xc422279af8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-glorious-mastodon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc422279d10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc422279d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  }],Message:,Reason:,HostIP:10.135.14.74,PodIP:172.31.0.11,StartTime:2018-12-08 05:38:23 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-08 05:38:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://c014d9f07e66c74557ff82d09a73522a475bd8513f30355646f290bb9761f44f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.858: INFO: Pod "nginx-deployment-7f9675fb8b-ctvsw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-ctvsw,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-ctvsw,UID:7e203523-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10622,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc42222c230 0xc42222c231}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-one-falcon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42222c4c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42222c4f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:29 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.859: INFO: Pod "nginx-deployment-7f9675fb8b-ctwtg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-ctwtg,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-ctwtg,UID:7a867851-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10499,Generation:0,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc42222c950 0xc42222c951}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-liberal-perch,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42222c9b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42222c9d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  }],Message:,Reason:,HostIP:10.135.11.179,PodIP:172.31.240.8,StartTime:2018-12-08 05:38:23 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-08 05:38:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://c43a6d55ce45ba0575a81b3323aea44b87e4436ab9ea412e9962e7a80f790a03}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.859: INFO: Pod "nginx-deployment-7f9675fb8b-kcnpj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-kcnpj,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-kcnpj,UID:7a99ef0d-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10501,Generation:0,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc42222cba7 0xc42222cba8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-liberal-perch,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42222cc10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42222cc90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  }],Message:,Reason:,HostIP:10.135.11.179,PodIP:172.31.240.9,StartTime:2018-12-08 05:38:23 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-08 05:38:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://dd577c64fa3fc679ef9270750202eb49df7d9596032c821fa9be215e893e4e43}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.859: INFO: Pod "nginx-deployment-7f9675fb8b-mzrfx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-mzrfx,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-mzrfx,UID:7e2bd907-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10635,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc42222d017 0xc42222d018}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42222d0f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42222d110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.859: INFO: Pod "nginx-deployment-7f9675fb8b-ngq2j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-ngq2j,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-ngq2j,UID:7a80ed50-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10505,Generation:0,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc42222d167 0xc42222d168}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-liberal-perch,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42222d2e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42222d300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  }],Message:,Reason:,HostIP:10.135.11.179,PodIP:172.31.240.7,StartTime:2018-12-08 05:38:23 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-08 05:38:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://72efc22b066f464cd9ffc1a95914cf4c5d9c4cb1cbe150150132796f0395d67a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.859: INFO: Pod "nginx-deployment-7f9675fb8b-pzvpj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-pzvpj,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-pzvpj,UID:7a9a21b9-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10535,Generation:0,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc42222d3c7 0xc42222d3c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-one-falcon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42222d5b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42222d640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  }],Message:,Reason:,HostIP:10.135.44.6,PodIP:172.31.192.11,StartTime:2018-12-08 05:38:23 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-08 05:38:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://06b144d290ec153c81671530de550b7672ca573bd066ca67baacd61fb2b7b12b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.859: INFO: Pod "nginx-deployment-7f9675fb8b-w2m9p" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-w2m9p,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-w2m9p,UID:7a99ba85-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10532,Generation:0,CreationTimestamp:2018-12-08 05:38:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc42222d770 0xc42222d771}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-glorious-mastodon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42222d7d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42222d7f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:23 +0000 UTC  }],Message:,Reason:,HostIP:10.135.14.74,PodIP:172.31.0.8,StartTime:2018-12-08 05:38:23 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-08 05:38:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 docker://4698c2ab571e180d0f9aaf5d3c4266181ced2b563cc19f0aa4df42d2948e9823}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.860: INFO: Pod "nginx-deployment-7f9675fb8b-wdcs4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-wdcs4,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-wdcs4,UID:7e20354b-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10626,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc42222d8b0 0xc42222d8b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-glorious-mastodon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42222d9e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42222da10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:29 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  8 05:38:29.860: INFO: Pod "nginx-deployment-7f9675fb8b-x42bs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-x42bs,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-qgjdb,SelfLink:/api/v1/namespaces/e2e-tests-deployment-qgjdb/pods/nginx-deployment-7f9675fb8b-x42bs,UID:7e1d1beb-faab-11e8-a103-96cd63cee3b8,ResourceVersion:10640,Generation:0,CreationTimestamp:2018-12-08 05:38:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 7a7b00ea-faab-11e8-a103-96cd63cee3b8 0xc42222da80 0xc42222da81}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-r86qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r86qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r86qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-one-falcon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc42222dba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc42222dbd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:38:29 +0000 UTC  }],Message:,Reason:,HostIP:10.135.44.6,PodIP:,StartTime:2018-12-08 05:38:29 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:38:29.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-qgjdb" for this suite.
Dec  8 05:38:38.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:38:38.126: INFO: namespace: e2e-tests-deployment-qgjdb, resource: bindings, ignored listing per whitelist
Dec  8 05:38:38.217: INFO: namespace e2e-tests-deployment-qgjdb deletion completed in 8.198757651s

• [SLOW TEST:14.860 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:38:38.218: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tv75k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:38:38.453: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83530e10-faab-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-tv75k" to be "success or failure"
Dec  8 05:38:38.463: INFO: Pod "downwardapi-volume-83530e10-faab-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 9.89445ms
Dec  8 05:38:40.467: INFO: Pod "downwardapi-volume-83530e10-faab-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014004665s
STEP: Saw pod success
Dec  8 05:38:40.468: INFO: Pod "downwardapi-volume-83530e10-faab-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:38:40.470: INFO: Trying to get logs from node phrs-one-falcon pod downwardapi-volume-83530e10-faab-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:38:40.496: INFO: Waiting for pod downwardapi-volume-83530e10-faab-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:38:40.499: INFO: Pod downwardapi-volume-83530e10-faab-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:38:40.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tv75k" for this suite.
Dec  8 05:38:46.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:38:46.561: INFO: namespace: e2e-tests-downward-api-tv75k, resource: bindings, ignored listing per whitelist
Dec  8 05:38:46.677: INFO: namespace e2e-tests-downward-api-tv75k deletion completed in 6.172505541s

• [SLOW TEST:8.459 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:38:46.677: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-vztkv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-vztkv
Dec  8 05:38:48.908: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-vztkv
STEP: checking the pod's current state and verifying that restartCount is present
Dec  8 05:38:48.911: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:42:49.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-vztkv" for this suite.
Dec  8 05:42:55.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:42:55.614: INFO: namespace: e2e-tests-container-probe-vztkv, resource: bindings, ignored listing per whitelist
Dec  8 05:42:55.713: INFO: namespace e2e-tests-container-probe-vztkv deletion completed in 6.162263173s

• [SLOW TEST:249.036 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:42:55.713: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-9wkk2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1208 05:43:05.946613      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  8 05:43:05.946: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:43:05.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-9wkk2" for this suite.
Dec  8 05:43:11.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:43:12.068: INFO: namespace: e2e-tests-gc-9wkk2, resource: bindings, ignored listing per whitelist
Dec  8 05:43:12.149: INFO: namespace e2e-tests-gc-9wkk2 deletion completed in 6.196112581s

• [SLOW TEST:16.436 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:43:12.150: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-4cg29
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-qj8k
STEP: Creating a pod to test atomic-volume-subpath
Dec  8 05:43:12.393: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qj8k" in namespace "e2e-tests-subpath-4cg29" to be "success or failure"
Dec  8 05:43:12.406: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Pending", Reason="", readiness=false. Elapsed: 13.471039ms
Dec  8 05:43:14.411: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018206951s
Dec  8 05:43:16.416: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 4.023622582s
Dec  8 05:43:18.421: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 6.028489014s
Dec  8 05:43:20.427: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 8.034063098s
Dec  8 05:43:22.432: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 10.039620303s
Dec  8 05:43:24.438: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 12.045228157s
Dec  8 05:43:26.443: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 14.050697592s
Dec  8 05:43:28.449: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 16.05594041s
Dec  8 05:43:30.453: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 18.059865589s
Dec  8 05:43:32.457: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 20.064267217s
Dec  8 05:43:34.484: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Running", Reason="", readiness=false. Elapsed: 22.091007122s
Dec  8 05:43:36.488: INFO: Pod "pod-subpath-test-configmap-qj8k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.095450195s
STEP: Saw pod success
Dec  8 05:43:36.488: INFO: Pod "pod-subpath-test-configmap-qj8k" satisfied condition "success or failure"
Dec  8 05:43:36.491: INFO: Trying to get logs from node phrs-one-falcon pod pod-subpath-test-configmap-qj8k container test-container-subpath-configmap-qj8k: <nil>
STEP: delete the pod
Dec  8 05:43:36.519: INFO: Waiting for pod pod-subpath-test-configmap-qj8k to disappear
Dec  8 05:43:36.523: INFO: Pod pod-subpath-test-configmap-qj8k no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qj8k
Dec  8 05:43:36.523: INFO: Deleting pod "pod-subpath-test-configmap-qj8k" in namespace "e2e-tests-subpath-4cg29"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:43:36.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4cg29" for this suite.
Dec  8 05:43:42.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:43:42.560: INFO: namespace: e2e-tests-subpath-4cg29, resource: bindings, ignored listing per whitelist
Dec  8 05:43:42.684: INFO: namespace e2e-tests-subpath-4cg29 deletion completed in 6.153531821s

• [SLOW TEST:30.535 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:43:42.685: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-ls7m8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  8 05:43:42.913: INFO: Waiting up to 5m0s for pod "pod-38cc91a7-faac-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-ls7m8" to be "success or failure"
Dec  8 05:43:42.925: INFO: Pod "pod-38cc91a7-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 11.932832ms
Dec  8 05:43:44.930: INFO: Pod "pod-38cc91a7-faac-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017029382s
STEP: Saw pod success
Dec  8 05:43:44.930: INFO: Pod "pod-38cc91a7-faac-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:43:44.934: INFO: Trying to get logs from node phrs-liberal-perch pod pod-38cc91a7-faac-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:43:44.967: INFO: Waiting for pod pod-38cc91a7-faac-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:43:44.970: INFO: Pod pod-38cc91a7-faac-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:43:44.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ls7m8" for this suite.
Dec  8 05:43:50.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:43:51.101: INFO: namespace: e2e-tests-emptydir-ls7m8, resource: bindings, ignored listing per whitelist
Dec  8 05:43:51.123: INFO: namespace e2e-tests-emptydir-ls7m8 deletion completed in 6.147718894s

• [SLOW TEST:8.438 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:43:51.123: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-j84g5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:43:51.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3dd1c569-faac-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-j84g5" to be "success or failure"
Dec  8 05:43:51.352: INFO: Pod "downwardapi-volume-3dd1c569-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 9.571217ms
Dec  8 05:43:53.356: INFO: Pod "downwardapi-volume-3dd1c569-faac-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013771354s
STEP: Saw pod success
Dec  8 05:43:53.356: INFO: Pod "downwardapi-volume-3dd1c569-faac-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:43:53.360: INFO: Trying to get logs from node phrs-one-falcon pod downwardapi-volume-3dd1c569-faac-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:43:53.390: INFO: Waiting for pod downwardapi-volume-3dd1c569-faac-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:43:53.394: INFO: Pod downwardapi-volume-3dd1c569-faac-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:43:53.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-j84g5" for this suite.
Dec  8 05:43:59.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:43:59.534: INFO: namespace: e2e-tests-downward-api-j84g5, resource: bindings, ignored listing per whitelist
Dec  8 05:43:59.551: INFO: namespace e2e-tests-downward-api-j84g5 deletion completed in 6.153555597s

• [SLOW TEST:8.428 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:43:59.551: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-swvzq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  8 05:43:59.770: INFO: Waiting up to 5m0s for pod "downward-api-42d84948-faac-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-swvzq" to be "success or failure"
Dec  8 05:43:59.782: INFO: Pod "downward-api-42d84948-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 12.057571ms
Dec  8 05:44:01.787: INFO: Pod "downward-api-42d84948-faac-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01650868s
STEP: Saw pod success
Dec  8 05:44:01.787: INFO: Pod "downward-api-42d84948-faac-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:44:01.791: INFO: Trying to get logs from node phrs-glorious-mastodon pod downward-api-42d84948-faac-11e8-b680-ca3d6f40e675 container dapi-container: <nil>
STEP: delete the pod
Dec  8 05:44:01.824: INFO: Waiting for pod downward-api-42d84948-faac-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:44:01.828: INFO: Pod downward-api-42d84948-faac-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:44:01.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-swvzq" for this suite.
Dec  8 05:44:07.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:44:07.881: INFO: namespace: e2e-tests-downward-api-swvzq, resource: bindings, ignored listing per whitelist
Dec  8 05:44:08.049: INFO: namespace e2e-tests-downward-api-swvzq deletion completed in 6.216158277s

• [SLOW TEST:8.498 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:44:08.050: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-9jmk5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  8 05:44:08.285: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:44:12.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-9jmk5" for this suite.
Dec  8 05:44:18.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:44:18.292: INFO: namespace: e2e-tests-init-container-9jmk5, resource: bindings, ignored listing per whitelist
Dec  8 05:44:18.401: INFO: namespace e2e-tests-init-container-9jmk5 deletion completed in 6.190803368s

• [SLOW TEST:10.351 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:44:18.401: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-hbkhs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  8 05:44:22.697: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:22.714: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:24.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:24.719: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:26.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:26.719: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:28.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:28.719: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:30.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:30.719: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:32.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:32.718: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:34.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:34.718: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:36.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:36.718: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:38.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:38.718: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:40.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:40.718: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  8 05:44:42.714: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  8 05:44:42.718: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:44:42.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-hbkhs" for this suite.
Dec  8 05:45:04.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:45:04.817: INFO: namespace: e2e-tests-container-lifecycle-hook-hbkhs, resource: bindings, ignored listing per whitelist
Dec  8 05:45:04.929: INFO: namespace e2e-tests-container-lifecycle-hook-hbkhs deletion completed in 22.196692077s

• [SLOW TEST:46.528 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:45:04.930: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-58rj9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:45:05.191: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69d6542b-faac-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-58rj9" to be "success or failure"
Dec  8 05:45:05.223: INFO: Pod "downwardapi-volume-69d6542b-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 32.28378ms
Dec  8 05:45:07.228: INFO: Pod "downwardapi-volume-69d6542b-faac-11e8-b680-ca3d6f40e675": Phase="Running", Reason="", readiness=true. Elapsed: 2.037331734s
Dec  8 05:45:09.233: INFO: Pod "downwardapi-volume-69d6542b-faac-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041853032s
STEP: Saw pod success
Dec  8 05:45:09.233: INFO: Pod "downwardapi-volume-69d6542b-faac-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:45:09.237: INFO: Trying to get logs from node phrs-liberal-perch pod downwardapi-volume-69d6542b-faac-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:45:09.276: INFO: Waiting for pod downwardapi-volume-69d6542b-faac-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:45:09.280: INFO: Pod downwardapi-volume-69d6542b-faac-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:45:09.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-58rj9" for this suite.
Dec  8 05:45:15.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:45:15.448: INFO: namespace: e2e-tests-projected-58rj9, resource: bindings, ignored listing per whitelist
Dec  8 05:45:15.529: INFO: namespace e2e-tests-projected-58rj9 deletion completed in 6.242311092s

• [SLOW TEST:10.599 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:45:15.529: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-2qtkc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  8 05:45:15.812: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:45:15.817: INFO: Number of nodes with available pods: 0
Dec  8 05:45:15.817: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:45:16.822: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:45:16.831: INFO: Number of nodes with available pods: 1
Dec  8 05:45:16.831: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 05:45:17.823: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:45:17.828: INFO: Number of nodes with available pods: 3
Dec  8 05:45:17.828: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  8 05:45:17.868: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:45:17.880: INFO: Number of nodes with available pods: 2
Dec  8 05:45:17.880: INFO: Node phrs-one-falcon is running more than one daemon pod
Dec  8 05:45:18.888: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 05:45:18.893: INFO: Number of nodes with available pods: 3
Dec  8 05:45:18.893: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-2qtkc, will wait for the garbage collector to delete the pods
Dec  8 05:45:18.966: INFO: Deleting {extensions DaemonSet} daemon-set took: 10.477ms
Dec  8 05:45:19.067: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.331466ms
Dec  8 05:45:58.470: INFO: Number of nodes with available pods: 0
Dec  8 05:45:58.470: INFO: Number of running nodes: 0, number of available pods: 0
Dec  8 05:45:58.474: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-2qtkc/daemonsets","resourceVersion":"12077"},"items":null}

Dec  8 05:45:58.478: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-2qtkc/pods","resourceVersion":"12077"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:45:58.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-2qtkc" for this suite.
Dec  8 05:46:04.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:46:04.574: INFO: namespace: e2e-tests-daemonsets-2qtkc, resource: bindings, ignored listing per whitelist
Dec  8 05:46:04.695: INFO: namespace e2e-tests-daemonsets-2qtkc deletion completed in 6.196314501s

• [SLOW TEST:49.166 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:46:04.695: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-cfn2w
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  8 05:46:04.972: INFO: Waiting up to 5m0s for pod "pod-8d77a1e6-faac-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-cfn2w" to be "success or failure"
Dec  8 05:46:04.984: INFO: Pod "pod-8d77a1e6-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 12.27428ms
Dec  8 05:46:06.989: INFO: Pod "pod-8d77a1e6-faac-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017175395s
STEP: Saw pod success
Dec  8 05:46:06.989: INFO: Pod "pod-8d77a1e6-faac-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:46:06.995: INFO: Trying to get logs from node phrs-one-falcon pod pod-8d77a1e6-faac-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:46:07.044: INFO: Waiting for pod pod-8d77a1e6-faac-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:46:07.051: INFO: Pod pod-8d77a1e6-faac-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:46:07.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cfn2w" for this suite.
Dec  8 05:46:13.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:46:13.193: INFO: namespace: e2e-tests-emptydir-cfn2w, resource: bindings, ignored listing per whitelist
Dec  8 05:46:13.271: INFO: namespace e2e-tests-emptydir-cfn2w deletion completed in 6.212619758s

• [SLOW TEST:8.576 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:46:13.272: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-cv556
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  8 05:46:13.547: INFO: Waiting up to 5m0s for pod "pod-92943c79-faac-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-cv556" to be "success or failure"
Dec  8 05:46:13.563: INFO: Pod "pod-92943c79-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 15.643163ms
Dec  8 05:46:15.572: INFO: Pod "pod-92943c79-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024734265s
Dec  8 05:46:17.576: INFO: Pod "pod-92943c79-faac-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029259649s
STEP: Saw pod success
Dec  8 05:46:17.576: INFO: Pod "pod-92943c79-faac-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:46:17.580: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-92943c79-faac-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:46:17.626: INFO: Waiting for pod pod-92943c79-faac-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:46:17.630: INFO: Pod pod-92943c79-faac-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:46:17.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cv556" for this suite.
Dec  8 05:46:23.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:46:23.730: INFO: namespace: e2e-tests-emptydir-cv556, resource: bindings, ignored listing per whitelist
Dec  8 05:46:23.894: INFO: namespace e2e-tests-emptydir-cv556 deletion completed in 6.25699028s

• [SLOW TEST:10.622 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:46:23.894: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5ccn6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Dec  8 05:46:24.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 --namespace=e2e-tests-kubectl-5ccn6 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  8 05:46:25.905: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  8 05:46:25.905: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:46:27.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5ccn6" for this suite.
Dec  8 05:46:41.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:46:41.979: INFO: namespace: e2e-tests-kubectl-5ccn6, resource: bindings, ignored listing per whitelist
Dec  8 05:46:42.135: INFO: namespace e2e-tests-kubectl-5ccn6 deletion completed in 14.213539886s

• [SLOW TEST:18.241 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:46:42.135: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-nsqr5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  8 05:46:44.912: INFO: Successfully updated pod "labelsupdatea3c444ba-faac-11e8-b680-ca3d6f40e675"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:46:46.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nsqr5" for this suite.
Dec  8 05:47:08.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:47:09.102: INFO: namespace: e2e-tests-downward-api-nsqr5, resource: bindings, ignored listing per whitelist
Dec  8 05:47:09.155: INFO: namespace e2e-tests-downward-api-nsqr5 deletion completed in 22.21475732s

• [SLOW TEST:27.020 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:47:09.157: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-4t84h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:47:09.421: INFO: Creating deployment "test-recreate-deployment"
Dec  8 05:47:09.432: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  8 05:47:09.451: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec  8 05:47:11.461: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  8 05:47:11.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844829, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844829, loc:(*time.Location)(0x6c43b60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844829, loc:(*time.Location)(0x6c43b60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679844829, loc:(*time.Location)(0x6c43b60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  8 05:47:13.471: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  8 05:47:13.480: INFO: Updating deployment test-recreate-deployment
Dec  8 05:47:13.480: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  8 05:47:13.627: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-4t84h,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4t84h/deployments/test-recreate-deployment,UID:b3e47b78-faac-11e8-a103-96cd63cee3b8,ResourceVersion:12392,Generation:2,CreationTimestamp:2018-12-08 05:47:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2018-12-08 05:47:13 +0000 UTC 2018-12-08 05:47:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2018-12-08 05:47:13 +0000 UTC 2018-12-08 05:47:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec  8 05:47:13.632: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-4t84h,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4t84h/replicasets/test-recreate-deployment-7cf749666b,UID:b659a212-faac-11e8-a103-96cd63cee3b8,ResourceVersion:12388,Generation:1,CreationTimestamp:2018-12-08 05:47:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b3e47b78-faac-11e8-a103-96cd63cee3b8 0xc422865007 0xc422865008}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  8 05:47:13.632: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  8 05:47:13.632: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-4t84h,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4t84h/replicasets/test-recreate-deployment-79f694ff59,UID:b3e73307-faac-11e8-a103-96cd63cee3b8,ResourceVersion:12380,Generation:2,CreationTimestamp:2018-12-08 05:47:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b3e47b78-faac-11e8-a103-96cd63cee3b8 0xc422864f47 0xc422864f48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  8 05:47:13.638: INFO: Pod "test-recreate-deployment-7cf749666b-6rd47" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-6rd47,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-4t84h,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4t84h/pods/test-recreate-deployment-7cf749666b-6rd47,UID:b65ab224-faac-11e8-a103-96cd63cee3b8,ResourceVersion:12391,Generation:0,CreationTimestamp:2018-12-08 05:47:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b b659a212-faac-11e8-a103-96cd63cee3b8 0xc422865827 0xc422865828}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-mr825 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mr825,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mr825 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-liberal-perch,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc4228658a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc4228658c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:13 +0000 UTC  }],Message:,Reason:,HostIP:10.135.11.179,PodIP:,StartTime:2018-12-08 05:47:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:47:13.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-4t84h" for this suite.
Dec  8 05:47:19.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:47:19.822: INFO: namespace: e2e-tests-deployment-4t84h, resource: bindings, ignored listing per whitelist
Dec  8 05:47:19.830: INFO: namespace e2e-tests-deployment-4t84h deletion completed in 6.185539766s

• [SLOW TEST:10.673 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:47:19.830: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-7qf65
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-ba3f5f16-faac-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 05:47:20.107: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba406028-faac-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-configmap-7qf65" to be "success or failure"
Dec  8 05:47:20.155: INFO: Pod "pod-configmaps-ba406028-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 48.356353ms
Dec  8 05:47:22.160: INFO: Pod "pod-configmaps-ba406028-faac-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.053539703s
STEP: Saw pod success
Dec  8 05:47:22.160: INFO: Pod "pod-configmaps-ba406028-faac-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:47:22.166: INFO: Trying to get logs from node phrs-one-falcon pod pod-configmaps-ba406028-faac-11e8-b680-ca3d6f40e675 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 05:47:22.208: INFO: Waiting for pod pod-configmaps-ba406028-faac-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:47:22.213: INFO: Pod pod-configmaps-ba406028-faac-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:47:22.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7qf65" for this suite.
Dec  8 05:47:28.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:47:28.280: INFO: namespace: e2e-tests-configmap-7qf65, resource: bindings, ignored listing per whitelist
Dec  8 05:47:28.443: INFO: namespace e2e-tests-configmap-7qf65 deletion completed in 6.223115952s

• [SLOW TEST:8.613 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:47:28.444: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-2q2ql
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Dec  8 05:47:28.668: INFO: Waiting up to 5m0s for pod "var-expansion-bf5acaa0-faac-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-var-expansion-2q2ql" to be "success or failure"
Dec  8 05:47:28.688: INFO: Pod "var-expansion-bf5acaa0-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 19.53711ms
Dec  8 05:47:30.692: INFO: Pod "var-expansion-bf5acaa0-faac-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023672572s
STEP: Saw pod success
Dec  8 05:47:30.692: INFO: Pod "var-expansion-bf5acaa0-faac-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:47:30.696: INFO: Trying to get logs from node phrs-glorious-mastodon pod var-expansion-bf5acaa0-faac-11e8-b680-ca3d6f40e675 container dapi-container: <nil>
STEP: delete the pod
Dec  8 05:47:30.725: INFO: Waiting for pod var-expansion-bf5acaa0-faac-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:47:30.730: INFO: Pod var-expansion-bf5acaa0-faac-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:47:30.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-2q2ql" for this suite.
Dec  8 05:47:36.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:47:36.799: INFO: namespace: e2e-tests-var-expansion-2q2ql, resource: bindings, ignored listing per whitelist
Dec  8 05:47:36.884: INFO: namespace e2e-tests-var-expansion-2q2ql deletion completed in 6.147978564s

• [SLOW TEST:8.440 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:47:36.884: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-vjpls
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 05:47:37.112: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c46400a4-faac-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-vjpls" to be "success or failure"
Dec  8 05:47:37.118: INFO: Pod "downwardapi-volume-c46400a4-faac-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.359704ms
Dec  8 05:47:39.122: INFO: Pod "downwardapi-volume-c46400a4-faac-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009592156s
STEP: Saw pod success
Dec  8 05:47:39.122: INFO: Pod "downwardapi-volume-c46400a4-faac-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:47:39.126: INFO: Trying to get logs from node phrs-liberal-perch pod downwardapi-volume-c46400a4-faac-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 05:47:39.160: INFO: Waiting for pod downwardapi-volume-c46400a4-faac-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:47:39.165: INFO: Pod downwardapi-volume-c46400a4-faac-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:47:39.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vjpls" for this suite.
Dec  8 05:47:45.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:47:45.218: INFO: namespace: e2e-tests-projected-vjpls, resource: bindings, ignored listing per whitelist
Dec  8 05:47:45.360: INFO: namespace e2e-tests-projected-vjpls deletion completed in 6.189157434s

• [SLOW TEST:8.476 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:47:45.361: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-b8mhw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-b8mhw
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-b8mhw
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-b8mhw
Dec  8 05:47:45.603: INFO: Found 0 stateful pods, waiting for 1
Dec  8 05:47:55.608: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  8 05:47:55.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:47:55.832: INFO: stderr: ""
Dec  8 05:47:55.832: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:47:55.832: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  8 05:47:55.837: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  8 05:48:05.842: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  8 05:48:05.842: INFO: Waiting for statefulset status.replicas updated to 0
Dec  8 05:48:05.861: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Dec  8 05:48:05.861: INFO: ss-0  phrs-one-falcon  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:05.861: INFO: 
Dec  8 05:48:05.861: INFO: StatefulSet ss has not reached scale 3, at 1
Dec  8 05:48:06.868: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992831807s
Dec  8 05:48:07.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986378285s
Dec  8 05:48:08.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981713489s
Dec  8 05:48:09.883: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975460036s
Dec  8 05:48:10.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97088132s
Dec  8 05:48:11.894: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965575094s
Dec  8 05:48:12.899: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960183812s
Dec  8 05:48:13.908: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954982432s
Dec  8 05:48:14.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.269558ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-b8mhw
Dec  8 05:48:15.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:48:16.130: INFO: stderr: ""
Dec  8 05:48:16.130: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  8 05:48:16.130: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  8 05:48:16.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:48:16.342: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Dec  8 05:48:16.342: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  8 05:48:16.342: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  8 05:48:16.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:48:16.553: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Dec  8 05:48:16.553: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  8 05:48:16.553: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  8 05:48:16.559: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec  8 05:48:26.563: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 05:48:26.563: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 05:48:26.563: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  8 05:48:26.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:48:26.780: INFO: stderr: ""
Dec  8 05:48:26.780: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:48:26.780: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  8 05:48:26.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:48:26.992: INFO: stderr: ""
Dec  8 05:48:26.992: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:48:26.992: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  8 05:48:26.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  8 05:48:27.239: INFO: stderr: ""
Dec  8 05:48:27.239: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  8 05:48:27.239: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  8 05:48:27.239: INFO: Waiting for statefulset status.replicas updated to 0
Dec  8 05:48:27.244: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  8 05:48:37.253: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  8 05:48:37.253: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  8 05:48:37.253: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  8 05:48:37.272: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:37.272: INFO: ss-0  phrs-one-falcon         Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:37.272: INFO: ss-1  phrs-glorious-mastodon  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:37.272: INFO: ss-2  phrs-liberal-perch      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:37.272: INFO: 
Dec  8 05:48:37.272: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  8 05:48:38.277: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:38.277: INFO: ss-0  phrs-one-falcon         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:38.277: INFO: ss-1  phrs-glorious-mastodon  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:38.277: INFO: ss-2  phrs-liberal-perch      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:38.277: INFO: 
Dec  8 05:48:38.277: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  8 05:48:39.281: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:39.281: INFO: ss-0  phrs-one-falcon         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:39.281: INFO: ss-1  phrs-glorious-mastodon  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:39.281: INFO: 
Dec  8 05:48:39.281: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  8 05:48:40.285: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:40.285: INFO: ss-0  phrs-one-falcon         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:40.285: INFO: ss-1  phrs-glorious-mastodon  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:40.285: INFO: 
Dec  8 05:48:40.285: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  8 05:48:41.290: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:41.290: INFO: ss-0  phrs-one-falcon         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:41.290: INFO: ss-1  phrs-glorious-mastodon  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:41.290: INFO: 
Dec  8 05:48:41.290: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  8 05:48:42.294: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:42.294: INFO: ss-0  phrs-one-falcon         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:42.294: INFO: ss-1  phrs-glorious-mastodon  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:42.294: INFO: 
Dec  8 05:48:42.294: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  8 05:48:43.299: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:43.299: INFO: ss-0  phrs-one-falcon         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:43.299: INFO: ss-1  phrs-glorious-mastodon  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:43.299: INFO: 
Dec  8 05:48:43.299: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  8 05:48:44.303: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:44.303: INFO: ss-0  phrs-one-falcon         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:44.303: INFO: ss-1  phrs-glorious-mastodon  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:44.303: INFO: 
Dec  8 05:48:44.303: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  8 05:48:45.307: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:45.307: INFO: ss-0  phrs-one-falcon         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:45.307: INFO: ss-1  phrs-glorious-mastodon  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:45.307: INFO: 
Dec  8 05:48:45.307: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  8 05:48:46.312: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec  8 05:48:46.312: INFO: ss-0  phrs-one-falcon         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:47:45 +0000 UTC  }]
Dec  8 05:48:46.312: INFO: ss-1  phrs-glorious-mastodon  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 05:48:05 +0000 UTC  }]
Dec  8 05:48:46.312: INFO: 
Dec  8 05:48:46.312: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-b8mhw
Dec  8 05:48:47.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:48:47.449: INFO: rc: 1
Dec  8 05:48:47.449: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc420a6fc50 exit status 1 <nil> <nil> true [0xc42152a5e8 0xc42152a608 0xc42152a660] [0xc42152a5e8 0xc42152a608 0xc42152a660] [0xc42152a5f8 0xc42152a648] [0x8fd520 0x8fd520] 0xc420fda9c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Dec  8 05:48:57.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:48:57.539: INFO: rc: 1
Dec  8 05:48:57.539: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc42239e390 exit status 1 <nil> <nil> true [0xc42104e4b8 0xc42104e4d0 0xc42104e4e8] [0xc42104e4b8 0xc42104e4d0 0xc42104e4e8] [0xc42104e4c8 0xc42104e4e0] [0x8fd520 0x8fd520] 0xc421b1ae40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:49:07.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:49:07.637: INFO: rc: 1
Dec  8 05:49:07.637: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc42239e7b0 exit status 1 <nil> <nil> true [0xc42104e4f0 0xc42104e508 0xc42104e520] [0xc42104e4f0 0xc42104e508 0xc42104e520] [0xc42104e500 0xc42104e518] [0x8fd520 0x8fd520] 0xc421b1afc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:49:17.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:49:17.751: INFO: rc: 1
Dec  8 05:49:17.751: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc42239ec00 exit status 1 <nil> <nil> true [0xc42104e528 0xc42104e540 0xc42104e558] [0xc42104e528 0xc42104e540 0xc42104e558] [0xc42104e538 0xc42104e550] [0x8fd520 0x8fd520] 0xc421b1b0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:49:27.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:49:27.865: INFO: rc: 1
Dec  8 05:49:27.865: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc4217b0180 exit status 1 <nil> <nil> true [0xc42152a668 0xc42152a690 0xc42152a6a8] [0xc42152a668 0xc42152a690 0xc42152a6a8] [0xc42152a688 0xc42152a6a0] [0x8fd520 0x8fd520] 0xc420fdaae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:49:37.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:49:37.959: INFO: rc: 1
Dec  8 05:49:37.959: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc4228f64e0 exit status 1 <nil> <nil> true [0xc420e8c008 0xc420e8c038 0xc420e8c070] [0xc420e8c008 0xc420e8c038 0xc420e8c070] [0xc420e8c020 0xc420e8c068] [0x8fd520 0x8fd520] 0xc42302a060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:49:47.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:49:48.075: INFO: rc: 1
Dec  8 05:49:48.075: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420857170 exit status 1 <nil> <nil> true [0xc4200ea000 0xc4200ea608 0xc4200eab70] [0xc4200ea000 0xc4200ea608 0xc4200eab70] [0xc4200ea5b8 0xc4200eaa78] [0x8fd520 0x8fd520] 0xc422060060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:49:58.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:49:58.170: INFO: rc: 1
Dec  8 05:49:58.170: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420857620 exit status 1 <nil> <nil> true [0xc4200eae60 0xc4200eb178 0xc4200eb4c8] [0xc4200eae60 0xc4200eb178 0xc4200eb4c8] [0xc4200eaf90 0xc4200eb4b0] [0x8fd520 0x8fd520] 0xc4220601e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:50:08.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:50:08.270: INFO: rc: 1
Dec  8 05:50:08.270: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc4228f6a50 exit status 1 <nil> <nil> true [0xc420e8c088 0xc420e8c0b8 0xc420e8c110] [0xc420e8c088 0xc420e8c0b8 0xc420e8c110] [0xc420e8c0b0 0xc420e8c0f8] [0x8fd520 0x8fd520] 0xc42302a1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:50:18.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:50:18.369: INFO: rc: 1
Dec  8 05:50:18.369: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420857d40 exit status 1 <nil> <nil> true [0xc4200eb510 0xc4200eb770 0xc4200eb808] [0xc4200eb510 0xc4200eb770 0xc4200eb808] [0xc4200eb588 0xc4200eb7c8] [0x8fd520 0x8fd520] 0xc4220603c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:50:28.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:50:28.461: INFO: rc: 1
Dec  8 05:50:28.461: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc4228f6f30 exit status 1 <nil> <nil> true [0xc420e8c118 0xc420e8c130 0xc420e8c148] [0xc420e8c118 0xc420e8c130 0xc420e8c148] [0xc420e8c128 0xc420e8c140] [0x8fd520 0x8fd520] 0xc42302a300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:50:38.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:50:38.580: INFO: rc: 1
Dec  8 05:50:38.580: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc421f0e510 exit status 1 <nil> <nil> true [0xc42000e108 0xc42000e1d8 0xc42000e298] [0xc42000e108 0xc42000e1d8 0xc42000e298] [0xc42000e1c8 0xc42000e258] [0x8fd520 0x8fd520] 0xc422786060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:50:48.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:50:48.674: INFO: rc: 1
Dec  8 05:50:48.674: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc421f0e900 exit status 1 <nil> <nil> true [0xc42000e2b8 0xc420928028 0xc4209280a0] [0xc42000e2b8 0xc420928028 0xc4209280a0] [0xc42000e2d8 0xc420928090] [0x8fd520 0x8fd520] 0xc422786180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:50:58.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:50:58.778: INFO: rc: 1
Dec  8 05:50:58.778: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420b701e0 exit status 1 <nil> <nil> true [0xc4200eb850 0xc4200eb908 0xc4200eba40] [0xc4200eb850 0xc4200eb908 0xc4200eba40] [0xc4200eb8e0 0xc4200eb9f0] [0x8fd520 0x8fd520] 0xc4220605a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:51:08.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:51:08.905: INFO: rc: 1
Dec  8 05:51:08.905: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420b70690 exit status 1 <nil> <nil> true [0xc4200eba48 0xc4200ebbf8 0xc4200ebcd8] [0xc4200eba48 0xc4200ebbf8 0xc4200ebcd8] [0xc4200ebbb8 0xc4200ebc88] [0x8fd520 0x8fd520] 0xc422060720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:51:18.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:51:19.003: INFO: rc: 1
Dec  8 05:51:19.003: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420b70b40 exit status 1 <nil> <nil> true [0xc4200ebcf0 0xc4200ebe10 0xc4200ebeb0] [0xc4200ebcf0 0xc4200ebe10 0xc4200ebeb0] [0xc4200ebdf0 0xc4200ebe80] [0x8fd520 0x8fd520] 0xc422060900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:51:29.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:51:29.092: INFO: rc: 1
Dec  8 05:51:29.092: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc4228f72c0 exit status 1 <nil> <nil> true [0xc420e8c150 0xc420e8c188 0xc420e8c1d0] [0xc420e8c150 0xc420e8c188 0xc420e8c1d0] [0xc420e8c168 0xc420e8c1b8] [0x8fd520 0x8fd520] 0xc42302a420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:51:39.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:51:39.208: INFO: rc: 1
Dec  8 05:51:39.208: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc4208571a0 exit status 1 <nil> <nil> true [0xc42000e198 0xc42000e208 0xc42000e2b8] [0xc42000e198 0xc42000e208 0xc42000e2b8] [0xc42000e1d8 0xc42000e298] [0x8fd520 0x8fd520] 0xc422060060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:51:49.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:51:49.303: INFO: rc: 1
Dec  8 05:51:49.303: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420b704e0 exit status 1 <nil> <nil> true [0xc4200ea000 0xc4200ea608 0xc4200eab70] [0xc4200ea000 0xc4200ea608 0xc4200eab70] [0xc4200ea5b8 0xc4200eaa78] [0x8fd520 0x8fd520] 0xc422786060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:51:59.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:51:59.411: INFO: rc: 1
Dec  8 05:51:59.411: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420857680 exit status 1 <nil> <nil> true [0xc42000e2c8 0xc420928080 0xc4209280b0] [0xc42000e2c8 0xc420928080 0xc4209280b0] [0xc420928028 0xc4209280a0] [0x8fd520 0x8fd520] 0xc4220601e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:52:09.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:52:09.518: INFO: rc: 1
Dec  8 05:52:09.519: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420b70900 exit status 1 <nil> <nil> true [0xc4200eae60 0xc4200eb178 0xc4200eb4c8] [0xc4200eae60 0xc4200eb178 0xc4200eb4c8] [0xc4200eaf90 0xc4200eb4b0] [0x8fd520 0x8fd520] 0xc422786180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:52:19.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:52:19.610: INFO: rc: 1
Dec  8 05:52:19.610: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420857e00 exit status 1 <nil> <nil> true [0xc4209280d8 0xc420928150 0xc420928220] [0xc4209280d8 0xc420928150 0xc420928220] [0xc420928140 0xc4209281b0] [0x8fd520 0x8fd520] 0xc4220603c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:52:29.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:52:29.712: INFO: rc: 1
Dec  8 05:52:29.712: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420a6fc50 exit status 1 <nil> <nil> true [0xc420928288 0xc4209282b0 0xc4209282f8] [0xc420928288 0xc4209282b0 0xc4209282f8] [0xc4209282a8 0xc4209282f0] [0x8fd520 0x8fd520] 0xc4220605a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:52:39.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:52:39.828: INFO: rc: 1
Dec  8 05:52:39.828: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc421f0e1e0 exit status 1 <nil> <nil> true [0xc420928308 0xc420928360 0xc420928400] [0xc420928308 0xc420928360 0xc420928400] [0xc420928338 0xc4209283f0] [0x8fd520 0x8fd520] 0xc422060720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:52:49.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:52:49.943: INFO: rc: 1
Dec  8 05:52:49.943: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420b70ed0 exit status 1 <nil> <nil> true [0xc4200eb510 0xc4200eb770 0xc4200eb808] [0xc4200eb510 0xc4200eb770 0xc4200eb808] [0xc4200eb588 0xc4200eb7c8] [0x8fd520 0x8fd520] 0xc4227862a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:52:59.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:53:00.067: INFO: rc: 1
Dec  8 05:53:00.067: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc421f0e690 exit status 1 <nil> <nil> true [0xc420928428 0xc420928490 0xc420928518] [0xc420928428 0xc420928490 0xc420928518] [0xc420928478 0xc4209284f8] [0x8fd520 0x8fd520] 0xc422060900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:53:10.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:53:10.167: INFO: rc: 1
Dec  8 05:53:10.167: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420b71530 exit status 1 <nil> <nil> true [0xc4200eb850 0xc4200eb908 0xc4200eba40] [0xc4200eb850 0xc4200eb908 0xc4200eba40] [0xc4200eb8e0 0xc4200eb9f0] [0x8fd520 0x8fd520] 0xc422786420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:53:20.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:53:20.258: INFO: rc: 1
Dec  8 05:53:20.258: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420b71950 exit status 1 <nil> <nil> true [0xc4200eba48 0xc4200ebbf8 0xc4200ebcd8] [0xc4200eba48 0xc4200ebbf8 0xc4200ebcd8] [0xc4200ebbb8 0xc4200ebc88] [0x8fd520 0x8fd520] 0xc4227865a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:53:30.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:53:30.357: INFO: rc: 1
Dec  8 05:53:30.357: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc421f0eba0 exit status 1 <nil> <nil> true [0xc420928558 0xc4209285b0 0xc4209285e0] [0xc420928558 0xc4209285b0 0xc4209285e0] [0xc4209285a8 0xc4209285d0] [0x8fd520 0x8fd520] 0xc422060a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:53:40.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:53:40.453: INFO: rc: 1
Dec  8 05:53:40.453: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc420857170 exit status 1 <nil> <nil> true [0xc42000e198 0xc42000e208 0xc42000e2b8] [0xc42000e198 0xc42000e208 0xc42000e2b8] [0xc42000e1d8 0xc42000e298] [0x8fd520 0x8fd520] 0xc422060060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  8 05:53:50.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 exec --namespace=e2e-tests-statefulset-b8mhw ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  8 05:53:50.561: INFO: rc: 1
Dec  8 05:53:50.561: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Dec  8 05:53:50.561: INFO: Scaling statefulset ss to 0
Dec  8 05:53:50.574: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  8 05:53:50.578: INFO: Deleting all statefulset in ns e2e-tests-statefulset-b8mhw
Dec  8 05:53:50.582: INFO: Scaling statefulset ss to 0
Dec  8 05:53:50.595: INFO: Waiting for statefulset status.replicas updated to 0
Dec  8 05:53:50.598: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:53:50.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-b8mhw" for this suite.
Dec  8 05:53:56.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:53:56.705: INFO: namespace: e2e-tests-statefulset-b8mhw, resource: bindings, ignored listing per whitelist
Dec  8 05:53:56.766: INFO: namespace e2e-tests-statefulset-b8mhw deletion completed in 6.134229926s

• [SLOW TEST:371.405 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:53:56.766: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-4jvnj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4jvnj
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  8 05:53:56.962: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  8 05:54:19.108: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://172.31.192.8:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4jvnj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:54:19.108: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:54:19.212: INFO: Found all expected endpoints: [netserver-0]
Dec  8 05:54:19.217: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://172.31.240.7:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4jvnj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:54:19.217: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:54:19.323: INFO: Found all expected endpoints: [netserver-1]
Dec  8 05:54:19.327: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://172.31.0.8:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4jvnj PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  8 05:54:19.327: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
Dec  8 05:54:19.439: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:54:19.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4jvnj" for this suite.
Dec  8 05:54:41.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:54:41.507: INFO: namespace: e2e-tests-pod-network-test-4jvnj, resource: bindings, ignored listing per whitelist
Dec  8 05:54:41.643: INFO: namespace e2e-tests-pod-network-test-4jvnj deletion completed in 22.197338525s

• [SLOW TEST:44.876 seconds]
[sig-network] Networking
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:54:41.643: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-xn4q4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Dec  8 05:54:41.855: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  8 05:54:41.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:54:42.099: INFO: stderr: ""
Dec  8 05:54:42.099: INFO: stdout: "service/redis-slave created\n"
Dec  8 05:54:42.099: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  8 05:54:42.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:54:42.317: INFO: stderr: ""
Dec  8 05:54:42.317: INFO: stdout: "service/redis-master created\n"
Dec  8 05:54:42.317: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  8 05:54:42.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:54:42.536: INFO: stderr: ""
Dec  8 05:54:42.536: INFO: stdout: "service/frontend created\n"
Dec  8 05:54:42.536: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  8 05:54:42.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:54:42.756: INFO: stderr: ""
Dec  8 05:54:42.756: INFO: stdout: "deployment.extensions/frontend created\n"
Dec  8 05:54:42.756: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  8 05:54:42.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:54:42.978: INFO: stderr: ""
Dec  8 05:54:42.978: INFO: stdout: "deployment.extensions/redis-master created\n"
Dec  8 05:54:42.979: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  8 05:54:42.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:54:43.193: INFO: stderr: ""
Dec  8 05:54:43.193: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Dec  8 05:54:43.193: INFO: Waiting for all frontend pods to be Running.
Dec  8 05:55:03.244: INFO: Waiting for frontend to serve content.
Dec  8 05:55:08.275: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  8 05:55:13.305: INFO: Trying to add a new entry to the guestbook.
Dec  8 05:55:13.325: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  8 05:55:13.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:55:13.508: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 05:55:13.508: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  8 05:55:13.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:55:13.666: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 05:55:13.666: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  8 05:55:13.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:55:13.817: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 05:55:13.817: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  8 05:55:13.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:55:13.980: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 05:55:13.980: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  8 05:55:13.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:55:14.147: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 05:55:14.148: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  8 05:55:14.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xn4q4'
Dec  8 05:55:14.348: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 05:55:14.348: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:55:14.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xn4q4" for this suite.
Dec  8 05:55:50.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:55:50.408: INFO: namespace: e2e-tests-kubectl-xn4q4, resource: bindings, ignored listing per whitelist
Dec  8 05:55:50.549: INFO: namespace e2e-tests-kubectl-xn4q4 deletion completed in 36.18312621s

• [SLOW TEST:68.907 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:55:50.549: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-rffkf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-rffkf
I1208 05:55:50.779456      17 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-rffkf, replica count: 1
I1208 05:55:51.829873      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  8 05:55:51.942: INFO: Created: latency-svc-k9mkt
Dec  8 05:55:51.957: INFO: Got endpoints: latency-svc-k9mkt [27.198236ms]
Dec  8 05:55:51.979: INFO: Created: latency-svc-k6tq8
Dec  8 05:55:51.981: INFO: Got endpoints: latency-svc-k6tq8 [23.648572ms]
Dec  8 05:55:51.998: INFO: Created: latency-svc-4cqfw
Dec  8 05:55:52.002: INFO: Got endpoints: latency-svc-4cqfw [44.610357ms]
Dec  8 05:55:52.017: INFO: Created: latency-svc-lzl4p
Dec  8 05:55:52.019: INFO: Got endpoints: latency-svc-lzl4p [61.948255ms]
Dec  8 05:55:52.035: INFO: Created: latency-svc-gfkfx
Dec  8 05:55:52.040: INFO: Got endpoints: latency-svc-gfkfx [82.390063ms]
Dec  8 05:55:52.070: INFO: Created: latency-svc-vqlb2
Dec  8 05:55:52.078: INFO: Got endpoints: latency-svc-vqlb2 [120.954742ms]
Dec  8 05:55:52.089: INFO: Created: latency-svc-q5znt
Dec  8 05:55:52.100: INFO: Got endpoints: latency-svc-q5znt [142.435642ms]
Dec  8 05:55:52.104: INFO: Created: latency-svc-97vsk
Dec  8 05:55:52.115: INFO: Created: latency-svc-928wm
Dec  8 05:55:52.116: INFO: Got endpoints: latency-svc-97vsk [158.43999ms]
Dec  8 05:55:52.125: INFO: Got endpoints: latency-svc-928wm [167.431451ms]
Dec  8 05:55:52.136: INFO: Created: latency-svc-4vh5n
Dec  8 05:55:52.141: INFO: Got endpoints: latency-svc-4vh5n [183.586169ms]
Dec  8 05:55:52.147: INFO: Created: latency-svc-9clxn
Dec  8 05:55:52.172: INFO: Created: latency-svc-c2f9m
Dec  8 05:55:52.173: INFO: Got endpoints: latency-svc-9clxn [215.473584ms]
Dec  8 05:55:52.179: INFO: Got endpoints: latency-svc-c2f9m [222.031965ms]
Dec  8 05:55:52.263: INFO: Created: latency-svc-j24jv
Dec  8 05:55:52.281: INFO: Created: latency-svc-6gcwf
Dec  8 05:55:52.281: INFO: Got endpoints: latency-svc-j24jv [324.075872ms]
Dec  8 05:55:52.294: INFO: Got endpoints: latency-svc-6gcwf [336.123118ms]
Dec  8 05:55:52.295: INFO: Created: latency-svc-4kdk2
Dec  8 05:55:52.298: INFO: Got endpoints: latency-svc-4kdk2 [340.7397ms]
Dec  8 05:55:52.307: INFO: Created: latency-svc-ks8z8
Dec  8 05:55:52.311: INFO: Got endpoints: latency-svc-ks8z8 [353.429467ms]
Dec  8 05:55:52.318: INFO: Created: latency-svc-2rjkb
Dec  8 05:55:52.322: INFO: Got endpoints: latency-svc-2rjkb [341.790739ms]
Dec  8 05:55:52.336: INFO: Created: latency-svc-p8l6w
Dec  8 05:55:52.343: INFO: Got endpoints: latency-svc-p8l6w [341.752245ms]
Dec  8 05:55:52.344: INFO: Created: latency-svc-cdj8f
Dec  8 05:55:52.351: INFO: Got endpoints: latency-svc-cdj8f [332.348544ms]
Dec  8 05:55:52.358: INFO: Created: latency-svc-w2hj7
Dec  8 05:55:52.365: INFO: Got endpoints: latency-svc-w2hj7 [286.385958ms]
Dec  8 05:55:52.372: INFO: Created: latency-svc-p8576
Dec  8 05:55:52.376: INFO: Got endpoints: latency-svc-p8576 [336.067304ms]
Dec  8 05:55:52.391: INFO: Created: latency-svc-z7b5j
Dec  8 05:55:52.396: INFO: Got endpoints: latency-svc-z7b5j [296.112517ms]
Dec  8 05:55:52.400: INFO: Created: latency-svc-hqzpq
Dec  8 05:55:52.411: INFO: Created: latency-svc-jf8nh
Dec  8 05:55:52.413: INFO: Got endpoints: latency-svc-hqzpq [297.79595ms]
Dec  8 05:55:52.423: INFO: Got endpoints: latency-svc-jf8nh [298.351192ms]
Dec  8 05:55:52.427: INFO: Created: latency-svc-zktrg
Dec  8 05:55:52.434: INFO: Created: latency-svc-pkz95
Dec  8 05:55:52.435: INFO: Got endpoints: latency-svc-zktrg [294.016273ms]
Dec  8 05:55:52.442: INFO: Got endpoints: latency-svc-pkz95 [269.563278ms]
Dec  8 05:55:52.447: INFO: Created: latency-svc-xjz54
Dec  8 05:55:52.454: INFO: Created: latency-svc-p2pc9
Dec  8 05:55:52.459: INFO: Got endpoints: latency-svc-xjz54 [36.263009ms]
Dec  8 05:55:52.463: INFO: Got endpoints: latency-svc-p2pc9 [283.540428ms]
Dec  8 05:55:52.472: INFO: Created: latency-svc-q8mpv
Dec  8 05:55:52.478: INFO: Got endpoints: latency-svc-q8mpv [196.789618ms]
Dec  8 05:55:52.487: INFO: Created: latency-svc-67s7d
Dec  8 05:55:52.490: INFO: Got endpoints: latency-svc-67s7d [196.881933ms]
Dec  8 05:55:52.503: INFO: Created: latency-svc-w66z8
Dec  8 05:55:52.514: INFO: Got endpoints: latency-svc-w66z8 [215.765654ms]
Dec  8 05:55:52.520: INFO: Created: latency-svc-mwpk2
Dec  8 05:55:52.525: INFO: Got endpoints: latency-svc-mwpk2 [213.948795ms]
Dec  8 05:55:52.538: INFO: Created: latency-svc-l92dq
Dec  8 05:55:52.546: INFO: Got endpoints: latency-svc-l92dq [223.399098ms]
Dec  8 05:55:52.552: INFO: Created: latency-svc-z9cws
Dec  8 05:55:52.556: INFO: Got endpoints: latency-svc-z9cws [212.560324ms]
Dec  8 05:55:52.573: INFO: Created: latency-svc-dnj2l
Dec  8 05:55:52.583: INFO: Got endpoints: latency-svc-dnj2l [231.218447ms]
Dec  8 05:55:52.590: INFO: Created: latency-svc-z76fc
Dec  8 05:55:52.591: INFO: Got endpoints: latency-svc-z76fc [226.441147ms]
Dec  8 05:55:52.599: INFO: Created: latency-svc-xs8th
Dec  8 05:55:52.604: INFO: Got endpoints: latency-svc-xs8th [227.564454ms]
Dec  8 05:55:52.613: INFO: Created: latency-svc-l9ppr
Dec  8 05:55:52.614: INFO: Got endpoints: latency-svc-l9ppr [218.399168ms]
Dec  8 05:55:52.623: INFO: Created: latency-svc-grnfx
Dec  8 05:55:52.630: INFO: Got endpoints: latency-svc-grnfx [216.818314ms]
Dec  8 05:55:52.634: INFO: Created: latency-svc-mqjfg
Dec  8 05:55:52.637: INFO: Got endpoints: latency-svc-mqjfg [201.37689ms]
Dec  8 05:55:52.646: INFO: Created: latency-svc-djpbj
Dec  8 05:55:52.649: INFO: Got endpoints: latency-svc-djpbj [206.57149ms]
Dec  8 05:55:52.667: INFO: Created: latency-svc-hnn5b
Dec  8 05:55:52.673: INFO: Got endpoints: latency-svc-hnn5b [213.222356ms]
Dec  8 05:55:52.681: INFO: Created: latency-svc-c2cd7
Dec  8 05:55:52.681: INFO: Got endpoints: latency-svc-c2cd7 [217.678764ms]
Dec  8 05:55:52.693: INFO: Created: latency-svc-78ngx
Dec  8 05:55:52.695: INFO: Got endpoints: latency-svc-78ngx [217.122209ms]
Dec  8 05:55:52.714: INFO: Created: latency-svc-lhwjg
Dec  8 05:55:52.716: INFO: Got endpoints: latency-svc-lhwjg [225.919844ms]
Dec  8 05:55:52.726: INFO: Created: latency-svc-9z66w
Dec  8 05:55:52.733: INFO: Created: latency-svc-xr9kn
Dec  8 05:55:52.745: INFO: Created: latency-svc-cmtqr
Dec  8 05:55:52.748: INFO: Got endpoints: latency-svc-9z66w [233.715493ms]
Dec  8 05:55:52.760: INFO: Created: latency-svc-547lc
Dec  8 05:55:52.774: INFO: Created: latency-svc-bqrkj
Dec  8 05:55:52.795: INFO: Got endpoints: latency-svc-xr9kn [269.816263ms]
Dec  8 05:55:52.797: INFO: Created: latency-svc-lcqb5
Dec  8 05:55:52.815: INFO: Created: latency-svc-956rq
Dec  8 05:55:52.830: INFO: Created: latency-svc-5q5dx
Dec  8 05:55:52.848: INFO: Created: latency-svc-h4slv
Dec  8 05:55:52.853: INFO: Got endpoints: latency-svc-cmtqr [306.733191ms]
Dec  8 05:55:52.866: INFO: Created: latency-svc-t6p2p
Dec  8 05:55:52.874: INFO: Created: latency-svc-gcpz9
Dec  8 05:55:52.882: INFO: Created: latency-svc-m8h8g
Dec  8 05:55:52.893: INFO: Got endpoints: latency-svc-547lc [337.126852ms]
Dec  8 05:55:52.894: INFO: Created: latency-svc-qn25m
Dec  8 05:55:52.920: INFO: Created: latency-svc-8nstt
Dec  8 05:55:52.920: INFO: Created: latency-svc-p94cg
Dec  8 05:55:52.942: INFO: Created: latency-svc-dm7mx
Dec  8 05:55:52.946: INFO: Got endpoints: latency-svc-bqrkj [363.172084ms]
Dec  8 05:55:52.957: INFO: Created: latency-svc-rnn48
Dec  8 05:55:52.969: INFO: Created: latency-svc-tglls
Dec  8 05:55:52.976: INFO: Created: latency-svc-7bwqb
Dec  8 05:55:52.986: INFO: Created: latency-svc-qzg78
Dec  8 05:55:52.994: INFO: Got endpoints: latency-svc-lcqb5 [402.891545ms]
Dec  8 05:55:53.011: INFO: Created: latency-svc-jnft8
Dec  8 05:55:53.042: INFO: Got endpoints: latency-svc-956rq [438.312007ms]
Dec  8 05:55:53.053: INFO: Created: latency-svc-x7hjg
Dec  8 05:55:53.094: INFO: Got endpoints: latency-svc-5q5dx [480.135341ms]
Dec  8 05:55:53.109: INFO: Created: latency-svc-4n2xk
Dec  8 05:55:53.143: INFO: Got endpoints: latency-svc-h4slv [512.286405ms]
Dec  8 05:55:53.161: INFO: Created: latency-svc-6d6hw
Dec  8 05:55:53.192: INFO: Got endpoints: latency-svc-t6p2p [554.980895ms]
Dec  8 05:55:53.206: INFO: Created: latency-svc-j26cm
Dec  8 05:55:53.243: INFO: Got endpoints: latency-svc-gcpz9 [594.078901ms]
Dec  8 05:55:53.259: INFO: Created: latency-svc-nxw64
Dec  8 05:55:53.295: INFO: Got endpoints: latency-svc-m8h8g [622.457149ms]
Dec  8 05:55:53.309: INFO: Created: latency-svc-4x4g4
Dec  8 05:55:53.344: INFO: Got endpoints: latency-svc-qn25m [663.28003ms]
Dec  8 05:55:53.362: INFO: Created: latency-svc-xt8bf
Dec  8 05:55:53.394: INFO: Got endpoints: latency-svc-8nstt [698.808226ms]
Dec  8 05:55:53.408: INFO: Created: latency-svc-gnk9n
Dec  8 05:55:53.443: INFO: Got endpoints: latency-svc-p94cg [726.948225ms]
Dec  8 05:55:53.460: INFO: Created: latency-svc-gb6sv
Dec  8 05:55:53.495: INFO: Got endpoints: latency-svc-dm7mx [747.360845ms]
Dec  8 05:55:53.518: INFO: Created: latency-svc-kn55b
Dec  8 05:55:53.544: INFO: Got endpoints: latency-svc-rnn48 [749.39406ms]
Dec  8 05:55:53.565: INFO: Created: latency-svc-l5zbn
Dec  8 05:55:53.594: INFO: Got endpoints: latency-svc-tglls [741.578446ms]
Dec  8 05:55:53.609: INFO: Created: latency-svc-bqfhn
Dec  8 05:55:53.643: INFO: Got endpoints: latency-svc-7bwqb [750.113761ms]
Dec  8 05:55:53.658: INFO: Created: latency-svc-g8npb
Dec  8 05:55:53.696: INFO: Got endpoints: latency-svc-qzg78 [749.793519ms]
Dec  8 05:55:53.711: INFO: Created: latency-svc-gqrvh
Dec  8 05:55:53.743: INFO: Got endpoints: latency-svc-jnft8 [749.188735ms]
Dec  8 05:55:53.774: INFO: Created: latency-svc-sbspf
Dec  8 05:55:53.796: INFO: Got endpoints: latency-svc-x7hjg [753.53964ms]
Dec  8 05:55:53.825: INFO: Created: latency-svc-lgs5d
Dec  8 05:55:53.843: INFO: Got endpoints: latency-svc-4n2xk [748.704411ms]
Dec  8 05:55:53.859: INFO: Created: latency-svc-jjgsw
Dec  8 05:55:53.893: INFO: Got endpoints: latency-svc-6d6hw [749.975446ms]
Dec  8 05:55:53.907: INFO: Created: latency-svc-grv7l
Dec  8 05:55:53.942: INFO: Got endpoints: latency-svc-j26cm [750.525982ms]
Dec  8 05:55:53.954: INFO: Created: latency-svc-llfm7
Dec  8 05:55:53.992: INFO: Got endpoints: latency-svc-nxw64 [748.573361ms]
Dec  8 05:55:54.006: INFO: Created: latency-svc-tcmdv
Dec  8 05:55:54.043: INFO: Got endpoints: latency-svc-4x4g4 [747.562295ms]
Dec  8 05:55:54.051: INFO: Created: latency-svc-5qjzd
Dec  8 05:55:54.093: INFO: Got endpoints: latency-svc-xt8bf [748.973131ms]
Dec  8 05:55:54.106: INFO: Created: latency-svc-z4ctk
Dec  8 05:55:54.145: INFO: Got endpoints: latency-svc-gnk9n [750.723904ms]
Dec  8 05:55:54.156: INFO: Created: latency-svc-6cpbv
Dec  8 05:55:54.193: INFO: Got endpoints: latency-svc-gb6sv [749.390239ms]
Dec  8 05:55:54.204: INFO: Created: latency-svc-8p2tj
Dec  8 05:55:54.242: INFO: Got endpoints: latency-svc-kn55b [747.070456ms]
Dec  8 05:55:54.252: INFO: Created: latency-svc-fv8bg
Dec  8 05:55:54.292: INFO: Got endpoints: latency-svc-l5zbn [747.697929ms]
Dec  8 05:55:54.303: INFO: Created: latency-svc-h6pct
Dec  8 05:55:54.344: INFO: Got endpoints: latency-svc-bqfhn [749.790367ms]
Dec  8 05:55:54.364: INFO: Created: latency-svc-jx9zx
Dec  8 05:55:54.396: INFO: Got endpoints: latency-svc-g8npb [751.964279ms]
Dec  8 05:55:54.412: INFO: Created: latency-svc-sks74
Dec  8 05:55:54.446: INFO: Got endpoints: latency-svc-gqrvh [749.809906ms]
Dec  8 05:55:54.456: INFO: Created: latency-svc-xm5v8
Dec  8 05:55:54.493: INFO: Got endpoints: latency-svc-sbspf [749.950162ms]
Dec  8 05:55:54.513: INFO: Created: latency-svc-gftkw
Dec  8 05:55:54.542: INFO: Got endpoints: latency-svc-lgs5d [746.220646ms]
Dec  8 05:55:54.554: INFO: Created: latency-svc-b5rmr
Dec  8 05:55:54.594: INFO: Got endpoints: latency-svc-jjgsw [750.273865ms]
Dec  8 05:55:54.604: INFO: Created: latency-svc-6ts7h
Dec  8 05:55:54.644: INFO: Got endpoints: latency-svc-grv7l [751.330526ms]
Dec  8 05:55:54.654: INFO: Created: latency-svc-8bl66
Dec  8 05:55:54.693: INFO: Got endpoints: latency-svc-llfm7 [750.646354ms]
Dec  8 05:55:54.709: INFO: Created: latency-svc-g2tsf
Dec  8 05:55:54.743: INFO: Got endpoints: latency-svc-tcmdv [750.54358ms]
Dec  8 05:55:54.758: INFO: Created: latency-svc-bn4kj
Dec  8 05:55:54.793: INFO: Got endpoints: latency-svc-5qjzd [750.570835ms]
Dec  8 05:55:54.806: INFO: Created: latency-svc-kbnnx
Dec  8 05:55:54.843: INFO: Got endpoints: latency-svc-z4ctk [749.36854ms]
Dec  8 05:55:54.856: INFO: Created: latency-svc-hs4fb
Dec  8 05:55:54.892: INFO: Got endpoints: latency-svc-6cpbv [747.213482ms]
Dec  8 05:55:54.904: INFO: Created: latency-svc-h77gm
Dec  8 05:55:54.942: INFO: Got endpoints: latency-svc-8p2tj [749.360978ms]
Dec  8 05:55:54.954: INFO: Created: latency-svc-2f2qz
Dec  8 05:55:54.994: INFO: Got endpoints: latency-svc-fv8bg [751.041611ms]
Dec  8 05:55:55.005: INFO: Created: latency-svc-lfpj9
Dec  8 05:55:55.042: INFO: Got endpoints: latency-svc-h6pct [750.503178ms]
Dec  8 05:55:55.064: INFO: Created: latency-svc-bvc9n
Dec  8 05:55:55.093: INFO: Got endpoints: latency-svc-jx9zx [748.578291ms]
Dec  8 05:55:55.103: INFO: Created: latency-svc-4wbs6
Dec  8 05:55:55.145: INFO: Got endpoints: latency-svc-sks74 [749.680166ms]
Dec  8 05:55:55.159: INFO: Created: latency-svc-mh89p
Dec  8 05:55:55.193: INFO: Got endpoints: latency-svc-xm5v8 [746.702915ms]
Dec  8 05:55:55.205: INFO: Created: latency-svc-q24bp
Dec  8 05:55:55.245: INFO: Got endpoints: latency-svc-gftkw [751.664204ms]
Dec  8 05:55:55.262: INFO: Created: latency-svc-c28bj
Dec  8 05:55:55.294: INFO: Got endpoints: latency-svc-b5rmr [752.098414ms]
Dec  8 05:55:55.303: INFO: Created: latency-svc-qvsnj
Dec  8 05:55:55.343: INFO: Got endpoints: latency-svc-6ts7h [749.031136ms]
Dec  8 05:55:55.358: INFO: Created: latency-svc-fhsfg
Dec  8 05:55:55.394: INFO: Got endpoints: latency-svc-8bl66 [749.215338ms]
Dec  8 05:55:55.411: INFO: Created: latency-svc-bxqbq
Dec  8 05:55:55.443: INFO: Got endpoints: latency-svc-g2tsf [749.44157ms]
Dec  8 05:55:55.462: INFO: Created: latency-svc-js82p
Dec  8 05:55:55.494: INFO: Got endpoints: latency-svc-bn4kj [751.409012ms]
Dec  8 05:55:55.509: INFO: Created: latency-svc-wj6n4
Dec  8 05:55:55.542: INFO: Got endpoints: latency-svc-kbnnx [748.824085ms]
Dec  8 05:55:55.551: INFO: Created: latency-svc-zsjj2
Dec  8 05:55:55.592: INFO: Got endpoints: latency-svc-hs4fb [749.862818ms]
Dec  8 05:55:55.603: INFO: Created: latency-svc-4zxd7
Dec  8 05:55:55.653: INFO: Got endpoints: latency-svc-h77gm [760.71321ms]
Dec  8 05:55:55.674: INFO: Created: latency-svc-qtbb9
Dec  8 05:55:55.692: INFO: Got endpoints: latency-svc-2f2qz [749.894429ms]
Dec  8 05:55:55.705: INFO: Created: latency-svc-d2ph7
Dec  8 05:55:55.743: INFO: Got endpoints: latency-svc-lfpj9 [749.672247ms]
Dec  8 05:55:55.765: INFO: Created: latency-svc-6jnl7
Dec  8 05:55:55.792: INFO: Got endpoints: latency-svc-bvc9n [749.377257ms]
Dec  8 05:55:55.805: INFO: Created: latency-svc-zvsnp
Dec  8 05:55:55.842: INFO: Got endpoints: latency-svc-4wbs6 [748.803997ms]
Dec  8 05:55:55.863: INFO: Created: latency-svc-ls7vh
Dec  8 05:55:55.892: INFO: Got endpoints: latency-svc-mh89p [746.876502ms]
Dec  8 05:55:55.906: INFO: Created: latency-svc-6m9ht
Dec  8 05:55:55.945: INFO: Got endpoints: latency-svc-q24bp [751.866944ms]
Dec  8 05:55:55.964: INFO: Created: latency-svc-x6tjv
Dec  8 05:55:55.994: INFO: Got endpoints: latency-svc-c28bj [748.75807ms]
Dec  8 05:55:56.008: INFO: Created: latency-svc-w4j9x
Dec  8 05:55:56.044: INFO: Got endpoints: latency-svc-qvsnj [749.776627ms]
Dec  8 05:55:56.056: INFO: Created: latency-svc-mfgss
Dec  8 05:55:56.092: INFO: Got endpoints: latency-svc-fhsfg [749.310422ms]
Dec  8 05:55:56.107: INFO: Created: latency-svc-cd59q
Dec  8 05:55:56.144: INFO: Got endpoints: latency-svc-bxqbq [749.935287ms]
Dec  8 05:55:56.157: INFO: Created: latency-svc-8xdcs
Dec  8 05:55:56.192: INFO: Got endpoints: latency-svc-js82p [749.351539ms]
Dec  8 05:55:56.212: INFO: Created: latency-svc-q6qvs
Dec  8 05:55:56.243: INFO: Got endpoints: latency-svc-wj6n4 [749.274873ms]
Dec  8 05:55:56.266: INFO: Created: latency-svc-q8wkf
Dec  8 05:55:56.294: INFO: Got endpoints: latency-svc-zsjj2 [752.037581ms]
Dec  8 05:55:56.309: INFO: Created: latency-svc-v2df4
Dec  8 05:55:56.345: INFO: Got endpoints: latency-svc-4zxd7 [752.673086ms]
Dec  8 05:55:56.359: INFO: Created: latency-svc-nxg2n
Dec  8 05:55:56.395: INFO: Got endpoints: latency-svc-qtbb9 [741.7141ms]
Dec  8 05:55:56.407: INFO: Created: latency-svc-fpfjs
Dec  8 05:55:56.444: INFO: Got endpoints: latency-svc-d2ph7 [751.414295ms]
Dec  8 05:55:56.478: INFO: Created: latency-svc-n8ssz
Dec  8 05:55:56.496: INFO: Got endpoints: latency-svc-6jnl7 [752.249193ms]
Dec  8 05:55:56.507: INFO: Created: latency-svc-nkvb6
Dec  8 05:55:56.544: INFO: Got endpoints: latency-svc-zvsnp [751.804155ms]
Dec  8 05:55:56.556: INFO: Created: latency-svc-ll4z6
Dec  8 05:55:56.593: INFO: Got endpoints: latency-svc-ls7vh [751.22578ms]
Dec  8 05:55:56.608: INFO: Created: latency-svc-j62p6
Dec  8 05:55:56.647: INFO: Got endpoints: latency-svc-6m9ht [754.452082ms]
Dec  8 05:55:56.663: INFO: Created: latency-svc-5s2nk
Dec  8 05:55:56.692: INFO: Got endpoints: latency-svc-x6tjv [747.47713ms]
Dec  8 05:55:56.701: INFO: Created: latency-svc-rdkcr
Dec  8 05:55:56.743: INFO: Got endpoints: latency-svc-w4j9x [749.210986ms]
Dec  8 05:55:56.761: INFO: Created: latency-svc-tl5dg
Dec  8 05:55:56.793: INFO: Got endpoints: latency-svc-mfgss [748.565058ms]
Dec  8 05:55:56.808: INFO: Created: latency-svc-4xkw8
Dec  8 05:55:56.844: INFO: Got endpoints: latency-svc-cd59q [751.712695ms]
Dec  8 05:55:56.854: INFO: Created: latency-svc-vlnml
Dec  8 05:55:56.892: INFO: Got endpoints: latency-svc-8xdcs [748.543349ms]
Dec  8 05:55:56.911: INFO: Created: latency-svc-8kss6
Dec  8 05:55:56.943: INFO: Got endpoints: latency-svc-q6qvs [750.851595ms]
Dec  8 05:55:56.957: INFO: Created: latency-svc-t8kgh
Dec  8 05:55:56.993: INFO: Got endpoints: latency-svc-q8wkf [749.7649ms]
Dec  8 05:55:57.007: INFO: Created: latency-svc-q2jsf
Dec  8 05:55:57.043: INFO: Got endpoints: latency-svc-v2df4 [748.2404ms]
Dec  8 05:55:57.061: INFO: Created: latency-svc-hpdbd
Dec  8 05:55:57.094: INFO: Got endpoints: latency-svc-nxg2n [748.98554ms]
Dec  8 05:55:57.108: INFO: Created: latency-svc-v6wlv
Dec  8 05:55:57.143: INFO: Got endpoints: latency-svc-fpfjs [747.782387ms]
Dec  8 05:55:57.162: INFO: Created: latency-svc-hwv8f
Dec  8 05:55:57.193: INFO: Got endpoints: latency-svc-n8ssz [748.903082ms]
Dec  8 05:55:57.208: INFO: Created: latency-svc-6d62x
Dec  8 05:55:57.246: INFO: Got endpoints: latency-svc-nkvb6 [749.940369ms]
Dec  8 05:55:57.269: INFO: Created: latency-svc-bmjxt
Dec  8 05:55:57.294: INFO: Got endpoints: latency-svc-ll4z6 [750.053914ms]
Dec  8 05:55:57.311: INFO: Created: latency-svc-q9z4t
Dec  8 05:55:57.343: INFO: Got endpoints: latency-svc-j62p6 [749.480249ms]
Dec  8 05:55:57.369: INFO: Created: latency-svc-gvmrt
Dec  8 05:55:57.392: INFO: Got endpoints: latency-svc-5s2nk [744.255127ms]
Dec  8 05:55:57.405: INFO: Created: latency-svc-4qhn2
Dec  8 05:55:57.444: INFO: Got endpoints: latency-svc-rdkcr [751.815804ms]
Dec  8 05:55:57.462: INFO: Created: latency-svc-q4gz8
Dec  8 05:55:57.496: INFO: Got endpoints: latency-svc-tl5dg [752.932428ms]
Dec  8 05:55:57.508: INFO: Created: latency-svc-ztrvf
Dec  8 05:55:57.544: INFO: Got endpoints: latency-svc-4xkw8 [751.49873ms]
Dec  8 05:55:57.555: INFO: Created: latency-svc-dc5mh
Dec  8 05:55:57.596: INFO: Got endpoints: latency-svc-vlnml [751.870855ms]
Dec  8 05:55:57.613: INFO: Created: latency-svc-bjggk
Dec  8 05:55:57.642: INFO: Got endpoints: latency-svc-8kss6 [749.791498ms]
Dec  8 05:55:57.659: INFO: Created: latency-svc-7vx5q
Dec  8 05:55:57.693: INFO: Got endpoints: latency-svc-t8kgh [749.838107ms]
Dec  8 05:55:57.703: INFO: Created: latency-svc-szdls
Dec  8 05:55:57.743: INFO: Got endpoints: latency-svc-q2jsf [748.77508ms]
Dec  8 05:55:57.757: INFO: Created: latency-svc-qks9p
Dec  8 05:55:57.793: INFO: Got endpoints: latency-svc-hpdbd [749.002361ms]
Dec  8 05:55:57.808: INFO: Created: latency-svc-7w2g9
Dec  8 05:55:57.842: INFO: Got endpoints: latency-svc-v6wlv [747.986899ms]
Dec  8 05:55:57.853: INFO: Created: latency-svc-z6qdk
Dec  8 05:55:57.894: INFO: Got endpoints: latency-svc-hwv8f [750.748731ms]
Dec  8 05:55:57.911: INFO: Created: latency-svc-t4lv9
Dec  8 05:55:57.942: INFO: Got endpoints: latency-svc-6d62x [749.185793ms]
Dec  8 05:55:57.956: INFO: Created: latency-svc-v4g68
Dec  8 05:55:57.994: INFO: Got endpoints: latency-svc-bmjxt [748.408897ms]
Dec  8 05:55:58.003: INFO: Created: latency-svc-vgldz
Dec  8 05:55:58.045: INFO: Got endpoints: latency-svc-q9z4t [750.851937ms]
Dec  8 05:55:58.059: INFO: Created: latency-svc-xt54m
Dec  8 05:55:58.094: INFO: Got endpoints: latency-svc-gvmrt [750.27722ms]
Dec  8 05:55:58.103: INFO: Created: latency-svc-hjlbq
Dec  8 05:55:58.143: INFO: Got endpoints: latency-svc-4qhn2 [751.330214ms]
Dec  8 05:55:58.158: INFO: Created: latency-svc-dfztb
Dec  8 05:55:58.197: INFO: Got endpoints: latency-svc-q4gz8 [752.59823ms]
Dec  8 05:55:58.211: INFO: Created: latency-svc-jpb8w
Dec  8 05:55:58.246: INFO: Got endpoints: latency-svc-ztrvf [749.454454ms]
Dec  8 05:55:58.262: INFO: Created: latency-svc-8nhwk
Dec  8 05:55:58.294: INFO: Got endpoints: latency-svc-dc5mh [749.672269ms]
Dec  8 05:55:58.307: INFO: Created: latency-svc-c9tc4
Dec  8 05:55:58.345: INFO: Got endpoints: latency-svc-bjggk [749.29998ms]
Dec  8 05:55:58.360: INFO: Created: latency-svc-gwq8h
Dec  8 05:55:58.394: INFO: Got endpoints: latency-svc-7vx5q [751.361659ms]
Dec  8 05:55:58.411: INFO: Created: latency-svc-c2rqq
Dec  8 05:55:58.445: INFO: Got endpoints: latency-svc-szdls [751.451403ms]
Dec  8 05:55:58.464: INFO: Created: latency-svc-pwzfc
Dec  8 05:55:58.493: INFO: Got endpoints: latency-svc-qks9p [749.70894ms]
Dec  8 05:55:58.513: INFO: Created: latency-svc-xc7fq
Dec  8 05:55:58.545: INFO: Got endpoints: latency-svc-7w2g9 [752.513413ms]
Dec  8 05:55:58.561: INFO: Created: latency-svc-hn8gq
Dec  8 05:55:58.593: INFO: Got endpoints: latency-svc-z6qdk [750.132159ms]
Dec  8 05:55:58.609: INFO: Created: latency-svc-vsk4g
Dec  8 05:55:58.644: INFO: Got endpoints: latency-svc-t4lv9 [749.936566ms]
Dec  8 05:55:58.661: INFO: Created: latency-svc-tw2ns
Dec  8 05:55:58.694: INFO: Got endpoints: latency-svc-v4g68 [751.414381ms]
Dec  8 05:55:58.716: INFO: Created: latency-svc-xsgdt
Dec  8 05:55:58.744: INFO: Got endpoints: latency-svc-vgldz [749.22792ms]
Dec  8 05:55:58.757: INFO: Created: latency-svc-zddhz
Dec  8 05:55:58.793: INFO: Got endpoints: latency-svc-xt54m [748.329575ms]
Dec  8 05:55:58.808: INFO: Created: latency-svc-kr6jr
Dec  8 05:55:58.843: INFO: Got endpoints: latency-svc-hjlbq [748.858641ms]
Dec  8 05:55:58.856: INFO: Created: latency-svc-wbbnk
Dec  8 05:55:58.892: INFO: Got endpoints: latency-svc-dfztb [749.049989ms]
Dec  8 05:55:58.916: INFO: Created: latency-svc-s77cj
Dec  8 05:55:58.943: INFO: Got endpoints: latency-svc-jpb8w [745.553979ms]
Dec  8 05:55:58.956: INFO: Created: latency-svc-b77bt
Dec  8 05:55:58.995: INFO: Got endpoints: latency-svc-8nhwk [749.261789ms]
Dec  8 05:55:59.013: INFO: Created: latency-svc-vsh4n
Dec  8 05:55:59.045: INFO: Got endpoints: latency-svc-c9tc4 [750.935851ms]
Dec  8 05:55:59.057: INFO: Created: latency-svc-k68hg
Dec  8 05:55:59.093: INFO: Got endpoints: latency-svc-gwq8h [747.990659ms]
Dec  8 05:55:59.111: INFO: Created: latency-svc-k988n
Dec  8 05:55:59.145: INFO: Got endpoints: latency-svc-c2rqq [751.523748ms]
Dec  8 05:55:59.160: INFO: Created: latency-svc-6jc6b
Dec  8 05:55:59.193: INFO: Got endpoints: latency-svc-pwzfc [748.282037ms]
Dec  8 05:55:59.206: INFO: Created: latency-svc-scj9f
Dec  8 05:55:59.242: INFO: Got endpoints: latency-svc-xc7fq [749.48716ms]
Dec  8 05:55:59.253: INFO: Created: latency-svc-p6wbj
Dec  8 05:55:59.294: INFO: Got endpoints: latency-svc-hn8gq [748.868809ms]
Dec  8 05:55:59.314: INFO: Created: latency-svc-v29mc
Dec  8 05:55:59.346: INFO: Got endpoints: latency-svc-vsk4g [753.699384ms]
Dec  8 05:55:59.499: INFO: Got endpoints: latency-svc-xsgdt [805.367361ms]
Dec  8 05:55:59.499: INFO: Got endpoints: latency-svc-tw2ns [855.540201ms]
Dec  8 05:55:59.501: INFO: Got endpoints: latency-svc-zddhz [757.003194ms]
Dec  8 05:55:59.504: INFO: Created: latency-svc-khclq
Dec  8 05:55:59.533: INFO: Created: latency-svc-grzxn
Dec  8 05:55:59.543: INFO: Created: latency-svc-nw9hz
Dec  8 05:55:59.545: INFO: Got endpoints: latency-svc-kr6jr [748.21287ms]
Dec  8 05:55:59.555: INFO: Created: latency-svc-mm7hx
Dec  8 05:55:59.561: INFO: Created: latency-svc-bvrh9
Dec  8 05:55:59.593: INFO: Got endpoints: latency-svc-wbbnk [750.157258ms]
Dec  8 05:55:59.607: INFO: Created: latency-svc-5cpwk
Dec  8 05:55:59.642: INFO: Got endpoints: latency-svc-s77cj [749.602891ms]
Dec  8 05:55:59.666: INFO: Created: latency-svc-wq7fp
Dec  8 05:55:59.692: INFO: Got endpoints: latency-svc-b77bt [748.893485ms]
Dec  8 05:55:59.710: INFO: Created: latency-svc-s6wrn
Dec  8 05:55:59.749: INFO: Got endpoints: latency-svc-vsh4n [753.985763ms]
Dec  8 05:55:59.761: INFO: Created: latency-svc-bvks5
Dec  8 05:55:59.794: INFO: Got endpoints: latency-svc-k68hg [748.290381ms]
Dec  8 05:55:59.846: INFO: Got endpoints: latency-svc-k988n [752.90497ms]
Dec  8 05:55:59.894: INFO: Got endpoints: latency-svc-6jc6b [748.020199ms]
Dec  8 05:55:59.945: INFO: Got endpoints: latency-svc-scj9f [751.628869ms]
Dec  8 05:55:59.994: INFO: Got endpoints: latency-svc-p6wbj [751.178929ms]
Dec  8 05:56:00.045: INFO: Got endpoints: latency-svc-v29mc [750.656906ms]
Dec  8 05:56:00.092: INFO: Got endpoints: latency-svc-khclq [745.542171ms]
Dec  8 05:56:00.142: INFO: Got endpoints: latency-svc-grzxn [642.771564ms]
Dec  8 05:56:00.193: INFO: Got endpoints: latency-svc-nw9hz [693.929904ms]
Dec  8 05:56:00.245: INFO: Got endpoints: latency-svc-mm7hx [744.731738ms]
Dec  8 05:56:00.295: INFO: Got endpoints: latency-svc-bvrh9 [749.215449ms]
Dec  8 05:56:00.343: INFO: Got endpoints: latency-svc-5cpwk [750.36012ms]
Dec  8 05:56:00.392: INFO: Got endpoints: latency-svc-wq7fp [750.403913ms]
Dec  8 05:56:00.443: INFO: Got endpoints: latency-svc-s6wrn [750.951824ms]
Dec  8 05:56:00.494: INFO: Got endpoints: latency-svc-bvks5 [744.536114ms]
Dec  8 05:56:00.494: INFO: Latencies: [23.648572ms 36.263009ms 44.610357ms 61.948255ms 82.390063ms 120.954742ms 142.435642ms 158.43999ms 167.431451ms 183.586169ms 196.789618ms 196.881933ms 201.37689ms 206.57149ms 212.560324ms 213.222356ms 213.948795ms 215.473584ms 215.765654ms 216.818314ms 217.122209ms 217.678764ms 218.399168ms 222.031965ms 223.399098ms 225.919844ms 226.441147ms 227.564454ms 231.218447ms 233.715493ms 269.563278ms 269.816263ms 283.540428ms 286.385958ms 294.016273ms 296.112517ms 297.79595ms 298.351192ms 306.733191ms 324.075872ms 332.348544ms 336.067304ms 336.123118ms 337.126852ms 340.7397ms 341.752245ms 341.790739ms 353.429467ms 363.172084ms 402.891545ms 438.312007ms 480.135341ms 512.286405ms 554.980895ms 594.078901ms 622.457149ms 642.771564ms 663.28003ms 693.929904ms 698.808226ms 726.948225ms 741.578446ms 741.7141ms 744.255127ms 744.536114ms 744.731738ms 745.542171ms 745.553979ms 746.220646ms 746.702915ms 746.876502ms 747.070456ms 747.213482ms 747.360845ms 747.47713ms 747.562295ms 747.697929ms 747.782387ms 747.986899ms 747.990659ms 748.020199ms 748.21287ms 748.2404ms 748.282037ms 748.290381ms 748.329575ms 748.408897ms 748.543349ms 748.565058ms 748.573361ms 748.578291ms 748.704411ms 748.75807ms 748.77508ms 748.803997ms 748.824085ms 748.858641ms 748.868809ms 748.893485ms 748.903082ms 748.973131ms 748.98554ms 749.002361ms 749.031136ms 749.049989ms 749.185793ms 749.188735ms 749.210986ms 749.215338ms 749.215449ms 749.22792ms 749.261789ms 749.274873ms 749.29998ms 749.310422ms 749.351539ms 749.360978ms 749.36854ms 749.377257ms 749.390239ms 749.39406ms 749.44157ms 749.454454ms 749.480249ms 749.48716ms 749.602891ms 749.672247ms 749.672269ms 749.680166ms 749.70894ms 749.7649ms 749.776627ms 749.790367ms 749.791498ms 749.793519ms 749.809906ms 749.838107ms 749.862818ms 749.894429ms 749.935287ms 749.936566ms 749.940369ms 749.950162ms 749.975446ms 750.053914ms 750.113761ms 750.132159ms 750.157258ms 750.273865ms 750.27722ms 750.36012ms 750.403913ms 750.503178ms 750.525982ms 750.54358ms 750.570835ms 750.646354ms 750.656906ms 750.723904ms 750.748731ms 750.851595ms 750.851937ms 750.935851ms 750.951824ms 751.041611ms 751.178929ms 751.22578ms 751.330214ms 751.330526ms 751.361659ms 751.409012ms 751.414295ms 751.414381ms 751.451403ms 751.49873ms 751.523748ms 751.628869ms 751.664204ms 751.712695ms 751.804155ms 751.815804ms 751.866944ms 751.870855ms 751.964279ms 752.037581ms 752.098414ms 752.249193ms 752.513413ms 752.59823ms 752.673086ms 752.90497ms 752.932428ms 753.53964ms 753.699384ms 753.985763ms 754.452082ms 757.003194ms 760.71321ms 805.367361ms 855.540201ms]
Dec  8 05:56:00.494: INFO: 50 %ile: 748.973131ms
Dec  8 05:56:00.494: INFO: 90 %ile: 751.815804ms
Dec  8 05:56:00.494: INFO: 99 %ile: 805.367361ms
Dec  8 05:56:00.494: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:56:00.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-rffkf" for this suite.
Dec  8 05:56:20.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:56:20.724: INFO: namespace: e2e-tests-svc-latency-rffkf, resource: bindings, ignored listing per whitelist
Dec  8 05:56:20.732: INFO: namespace e2e-tests-svc-latency-rffkf deletion completed in 20.231508463s

• [SLOW TEST:30.183 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:56:20.732: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-fkxs6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-9kd8
STEP: Creating a pod to test atomic-volume-subpath
Dec  8 05:56:20.994: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9kd8" in namespace "e2e-tests-subpath-fkxs6" to be "success or failure"
Dec  8 05:56:21.016: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Pending", Reason="", readiness=false. Elapsed: 21.616342ms
Dec  8 05:56:23.021: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026996871s
Dec  8 05:56:25.033: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 4.039286708s
Dec  8 05:56:27.038: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 6.044302563s
Dec  8 05:56:29.042: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 8.048348363s
Dec  8 05:56:31.047: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 10.053021748s
Dec  8 05:56:33.051: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 12.05711652s
Dec  8 05:56:35.059: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 14.064656505s
Dec  8 05:56:37.062: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 16.068417955s
Dec  8 05:56:39.067: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 18.072734138s
Dec  8 05:56:41.071: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 20.076663537s
Dec  8 05:56:43.076: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Running", Reason="", readiness=false. Elapsed: 22.081773135s
Dec  8 05:56:45.080: INFO: Pod "pod-subpath-test-projected-9kd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.085956743s
STEP: Saw pod success
Dec  8 05:56:45.080: INFO: Pod "pod-subpath-test-projected-9kd8" satisfied condition "success or failure"
Dec  8 05:56:45.084: INFO: Trying to get logs from node phrs-one-falcon pod pod-subpath-test-projected-9kd8 container test-container-subpath-projected-9kd8: <nil>
STEP: delete the pod
Dec  8 05:56:45.116: INFO: Waiting for pod pod-subpath-test-projected-9kd8 to disappear
Dec  8 05:56:45.121: INFO: Pod pod-subpath-test-projected-9kd8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-9kd8
Dec  8 05:56:45.121: INFO: Deleting pod "pod-subpath-test-projected-9kd8" in namespace "e2e-tests-subpath-fkxs6"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:56:45.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-fkxs6" for this suite.
Dec  8 05:56:51.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:56:51.288: INFO: namespace: e2e-tests-subpath-fkxs6, resource: bindings, ignored listing per whitelist
Dec  8 05:56:51.308: INFO: namespace e2e-tests-subpath-fkxs6 deletion completed in 6.165253012s

• [SLOW TEST:30.576 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:56:51.308: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-bwn6b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Dec  8 05:56:51.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-bwn6b'
Dec  8 05:56:51.994: INFO: stderr: ""
Dec  8 05:56:51.994: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  8 05:56:52.999: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 05:56:52.999: INFO: Found 0 / 1
Dec  8 05:56:53.998: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 05:56:53.998: INFO: Found 1 / 1
Dec  8 05:56:53.998: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  8 05:56:54.001: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 05:56:54.001: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  8 05:56:54.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 patch pod redis-master-8xqgf --namespace=e2e-tests-kubectl-bwn6b -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  8 05:56:54.123: INFO: stderr: ""
Dec  8 05:56:54.123: INFO: stdout: "pod/redis-master-8xqgf patched\n"
STEP: checking annotations
Dec  8 05:56:54.128: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 05:56:54.128: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:56:54.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bwn6b" for this suite.
Dec  8 05:57:16.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:57:16.266: INFO: namespace: e2e-tests-kubectl-bwn6b, resource: bindings, ignored listing per whitelist
Dec  8 05:57:16.346: INFO: namespace e2e-tests-kubectl-bwn6b deletion completed in 22.212797018s

• [SLOW TEST:25.038 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:57:16.346: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-gm45x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Dec  8 05:57:16.627: INFO: Waiting up to 5m0s for pod "client-containers-1dce48d9-faae-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-containers-gm45x" to be "success or failure"
Dec  8 05:57:16.635: INFO: Pod "client-containers-1dce48d9-faae-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.636578ms
Dec  8 05:57:18.647: INFO: Pod "client-containers-1dce48d9-faae-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020207792s
Dec  8 05:57:20.652: INFO: Pod "client-containers-1dce48d9-faae-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024937748s
STEP: Saw pod success
Dec  8 05:57:20.652: INFO: Pod "client-containers-1dce48d9-faae-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:57:20.656: INFO: Trying to get logs from node phrs-liberal-perch pod client-containers-1dce48d9-faae-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 05:57:20.688: INFO: Waiting for pod client-containers-1dce48d9-faae-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:57:20.692: INFO: Pod client-containers-1dce48d9-faae-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:57:20.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-gm45x" for this suite.
Dec  8 05:57:26.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:57:26.834: INFO: namespace: e2e-tests-containers-gm45x, resource: bindings, ignored listing per whitelist
Dec  8 05:57:26.921: INFO: namespace e2e-tests-containers-gm45x deletion completed in 6.223303691s

• [SLOW TEST:10.575 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:57:26.922: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-z8rhw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 05:57:29.240: INFO: Waiting up to 5m0s for pod "client-envvars-2551ed03-faae-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-pods-z8rhw" to be "success or failure"
Dec  8 05:57:29.250: INFO: Pod "client-envvars-2551ed03-faae-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021003ms
Dec  8 05:57:31.254: INFO: Pod "client-envvars-2551ed03-faae-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014164223s
STEP: Saw pod success
Dec  8 05:57:31.254: INFO: Pod "client-envvars-2551ed03-faae-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:57:31.258: INFO: Trying to get logs from node phrs-one-falcon pod client-envvars-2551ed03-faae-11e8-b680-ca3d6f40e675 container env3cont: <nil>
STEP: delete the pod
Dec  8 05:57:31.281: INFO: Waiting for pod client-envvars-2551ed03-faae-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:57:31.285: INFO: Pod client-envvars-2551ed03-faae-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:57:31.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-z8rhw" for this suite.
Dec  8 05:58:25.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:58:25.474: INFO: namespace: e2e-tests-pods-z8rhw, resource: bindings, ignored listing per whitelist
Dec  8 05:58:25.486: INFO: namespace e2e-tests-pods-z8rhw deletion completed in 54.197109661s

• [SLOW TEST:58.564 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:58:25.486: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-m5psr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-46ff17b5-faae-11e8-b680-ca3d6f40e675
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-46ff17b5-faae-11e8-b680-ca3d6f40e675
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:58:29.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-m5psr" for this suite.
Dec  8 05:58:51.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:58:51.837: INFO: namespace: e2e-tests-projected-m5psr, resource: bindings, ignored listing per whitelist
Dec  8 05:58:51.979: INFO: namespace e2e-tests-projected-m5psr deletion completed in 22.172206176s

• [SLOW TEST:26.493 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:58:51.980: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-xtqcz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-9j72b
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-hwvnq
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:58:58.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-xtqcz" for this suite.
Dec  8 05:59:04.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:59:04.620: INFO: namespace: e2e-tests-namespaces-xtqcz, resource: bindings, ignored listing per whitelist
Dec  8 05:59:04.780: INFO: namespace e2e-tests-namespaces-xtqcz deletion completed in 6.210446864s
STEP: Destroying namespace "e2e-tests-nsdeletetest-9j72b" for this suite.
Dec  8 05:59:04.785: INFO: Namespace e2e-tests-nsdeletetest-9j72b was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-hwvnq" for this suite.
Dec  8 05:59:10.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:59:10.956: INFO: namespace: e2e-tests-nsdeletetest-hwvnq, resource: bindings, ignored listing per whitelist
Dec  8 05:59:10.972: INFO: namespace e2e-tests-nsdeletetest-hwvnq deletion completed in 6.187002045s

• [SLOW TEST:18.992 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:59:10.973: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-7j8vk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-7j8vk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-7j8vk to expose endpoints map[]
Dec  8 05:59:11.236: INFO: Get endpoints failed (9.388287ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec  8 05:59:12.240: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-7j8vk exposes endpoints map[] (1.013437582s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-7j8vk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-7j8vk to expose endpoints map[pod1:[80]]
Dec  8 05:59:14.298: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-7j8vk exposes endpoints map[pod1:[80]] (2.041185995s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-7j8vk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-7j8vk to expose endpoints map[pod1:[80] pod2:[80]]
Dec  8 05:59:16.412: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-7j8vk exposes endpoints map[pod1:[80] pod2:[80]] (2.10379662s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-7j8vk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-7j8vk to expose endpoints map[pod2:[80]]
Dec  8 05:59:16.490: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-7j8vk exposes endpoints map[pod2:[80]] (50.680753ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-7j8vk
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-7j8vk to expose endpoints map[]
Dec  8 05:59:16.537: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-7j8vk exposes endpoints map[] (14.39797ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:59:16.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-7j8vk" for this suite.
Dec  8 05:59:38.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:59:38.723: INFO: namespace: e2e-tests-services-7j8vk, resource: bindings, ignored listing per whitelist
Dec  8 05:59:38.781: INFO: namespace e2e-tests-services-7j8vk deletion completed in 22.19377579s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

• [SLOW TEST:27.809 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:59:38.782: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bgj8g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-72abc6be-faae-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 05:59:39.011: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-72ac7c59-faae-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-bgj8g" to be "success or failure"
Dec  8 05:59:39.019: INFO: Pod "pod-projected-secrets-72ac7c59-faae-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.718306ms
Dec  8 05:59:41.024: INFO: Pod "pod-projected-secrets-72ac7c59-faae-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01221658s
STEP: Saw pod success
Dec  8 05:59:41.024: INFO: Pod "pod-projected-secrets-72ac7c59-faae-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 05:59:41.027: INFO: Trying to get logs from node phrs-liberal-perch pod pod-projected-secrets-72ac7c59-faae-11e8-b680-ca3d6f40e675 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  8 05:59:41.049: INFO: Waiting for pod pod-projected-secrets-72ac7c59-faae-11e8-b680-ca3d6f40e675 to disappear
Dec  8 05:59:41.053: INFO: Pod pod-projected-secrets-72ac7c59-faae-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:59:41.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bgj8g" for this suite.
Dec  8 05:59:47.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 05:59:47.106: INFO: namespace: e2e-tests-projected-bgj8g, resource: bindings, ignored listing per whitelist
Dec  8 05:59:47.212: INFO: namespace e2e-tests-projected-bgj8g deletion completed in 6.153479505s

• [SLOW TEST:8.430 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 05:59:47.212: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-xzpjc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-xzpjc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xzpjc to expose endpoints map[]
Dec  8 05:59:47.433: INFO: Get endpoints failed (6.435863ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec  8 05:59:48.438: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xzpjc exposes endpoints map[] (1.010709973s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-xzpjc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xzpjc to expose endpoints map[pod1:[100]]
Dec  8 05:59:50.470: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xzpjc exposes endpoints map[pod1:[100]] (2.024042023s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-xzpjc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xzpjc to expose endpoints map[pod1:[100] pod2:[101]]
Dec  8 05:59:51.512: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xzpjc exposes endpoints map[pod1:[100] pod2:[101]] (1.030882458s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-xzpjc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xzpjc to expose endpoints map[pod2:[101]]
Dec  8 05:59:51.552: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xzpjc exposes endpoints map[pod2:[101]] (10.337917ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-xzpjc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-xzpjc to expose endpoints map[]
Dec  8 05:59:51.572: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-xzpjc exposes endpoints map[] (12.212367ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 05:59:51.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-xzpjc" for this suite.
Dec  8 06:00:13.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:00:13.664: INFO: namespace: e2e-tests-services-xzpjc, resource: bindings, ignored listing per whitelist
Dec  8 06:00:13.825: INFO: namespace e2e-tests-services-xzpjc deletion completed in 22.217424657s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

• [SLOW TEST:26.613 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:00:13.826: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-xfd8c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-xfd8c
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Dec  8 06:00:14.111: INFO: Found 0 stateful pods, waiting for 3
Dec  8 06:00:24.117: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 06:00:24.117: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 06:00:24.117: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  8 06:00:24.169: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  8 06:00:34.222: INFO: Updating stateful set ss2
Dec  8 06:00:34.235: INFO: Waiting for Pod e2e-tests-statefulset-xfd8c/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Dec  8 06:00:44.390: INFO: Found 2 stateful pods, waiting for 3
Dec  8 06:00:54.396: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 06:00:54.396: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  8 06:00:54.396: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  8 06:00:54.425: INFO: Updating stateful set ss2
Dec  8 06:00:54.436: INFO: Waiting for Pod e2e-tests-statefulset-xfd8c/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec  8 06:01:04.466: INFO: Updating stateful set ss2
Dec  8 06:01:04.481: INFO: Waiting for StatefulSet e2e-tests-statefulset-xfd8c/ss2 to complete update
Dec  8 06:01:04.481: INFO: Waiting for Pod e2e-tests-statefulset-xfd8c/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  8 06:01:14.493: INFO: Deleting all statefulset in ns e2e-tests-statefulset-xfd8c
Dec  8 06:01:14.498: INFO: Scaling statefulset ss2 to 0
Dec  8 06:01:44.535: INFO: Waiting for statefulset status.replicas updated to 0
Dec  8 06:01:44.540: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:01:44.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-xfd8c" for this suite.
Dec  8 06:01:50.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:01:50.692: INFO: namespace: e2e-tests-statefulset-xfd8c, resource: bindings, ignored listing per whitelist
Dec  8 06:01:50.749: INFO: namespace e2e-tests-statefulset-xfd8c deletion completed in 6.171883313s

• [SLOW TEST:96.923 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:01:50.750: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wzrnw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-c152fe60-faae-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 06:01:50.967: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c153a02e-faae-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-wzrnw" to be "success or failure"
Dec  8 06:01:50.986: INFO: Pod "pod-projected-configmaps-c153a02e-faae-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 18.641279ms
Dec  8 06:01:52.991: INFO: Pod "pod-projected-configmaps-c153a02e-faae-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02298854s
STEP: Saw pod success
Dec  8 06:01:52.991: INFO: Pod "pod-projected-configmaps-c153a02e-faae-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:01:52.995: INFO: Trying to get logs from node phrs-liberal-perch pod pod-projected-configmaps-c153a02e-faae-11e8-b680-ca3d6f40e675 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 06:01:53.031: INFO: Waiting for pod pod-projected-configmaps-c153a02e-faae-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:01:53.035: INFO: Pod pod-projected-configmaps-c153a02e-faae-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:01:53.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wzrnw" for this suite.
Dec  8 06:01:59.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:01:59.171: INFO: namespace: e2e-tests-projected-wzrnw, resource: bindings, ignored listing per whitelist
Dec  8 06:01:59.214: INFO: namespace e2e-tests-projected-wzrnw deletion completed in 6.172535538s

• [SLOW TEST:8.464 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:01:59.214: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-z9zff
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-c65def33-faae-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 06:01:59.432: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c65eb254-faae-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-z9zff" to be "success or failure"
Dec  8 06:01:59.440: INFO: Pod "pod-projected-configmaps-c65eb254-faae-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 8.269019ms
Dec  8 06:02:01.444: INFO: Pod "pod-projected-configmaps-c65eb254-faae-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011887047s
STEP: Saw pod success
Dec  8 06:02:01.444: INFO: Pod "pod-projected-configmaps-c65eb254-faae-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:02:01.447: INFO: Trying to get logs from node phrs-one-falcon pod pod-projected-configmaps-c65eb254-faae-11e8-b680-ca3d6f40e675 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 06:02:01.479: INFO: Waiting for pod pod-projected-configmaps-c65eb254-faae-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:02:01.485: INFO: Pod pod-projected-configmaps-c65eb254-faae-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:02:01.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z9zff" for this suite.
Dec  8 06:02:07.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:02:07.659: INFO: namespace: e2e-tests-projected-z9zff, resource: bindings, ignored listing per whitelist
Dec  8 06:02:07.717: INFO: namespace e2e-tests-projected-z9zff deletion completed in 6.225871323s

• [SLOW TEST:8.503 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:02:07.717: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-dfbdk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  8 06:02:07.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-dfbdk'
Dec  8 06:02:08.095: INFO: stderr: ""
Dec  8 06:02:08.095: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec  8 06:02:13.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-dfbdk -o json'
Dec  8 06:02:13.253: INFO: stderr: ""
Dec  8 06:02:13.253: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"00-pharos-privileged\"\n        },\n        \"creationTimestamp\": \"2018-12-08T06:02:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-dfbdk\",\n        \"resourceVersion\": \"16502\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-dfbdk/pods/e2e-test-nginx-pod\",\n        \"uid\": \"cb884405-faae-11e8-a103-96cd63cee3b8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-qx5ln\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"nodeName\": \"phrs-glorious-mastodon\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-qx5ln\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-qx5ln\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-08T06:02:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-08T06:02:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-08T06:02:09Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-08T06:02:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://5651aa329874e32ab708586b21f25b0caa0e17bceb64411ec1d2cd7b404dfa34\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2018-12-08T06:02:08Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.135.14.74\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.31.0.8\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2018-12-08T06:02:08Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  8 06:02:13.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 replace -f - --namespace=e2e-tests-kubectl-dfbdk'
Dec  8 06:02:13.471: INFO: stderr: ""
Dec  8 06:02:13.471: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Dec  8 06:02:13.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-dfbdk'
Dec  8 06:02:18.333: INFO: stderr: ""
Dec  8 06:02:18.333: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:02:18.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dfbdk" for this suite.
Dec  8 06:02:24.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:02:24.385: INFO: namespace: e2e-tests-kubectl-dfbdk, resource: bindings, ignored listing per whitelist
Dec  8 06:02:24.546: INFO: namespace e2e-tests-kubectl-dfbdk deletion completed in 6.197915753s

• [SLOW TEST:16.829 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:02:24.547: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-9d8cd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec  8 06:02:24.834: INFO: PodSpec: initContainers in spec.initContainers
Dec  8 06:03:08.066: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d5855ce7-faae-11e8-b680-ca3d6f40e675", GenerateName:"", Namespace:"e2e-tests-init-container-9d8cd", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-9d8cd/pods/pod-init-d5855ce7-faae-11e8-b680-ca3d6f40e675", UID:"d585d19c-faae-11e8-a103-96cd63cee3b8", ResourceVersion:"16653", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63679845744, loc:(*time.Location)(0x6c43b60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"834077100"}, Annotations:map[string]string{"kubernetes.io/psp":"00-pharos-privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-946sb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc422660240), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-946sb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-946sb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-946sb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc42275fc68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"phrs-liberal-perch", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc421f09200), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc42275fd80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc42275fdd0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc42275fdd8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679845744, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679845744, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679845744, loc:(*time.Location)(0x6c43b60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63679845744, loc:(*time.Location)(0x6c43b60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.135.11.179", PodIP:"172.31.240.7", StartTime:(*v1.Time)(0xc4227711c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc42184a9a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc42184aa10)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:2a03a6059f21e150ae84b0973863609494aad70f0a80eaeb64bddd8d92465812", ContainerID:"docker://eb08fb88c05383396be471dd47811b6951c1afc8a3e5ae4c955968acd959d22d"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc422771200), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc4227711e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:03:08.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-9d8cd" for this suite.
Dec  8 06:03:30.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:03:30.240: INFO: namespace: e2e-tests-init-container-9d8cd, resource: bindings, ignored listing per whitelist
Dec  8 06:03:30.252: INFO: namespace e2e-tests-init-container-9d8cd deletion completed in 22.171515112s

• [SLOW TEST:65.706 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:03:30.253: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2mmw6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Dec  8 06:03:30.461: INFO: namespace e2e-tests-kubectl-2mmw6
Dec  8 06:03:30.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-2mmw6'
Dec  8 06:03:30.658: INFO: stderr: ""
Dec  8 06:03:30.658: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  8 06:03:31.662: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 06:03:31.663: INFO: Found 1 / 1
Dec  8 06:03:31.663: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  8 06:03:31.666: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 06:03:31.666: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  8 06:03:31.666: INFO: wait on redis-master startup in e2e-tests-kubectl-2mmw6 
Dec  8 06:03:31.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 logs redis-master-n6j9c redis-master --namespace=e2e-tests-kubectl-2mmw6'
Dec  8 06:03:31.803: INFO: stderr: ""
Dec  8 06:03:31.803: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Dec 06:03:31.429 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Dec 06:03:31.429 # Server started, Redis version 3.2.12\n1:M 08 Dec 06:03:31.429 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Dec 06:03:31.429 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec  8 06:03:31.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-2mmw6'
Dec  8 06:03:31.960: INFO: stderr: ""
Dec  8 06:03:31.960: INFO: stdout: "service/rm2 exposed\n"
Dec  8 06:03:31.963: INFO: Service rm2 in namespace e2e-tests-kubectl-2mmw6 found.
STEP: exposing service
Dec  8 06:03:33.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-2mmw6'
Dec  8 06:03:34.110: INFO: stderr: ""
Dec  8 06:03:34.110: INFO: stdout: "service/rm3 exposed\n"
Dec  8 06:03:34.114: INFO: Service rm3 in namespace e2e-tests-kubectl-2mmw6 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:03:36.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2mmw6" for this suite.
Dec  8 06:03:58.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:03:58.243: INFO: namespace: e2e-tests-kubectl-2mmw6, resource: bindings, ignored listing per whitelist
Dec  8 06:03:58.270: INFO: namespace e2e-tests-kubectl-2mmw6 deletion completed in 22.142896004s

• [SLOW TEST:28.017 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:03:58.270: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-4kkrc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Dec  8 06:03:58.462: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  8 06:03:58.474: INFO: Waiting for terminating namespaces to be deleted...
Dec  8 06:03:58.478: INFO: 
Logging pods the kubelet thinks is on node phrs-glorious-mastodon before test
Dec  8 06:03:58.491: INFO: metrics-server-66d95b8778-t9z2j from kube-system started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container metrics-server ready: true, restart count 0
Dec  8 06:03:58.492: INFO: rook-ceph-mon0-4rg6w from kontena-storage started at 2018-12-08 04:56:23 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Dec  8 06:03:58.492: INFO: pharos-proxy-phrs-glorious-mastodon from kube-system started at <nil> (0 container statuses recorded)
Dec  8 06:03:58.492: INFO: weave-net-p7lbn from kube-system started at 2018-12-08 04:55:28 +0000 UTC (3 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container weave ready: true, restart count 0
Dec  8 06:03:58.492: INFO: 	Container weave-flying-shuttle ready: true, restart count 0
Dec  8 06:03:58.492: INFO: 	Container weave-npc ready: true, restart count 0
Dec  8 06:03:58.492: INFO: rook-ceph-osd-prepare-phrs-glorious-mastodon-zglms from kontena-storage started at 2018-12-08 04:57:44 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container rook-ceph-osd ready: false, restart count 0
Dec  8 06:03:58.492: INFO: default-http-backend-78796f9b55-xpm76 from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container default-http-backend ready: true, restart count 0
Dec  8 06:03:58.492: INFO: nginx-ingress-controller-br6h6 from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  8 06:03:58.492: INFO: rook-ceph-agent-8nlgc from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container rook-ceph-agent ready: true, restart count 0
Dec  8 06:03:58.492: INFO: rook-ceph-osd-id-0-8565c4d54-mhrr2 from kontena-storage started at 2018-12-08 04:57:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Dec  8 06:03:58.492: INFO: kube-proxy-2wrhj from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  8 06:03:58.492: INFO: dashboard-5fd9996979-b8dtn from kontena-lens started at 2018-12-08 04:55:48 +0000 UTC (2 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container dashboard ready: true, restart count 0
Dec  8 06:03:58.492: INFO: 	Container terminal-gateway ready: true, restart count 0
Dec  8 06:03:58.492: INFO: sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-q6qrz from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec  8 06:03:58.492: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  8 06:03:58.492: INFO: user-management-6f95f557c-clh6k from kontena-lens started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container user-management ready: true, restart count 0
Dec  8 06:03:58.492: INFO: rook-discover-swkpp from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.492: INFO: 	Container rook-discover ready: true, restart count 0
Dec  8 06:03:58.492: INFO: 
Logging pods the kubelet thinks is on node phrs-liberal-perch before test
Dec  8 06:03:58.506: INFO: kube-proxy-kflkl from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  8 06:03:58.506: INFO: nginx-ingress-controller-phwbj from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  8 06:03:58.506: INFO: rook-ceph-mon1-svdpg from kontena-storage started at 2018-12-08 04:57:18 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Dec  8 06:03:58.506: INFO: pharos-telemetry-1544248800-hrnd5 from kube-system started at 2018-12-08 06:00:05 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container agent ready: false, restart count 0
Dec  8 06:03:58.506: INFO: weave-net-nxwh9 from kube-system started at 2018-12-08 04:55:28 +0000 UTC (3 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container weave ready: true, restart count 0
Dec  8 06:03:58.506: INFO: 	Container weave-flying-shuttle ready: true, restart count 0
Dec  8 06:03:58.506: INFO: 	Container weave-npc ready: true, restart count 0
Dec  8 06:03:58.506: INFO: rook-ceph-osd-id-1-576859ddb-5xgt4 from kontena-storage started at 2018-12-08 04:57:49 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Dec  8 06:03:58.506: INFO: sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-d2p6d from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec  8 06:03:58.506: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  8 06:03:58.506: INFO: rook-ceph-osd-prepare-phrs-liberal-perch-kw6qh from kontena-storage started at 2018-12-08 04:57:44 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container rook-ceph-osd ready: false, restart count 0
Dec  8 06:03:58.506: INFO: rook-ceph-agent-76246 from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container rook-ceph-agent ready: true, restart count 0
Dec  8 06:03:58.506: INFO: sonobuoy-e2e-job-31fab27f508240cf from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container e2e ready: true, restart count 0
Dec  8 06:03:58.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  8 06:03:58.506: INFO: rook-discover-b5h6d from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.506: INFO: 	Container rook-discover ready: true, restart count 0
Dec  8 06:03:58.506: INFO: redis-76b74cb777-kzwbw from kontena-lens started at 2018-12-08 04:58:03 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.507: INFO: 	Container redis ready: true, restart count 0
Dec  8 06:03:58.507: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-08 04:59:40 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.507: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  8 06:03:58.507: INFO: pharos-proxy-phrs-liberal-perch from kube-system started at <nil> (0 container statuses recorded)
Dec  8 06:03:58.507: INFO: 
Logging pods the kubelet thinks is on node phrs-one-falcon before test
Dec  8 06:03:58.518: INFO: rook-ceph-agent-jw287 from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.518: INFO: 	Container rook-ceph-agent ready: true, restart count 0
Dec  8 06:03:58.518: INFO: rook-discover-5v5d2 from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.518: INFO: 	Container rook-discover ready: true, restart count 0
Dec  8 06:03:58.518: INFO: kontena-storage-operator-6bf6b8979d-cp4sp from kontena-storage-system started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec  8 06:03:58.519: INFO: kontena-storage-tools-59d9cfbff7-qbvq6 from kontena-storage started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Dec  8 06:03:58.519: INFO: sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-cjzdv from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec  8 06:03:58.519: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  8 06:03:58.519: INFO: rook-ceph-osd-prepare-phrs-one-falcon-7b6gx from kontena-storage started at 2018-12-08 04:57:44 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container rook-ceph-osd ready: false, restart count 0
Dec  8 06:03:58.519: INFO: pharos-proxy-phrs-one-falcon from kube-system started at <nil> (0 container statuses recorded)
Dec  8 06:03:58.519: INFO: weave-net-ck2kg from kube-system started at 2018-12-08 04:55:28 +0000 UTC (3 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container weave ready: true, restart count 0
Dec  8 06:03:58.519: INFO: 	Container weave-flying-shuttle ready: true, restart count 0
Dec  8 06:03:58.519: INFO: 	Container weave-npc ready: true, restart count 0
Dec  8 06:03:58.519: INFO: rook-ceph-osd-id-2-6c96b77d74-4c876 from kontena-storage started at 2018-12-08 04:57:49 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Dec  8 06:03:58.519: INFO: rook-ceph-mon2-7qfn7 from kontena-storage started at 2018-12-08 04:57:26 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Dec  8 06:03:58.519: INFO: kube-proxy-xmcsc from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  8 06:03:58.519: INFO: default-http-backend-78796f9b55-g5z5h from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container default-http-backend ready: true, restart count 0
Dec  8 06:03:58.519: INFO: nginx-ingress-controller-hmqdb from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  8 06:03:58.519: INFO: rook-ceph-mgr-a-b49c8bf4-pthbl from kontena-storage started at 2018-12-08 04:57:40 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container rook-ceph-mgr-a ready: true, restart count 0
Dec  8 06:03:58.519: INFO: coredns-5784d68866-d7qp2 from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container coredns ready: true, restart count 0
Dec  8 06:03:58.519: INFO: tiller-deploy-86bbfc6cdc-p6cg7 from kube-system started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container tiller ready: true, restart count 0
Dec  8 06:03:58.519: INFO: pharos-telemetry-1544245200-b4zhz from kube-system started at 2018-12-08 05:00:05 +0000 UTC (1 container statuses recorded)
Dec  8 06:03:58.519: INFO: 	Container agent ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.156e46519b530584], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:03:59.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-4kkrc" for this suite.
Dec  8 06:04:05.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:04:05.658: INFO: namespace: e2e-tests-sched-pred-4kkrc, resource: bindings, ignored listing per whitelist
Dec  8 06:04:05.779: INFO: namespace e2e-tests-sched-pred-4kkrc deletion completed in 6.215529189s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:7.509 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:04:05.781: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-4jpdb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  8 06:04:06.034: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-a,UID:11d67c00-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16834,Generation:0,CreationTimestamp:2018-12-08 06:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  8 06:04:06.034: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-a,UID:11d67c00-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16834,Generation:0,CreationTimestamp:2018-12-08 06:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  8 06:04:16.046: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-a,UID:11d67c00-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16852,Generation:0,CreationTimestamp:2018-12-08 06:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  8 06:04:16.046: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-a,UID:11d67c00-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16852,Generation:0,CreationTimestamp:2018-12-08 06:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  8 06:04:26.058: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-a,UID:11d67c00-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16869,Generation:0,CreationTimestamp:2018-12-08 06:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  8 06:04:26.059: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-a,UID:11d67c00-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16869,Generation:0,CreationTimestamp:2018-12-08 06:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  8 06:04:36.066: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-a,UID:11d67c00-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16886,Generation:0,CreationTimestamp:2018-12-08 06:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  8 06:04:36.066: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-a,UID:11d67c00-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16886,Generation:0,CreationTimestamp:2018-12-08 06:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  8 06:04:46.083: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-b,UID:29b3d21b-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16904,Generation:0,CreationTimestamp:2018-12-08 06:04:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  8 06:04:46.083: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-b,UID:29b3d21b-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16904,Generation:0,CreationTimestamp:2018-12-08 06:04:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  8 06:04:56.095: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-b,UID:29b3d21b-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16922,Generation:0,CreationTimestamp:2018-12-08 06:04:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  8 06:04:56.095: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-4jpdb,SelfLink:/api/v1/namespaces/e2e-tests-watch-4jpdb/configmaps/e2e-watch-test-configmap-b,UID:29b3d21b-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:16922,Generation:0,CreationTimestamp:2018-12-08 06:04:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:05:06.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-4jpdb" for this suite.
Dec  8 06:05:12.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:05:12.220: INFO: namespace: e2e-tests-watch-4jpdb, resource: bindings, ignored listing per whitelist
Dec  8 06:05:12.316: INFO: namespace e2e-tests-watch-4jpdb deletion completed in 6.20382577s

• [SLOW TEST:66.535 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:05:12.316: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-vhn9f
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  8 06:05:12.559: INFO: Waiting up to 5m0s for pod "pod-397bd3f8-faaf-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-vhn9f" to be "success or failure"
Dec  8 06:05:12.573: INFO: Pod "pod-397bd3f8-faaf-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 14.426952ms
Dec  8 06:05:14.578: INFO: Pod "pod-397bd3f8-faaf-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018771432s
STEP: Saw pod success
Dec  8 06:05:14.578: INFO: Pod "pod-397bd3f8-faaf-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:05:14.582: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-397bd3f8-faaf-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 06:05:14.619: INFO: Waiting for pod pod-397bd3f8-faaf-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:05:14.624: INFO: Pod pod-397bd3f8-faaf-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:05:14.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vhn9f" for this suite.
Dec  8 06:05:20.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:05:20.666: INFO: namespace: e2e-tests-emptydir-vhn9f, resource: bindings, ignored listing per whitelist
Dec  8 06:05:20.841: INFO: namespace e2e-tests-emptydir-vhn9f deletion completed in 6.210847919s

• [SLOW TEST:8.525 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:05:20.842: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-zmg7x
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  8 06:05:21.121: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-zmg7x,SelfLink:/api/v1/namespaces/e2e-tests-watch-zmg7x/configmaps/e2e-watch-test-resource-version,UID:3e933f01-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:17005,Generation:0,CreationTimestamp:2018-12-08 06:05:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  8 06:05:21.121: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-zmg7x,SelfLink:/api/v1/namespaces/e2e-tests-watch-zmg7x/configmaps/e2e-watch-test-resource-version,UID:3e933f01-faaf-11e8-a103-96cd63cee3b8,ResourceVersion:17006,Generation:0,CreationTimestamp:2018-12-08 06:05:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:05:21.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-zmg7x" for this suite.
Dec  8 06:05:27.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:05:27.258: INFO: namespace: e2e-tests-watch-zmg7x, resource: bindings, ignored listing per whitelist
Dec  8 06:05:27.303: INFO: namespace e2e-tests-watch-zmg7x deletion completed in 6.175090139s

• [SLOW TEST:6.461 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:05:27.303: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gdmxp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-42682792-faaf-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 06:05:27.528: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4268f6d9-faaf-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-gdmxp" to be "success or failure"
Dec  8 06:05:27.534: INFO: Pod "pod-projected-secrets-4268f6d9-faaf-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.625722ms
Dec  8 06:05:29.538: INFO: Pod "pod-projected-secrets-4268f6d9-faaf-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009615185s
STEP: Saw pod success
Dec  8 06:05:29.538: INFO: Pod "pod-projected-secrets-4268f6d9-faaf-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:05:29.542: INFO: Trying to get logs from node phrs-liberal-perch pod pod-projected-secrets-4268f6d9-faaf-11e8-b680-ca3d6f40e675 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  8 06:05:29.565: INFO: Waiting for pod pod-projected-secrets-4268f6d9-faaf-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:05:29.568: INFO: Pod pod-projected-secrets-4268f6d9-faaf-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:05:29.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gdmxp" for this suite.
Dec  8 06:05:35.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:05:35.696: INFO: namespace: e2e-tests-projected-gdmxp, resource: bindings, ignored listing per whitelist
Dec  8 06:05:35.710: INFO: namespace e2e-tests-projected-gdmxp deletion completed in 6.136452877s

• [SLOW TEST:8.407 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:05:35.710: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8qh6f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
Dec  8 06:05:35.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-8qh6f'
Dec  8 06:05:36.108: INFO: stderr: ""
Dec  8 06:05:36.109: INFO: stdout: "pod/pause created\n"
Dec  8 06:05:36.109: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  8 06:05:36.109: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-8qh6f" to be "running and ready"
Dec  8 06:05:36.114: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.417465ms
Dec  8 06:05:38.117: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008869056s
Dec  8 06:05:38.117: INFO: Pod "pause" satisfied condition "running and ready"
Dec  8 06:05:38.117: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  8 06:05:38.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-8qh6f'
Dec  8 06:05:38.229: INFO: stderr: ""
Dec  8 06:05:38.229: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  8 06:05:38.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pod pause -L testing-label --namespace=e2e-tests-kubectl-8qh6f'
Dec  8 06:05:38.348: INFO: stderr: ""
Dec  8 06:05:38.348: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  8 06:05:38.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 label pods pause testing-label- --namespace=e2e-tests-kubectl-8qh6f'
Dec  8 06:05:38.480: INFO: stderr: ""
Dec  8 06:05:38.480: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  8 06:05:38.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pod pause -L testing-label --namespace=e2e-tests-kubectl-8qh6f'
Dec  8 06:05:38.589: INFO: stderr: ""
Dec  8 06:05:38.589: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
Dec  8 06:05:38.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-8qh6f'
Dec  8 06:05:38.712: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  8 06:05:38.712: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  8 06:05:38.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-8qh6f'
Dec  8 06:05:38.823: INFO: stderr: "No resources found.\n"
Dec  8 06:05:38.823: INFO: stdout: ""
Dec  8 06:05:38.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 get pods -l name=pause --namespace=e2e-tests-kubectl-8qh6f -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  8 06:05:38.927: INFO: stderr: ""
Dec  8 06:05:38.927: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:05:38.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8qh6f" for this suite.
Dec  8 06:05:44.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:05:44.994: INFO: namespace: e2e-tests-kubectl-8qh6f, resource: bindings, ignored listing per whitelist
Dec  8 06:05:45.077: INFO: namespace e2e-tests-kubectl-8qh6f deletion completed in 6.144339567s

• [SLOW TEST:9.367 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:05:45.077: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hz9hb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4cff774b-faaf-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 06:05:45.296: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4d00176e-faaf-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-hz9hb" to be "success or failure"
Dec  8 06:05:45.303: INFO: Pod "pod-projected-configmaps-4d00176e-faaf-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.027662ms
Dec  8 06:05:47.308: INFO: Pod "pod-projected-configmaps-4d00176e-faaf-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011635674s
STEP: Saw pod success
Dec  8 06:05:47.308: INFO: Pod "pod-projected-configmaps-4d00176e-faaf-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:05:47.311: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-projected-configmaps-4d00176e-faaf-11e8-b680-ca3d6f40e675 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 06:05:47.338: INFO: Waiting for pod pod-projected-configmaps-4d00176e-faaf-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:05:47.341: INFO: Pod pod-projected-configmaps-4d00176e-faaf-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:05:47.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hz9hb" for this suite.
Dec  8 06:05:53.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:05:53.389: INFO: namespace: e2e-tests-projected-hz9hb, resource: bindings, ignored listing per whitelist
Dec  8 06:05:53.515: INFO: namespace e2e-tests-projected-hz9hb deletion completed in 6.16891884s

• [SLOW TEST:8.438 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:05:53.516: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-qglm2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Dec  8 06:05:53.714: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  8 06:05:53.726: INFO: Waiting for terminating namespaces to be deleted...
Dec  8 06:05:53.732: INFO: 
Logging pods the kubelet thinks is on node phrs-glorious-mastodon before test
Dec  8 06:05:53.743: INFO: rook-discover-swkpp from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.743: INFO: 	Container rook-discover ready: true, restart count 0
Dec  8 06:05:53.743: INFO: sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-q6qrz from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 06:05:53.743: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec  8 06:05:53.743: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  8 06:05:53.743: INFO: user-management-6f95f557c-clh6k from kontena-lens started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.743: INFO: 	Container user-management ready: true, restart count 0
Dec  8 06:05:53.743: INFO: weave-net-p7lbn from kube-system started at 2018-12-08 04:55:28 +0000 UTC (3 container statuses recorded)
Dec  8 06:05:53.743: INFO: 	Container weave ready: true, restart count 0
Dec  8 06:05:53.743: INFO: 	Container weave-flying-shuttle ready: true, restart count 0
Dec  8 06:05:53.743: INFO: 	Container weave-npc ready: true, restart count 0
Dec  8 06:05:53.743: INFO: metrics-server-66d95b8778-t9z2j from kube-system started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.743: INFO: 	Container metrics-server ready: true, restart count 0
Dec  8 06:05:53.743: INFO: rook-ceph-mon0-4rg6w from kontena-storage started at 2018-12-08 04:56:23 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.743: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Dec  8 06:05:53.743: INFO: pharos-proxy-phrs-glorious-mastodon from kube-system started at <nil> (0 container statuses recorded)
Dec  8 06:05:53.743: INFO: nginx-ingress-controller-br6h6 from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.743: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  8 06:05:53.743: INFO: rook-ceph-osd-prepare-phrs-glorious-mastodon-zglms from kontena-storage started at 2018-12-08 04:57:44 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.744: INFO: 	Container rook-ceph-osd ready: false, restart count 0
Dec  8 06:05:53.744: INFO: default-http-backend-78796f9b55-xpm76 from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.744: INFO: 	Container default-http-backend ready: true, restart count 0
Dec  8 06:05:53.744: INFO: dashboard-5fd9996979-b8dtn from kontena-lens started at 2018-12-08 04:55:48 +0000 UTC (2 container statuses recorded)
Dec  8 06:05:53.744: INFO: 	Container dashboard ready: true, restart count 0
Dec  8 06:05:53.744: INFO: 	Container terminal-gateway ready: true, restart count 0
Dec  8 06:05:53.744: INFO: rook-ceph-agent-8nlgc from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.744: INFO: 	Container rook-ceph-agent ready: true, restart count 0
Dec  8 06:05:53.744: INFO: rook-ceph-osd-id-0-8565c4d54-mhrr2 from kontena-storage started at 2018-12-08 04:57:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.744: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Dec  8 06:05:53.744: INFO: kube-proxy-2wrhj from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.744: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  8 06:05:53.744: INFO: 
Logging pods the kubelet thinks is on node phrs-liberal-perch before test
Dec  8 06:05:53.756: INFO: sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-d2p6d from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 06:05:53.756: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec  8 06:05:53.756: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  8 06:05:53.756: INFO: rook-ceph-osd-prepare-phrs-liberal-perch-kw6qh from kontena-storage started at 2018-12-08 04:57:44 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.756: INFO: 	Container rook-ceph-osd ready: false, restart count 0
Dec  8 06:05:53.756: INFO: rook-ceph-osd-id-1-576859ddb-5xgt4 from kontena-storage started at 2018-12-08 04:57:49 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.756: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Dec  8 06:05:53.756: INFO: sonobuoy-e2e-job-31fab27f508240cf from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 06:05:53.756: INFO: 	Container e2e ready: true, restart count 0
Dec  8 06:05:53.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  8 06:05:53.756: INFO: rook-discover-b5h6d from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.756: INFO: 	Container rook-discover ready: true, restart count 0
Dec  8 06:05:53.756: INFO: rook-ceph-agent-76246 from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.756: INFO: 	Container rook-ceph-agent ready: true, restart count 0
Dec  8 06:05:53.756: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-08 04:59:40 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.756: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  8 06:05:53.756: INFO: pharos-proxy-phrs-liberal-perch from kube-system started at <nil> (0 container statuses recorded)
Dec  8 06:05:53.756: INFO: redis-76b74cb777-kzwbw from kontena-lens started at 2018-12-08 04:58:03 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.756: INFO: 	Container redis ready: true, restart count 0
Dec  8 06:05:53.756: INFO: nginx-ingress-controller-phwbj from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.756: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  8 06:05:53.757: INFO: rook-ceph-mon1-svdpg from kontena-storage started at 2018-12-08 04:57:18 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.757: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Dec  8 06:05:53.757: INFO: pharos-telemetry-1544248800-hrnd5 from kube-system started at 2018-12-08 06:00:05 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.757: INFO: 	Container agent ready: false, restart count 0
Dec  8 06:05:53.757: INFO: weave-net-nxwh9 from kube-system started at 2018-12-08 04:55:28 +0000 UTC (3 container statuses recorded)
Dec  8 06:05:53.757: INFO: 	Container weave ready: true, restart count 0
Dec  8 06:05:53.757: INFO: 	Container weave-flying-shuttle ready: true, restart count 0
Dec  8 06:05:53.757: INFO: 	Container weave-npc ready: true, restart count 0
Dec  8 06:05:53.757: INFO: kube-proxy-kflkl from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.757: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  8 06:05:53.757: INFO: 
Logging pods the kubelet thinks is on node phrs-one-falcon before test
Dec  8 06:05:53.768: INFO: rook-ceph-osd-prepare-phrs-one-falcon-7b6gx from kontena-storage started at 2018-12-08 04:57:44 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container rook-ceph-osd ready: false, restart count 0
Dec  8 06:05:53.768: INFO: rook-ceph-osd-id-2-6c96b77d74-4c876 from kontena-storage started at 2018-12-08 04:57:49 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container rook-ceph-osd ready: true, restart count 0
Dec  8 06:05:53.768: INFO: pharos-proxy-phrs-one-falcon from kube-system started at <nil> (0 container statuses recorded)
Dec  8 06:05:53.768: INFO: weave-net-ck2kg from kube-system started at 2018-12-08 04:55:28 +0000 UTC (3 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container weave ready: true, restart count 0
Dec  8 06:05:53.768: INFO: 	Container weave-flying-shuttle ready: true, restart count 0
Dec  8 06:05:53.768: INFO: 	Container weave-npc ready: true, restart count 0
Dec  8 06:05:53.768: INFO: rook-ceph-mon2-7qfn7 from kontena-storage started at 2018-12-08 04:57:26 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container rook-ceph-mon ready: true, restart count 0
Dec  8 06:05:53.768: INFO: nginx-ingress-controller-hmqdb from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  8 06:05:53.768: INFO: rook-ceph-mgr-a-b49c8bf4-pthbl from kontena-storage started at 2018-12-08 04:57:40 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container rook-ceph-mgr-a ready: true, restart count 0
Dec  8 06:05:53.768: INFO: kube-proxy-xmcsc from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  8 06:05:53.768: INFO: default-http-backend-78796f9b55-g5z5h from ingress-nginx started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container default-http-backend ready: true, restart count 0
Dec  8 06:05:53.768: INFO: pharos-telemetry-1544245200-b4zhz from kube-system started at 2018-12-08 05:00:05 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container agent ready: false, restart count 0
Dec  8 06:05:53.768: INFO: coredns-5784d68866-d7qp2 from kube-system started at 2018-12-08 04:55:28 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container coredns ready: true, restart count 0
Dec  8 06:05:53.768: INFO: tiller-deploy-86bbfc6cdc-p6cg7 from kube-system started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container tiller ready: true, restart count 0
Dec  8 06:05:53.768: INFO: rook-ceph-agent-jw287 from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container rook-ceph-agent ready: true, restart count 0
Dec  8 06:05:53.768: INFO: rook-discover-5v5d2 from kontena-storage-system started at 2018-12-08 04:56:16 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container rook-discover ready: true, restart count 0
Dec  8 06:05:53.768: INFO: sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-cjzdv from heptio-sonobuoy started at 2018-12-08 04:59:45 +0000 UTC (2 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec  8 06:05:53.768: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  8 06:05:53.768: INFO: kontena-storage-operator-6bf6b8979d-cp4sp from kontena-storage-system started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec  8 06:05:53.768: INFO: kontena-storage-tools-59d9cfbff7-qbvq6 from kontena-storage started at 2018-12-08 04:55:48 +0000 UTC (1 container statuses recorded)
Dec  8 06:05:53.768: INFO: 	Container rook-ceph-tools ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node phrs-glorious-mastodon
STEP: verifying the node has the label node phrs-liberal-perch
STEP: verifying the node has the label node phrs-one-falcon
Dec  8 06:05:53.841: INFO: Pod sonobuoy requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.841: INFO: Pod sonobuoy-e2e-job-31fab27f508240cf requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.842: INFO: Pod sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-cjzdv requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.842: INFO: Pod sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-d2p6d requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.842: INFO: Pod sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-q6qrz requesting resource cpu=0m on Node phrs-glorious-mastodon
Dec  8 06:05:53.842: INFO: Pod default-http-backend-78796f9b55-g5z5h requesting resource cpu=10m on Node phrs-one-falcon
Dec  8 06:05:53.842: INFO: Pod default-http-backend-78796f9b55-xpm76 requesting resource cpu=10m on Node phrs-glorious-mastodon
Dec  8 06:05:53.842: INFO: Pod nginx-ingress-controller-br6h6 requesting resource cpu=0m on Node phrs-glorious-mastodon
Dec  8 06:05:53.842: INFO: Pod nginx-ingress-controller-hmqdb requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.842: INFO: Pod nginx-ingress-controller-phwbj requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.842: INFO: Pod dashboard-5fd9996979-b8dtn requesting resource cpu=150m on Node phrs-glorious-mastodon
Dec  8 06:05:53.842: INFO: Pod redis-76b74cb777-kzwbw requesting resource cpu=20m on Node phrs-liberal-perch
Dec  8 06:05:53.842: INFO: Pod user-management-6f95f557c-clh6k requesting resource cpu=20m on Node phrs-glorious-mastodon
Dec  8 06:05:53.842: INFO: Pod kontena-storage-operator-6bf6b8979d-cp4sp requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.842: INFO: Pod rook-ceph-agent-76246 requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.842: INFO: Pod rook-ceph-agent-8nlgc requesting resource cpu=0m on Node phrs-glorious-mastodon
Dec  8 06:05:53.842: INFO: Pod rook-ceph-agent-jw287 requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.842: INFO: Pod rook-discover-5v5d2 requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.842: INFO: Pod rook-discover-b5h6d requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.842: INFO: Pod rook-discover-swkpp requesting resource cpu=0m on Node phrs-glorious-mastodon
Dec  8 06:05:53.843: INFO: Pod kontena-storage-tools-59d9cfbff7-qbvq6 requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.843: INFO: Pod rook-ceph-mgr-a-b49c8bf4-pthbl requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.843: INFO: Pod rook-ceph-mon0-4rg6w requesting resource cpu=0m on Node phrs-glorious-mastodon
Dec  8 06:05:53.843: INFO: Pod rook-ceph-mon1-svdpg requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.843: INFO: Pod rook-ceph-mon2-7qfn7 requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.843: INFO: Pod rook-ceph-osd-id-0-8565c4d54-mhrr2 requesting resource cpu=0m on Node phrs-glorious-mastodon
Dec  8 06:05:53.843: INFO: Pod rook-ceph-osd-id-1-576859ddb-5xgt4 requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.843: INFO: Pod rook-ceph-osd-id-2-6c96b77d74-4c876 requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.843: INFO: Pod coredns-5784d68866-d7qp2 requesting resource cpu=100m on Node phrs-one-falcon
Dec  8 06:05:53.843: INFO: Pod kube-proxy-2wrhj requesting resource cpu=0m on Node phrs-glorious-mastodon
Dec  8 06:05:53.843: INFO: Pod kube-proxy-kflkl requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.843: INFO: Pod kube-proxy-xmcsc requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.843: INFO: Pod metrics-server-66d95b8778-t9z2j requesting resource cpu=10m on Node phrs-glorious-mastodon
Dec  8 06:05:53.843: INFO: Pod pharos-proxy-phrs-glorious-mastodon requesting resource cpu=0m on Node phrs-glorious-mastodon
Dec  8 06:05:53.843: INFO: Pod pharos-proxy-phrs-liberal-perch requesting resource cpu=0m on Node phrs-liberal-perch
Dec  8 06:05:53.843: INFO: Pod pharos-proxy-phrs-one-falcon requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.843: INFO: Pod tiller-deploy-86bbfc6cdc-p6cg7 requesting resource cpu=0m on Node phrs-one-falcon
Dec  8 06:05:53.843: INFO: Pod weave-net-ck2kg requesting resource cpu=30m on Node phrs-one-falcon
Dec  8 06:05:53.844: INFO: Pod weave-net-nxwh9 requesting resource cpu=30m on Node phrs-liberal-perch
Dec  8 06:05:53.844: INFO: Pod weave-net-p7lbn requesting resource cpu=30m on Node phrs-glorious-mastodon
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5219c678-faaf-11e8-b680-ca3d6f40e675.156e466c74b9b725], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-qglm2/filler-pod-5219c678-faaf-11e8-b680-ca3d6f40e675 to phrs-glorious-mastodon]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5219c678-faaf-11e8-b680-ca3d6f40e675.156e466c9a310c20], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5219c678-faaf-11e8-b680-ca3d6f40e675.156e466c9c9e01a8], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5219c678-faaf-11e8-b680-ca3d6f40e675.156e466ca2ae1491], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521d6507-faaf-11e8-b680-ca3d6f40e675.156e466c75f5b2a0], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-qglm2/filler-pod-521d6507-faaf-11e8-b680-ca3d6f40e675 to phrs-liberal-perch]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521d6507-faaf-11e8-b680-ca3d6f40e675.156e466c9c9cec49], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521d6507-faaf-11e8-b680-ca3d6f40e675.156e466c9f80af84], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521d6507-faaf-11e8-b680-ca3d6f40e675.156e466ca56bef03], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521e5b13-faaf-11e8-b680-ca3d6f40e675.156e466c77687234], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-qglm2/filler-pod-521e5b13-faaf-11e8-b680-ca3d6f40e675 to phrs-one-falcon]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521e5b13-faaf-11e8-b680-ca3d6f40e675.156e466c9e321dff], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521e5b13-faaf-11e8-b680-ca3d6f40e675.156e466ca0a96db2], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521e5b13-faaf-11e8-b680-ca3d6f40e675.156e466ca85f4242], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.156e466cf1a619d2], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node phrs-glorious-mastodon
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node phrs-liberal-perch
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node phrs-one-falcon
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:05:57.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-qglm2" for this suite.
Dec  8 06:06:03.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:06:03.174: INFO: namespace: e2e-tests-sched-pred-qglm2, resource: bindings, ignored listing per whitelist
Dec  8 06:06:03.216: INFO: namespace e2e-tests-sched-pred-qglm2 deletion completed in 6.175823309s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:9.701 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:06:03.217: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5lhfw
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-57d351ba-faaf-11e8-b680-ca3d6f40e675
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:06:05.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5lhfw" for this suite.
Dec  8 06:06:27.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:06:27.687: INFO: namespace: e2e-tests-configmap-5lhfw, resource: bindings, ignored listing per whitelist
Dec  8 06:06:27.746: INFO: namespace e2e-tests-configmap-5lhfw deletion completed in 22.211009422s

• [SLOW TEST:24.529 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:06:27.747: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8h9vn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Dec  8 06:06:27.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 api-versions'
Dec  8 06:06:28.085: INFO: stderr: ""
Dec  8 06:06:28.085: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbeta.kontena.io/v1\nceph.rook.io/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrook.io/v1alpha2\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:06:28.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8h9vn" for this suite.
Dec  8 06:06:34.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:06:34.210: INFO: namespace: e2e-tests-kubectl-8h9vn, resource: bindings, ignored listing per whitelist
Dec  8 06:06:34.215: INFO: namespace e2e-tests-kubectl-8h9vn deletion completed in 6.124664686s

• [SLOW TEST:6.468 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:06:34.215: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-vpddb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  8 06:06:40.452: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  8 06:06:40.456: INFO: Pod pod-with-poststart-http-hook still exists
Dec  8 06:06:42.456: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  8 06:06:42.460: INFO: Pod pod-with-poststart-http-hook still exists
Dec  8 06:06:44.456: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  8 06:06:44.459: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:06:44.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-vpddb" for this suite.
Dec  8 06:07:06.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:07:06.545: INFO: namespace: e2e-tests-container-lifecycle-hook-vpddb, resource: bindings, ignored listing per whitelist
Dec  8 06:07:06.663: INFO: namespace e2e-tests-container-lifecycle-hook-vpddb deletion completed in 22.199322889s

• [SLOW TEST:32.448 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:07:06.663: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-x7vxv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-7da65ca2-faaf-11e8-b680-ca3d6f40e675
STEP: Creating secret with name s-test-opt-upd-7da65ced-faaf-11e8-b680-ca3d6f40e675
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7da65ca2-faaf-11e8-b680-ca3d6f40e675
STEP: Updating secret s-test-opt-upd-7da65ced-faaf-11e8-b680-ca3d6f40e675
STEP: Creating secret with name s-test-opt-create-7da65d08-faaf-11e8-b680-ca3d6f40e675
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:07:11.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x7vxv" for this suite.
Dec  8 06:07:33.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:07:33.187: INFO: namespace: e2e-tests-projected-x7vxv, resource: bindings, ignored listing per whitelist
Dec  8 06:07:33.260: INFO: namespace e2e-tests-projected-x7vxv deletion completed in 22.168597523s

• [SLOW TEST:26.597 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:07:33.260: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-7gdg2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-8d79b7a3-faaf-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 06:07:33.478: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d7a878d-faaf-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-configmap-7gdg2" to be "success or failure"
Dec  8 06:07:33.493: INFO: Pod "pod-configmaps-8d7a878d-faaf-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 15.125391ms
Dec  8 06:07:35.498: INFO: Pod "pod-configmaps-8d7a878d-faaf-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019628149s
STEP: Saw pod success
Dec  8 06:07:35.498: INFO: Pod "pod-configmaps-8d7a878d-faaf-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:07:35.501: INFO: Trying to get logs from node phrs-one-falcon pod pod-configmaps-8d7a878d-faaf-11e8-b680-ca3d6f40e675 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 06:07:35.527: INFO: Waiting for pod pod-configmaps-8d7a878d-faaf-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:07:35.536: INFO: Pod pod-configmaps-8d7a878d-faaf-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:07:35.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7gdg2" for this suite.
Dec  8 06:07:41.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:07:41.673: INFO: namespace: e2e-tests-configmap-7gdg2, resource: bindings, ignored listing per whitelist
Dec  8 06:07:41.713: INFO: namespace e2e-tests-configmap-7gdg2 deletion completed in 6.172161678s

• [SLOW TEST:8.453 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:07:41.714: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-7s9xf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-9286e30f-faaf-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 06:07:41.951: INFO: Waiting up to 5m0s for pod "pod-configmaps-92883b56-faaf-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-configmap-7s9xf" to be "success or failure"
Dec  8 06:07:41.956: INFO: Pod "pod-configmaps-92883b56-faaf-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.102688ms
Dec  8 06:07:43.960: INFO: Pod "pod-configmaps-92883b56-faaf-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009232767s
STEP: Saw pod success
Dec  8 06:07:43.961: INFO: Pod "pod-configmaps-92883b56-faaf-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:07:43.964: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-configmaps-92883b56-faaf-11e8-b680-ca3d6f40e675 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 06:07:43.988: INFO: Waiting for pod pod-configmaps-92883b56-faaf-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:07:43.992: INFO: Pod pod-configmaps-92883b56-faaf-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:07:43.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7s9xf" for this suite.
Dec  8 06:07:50.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:07:50.128: INFO: namespace: e2e-tests-configmap-7s9xf, resource: bindings, ignored listing per whitelist
Dec  8 06:07:50.128: INFO: namespace e2e-tests-configmap-7s9xf deletion completed in 6.129620085s

• [SLOW TEST:8.414 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:07:50.129: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-msfcg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-9787ab36-faaf-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 06:07:50.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97885d58-faaf-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-msfcg" to be "success or failure"
Dec  8 06:07:50.352: INFO: Pod "pod-projected-secrets-97885d58-faaf-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 9.386689ms
Dec  8 06:07:52.356: INFO: Pod "pod-projected-secrets-97885d58-faaf-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013037704s
STEP: Saw pod success
Dec  8 06:07:52.356: INFO: Pod "pod-projected-secrets-97885d58-faaf-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:07:52.359: INFO: Trying to get logs from node phrs-liberal-perch pod pod-projected-secrets-97885d58-faaf-11e8-b680-ca3d6f40e675 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  8 06:07:52.387: INFO: Waiting for pod pod-projected-secrets-97885d58-faaf-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:07:52.390: INFO: Pod pod-projected-secrets-97885d58-faaf-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:07:52.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-msfcg" for this suite.
Dec  8 06:07:58.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:07:58.523: INFO: namespace: e2e-tests-projected-msfcg, resource: bindings, ignored listing per whitelist
Dec  8 06:07:58.547: INFO: namespace e2e-tests-projected-msfcg deletion completed in 6.152363786s

• [SLOW TEST:8.418 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:07:58.547: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-h6cm4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Dec  8 06:07:58.753: INFO: Waiting up to 5m0s for pod "client-containers-9c8b095f-faaf-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-containers-h6cm4" to be "success or failure"
Dec  8 06:07:58.757: INFO: Pod "client-containers-9c8b095f-faaf-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 4.606128ms
Dec  8 06:08:00.767: INFO: Pod "client-containers-9c8b095f-faaf-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013997897s
STEP: Saw pod success
Dec  8 06:08:00.767: INFO: Pod "client-containers-9c8b095f-faaf-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:08:00.770: INFO: Trying to get logs from node phrs-one-falcon pod client-containers-9c8b095f-faaf-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 06:08:00.801: INFO: Waiting for pod client-containers-9c8b095f-faaf-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:08:00.805: INFO: Pod client-containers-9c8b095f-faaf-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:08:00.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-h6cm4" for this suite.
Dec  8 06:08:06.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:08:06.992: INFO: namespace: e2e-tests-containers-h6cm4, resource: bindings, ignored listing per whitelist
Dec  8 06:08:07.032: INFO: namespace e2e-tests-containers-h6cm4 deletion completed in 6.222166637s

• [SLOW TEST:8.485 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:08:07.033: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jjhvq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  8 06:08:07.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-jjhvq'
Dec  8 06:08:07.594: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Dec  8 06:08:07.594: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
Dec  8 06:08:09.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-jjhvq'
Dec  8 06:08:09.735: INFO: stderr: ""
Dec  8 06:08:09.735: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:08:09.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jjhvq" for this suite.
Dec  8 06:08:15.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:08:15.878: INFO: namespace: e2e-tests-kubectl-jjhvq, resource: bindings, ignored listing per whitelist
Dec  8 06:08:15.978: INFO: namespace e2e-tests-kubectl-jjhvq deletion completed in 6.235425522s

• [SLOW TEST:8.945 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:08:15.978: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-qsrxf
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-a6f378f0-faaf-11e8-b680-ca3d6f40e675
STEP: Creating secret with name s-test-opt-upd-a6f37956-faaf-11e8-b680-ca3d6f40e675
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a6f378f0-faaf-11e8-b680-ca3d6f40e675
STEP: Updating secret s-test-opt-upd-a6f37956-faaf-11e8-b680-ca3d6f40e675
STEP: Creating secret with name s-test-opt-create-a6f3797d-faaf-11e8-b680-ca3d6f40e675
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:08:20.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qsrxf" for this suite.
Dec  8 06:08:42.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:08:42.529: INFO: namespace: e2e-tests-secrets-qsrxf, resource: bindings, ignored listing per whitelist
Dec  8 06:08:42.538: INFO: namespace e2e-tests-secrets-qsrxf deletion completed in 22.173311887s

• [SLOW TEST:26.560 seconds]
[sig-storage] Secrets
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:08:42.538: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-jdq8c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:98
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 06:08:42.755: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  8 06:08:42.769: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:42.772: INFO: Number of nodes with available pods: 0
Dec  8 06:08:42.772: INFO: Node phrs-glorious-mastodon is running more than one daemon pod
Dec  8 06:08:43.903: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:43.907: INFO: Number of nodes with available pods: 1
Dec  8 06:08:43.907: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 06:08:44.777: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:44.782: INFO: Number of nodes with available pods: 3
Dec  8 06:08:44.782: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  8 06:08:44.812: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:44.812: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:44.812: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:44.822: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:45.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:45.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:45.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:45.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:46.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:46.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:46.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:46.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:47.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:47.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:47.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:47.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:48.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:48.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:48.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:48.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:49.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:49.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:49.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:49.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:50.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:50.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:50.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:50.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:51.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:51.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:51.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:51.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:52.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:52.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:52.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:52.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:53.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:53.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:53.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:53.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:54.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:54.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:54.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:54.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:55.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:55.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:55.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:55.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:56.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:56.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:56.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:56.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:57.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:57.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:57.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:57.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:58.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:58.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:58.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:58.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:08:59.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:59.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:59.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:08:59.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:00.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:00.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:00.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:00.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:01.840: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:01.841: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:01.841: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:01.852: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:02.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:02.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:02.828: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:02.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:03.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:03.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:03.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:03.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:04.836: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:04.836: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:04.836: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:04.842: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:05.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:05.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:05.828: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:05.835: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:06.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:06.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:06.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:06.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:07.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:07.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:07.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:07.836: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:08.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:08.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:08.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:08.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:09.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:09.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:09.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:09.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:10.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:10.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:10.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:10.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:11.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:11.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:11.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:11.838: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:12.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:12.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:12.828: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:12.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:13.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:13.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:13.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:13.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:14.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:14.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:14.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:14.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:15.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:15.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:15.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:15.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:16.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:16.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:16.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:16.827: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:16.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:17.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:17.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:17.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:17.827: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:17.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:18.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:18.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:18.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:18.827: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:18.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:19.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:19.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:19.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:19.827: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:19.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:20.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:20.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:20.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:20.827: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:20.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:21.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:21.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:21.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:21.826: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:21.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:22.832: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:22.832: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:22.832: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:22.832: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:22.838: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:23.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:23.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:23.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:23.826: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:23.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:24.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:24.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:24.826: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:24.826: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:24.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:25.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:25.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:25.828: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:25.828: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:25.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:26.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:26.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:26.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:26.827: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:26.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:27.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:27.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:27.827: INFO: Wrong image for pod: daemon-set-p869k. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:27.827: INFO: Pod daemon-set-p869k is not available
Dec  8 06:09:27.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:28.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:28.827: INFO: Pod daemon-set-fz889 is not available
Dec  8 06:09:28.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:28.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:29.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:29.826: INFO: Pod daemon-set-fz889 is not available
Dec  8 06:09:29.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:29.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:30.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:30.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:30.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:31.834: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:31.834: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:31.841: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:32.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:32.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:32.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:33.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:33.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:33.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:34.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:34.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:34.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:35.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:35.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:35.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:36.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:36.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:36.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:37.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:37.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:37.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:38.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:38.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:38.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:39.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:39.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:39.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:40.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:40.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:40.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:41.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:41.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:41.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:42.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:42.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:42.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:43.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:43.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:43.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:44.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:44.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:44.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:45.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:45.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:45.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:46.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:46.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:46.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:47.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:47.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:47.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:48.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:48.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:48.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:49.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:49.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:49.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:50.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:50.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:50.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:51.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:51.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:51.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:52.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:52.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:52.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:53.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:53.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:53.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:54.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:54.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:54.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:55.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:55.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:55.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:56.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:56.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:56.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:57.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:57.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:57.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:58.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:58.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:58.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:09:59.829: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:59.829: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:09:59.836: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:00.826: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:00.826: INFO: Pod daemon-set-9mxlt is not available
Dec  8 06:10:00.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:00.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:01.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:01.828: INFO: Pod daemon-set-9mxlt is not available
Dec  8 06:10:01.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:01.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:02.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:02.828: INFO: Pod daemon-set-9mxlt is not available
Dec  8 06:10:02.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:02.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:03.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:03.827: INFO: Pod daemon-set-9mxlt is not available
Dec  8 06:10:03.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:03.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:04.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:04.828: INFO: Pod daemon-set-9mxlt is not available
Dec  8 06:10:04.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:04.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:05.828: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:05.828: INFO: Pod daemon-set-9mxlt is not available
Dec  8 06:10:05.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:05.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:06.827: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:06.827: INFO: Pod daemon-set-9mxlt is not available
Dec  8 06:10:06.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:06.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:07.832: INFO: Wrong image for pod: daemon-set-9mxlt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:07.832: INFO: Pod daemon-set-9mxlt is not available
Dec  8 06:10:07.832: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:07.940: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:08.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:08.827: INFO: Pod daemon-set-r2h2f is not available
Dec  8 06:10:08.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:09.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:09.828: INFO: Pod daemon-set-r2h2f is not available
Dec  8 06:10:09.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:10.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:10.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:11.848: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:11.854: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:12.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:12.852: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:13.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:13.854: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:14.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:14.836: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:15.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:15.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:16.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:16.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:17.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:17.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:18.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:18.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:19.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:19.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:20.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:20.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:21.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:21.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:22.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:22.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:23.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:23.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:24.829: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:24.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:25.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:25.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:26.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:26.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:27.828: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:27.840: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:28.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:28.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:29.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:29.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:30.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:30.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:31.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:31.830: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:32.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:32.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:33.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:33.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:34.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:34.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:35.829: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:35.833: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:36.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:36.832: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:37.826: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:37.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:38.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:38.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:39.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:39.834: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:40.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:40.835: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:41.827: INFO: Wrong image for pod: daemon-set-g47n8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec  8 06:10:41.827: INFO: Pod daemon-set-g47n8 is not available
Dec  8 06:10:41.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:42.827: INFO: Pod daemon-set-ktltc is not available
Dec  8 06:10:42.831: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  8 06:10:42.835: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:42.839: INFO: Number of nodes with available pods: 2
Dec  8 06:10:42.839: INFO: Node phrs-liberal-perch is running more than one daemon pod
Dec  8 06:10:43.845: INFO: DaemonSet pods can't tolerate node phrs-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  8 06:10:43.850: INFO: Number of nodes with available pods: 3
Dec  8 06:10:43.850: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:64
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-jdq8c, will wait for the garbage collector to delete the pods
Dec  8 06:10:43.958: INFO: Deleting {extensions DaemonSet} daemon-set took: 33.693606ms
Dec  8 06:10:44.059: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.33985ms
Dec  8 06:10:58.462: INFO: Number of nodes with available pods: 0
Dec  8 06:10:58.462: INFO: Number of running nodes: 0, number of available pods: 0
Dec  8 06:10:58.465: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-jdq8c/daemonsets","resourceVersion":"18177"},"items":null}

Dec  8 06:10:58.469: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-jdq8c/pods","resourceVersion":"18177"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:10:58.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-jdq8c" for this suite.
Dec  8 06:11:04.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:11:04.662: INFO: namespace: e2e-tests-daemonsets-jdq8c, resource: bindings, ignored listing per whitelist
Dec  8 06:11:04.673: INFO: namespace e2e-tests-daemonsets-jdq8c deletion completed in 6.184919241s

• [SLOW TEST:142.135 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:11:04.673: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-fsvcm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 06:11:04.906: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b7ffd02-fab0-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-fsvcm" to be "success or failure"
Dec  8 06:11:04.911: INFO: Pod "downwardapi-volume-0b7ffd02-fab0-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.556135ms
Dec  8 06:11:06.917: INFO: Pod "downwardapi-volume-0b7ffd02-fab0-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010854797s
STEP: Saw pod success
Dec  8 06:11:06.917: INFO: Pod "downwardapi-volume-0b7ffd02-fab0-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:11:06.921: INFO: Trying to get logs from node phrs-one-falcon pod downwardapi-volume-0b7ffd02-fab0-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 06:11:06.951: INFO: Waiting for pod downwardapi-volume-0b7ffd02-fab0-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:11:06.955: INFO: Pod downwardapi-volume-0b7ffd02-fab0-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:11:06.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fsvcm" for this suite.
Dec  8 06:11:12.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:11:13.119: INFO: namespace: e2e-tests-downward-api-fsvcm, resource: bindings, ignored listing per whitelist
Dec  8 06:11:13.156: INFO: namespace e2e-tests-downward-api-fsvcm deletion completed in 6.195578148s

• [SLOW TEST:8.483 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:11:13.157: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vkzqp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 06:11:13.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 version --client'
Dec  8 06:11:13.476: INFO: stderr: ""
Dec  8 06:11:13.476: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.1\", GitCommit:\"4ed3216f3ec431b140b1d899130a69fc671678f4\", GitTreeState:\"clean\", BuildDate:\"2018-10-05T16:46:06Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Dec  8 06:11:13.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-vkzqp'
Dec  8 06:11:13.750: INFO: stderr: ""
Dec  8 06:11:13.750: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  8 06:11:13.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 create -f - --namespace=e2e-tests-kubectl-vkzqp'
Dec  8 06:11:13.955: INFO: stderr: ""
Dec  8 06:11:13.955: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  8 06:11:14.961: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 06:11:14.961: INFO: Found 0 / 1
Dec  8 06:11:15.960: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 06:11:15.960: INFO: Found 0 / 1
Dec  8 06:11:16.960: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 06:11:16.960: INFO: Found 1 / 1
Dec  8 06:11:16.960: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  8 06:11:16.965: INFO: Selector matched 1 pods for map[app:redis]
Dec  8 06:11:16.965: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  8 06:11:16.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 describe pod redis-master-v72mt --namespace=e2e-tests-kubectl-vkzqp'
Dec  8 06:11:17.087: INFO: stderr: ""
Dec  8 06:11:17.087: INFO: stdout: "Name:               redis-master-v72mt\nNamespace:          e2e-tests-kubectl-vkzqp\nPriority:           0\nPriorityClassName:  <none>\nNode:               phrs-glorious-mastodon/10.135.14.74\nStart Time:         Sat, 08 Dec 2018 06:11:13 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: 00-pharos-privileged\nStatus:             Running\nIP:                 172.31.0.8\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://624c439c4ff551706b428946e766db20e6d032c92282775a86765f07ac72596b\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 08 Dec 2018 06:11:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-l86d5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-l86d5:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-l86d5\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                             Message\n  ----    ------     ----  ----                             -------\n  Normal  Scheduled  4s    default-scheduler                Successfully assigned e2e-tests-kubectl-vkzqp/redis-master-v72mt to phrs-glorious-mastodon\n  Normal  Pulled     2s    kubelet, phrs-glorious-mastodon  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, phrs-glorious-mastodon  Created container\n  Normal  Started    2s    kubelet, phrs-glorious-mastodon  Started container\n"
Dec  8 06:11:17.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 describe rc redis-master --namespace=e2e-tests-kubectl-vkzqp'
Dec  8 06:11:17.225: INFO: stderr: ""
Dec  8 06:11:17.225: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-vkzqp\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-v72mt\n"
Dec  8 06:11:17.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 describe service redis-master --namespace=e2e-tests-kubectl-vkzqp'
Dec  8 06:11:17.366: INFO: stderr: ""
Dec  8 06:11:17.366: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-vkzqp\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.32.106.80\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.31.0.8:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  8 06:11:17.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 describe node phrs-glorious-mastodon'
Dec  8 06:11:17.558: INFO: stderr: ""
Dec  8 06:11:17.558: INFO: stdout: "Name:               phrs-glorious-mastodon\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=8gb\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=fra1\n                    kubernetes.io/hostname=phrs-glorious-mastodon\n                    node-address.kontena.io/external-ip=142.93.161.241\n                    node-role.kubernetes.io/worker=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 08 Dec 2018 04:55:28 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 08 Dec 2018 04:55:41 +0000   Sat, 08 Dec 2018 04:55:41 +0000   WeaveIsUp                    Weave pod has set this\n  OutOfDisk            False   Sat, 08 Dec 2018 06:11:16 +0000   Sat, 08 Dec 2018 04:55:28 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure       False   Sat, 08 Dec 2018 06:11:16 +0000   Sat, 08 Dec 2018 04:55:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 08 Dec 2018 06:11:16 +0000   Sat, 08 Dec 2018 04:55:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 08 Dec 2018 06:11:16 +0000   Sat, 08 Dec 2018 04:55:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 08 Dec 2018 06:11:16 +0000   Sat, 08 Dec 2018 04:55:48 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.135.14.74\n  Hostname:    phrs-glorious-mastodon\nCapacity:\n cpu:                4\n ephemeral-storage:  81120924Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8174876Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  74761043435\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8072476Ki\n pods:               110\nSystem Info:\n Machine ID:                 7c5070c58ea54b4d8e147892279bdeed\n System UUID:                7C5070C5-8EA5-4B4D-8E14-7892279BDEED\n Boot ID:                    8189f94e-ad0e-4736-afaa-7efdf820098e\n Kernel Version:             4.4.0-140-generic\n OS Image:                   Ubuntu 16.04.5 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://17.3.2\n Kubelet Version:            v1.12.3\n Kube-Proxy Version:         v1.12.3\nPodCIDR:                     172.31.2.0/24\nNon-terminated Pods:         (14 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------\n  e2e-tests-kubectl-vkzqp    redis-master-v72mt                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-f0d1361ec09d4d5d-q6qrz    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  ingress-nginx              default-http-backend-78796f9b55-xpm76                      10m (0%)      10m (0%)    20Mi (0%)        20Mi (0%)\n  ingress-nginx              nginx-ingress-controller-br6h6                             0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kontena-lens               dashboard-5fd9996979-b8dtn                                 150m (3%)     300m (7%)   384Mi (4%)       768Mi (9%)\n  kontena-lens               user-management-6f95f557c-clh6k                            20m (0%)      50m (1%)    128Mi (1%)       256Mi (3%)\n  kontena-storage-system     rook-ceph-agent-8nlgc                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kontena-storage-system     rook-discover-swkpp                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kontena-storage            rook-ceph-mon0-4rg6w                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kontena-storage            rook-ceph-osd-id-0-8565c4d54-mhrr2                         0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                kube-proxy-2wrhj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                metrics-server-66d95b8778-t9z2j                            10m (0%)      0 (0%)      32Mi (0%)        0 (0%)\n  kube-system                pharos-proxy-phrs-glorious-mastodon                        0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                weave-net-p7lbn                                            30m (0%)      0 (0%)      0 (0%)           0 (0%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource  Requests    Limits\n  --------  --------    ------\n  cpu       220m (5%)   360m (9%)\n  memory    564Mi (7%)  1044Mi (13%)\nEvents:     <none>\n"
Dec  8 06:11:17.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 describe namespace e2e-tests-kubectl-vkzqp'
Dec  8 06:11:17.703: INFO: stderr: ""
Dec  8 06:11:17.703: INFO: stdout: "Name:         e2e-tests-kubectl-vkzqp\nLabels:       e2e-framework=kubectl\n              e2e-run=20a32d24-faa6-11e8-b680-ca3d6f40e675\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:11:17.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vkzqp" for this suite.
Dec  8 06:11:39.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:11:39.795: INFO: namespace: e2e-tests-kubectl-vkzqp, resource: bindings, ignored listing per whitelist
Dec  8 06:11:39.895: INFO: namespace e2e-tests-kubectl-vkzqp deletion completed in 22.183456354s

• [SLOW TEST:26.738 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:11:39.895: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-lsdbn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Dec  8 06:11:42.145: INFO: Pod pod-hostip-207e4f3f-fab0-11e8-b680-ca3d6f40e675 has hostIP: 10.135.11.179
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:11:42.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lsdbn" for this suite.
Dec  8 06:12:04.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:12:04.250: INFO: namespace: e2e-tests-pods-lsdbn, resource: bindings, ignored listing per whitelist
Dec  8 06:12:04.346: INFO: namespace e2e-tests-pods-lsdbn deletion completed in 22.195175081s

• [SLOW TEST:24.451 seconds]
[k8s.io] Pods
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:12:04.346: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-g2vn8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  8 06:12:04.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-g2vn8'
Dec  8 06:12:04.680: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Dec  8 06:12:04.680: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec  8 06:12:04.717: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-rk4zt]
Dec  8 06:12:04.717: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-rk4zt" in namespace "e2e-tests-kubectl-g2vn8" to be "running and ready"
Dec  8 06:12:04.735: INFO: Pod "e2e-test-nginx-rc-rk4zt": Phase="Pending", Reason="", readiness=false. Elapsed: 17.84775ms
Dec  8 06:12:06.745: INFO: Pod "e2e-test-nginx-rc-rk4zt": Phase="Running", Reason="", readiness=true. Elapsed: 2.02774706s
Dec  8 06:12:06.745: INFO: Pod "e2e-test-nginx-rc-rk4zt" satisfied condition "running and ready"
Dec  8 06:12:06.745: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-rk4zt]
Dec  8 06:12:06.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-g2vn8'
Dec  8 06:12:06.876: INFO: stderr: ""
Dec  8 06:12:06.876: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
Dec  8 06:12:06.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-g2vn8'
Dec  8 06:12:07.013: INFO: stderr: ""
Dec  8 06:12:07.013: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:12:07.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-g2vn8" for this suite.
Dec  8 06:12:29.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:12:29.137: INFO: namespace: e2e-tests-kubectl-g2vn8, resource: bindings, ignored listing per whitelist
Dec  8 06:12:29.190: INFO: namespace e2e-tests-kubectl-g2vn8 deletion completed in 22.165770749s

• [SLOW TEST:24.845 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:12:29.191: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-xt582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 06:12:29.385: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  8 06:12:29.395: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  8 06:12:34.400: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  8 06:12:34.400: INFO: Creating deployment "test-rolling-update-deployment"
Dec  8 06:12:34.407: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  8 06:12:34.428: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec  8 06:12:36.434: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  8 06:12:36.437: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  8 06:12:36.445: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-xt582,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xt582/deployments/test-rolling-update-deployment,UID:40d9deb8-fab0-11e8-a103-96cd63cee3b8,ResourceVersion:18555,Generation:1,CreationTimestamp:2018-12-08 06:12:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2018-12-08 06:12:34 +0000 UTC 2018-12-08 06:12:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2018-12-08 06:12:36 +0000 UTC 2018-12-08 06:12:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  8 06:12:36.449: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-xt582,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xt582/replicasets/test-rolling-update-deployment-65b7695dcf,UID:40dd5b35-fab0-11e8-a103-96cd63cee3b8,ResourceVersion:18546,Generation:1,CreationTimestamp:2018-12-08 06:12:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 40d9deb8-fab0-11e8-a103-96cd63cee3b8 0xc421eb4937 0xc421eb4938}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  8 06:12:36.449: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  8 06:12:36.449: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-xt582,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-xt582/replicasets/test-rolling-update-controller,UID:3ddc9ec0-fab0-11e8-a103-96cd63cee3b8,ResourceVersion:18554,Generation:2,CreationTimestamp:2018-12-08 06:12:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 40d9deb8-fab0-11e8-a103-96cd63cee3b8 0xc421eb486e 0xc421eb486f}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  8 06:12:36.453: INFO: Pod "test-rolling-update-deployment-65b7695dcf-j77rk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-j77rk,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-xt582,SelfLink:/api/v1/namespaces/e2e-tests-deployment-xt582/pods/test-rolling-update-deployment-65b7695dcf-j77rk,UID:40de4611-fab0-11e8-a103-96cd63cee3b8,ResourceVersion:18545,Generation:0,CreationTimestamp:2018-12-08 06:12:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{kubernetes.io/psp: 00-pharos-privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf 40dd5b35-fab0-11e8-a103-96cd63cee3b8 0xc421eb51b7 0xc421eb51b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-2d8wf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-2d8wf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-2d8wf true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phrs-one-falcon,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc421eb5230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc421eb5250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 06:12:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 06:12:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 06:12:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-08 06:12:34 +0000 UTC  }],Message:,Reason:,HostIP:10.135.44.6,PodIP:172.31.192.8,StartTime:2018-12-08 06:12:34 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2018-12-08 06:12:35 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://53b77de742f62f2a00429bb4143547341aa1c488a33e336d400f6269ac4fe047}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:12:36.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-xt582" for this suite.
Dec  8 06:12:42.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:12:42.490: INFO: namespace: e2e-tests-deployment-xt582, resource: bindings, ignored listing per whitelist
Dec  8 06:12:42.581: INFO: namespace e2e-tests-deployment-xt582 deletion completed in 6.123710442s

• [SLOW TEST:13.390 seconds]
[sig-apps] Deployment
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:12:42.581: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-fl8vn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Dec  8 06:12:42.778: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-fl8vn" to be "success or failure"
Dec  8 06:12:42.820: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 41.445817ms
Dec  8 06:12:44.824: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045483389s
STEP: Saw pod success
Dec  8 06:12:44.824: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  8 06:12:44.827: INFO: Trying to get logs from node phrs-one-falcon pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  8 06:12:44.849: INFO: Waiting for pod pod-host-path-test to disappear
Dec  8 06:12:44.854: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:12:44.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-fl8vn" for this suite.
Dec  8 06:12:50.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:12:50.950: INFO: namespace: e2e-tests-hostpath-fl8vn, resource: bindings, ignored listing per whitelist
Dec  8 06:12:50.988: INFO: namespace e2e-tests-hostpath-fl8vn deletion completed in 6.129884644s

• [SLOW TEST:8.407 seconds]
[sig-storage] HostPath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:12:50.988: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ld6jz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-4ad894f2-fab0-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 06:12:51.179: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4ad92fba-fab0-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-ld6jz" to be "success or failure"
Dec  8 06:12:51.184: INFO: Pod "pod-projected-configmaps-4ad92fba-fab0-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 4.811301ms
Dec  8 06:12:53.188: INFO: Pod "pod-projected-configmaps-4ad92fba-fab0-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009025178s
STEP: Saw pod success
Dec  8 06:12:53.188: INFO: Pod "pod-projected-configmaps-4ad92fba-fab0-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:12:53.191: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-projected-configmaps-4ad92fba-fab0-11e8-b680-ca3d6f40e675 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 06:12:53.250: INFO: Waiting for pod pod-projected-configmaps-4ad92fba-fab0-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:12:53.273: INFO: Pod pod-projected-configmaps-4ad92fba-fab0-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:12:53.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ld6jz" for this suite.
Dec  8 06:12:59.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:12:59.321: INFO: namespace: e2e-tests-projected-ld6jz, resource: bindings, ignored listing per whitelist
Dec  8 06:12:59.421: INFO: namespace e2e-tests-projected-ld6jz deletion completed in 6.141497329s

• [SLOW TEST:8.432 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:12:59.421: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jnd45
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-4fe1ab38-fab0-11e8-b680-ca3d6f40e675
STEP: Creating configMap with name cm-test-opt-upd-4fe1ab91-fab0-11e8-b680-ca3d6f40e675
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4fe1ab38-fab0-11e8-b680-ca3d6f40e675
STEP: Updating configmap cm-test-opt-upd-4fe1ab91-fab0-11e8-b680-ca3d6f40e675
STEP: Creating configMap with name cm-test-opt-create-4fe1abb6-fab0-11e8-b680-ca3d6f40e675
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:13:03.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jnd45" for this suite.
Dec  8 06:13:25.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:13:25.884: INFO: namespace: e2e-tests-projected-jnd45, resource: bindings, ignored listing per whitelist
Dec  8 06:13:25.913: INFO: namespace e2e-tests-projected-jnd45 deletion completed in 22.171167733s

• [SLOW TEST:26.493 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:13:25.914: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-kns6f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-kns6f
Dec  8 06:13:28.150: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-kns6f
STEP: checking the pod's current state and verifying that restartCount is present
Dec  8 06:13:28.153: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:17:28.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-kns6f" for this suite.
Dec  8 06:17:34.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:17:34.834: INFO: namespace: e2e-tests-container-probe-kns6f, resource: bindings, ignored listing per whitelist
Dec  8 06:17:34.880: INFO: namespace e2e-tests-container-probe-kns6f deletion completed in 6.167440765s

• [SLOW TEST:248.967 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:17:34.881: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-h89nh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-gmc5
STEP: Creating a pod to test atomic-volume-subpath
Dec  8 06:17:35.093: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gmc5" in namespace "e2e-tests-subpath-h89nh" to be "success or failure"
Dec  8 06:17:35.103: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.379312ms
Dec  8 06:17:37.107: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013254259s
Dec  8 06:17:39.111: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Running", Reason="", readiness=false. Elapsed: 4.017726044s
Dec  8 06:17:41.115: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Running", Reason="", readiness=false. Elapsed: 6.021584262s
Dec  8 06:17:43.119: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Running", Reason="", readiness=false. Elapsed: 8.025459735s
Dec  8 06:17:45.124: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Running", Reason="", readiness=false. Elapsed: 10.030057283s
Dec  8 06:17:47.128: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Running", Reason="", readiness=false. Elapsed: 12.034738255s
Dec  8 06:17:49.135: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Running", Reason="", readiness=false. Elapsed: 14.041650657s
Dec  8 06:17:51.139: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Running", Reason="", readiness=false. Elapsed: 16.045779091s
Dec  8 06:17:53.144: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Running", Reason="", readiness=false. Elapsed: 18.050032036s
Dec  8 06:17:55.149: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Running", Reason="", readiness=false. Elapsed: 20.055105962s
Dec  8 06:17:57.154: INFO: Pod "pod-subpath-test-secret-gmc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.060104356s
STEP: Saw pod success
Dec  8 06:17:57.154: INFO: Pod "pod-subpath-test-secret-gmc5" satisfied condition "success or failure"
Dec  8 06:17:57.162: INFO: Trying to get logs from node phrs-one-falcon pod pod-subpath-test-secret-gmc5 container test-container-subpath-secret-gmc5: <nil>
STEP: delete the pod
Dec  8 06:17:57.189: INFO: Waiting for pod pod-subpath-test-secret-gmc5 to disappear
Dec  8 06:17:57.193: INFO: Pod pod-subpath-test-secret-gmc5 no longer exists
STEP: Deleting pod pod-subpath-test-secret-gmc5
Dec  8 06:17:57.193: INFO: Deleting pod "pod-subpath-test-secret-gmc5" in namespace "e2e-tests-subpath-h89nh"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:17:57.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-h89nh" for this suite.
Dec  8 06:18:03.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:18:03.293: INFO: namespace: e2e-tests-subpath-h89nh, resource: bindings, ignored listing per whitelist
Dec  8 06:18:03.380: INFO: namespace e2e-tests-subpath-h89nh deletion completed in 6.178211303s

• [SLOW TEST:28.499 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:18:03.381: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-7ttqd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 06:18:03.626: INFO: (0) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 9.673023ms)
Dec  8 06:18:03.632: INFO: (1) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.800336ms)
Dec  8 06:18:03.638: INFO: (2) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.754392ms)
Dec  8 06:18:03.642: INFO: (3) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 4.806961ms)
Dec  8 06:18:03.649: INFO: (4) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.618109ms)
Dec  8 06:18:03.656: INFO: (5) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.442707ms)
Dec  8 06:18:03.662: INFO: (6) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.492731ms)
Dec  8 06:18:03.669: INFO: (7) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.564801ms)
Dec  8 06:18:03.676: INFO: (8) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.340341ms)
Dec  8 06:18:03.681: INFO: (9) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.391873ms)
Dec  8 06:18:03.687: INFO: (10) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.442383ms)
Dec  8 06:18:03.693: INFO: (11) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.824849ms)
Dec  8 06:18:03.698: INFO: (12) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.269225ms)
Dec  8 06:18:03.703: INFO: (13) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.43293ms)
Dec  8 06:18:03.709: INFO: (14) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.101285ms)
Dec  8 06:18:03.716: INFO: (15) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.145235ms)
Dec  8 06:18:03.722: INFO: (16) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.28779ms)
Dec  8 06:18:03.728: INFO: (17) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.446472ms)
Dec  8 06:18:03.736: INFO: (18) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 7.819118ms)
Dec  8 06:18:03.743: INFO: (19) /api/v1/nodes/phrs-glorious-mastodon/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 7.166184ms)
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:18:03.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-7ttqd" for this suite.
Dec  8 06:18:09.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:18:09.888: INFO: namespace: e2e-tests-proxy-7ttqd, resource: bindings, ignored listing per whitelist
Dec  8 06:18:09.946: INFO: namespace e2e-tests-proxy-7ttqd deletion completed in 6.196170829s

• [SLOW TEST:6.565 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:18:09.946: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-tvj2g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  8 06:18:12.725: INFO: Successfully updated pod "annotationupdate08fafc9f-fab1-11e8-b680-ca3d6f40e675"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:18:16.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tvj2g" for this suite.
Dec  8 06:18:38.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:18:38.891: INFO: namespace: e2e-tests-downward-api-tvj2g, resource: bindings, ignored listing per whitelist
Dec  8 06:18:38.923: INFO: namespace e2e-tests-downward-api-tvj2g deletion completed in 22.16065848s

• [SLOW TEST:28.977 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:18:38.923: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-wbntj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1208 06:19:19.174556      17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  8 06:19:19.174: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:19:19.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-wbntj" for this suite.
Dec  8 06:19:25.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:19:25.348: INFO: namespace: e2e-tests-gc-wbntj, resource: bindings, ignored listing per whitelist
Dec  8 06:19:25.388: INFO: namespace e2e-tests-gc-wbntj deletion completed in 6.206662454s

• [SLOW TEST:46.464 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:19:25.388: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-b2bld
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Dec  8 06:19:25.629: INFO: Waiting up to 5m0s for pod "var-expansion-35f45345-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-var-expansion-b2bld" to be "success or failure"
Dec  8 06:19:25.638: INFO: Pod "var-expansion-35f45345-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 8.830442ms
Dec  8 06:19:27.641: INFO: Pod "var-expansion-35f45345-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012496223s
STEP: Saw pod success
Dec  8 06:19:27.641: INFO: Pod "var-expansion-35f45345-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:19:27.644: INFO: Trying to get logs from node phrs-glorious-mastodon pod var-expansion-35f45345-fab1-11e8-b680-ca3d6f40e675 container dapi-container: <nil>
STEP: delete the pod
Dec  8 06:19:27.673: INFO: Waiting for pod var-expansion-35f45345-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:19:27.678: INFO: Pod var-expansion-35f45345-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:19:27.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-b2bld" for this suite.
Dec  8 06:19:33.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:19:33.797: INFO: namespace: e2e-tests-var-expansion-b2bld, resource: bindings, ignored listing per whitelist
Dec  8 06:19:33.806: INFO: namespace e2e-tests-var-expansion-b2bld deletion completed in 6.124088527s

• [SLOW TEST:8.419 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:19:33.807: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sglbj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-3af3541c-fab1-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 06:19:34.015: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3af41fcb-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-sglbj" to be "success or failure"
Dec  8 06:19:34.025: INFO: Pod "pod-projected-secrets-3af41fcb-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 10.182054ms
Dec  8 06:19:36.029: INFO: Pod "pod-projected-secrets-3af41fcb-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014032156s
STEP: Saw pod success
Dec  8 06:19:36.029: INFO: Pod "pod-projected-secrets-3af41fcb-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:19:36.032: INFO: Trying to get logs from node phrs-liberal-perch pod pod-projected-secrets-3af41fcb-fab1-11e8-b680-ca3d6f40e675 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  8 06:19:36.055: INFO: Waiting for pod pod-projected-secrets-3af41fcb-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:19:36.057: INFO: Pod pod-projected-secrets-3af41fcb-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:19:36.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sglbj" for this suite.
Dec  8 06:19:42.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:19:42.169: INFO: namespace: e2e-tests-projected-sglbj, resource: bindings, ignored listing per whitelist
Dec  8 06:19:42.207: INFO: namespace e2e-tests-projected-sglbj deletion completed in 6.146582943s

• [SLOW TEST:8.400 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:19:42.208: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-4hf65
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-j6bc
STEP: Creating a pod to test atomic-volume-subpath
Dec  8 06:19:42.419: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-j6bc" in namespace "e2e-tests-subpath-4hf65" to be "success or failure"
Dec  8 06:19:42.426: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.141392ms
Dec  8 06:19:44.431: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012343927s
Dec  8 06:19:46.434: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 4.015822504s
Dec  8 06:19:48.443: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 6.02415885s
Dec  8 06:19:50.447: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 8.028333264s
Dec  8 06:19:52.452: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 10.033255951s
Dec  8 06:19:54.456: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 12.037476225s
Dec  8 06:19:56.460: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 14.041768093s
Dec  8 06:19:58.464: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 16.045749393s
Dec  8 06:20:00.469: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 18.050128463s
Dec  8 06:20:02.474: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 20.05510776s
Dec  8 06:20:04.479: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Running", Reason="", readiness=false. Elapsed: 22.060567089s
Dec  8 06:20:06.483: INFO: Pod "pod-subpath-test-downwardapi-j6bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.064640816s
STEP: Saw pod success
Dec  8 06:20:06.483: INFO: Pod "pod-subpath-test-downwardapi-j6bc" satisfied condition "success or failure"
Dec  8 06:20:06.487: INFO: Trying to get logs from node phrs-one-falcon pod pod-subpath-test-downwardapi-j6bc container test-container-subpath-downwardapi-j6bc: <nil>
STEP: delete the pod
Dec  8 06:20:06.522: INFO: Waiting for pod pod-subpath-test-downwardapi-j6bc to disappear
Dec  8 06:20:06.530: INFO: Pod pod-subpath-test-downwardapi-j6bc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-j6bc
Dec  8 06:20:06.530: INFO: Deleting pod "pod-subpath-test-downwardapi-j6bc" in namespace "e2e-tests-subpath-4hf65"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:20:06.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4hf65" for this suite.
Dec  8 06:20:12.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:20:12.690: INFO: namespace: e2e-tests-subpath-4hf65, resource: bindings, ignored listing per whitelist
Dec  8 06:20:12.715: INFO: namespace e2e-tests-subpath-4hf65 deletion completed in 6.174631862s

• [SLOW TEST:30.508 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:20:12.716: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-clv95
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-clv95 A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-clv95;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-clv95 A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-clv95;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-clv95.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-clv95.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-clv95.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-clv95.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-clv95.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-clv95.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-clv95.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-clv95.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-clv95.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-clv95.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-clv95.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-clv95.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-clv95.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 102.59.32.172.in-addr.arpa. PTR)" && echo OK > /results/172.32.59.102_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 102.59.32.172.in-addr.arpa. PTR)" && echo OK > /results/172.32.59.102_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-clv95 A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-clv95;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-clv95 A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-clv95;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-clv95.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-clv95.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-clv95.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-clv95.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-clv95.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-clv95.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-clv95.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-clv95.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-clv95.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-clv95.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-clv95.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-clv95.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-clv95.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 102.59.32.172.in-addr.arpa. PTR)" && echo OK > /results/172.32.59.102_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 102.59.32.172.in-addr.arpa. PTR)" && echo OK > /results/172.32.59.102_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  8 06:20:37.144: INFO: DNS probes using e2e-tests-dns-clv95/dns-test-522eb47c-fab1-11e8-b680-ca3d6f40e675 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:20:37.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-clv95" for this suite.
Dec  8 06:20:43.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:20:43.427: INFO: namespace: e2e-tests-dns-clv95, resource: bindings, ignored listing per whitelist
Dec  8 06:20:43.449: INFO: namespace e2e-tests-dns-clv95 deletion completed in 6.161219959s

• [SLOW TEST:30.733 seconds]
[sig-network] DNS
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:20:43.449: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rp5q8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  8 06:20:43.655: INFO: Waiting up to 5m0s for pod "downward-api-64767961-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-rp5q8" to be "success or failure"
Dec  8 06:20:43.672: INFO: Pod "downward-api-64767961-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 16.541835ms
Dec  8 06:20:45.677: INFO: Pod "downward-api-64767961-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021276063s
STEP: Saw pod success
Dec  8 06:20:45.677: INFO: Pod "downward-api-64767961-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:20:45.680: INFO: Trying to get logs from node phrs-liberal-perch pod downward-api-64767961-fab1-11e8-b680-ca3d6f40e675 container dapi-container: <nil>
STEP: delete the pod
Dec  8 06:20:45.719: INFO: Waiting for pod downward-api-64767961-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:20:45.723: INFO: Pod downward-api-64767961-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:20:45.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rp5q8" for this suite.
Dec  8 06:20:51.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:20:51.849: INFO: namespace: e2e-tests-downward-api-rp5q8, resource: bindings, ignored listing per whitelist
Dec  8 06:20:51.893: INFO: namespace e2e-tests-downward-api-rp5q8 deletion completed in 6.163860141s

• [SLOW TEST:8.444 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:20:51.893: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-4gcbp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-jmk4
STEP: Creating a pod to test atomic-volume-subpath
Dec  8 06:20:52.221: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jmk4" in namespace "e2e-tests-subpath-4gcbp" to be "success or failure"
Dec  8 06:20:52.248: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Pending", Reason="", readiness=false. Elapsed: 27.55374ms
Dec  8 06:20:54.253: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031809346s
Dec  8 06:20:56.257: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 4.035987901s
Dec  8 06:20:58.261: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 6.039953088s
Dec  8 06:21:00.267: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 8.04574988s
Dec  8 06:21:02.271: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 10.049793375s
Dec  8 06:21:04.276: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 12.055077321s
Dec  8 06:21:06.281: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 14.059917552s
Dec  8 06:21:08.286: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 16.064688464s
Dec  8 06:21:10.290: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 18.069234375s
Dec  8 06:21:12.294: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 20.073477368s
Dec  8 06:21:14.299: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Running", Reason="", readiness=false. Elapsed: 22.077892664s
Dec  8 06:21:16.305: INFO: Pod "pod-subpath-test-configmap-jmk4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.083784609s
STEP: Saw pod success
Dec  8 06:21:16.305: INFO: Pod "pod-subpath-test-configmap-jmk4" satisfied condition "success or failure"
Dec  8 06:21:16.308: INFO: Trying to get logs from node phrs-one-falcon pod pod-subpath-test-configmap-jmk4 container test-container-subpath-configmap-jmk4: <nil>
STEP: delete the pod
Dec  8 06:21:16.353: INFO: Waiting for pod pod-subpath-test-configmap-jmk4 to disappear
Dec  8 06:21:16.358: INFO: Pod pod-subpath-test-configmap-jmk4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jmk4
Dec  8 06:21:16.358: INFO: Deleting pod "pod-subpath-test-configmap-jmk4" in namespace "e2e-tests-subpath-4gcbp"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:21:16.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4gcbp" for this suite.
Dec  8 06:21:22.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:21:22.501: INFO: namespace: e2e-tests-subpath-4gcbp, resource: bindings, ignored listing per whitelist
Dec  8 06:21:22.575: INFO: namespace e2e-tests-subpath-4gcbp deletion completed in 6.207220641s

• [SLOW TEST:30.682 seconds]
[sig-storage] Subpath
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:21:22.576: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-zbdw9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-zbdw9
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-zbdw9
STEP: Deleting pre-stop pod
Dec  8 06:21:33.874: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:21:33.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-zbdw9" for this suite.
Dec  8 06:22:11.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:22:11.971: INFO: namespace: e2e-tests-prestop-zbdw9, resource: bindings, ignored listing per whitelist
Dec  8 06:22:12.112: INFO: namespace e2e-tests-prestop-zbdw9 deletion completed in 38.220739932s

• [SLOW TEST:49.536 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:22:12.113: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-fsxgq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 06:22:12.375: INFO: (0) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 7.472557ms)
Dec  8 06:22:12.384: INFO: (1) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 8.830347ms)
Dec  8 06:22:12.392: INFO: (2) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 7.398295ms)
Dec  8 06:22:12.398: INFO: (3) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.320317ms)
Dec  8 06:22:12.410: INFO: (4) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 12.289356ms)
Dec  8 06:22:12.419: INFO: (5) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 8.599622ms)
Dec  8 06:22:12.426: INFO: (6) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 7.178801ms)
Dec  8 06:22:12.433: INFO: (7) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.461255ms)
Dec  8 06:22:12.440: INFO: (8) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 7.393926ms)
Dec  8 06:22:12.446: INFO: (9) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 5.31679ms)
Dec  8 06:22:12.452: INFO: (10) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.21913ms)
Dec  8 06:22:12.459: INFO: (11) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.351409ms)
Dec  8 06:22:12.465: INFO: (12) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.651563ms)
Dec  8 06:22:12.472: INFO: (13) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.943399ms)
Dec  8 06:22:12.479: INFO: (14) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.629417ms)
Dec  8 06:22:12.486: INFO: (15) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 7.415738ms)
Dec  8 06:22:12.493: INFO: (16) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.740405ms)
Dec  8 06:22:12.500: INFO: (17) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 6.55071ms)
Dec  8 06:22:12.510: INFO: (18) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 10.353535ms)
Dec  8 06:22:12.523: INFO: (19) /api/v1/nodes/phrs-glorious-mastodon:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="cl... (200; 12.375977ms)
[AfterEach] version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:22:12.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-fsxgq" for this suite.
Dec  8 06:22:18.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:22:18.683: INFO: namespace: e2e-tests-proxy-fsxgq, resource: bindings, ignored listing per whitelist
Dec  8 06:22:18.718: INFO: namespace e2e-tests-proxy-fsxgq deletion completed in 6.18554731s

• [SLOW TEST:6.604 seconds]
[sig-network] Proxy
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:22:18.718: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-d48p7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 06:22:18.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d42dff0-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-d48p7" to be "success or failure"
Dec  8 06:22:18.962: INFO: Pod "downwardapi-volume-9d42dff0-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 9.070612ms
Dec  8 06:22:20.967: INFO: Pod "downwardapi-volume-9d42dff0-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014672114s
STEP: Saw pod success
Dec  8 06:22:20.967: INFO: Pod "downwardapi-volume-9d42dff0-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:22:20.971: INFO: Trying to get logs from node phrs-glorious-mastodon pod downwardapi-volume-9d42dff0-fab1-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 06:22:21.008: INFO: Waiting for pod downwardapi-volume-9d42dff0-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:22:21.014: INFO: Pod downwardapi-volume-9d42dff0-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:22:21.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-d48p7" for this suite.
Dec  8 06:22:27.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:22:27.185: INFO: namespace: e2e-tests-projected-d48p7, resource: bindings, ignored listing per whitelist
Dec  8 06:22:27.201: INFO: namespace e2e-tests-projected-d48p7 deletion completed in 6.180920637s

• [SLOW TEST:8.483 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:22:27.201: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-8cmsm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  8 06:22:27.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-8cmsm'
Dec  8 06:22:27.684: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Dec  8 06:22:27.684: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
Dec  8 06:22:31.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-036194578 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-8cmsm'
Dec  8 06:22:31.818: INFO: stderr: ""
Dec  8 06:22:31.819: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:22:31.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-8cmsm" for this suite.
Dec  8 06:22:53.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:22:53.889: INFO: namespace: e2e-tests-kubectl-8cmsm, resource: bindings, ignored listing per whitelist
Dec  8 06:22:54.148: INFO: namespace e2e-tests-kubectl-8cmsm deletion completed in 22.321497984s

• [SLOW TEST:26.947 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:22:54.148: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-p9v66
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 06:22:54.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b25fb3b1-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-p9v66" to be "success or failure"
Dec  8 06:22:54.382: INFO: Pod "downwardapi-volume-b25fb3b1-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 10.597767ms
Dec  8 06:22:56.386: INFO: Pod "downwardapi-volume-b25fb3b1-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014700183s
Dec  8 06:22:58.390: INFO: Pod "downwardapi-volume-b25fb3b1-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018495373s
STEP: Saw pod success
Dec  8 06:22:58.390: INFO: Pod "downwardapi-volume-b25fb3b1-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:22:58.393: INFO: Trying to get logs from node phrs-liberal-perch pod downwardapi-volume-b25fb3b1-fab1-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 06:22:58.417: INFO: Waiting for pod downwardapi-volume-b25fb3b1-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:22:58.420: INFO: Pod downwardapi-volume-b25fb3b1-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:22:58.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-p9v66" for this suite.
Dec  8 06:23:04.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:23:04.600: INFO: namespace: e2e-tests-downward-api-p9v66, resource: bindings, ignored listing per whitelist
Dec  8 06:23:04.619: INFO: namespace e2e-tests-downward-api-p9v66 deletion completed in 6.19460611s

• [SLOW TEST:10.471 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:23:04.619: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ncqbp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  8 06:23:04.874: INFO: Waiting up to 5m0s for pod "downward-api-b8a2e977-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-ncqbp" to be "success or failure"
Dec  8 06:23:04.886: INFO: Pod "downward-api-b8a2e977-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 11.777075ms
Dec  8 06:23:06.892: INFO: Pod "downward-api-b8a2e977-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01735842s
STEP: Saw pod success
Dec  8 06:23:06.892: INFO: Pod "downward-api-b8a2e977-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:23:06.897: INFO: Trying to get logs from node phrs-one-falcon pod downward-api-b8a2e977-fab1-11e8-b680-ca3d6f40e675 container dapi-container: <nil>
STEP: delete the pod
Dec  8 06:23:06.940: INFO: Waiting for pod downward-api-b8a2e977-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:23:06.945: INFO: Pod downward-api-b8a2e977-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:23:06.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ncqbp" for this suite.
Dec  8 06:23:12.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:23:13.047: INFO: namespace: e2e-tests-downward-api-ncqbp, resource: bindings, ignored listing per whitelist
Dec  8 06:23:13.187: INFO: namespace e2e-tests-downward-api-ncqbp deletion completed in 6.236175776s

• [SLOW TEST:8.568 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:23:13.188: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dp8pg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec  8 06:23:16.021: INFO: Successfully updated pod "labelsupdatebdbe2d77-fab1-11e8-b680-ca3d6f40e675"
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:23:20.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dp8pg" for this suite.
Dec  8 06:23:42.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:23:42.128: INFO: namespace: e2e-tests-projected-dp8pg, resource: bindings, ignored listing per whitelist
Dec  8 06:23:42.212: INFO: namespace e2e-tests-projected-dp8pg deletion completed in 22.151246266s

• [SLOW TEST:29.024 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:23:42.212: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-h4fd5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  8 06:23:42.426: INFO: Waiting up to 5m0s for pod "pod-cf049034-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-h4fd5" to be "success or failure"
Dec  8 06:23:42.436: INFO: Pod "pod-cf049034-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 9.373753ms
Dec  8 06:23:44.439: INFO: Pod "pod-cf049034-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012795986s
STEP: Saw pod success
Dec  8 06:23:44.439: INFO: Pod "pod-cf049034-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:23:44.442: INFO: Trying to get logs from node phrs-liberal-perch pod pod-cf049034-fab1-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 06:23:44.464: INFO: Waiting for pod pod-cf049034-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:23:44.470: INFO: Pod pod-cf049034-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:23:44.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-h4fd5" for this suite.
Dec  8 06:23:50.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:23:50.550: INFO: namespace: e2e-tests-emptydir-h4fd5, resource: bindings, ignored listing per whitelist
Dec  8 06:23:50.613: INFO: namespace e2e-tests-emptydir-h4fd5 deletion completed in 6.13841918s

• [SLOW TEST:8.401 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:23:50.613: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wwhbx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-d404e974-fab1-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume configMaps
Dec  8 06:23:50.818: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d40598d4-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-wwhbx" to be "success or failure"
Dec  8 06:23:50.824: INFO: Pod "pod-projected-configmaps-d40598d4-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 5.987185ms
Dec  8 06:23:52.828: INFO: Pod "pod-projected-configmaps-d40598d4-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010284098s
STEP: Saw pod success
Dec  8 06:23:52.828: INFO: Pod "pod-projected-configmaps-d40598d4-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:23:52.831: INFO: Trying to get logs from node phrs-one-falcon pod pod-projected-configmaps-d40598d4-fab1-11e8-b680-ca3d6f40e675 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  8 06:23:52.856: INFO: Waiting for pod pod-projected-configmaps-d40598d4-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:23:52.859: INFO: Pod pod-projected-configmaps-d40598d4-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:23:52.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wwhbx" for this suite.
Dec  8 06:23:58.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:23:58.988: INFO: namespace: e2e-tests-projected-wwhbx, resource: bindings, ignored listing per whitelist
Dec  8 06:23:59.313: INFO: namespace e2e-tests-projected-wwhbx deletion completed in 6.44999465s

• [SLOW TEST:8.700 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:23:59.314: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-d9l59
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-d9l59
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-d9l59
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-d9l59
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-d9l59
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-d9l59
Dec  8 06:24:03.561: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-d9l59, name: ss-0, uid: db6576ac-fab1-11e8-a103-96cd63cee3b8, status phase: Pending. Waiting for statefulset controller to delete.
Dec  8 06:24:03.751: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-d9l59, name: ss-0, uid: db6576ac-fab1-11e8-a103-96cd63cee3b8, status phase: Failed. Waiting for statefulset controller to delete.
Dec  8 06:24:03.765: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-d9l59, name: ss-0, uid: db6576ac-fab1-11e8-a103-96cd63cee3b8, status phase: Failed. Waiting for statefulset controller to delete.
Dec  8 06:24:03.772: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-d9l59
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-d9l59
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-d9l59 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  8 06:24:07.821: INFO: Deleting all statefulset in ns e2e-tests-statefulset-d9l59
Dec  8 06:24:07.825: INFO: Scaling statefulset ss to 0
Dec  8 06:24:27.846: INFO: Waiting for statefulset status.replicas updated to 0
Dec  8 06:24:27.849: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:24:27.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-d9l59" for this suite.
Dec  8 06:24:33.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:24:33.933: INFO: namespace: e2e-tests-statefulset-d9l59, resource: bindings, ignored listing per whitelist
Dec  8 06:24:34.031: INFO: namespace e2e-tests-statefulset-d9l59 deletion completed in 6.155280911s

• [SLOW TEST:34.717 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:24:34.032: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-cxsbs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 06:24:34.281: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edec0783-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-cxsbs" to be "success or failure"
Dec  8 06:24:34.297: INFO: Pod "downwardapi-volume-edec0783-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 15.500018ms
Dec  8 06:24:36.301: INFO: Pod "downwardapi-volume-edec0783-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019821168s
STEP: Saw pod success
Dec  8 06:24:36.301: INFO: Pod "downwardapi-volume-edec0783-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:24:36.304: INFO: Trying to get logs from node phrs-glorious-mastodon pod downwardapi-volume-edec0783-fab1-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 06:24:36.325: INFO: Waiting for pod downwardapi-volume-edec0783-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:24:36.329: INFO: Pod downwardapi-volume-edec0783-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:24:36.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cxsbs" for this suite.
Dec  8 06:24:42.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:24:42.383: INFO: namespace: e2e-tests-projected-cxsbs, resource: bindings, ignored listing per whitelist
Dec  8 06:24:42.472: INFO: namespace e2e-tests-projected-cxsbs deletion completed in 6.138849954s

• [SLOW TEST:8.441 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:24:42.473: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-jjqnr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:24:42.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-jjqnr" for this suite.
Dec  8 06:24:48.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:24:48.808: INFO: namespace: e2e-tests-services-jjqnr, resource: bindings, ignored listing per whitelist
Dec  8 06:24:48.811: INFO: namespace e2e-tests-services-jjqnr deletion completed in 6.137154316s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88

• [SLOW TEST:6.338 seconds]
[sig-network] Services
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:24:48.811: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-x4rs8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec  8 06:24:49.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6b712a4-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-x4rs8" to be "success or failure"
Dec  8 06:24:49.041: INFO: Pod "downwardapi-volume-f6b712a4-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 12.192796ms
Dec  8 06:24:51.045: INFO: Pod "downwardapi-volume-f6b712a4-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016370363s
STEP: Saw pod success
Dec  8 06:24:51.045: INFO: Pod "downwardapi-volume-f6b712a4-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:24:51.048: INFO: Trying to get logs from node phrs-liberal-perch pod downwardapi-volume-f6b712a4-fab1-11e8-b680-ca3d6f40e675 container client-container: <nil>
STEP: delete the pod
Dec  8 06:24:51.071: INFO: Waiting for pod downwardapi-volume-f6b712a4-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:24:51.076: INFO: Pod downwardapi-volume-f6b712a4-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:24:51.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-x4rs8" for this suite.
Dec  8 06:24:57.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:24:57.208: INFO: namespace: e2e-tests-downward-api-x4rs8, resource: bindings, ignored listing per whitelist
Dec  8 06:24:57.248: INFO: namespace e2e-tests-downward-api-x4rs8 deletion completed in 6.167060303s

• [SLOW TEST:8.437 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:24:57.248: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-qtlnp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-fbbbfe00-fab1-11e8-b680-ca3d6f40e675
STEP: Creating a pod to test consume secrets
Dec  8 06:24:57.468: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fbbcb941-fab1-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-projected-qtlnp" to be "success or failure"
Dec  8 06:24:57.471: INFO: Pod "pod-projected-secrets-fbbcb941-fab1-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.988935ms
Dec  8 06:24:59.475: INFO: Pod "pod-projected-secrets-fbbcb941-fab1-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006628071s
STEP: Saw pod success
Dec  8 06:24:59.475: INFO: Pod "pod-projected-secrets-fbbcb941-fab1-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:24:59.478: INFO: Trying to get logs from node phrs-one-falcon pod pod-projected-secrets-fbbcb941-fab1-11e8-b680-ca3d6f40e675 container secret-volume-test: <nil>
STEP: delete the pod
Dec  8 06:24:59.503: INFO: Waiting for pod pod-projected-secrets-fbbcb941-fab1-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:24:59.507: INFO: Pod pod-projected-secrets-fbbcb941-fab1-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] Projected
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:24:59.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qtlnp" for this suite.
Dec  8 06:25:07.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:25:08.249: INFO: namespace: e2e-tests-projected-qtlnp, resource: bindings, ignored listing per whitelist
Dec  8 06:25:08.859: INFO: namespace e2e-tests-projected-qtlnp deletion completed in 9.345223508s

• [SLOW TEST:11.611 seconds]
[sig-storage] Projected
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:25:08.860: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-kkbbb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  8 06:25:09.736: INFO: Waiting up to 5m0s for pod "pod-02ffcc82-fab2-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-kkbbb" to be "success or failure"
Dec  8 06:25:09.783: INFO: Pod "pod-02ffcc82-fab2-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 47.428726ms
Dec  8 06:25:11.825: INFO: Pod "pod-02ffcc82-fab2-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.089255338s
STEP: Saw pod success
Dec  8 06:25:11.825: INFO: Pod "pod-02ffcc82-fab2-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:25:11.831: INFO: Trying to get logs from node phrs-glorious-mastodon pod pod-02ffcc82-fab2-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 06:25:11.947: INFO: Waiting for pod pod-02ffcc82-fab2-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:25:12.081: INFO: Pod pod-02ffcc82-fab2-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:25:12.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kkbbb" for this suite.
Dec  8 06:25:18.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:25:18.458: INFO: namespace: e2e-tests-emptydir-kkbbb, resource: bindings, ignored listing per whitelist
Dec  8 06:25:18.544: INFO: namespace e2e-tests-emptydir-kkbbb deletion completed in 6.448957784s

• [SLOW TEST:9.684 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:25:18.544: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-k567t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  8 06:25:18.948: INFO: Waiting up to 5m0s for pod "pod-0889f7f5-fab2-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-emptydir-k567t" to be "success or failure"
Dec  8 06:25:19.006: INFO: Pod "pod-0889f7f5-fab2-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 58.548647ms
Dec  8 06:25:21.011: INFO: Pod "pod-0889f7f5-fab2-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.06380361s
STEP: Saw pod success
Dec  8 06:25:21.011: INFO: Pod "pod-0889f7f5-fab2-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:25:21.019: INFO: Trying to get logs from node phrs-liberal-perch pod pod-0889f7f5-fab2-11e8-b680-ca3d6f40e675 container test-container: <nil>
STEP: delete the pod
Dec  8 06:25:21.118: INFO: Waiting for pod pod-0889f7f5-fab2-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:25:21.138: INFO: Pod pod-0889f7f5-fab2-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:25:21.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-k567t" for this suite.
Dec  8 06:25:27.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:25:27.506: INFO: namespace: e2e-tests-emptydir-k567t, resource: bindings, ignored listing per whitelist
Dec  8 06:25:27.574: INFO: namespace e2e-tests-emptydir-k567t deletion completed in 6.41905777s

• [SLOW TEST:9.030 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:25:27.574: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-d47xc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec  8 06:25:27.824: INFO: Waiting up to 5m0s for pod "downward-api-0dd5eaf8-fab2-11e8-b680-ca3d6f40e675" in namespace "e2e-tests-downward-api-d47xc" to be "success or failure"
Dec  8 06:25:27.846: INFO: Pod "downward-api-0dd5eaf8-fab2-11e8-b680-ca3d6f40e675": Phase="Pending", Reason="", readiness=false. Elapsed: 21.620882ms
Dec  8 06:25:29.851: INFO: Pod "downward-api-0dd5eaf8-fab2-11e8-b680-ca3d6f40e675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026298261s
STEP: Saw pod success
Dec  8 06:25:29.851: INFO: Pod "downward-api-0dd5eaf8-fab2-11e8-b680-ca3d6f40e675" satisfied condition "success or failure"
Dec  8 06:25:29.854: INFO: Trying to get logs from node phrs-one-falcon pod downward-api-0dd5eaf8-fab2-11e8-b680-ca3d6f40e675 container dapi-container: <nil>
STEP: delete the pod
Dec  8 06:25:29.893: INFO: Waiting for pod downward-api-0dd5eaf8-fab2-11e8-b680-ca3d6f40e675 to disappear
Dec  8 06:25:29.898: INFO: Pod downward-api-0dd5eaf8-fab2-11e8-b680-ca3d6f40e675 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:25:29.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-d47xc" for this suite.
Dec  8 06:25:35.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:25:36.024: INFO: namespace: e2e-tests-downward-api-d47xc, resource: bindings, ignored listing per whitelist
Dec  8 06:25:36.113: INFO: namespace e2e-tests-downward-api-d47xc deletion completed in 6.206926081s

• [SLOW TEST:8.539 seconds]
[sig-api-machinery] Downward API
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Dec  8 06:25:36.113: INFO: >>> kubeConfig: /tmp/kubeconfig-036194578
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-2jdhl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec  8 06:25:56.366: INFO: Container started at 2018-12-08 06:25:37 +0000 UTC, pod became ready at 2018-12-08 06:25:55 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Dec  8 06:25:56.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-2jdhl" for this suite.
Dec  8 06:26:18.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  8 06:26:18.543: INFO: namespace: e2e-tests-container-probe-2jdhl, resource: bindings, ignored listing per whitelist
Dec  8 06:26:18.585: INFO: namespace e2e-tests-container-probe-2jdhl deletion completed in 22.213620091s

• [SLOW TEST:42.472 seconds]
[k8s.io] Probing container
/workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.12.1-beta.0.52+4ed3216f3ec431/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSDec  8 06:26:18.586: INFO: Running AfterSuite actions on all node
Dec  8 06:26:18.586: INFO: Running AfterSuite actions on node 1
Dec  8 06:26:18.586: INFO: Skipping dumping logs from cluster

Ran 187 of 1814 Specs in 5172.369 seconds
SUCCESS! -- 187 Passed | 0 Failed | 0 Pending | 1627 Skipped PASS

Ginkgo ran 1 suite in 1h26m13.330774004s
Test Suite Passed
